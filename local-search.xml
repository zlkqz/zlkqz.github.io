<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>激活函数的作用和比较</title>
    <link href="/2021/11/11/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E6%AF%94%E8%BE%83/"/>
    <url>/2021/11/11/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E6%AF%94%E8%BE%83/</url>
    
    <content type="html"><![CDATA[<h1 id="1-线性结构"><a href="#1-线性结构" class="headerlink" title="1 线性结构"></a>1 线性结构</h1><p>如果满足：</p><script type="math/tex; mode=display">y = wx + b</script><p>则可称y、x间具有线性关系</p><p>而对于神经网络，相邻两层之间的输出之间满足：</p><script type="math/tex; mode=display">X^{[l]} = w^{[l]}X^{[l - 1]} + b^{[l]}</script><p>则可称其满足线性结构</p><h1 id="2-激活函数的作用"><a href="#2-激活函数的作用" class="headerlink" title="2 激活函数的作用"></a>2 激活函数的作用</h1><ul><li>我们先假设不用激活函数，假设神经网络有3层，每层的权重为$w^{[l]}$，偏差都为$b^{[l]}$，输入为$X$，输出为$Y$，则从头到尾的计算为：</li></ul><script type="math/tex; mode=display">Y = w^{[3]}(w^{[2]}(w^{[1]}X + b^{[1]}) + b^{[2]}) + b^{[3]}</script><p>就相当于做一次线性运算：</p><script type="math/tex; mode=display">Y = w'X + b'</script><p>则从头到尾都是线性结构，这在某些情况是适用的，但是大多数时候输入和输出的关系是非线性的，<strong>这时候我们就需要引入非线性因素，即激活函数，来使得神经网络有足够的capacity来抓取复杂的pattern</strong></p><p>所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。</p><ul><li><strong>能用线性拟合的情况：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/NdbYFTOkwyBj7xh.png" alt="image-20211111115632781" style="zoom:50%;" /></p><ul><li><strong>无法用线性拟合的情况：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/Zvo5O17LesqBbVF.png" alt="image-20211111115745462" style="zoom:50%;" /></p><h1 id="3-常用激活函数的优缺点"><a href="#3-常用激活函数的优缺点" class="headerlink" title="3 常用激活函数的优缺点"></a>3 常用激活函数的优缺点</h1><h3 id="3-1-Sigmoid函数"><a href="#3-1-Sigmoid函数" class="headerlink" title="3.1 Sigmoid函数"></a>3.1 Sigmoid函数</h3><ol><li><p>Sigmoid具有一个很好的特性就是能将输入值映射到$(0, 1)$，便于我们将值转化为概率，并且sigmoid平滑，便于求导</p></li><li><p>但sigmoid还有三大缺点：</p></li></ol><ul><li><strong>Gradient Vanishing：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/TWxNvl6CrD4kuAU.png" alt="image-20211111155149051"></p><p><strong>由图可以看出，在输入值较大或者较小时，其导数是趋于0，而在梯度下降时，其值就基本不会被更新</strong></p><ul><li><strong>函数输出并不是zero-centered</strong></li></ul><p>我们以一个二维的情况举例：</p><script type="math/tex; mode=display">f(\vec{x} ; \vec{w}, b)=f\left(w_{0} x_{0}+w_{1} x_{1}+b\right)</script><p>现在假设，参数 $w_0, w_1$ 的最优解 $w_0^∗, w_1^∗$ 满足条件:</p><script type="math/tex; mode=display">\left\{\begin{array}{l}w_{0}<w_{0}^{*} \\w_{1} \geqslant w_{1}^{*}\end{array}\right.</script><p>所以我们现在就是要让$w_0$变大，$w_1$变小，而：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_0} = x_0 \frac{\partial L}{\partial f} \\\frac{\partial L}{\partial w_1} = x_1 \frac{\partial L}{\partial f}</script><p>由于上一层sigmoid输出的$x_0, x_1$为正值，所以$w_0, w_1$的梯度不可能符号相同，而我们要逼近最小值点，只能走下图红色线路：</p><p><img src="https://i.loli.net/2021/11/11/IXqb8Brk9KOH7WG.png" alt="image-20211111160220315" style="zoom:50%;" /></p><p>而显然绿色线路才是最快的，所以这会<strong>影响梯度下降的速度</strong></p><ul><li><strong>幂运算相对来说比较耗时</strong></li></ul><h3 id="3-2-Tanh函数"><a href="#3-2-Tanh函数" class="headerlink" title="3.2 Tanh函数"></a>3.2 Tanh函数</h3><ul><li>tanh一般优于sigmoid，就是因为tanh是zero-centered，但是tanh仍然没有解决sigmoid的其他两个问题</li></ul><h3 id="3-3-ReLu函数"><a href="#3-3-ReLu函数" class="headerlink" title="3.3 ReLu函数"></a>3.3 ReLu函数</h3><ol><li>优点：</li></ol><ul><li>解决了gradient vanishing问题 (在正区间)</li><li>计算速度非常快，只需要判断输入是否大于0</li><li>收敛速度远快于sigmoid和tanh</li></ul><ol><li>缺点：</li></ol><ul><li>ReLU的输出不是zero-centered</li><li><strong>Dead ReLU Problem：</strong></li></ul><p>指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。</p><p>在x&lt;0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应</p><p><strong>产生原因：</strong></p><p>参数初始化问题（比较少见）</p><p>learning rate太高导致在训练过程中参数更新太大</p><p>而为了防止这种情况，我们可以使用改进的ReLu函数，如Leaky ReLu：</p><script type="math/tex; mode=display">f(x) = max(0.01x, x)</script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>常用损失函数和评估模型的指标</title>
    <link href="/2021/11/11/%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8C%87%E6%A0%87/"/>
    <url>/2021/11/11/%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h1 id="1-常用损失函数"><a href="#1-常用损失函数" class="headerlink" title="1 常用损失函数"></a>1 常用损失函数</h1><h3 id="1-1-0-1损失函数"><a href="#1-1-0-1损失函数" class="headerlink" title="1.1 0-1损失函数"></a>1.1 0-1损失函数</h3><script type="math/tex; mode=display">L(y, \hat{y}) = \begin{cases}1 & y\neq \hat{y}\\0 & y = \hat{y}\end{cases}</script><ul><li>0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用</li><li>感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足$|y - \hat{y}| &lt; T$时认为相等，即：</li></ul><script type="math/tex; mode=display">L(y, \hat{y}) = \begin{cases}1 & |y - \hat{y}| \geq T\\0 & |y - \hat{y}| < T\end{cases}</script><h3 id="1-2-均方差损失函数（MSE）"><a href="#1-2-均方差损失函数（MSE）" class="headerlink" title="1.2 均方差损失函数（MSE）"></a>1.2 均方差损失函数（MSE）</h3><script type="math/tex; mode=display">J_{MSE} = \frac{1}{N} \sum_{i = 1}^N(y_i - \hat{y_i})^2</script><ul><li>也称L2 Loss</li></ul><h5 id="1-2-1-证明"><a href="#1-2-1-证明" class="headerlink" title="1.2.1 证明"></a>1.2.1 证明</h5><p>假设预测值和真实值的误差服从标准正态分布，则给定一个$x_i$输出真实值$y_i$的概率为：</p><script type="math/tex; mode=display">p(y_i|x_i) = \frac{1}{\sqrt{2\pi}}exp(-\frac{(y_i - \hat{y_i})^2}{2})</script><p>其实就是极大似然估计，我们要寻找一组参数，使$p(y_i|x_i)$最大</p><p>进一步对所有样本，由于他们相互独立，所以所有样本都正好取到真实值$y$的概率为：</p><script type="math/tex; mode=display">L(x, y) = \prod_{i = 1}^N\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_i - \hat{y_i})^2}{2})</script><p>现在我们就要使$L(x, y)$最大，为了方便计算，我们取对数：</p><script type="math/tex; mode=display">LL(x, y) = log(L(x, y)) = -\frac{N}{2}log2\pi - \frac{1}{2}\sum_{i = 1}^N(y_i - \hat{y_i})^2</script><p>把第一项无关项去掉，再取负：</p><script type="math/tex; mode=display">NLL(x, y) = \frac{1}{2}\sum_{i = 1}^N(y_i - \hat{y_i})^2</script><p>即得到均方差形式</p><h5 id="1-2-2-为什么可以用极大似然"><a href="#1-2-2-为什么可以用极大似然" class="headerlink" title="1.2.2 为什么可以用极大似然"></a>1.2.2 为什么可以用极大似然</h5><p><strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，拟合数据最好的情况就是所有的$y_i = \hat{y_i}$，即每个样本的$p(y_i|x_i)$取最大，即$L(x, y)$取最大，由于对数运算不改变单调性，并且最后取了个负值，所以即$NLL(x, y)$取最小</strong></p><h3 id="1-3-平均绝对误差损失（MAE）"><a href="#1-3-平均绝对误差损失（MAE）" class="headerlink" title="1.3 平均绝对误差损失（MAE）"></a>1.3 平均绝对误差损失（MAE）</h3><script type="math/tex; mode=display">J_{MAE} = \frac{1}{N}\sum_{i = 1}^N|y_i - \hat{y_i}|</script><ul><li>也称L1 Loss</li></ul><h5 id="1-3-1-拉普拉斯分布"><a href="#1-3-1-拉普拉斯分布" class="headerlink" title="1.3.1 拉普拉斯分布"></a>1.3.1 拉普拉斯分布</h5><script type="math/tex; mode=display">f(x|\mu, b) = \frac{1}{2b}exp(-\frac{|x - \mu|}{b})</script><p>期望值：$\mu$             方差：$2b^2$</p><p><img src="https://i.loli.net/2021/11/08/yWY3hI4ioKpADRF.png" alt="Laplace_distribution_pdf" style="zoom: 25%;" /></p><h5 id="1-3-2-证明"><a href="#1-3-2-证明" class="headerlink" title="1.3.2 证明"></a>1.3.2 证明</h5><p>假设预测值和真实值的误差服从拉普拉斯分布（$\mu = 0, b = 1$）</p><script type="math/tex; mode=display">p(y_i | x_i) = \frac{1}{2}exp(-{|y_i - \hat{y_i}|})</script><p>剩余证明和上述MSE证明过程一样</p><h5 id="1-3-3-MSE和MAE的区别："><a href="#1-3-3-MSE和MAE的区别：" class="headerlink" title="1.3.3 MSE和MAE的区别："></a>1.3.3 MSE和MAE的区别：</h5><ul><li><strong>MSE 损失相比 MAE 通常可以更快地收敛</strong></li></ul><p>关于$\hat{y_i}$求导时，MSE为$-(y_i - \hat{y_i})$，MAE为$\pm1$，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的（当然也可以通过调整学习率缓解这个问题，但总体来说还是MAE更快）。</p><ul><li><strong>MAE对于离群值更加健壮，即更加不易受到离群值影响</strong></li></ul><ol><li>由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失</li><li>MSE 假设了误差服从正态分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于离群值更加健壮</li></ol><h5 id="1-3-4-MSE和MAE的收敛"><a href="#1-3-4-MSE和MAE的收敛" class="headerlink" title="1.3.4 MSE和MAE的收敛"></a>1.3.4 MSE和MAE的收敛</h5><ul><li>MSE收敛于均值</li></ul><p>将$\hat{y_i}$设为变量$t$：</p><script type="math/tex; mode=display">J_{MSE} = \frac{1}{N}\sum_{i = 1}^N(t - y_i)^2</script><p>关于$t$求导：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial t} = \frac{2}{N}\sum_{i = 1}^N(t - y_i) = 0</script><p>求得：</p><script type="math/tex; mode=display">t = \frac{1}{N}\sum_{i = 1}^Ny_i = E(y)</script><ul><li>MAE收敛于中值</li></ul><p>将$\hat{y_i}$设为变量$t$：</p><script type="math/tex; mode=display">J_{MAE} = \frac{1}{N}\sum_{i = 1}^N|t - y_i|</script><p>关于$t$求导：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial t} = \frac{1}{N}\sum_{i = 1}^Nsgn(t - y_i) = 0</script><p>显然在该种情况下应该取$t$为中值</p><h3 id="1-4-Huber-Loss"><a href="#1-4-Huber-Loss" class="headerlink" title="1.4 Huber Loss"></a>1.4 Huber Loss</h3><ul><li>上面介绍了MSE和MAE，他们各有各的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，而Huber Loss则是将两者结合起来，原理很简单，就是误差接近 0 时使用 MSE，误差较大时使用 MAE：</li></ul><script type="math/tex; mode=display">J_{huber} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right| \leq \delta} \frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}+\mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right|>\delta}\left(\delta\left|y_{i}-\hat{y}_{i}\right|-\frac{1}{2} \delta^{2}\right)</script><ul><li>前半部分是MSE部分，后半部分是MAE部分，超参数$\delta$为两个部分的连接处</li><li>MAE部分为$\delta |y_i - \hat{y_i}| - \frac{1}{2}\delta ^2$是为了在$|y_i - \hat{y_i}| = \delta$ 端点处连续可导</li></ul><p><img src="https://i.loli.net/2021/11/09/e6FQMBY42PDGxtI.png" alt="超参数为1的Huber Loss"></p><ul><li>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个$\delta$超参数。</li></ul><h3 id="1-5-分位数损失（Quantile-Loss）"><a href="#1-5-分位数损失（Quantile-Loss）" class="headerlink" title="1.5 分位数损失（Quantile Loss）"></a>1.5 分位数损失（Quantile Loss）</h3><script type="math/tex; mode=display">J_{\text {quant }}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\hat{y}_{i} \geq y_{i}}(1-r)\left|y_{i}-\hat{y}_{i}\right|+\mathbb{I}_{\hat{y}_{i}<y_{i}} r\left|y_{i}-\hat{y}_{i}\right|</script><ul><li>这是一个分段函数，这个损失函数是一个分段的函数 ，将$\hat{y_i} \geq y_i$（高估） 和$\hat{y_i} &lt; y_i$（低估) 两种情况分开来，并分别给予不同的系数</li><li>分位数损失实现了分别用不同的系数（r和1-r）控制高估和低估的损失</li><li>特别地，当$r = 0.5$时分位数损失退化为 MAE 损失</li></ul><p><img src="https://i.loli.net/2021/11/09/BvpTeXqWFYRKoMU.png" alt="Quantile Loss"></p><h3 id="1-6-交叉熵损失（Cross-Entropy-Loss）"><a href="#1-6-交叉熵损失（Cross-Entropy-Loss）" class="headerlink" title="1.6 交叉熵损失（Cross Entropy Loss）"></a>1.6 交叉熵损失（Cross Entropy Loss）</h3><ul><li>上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 </li></ul><h5 id="1-6-1-二分类"><a href="#1-6-1-二分类" class="headerlink" title="1.6.1 二分类"></a>1.6.1 二分类</h5><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N}\left(y_{i} \log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right)\right)</script><p><img src="https://i.loli.net/2021/11/09/hPt1nITJ624Llfp.png" alt="二分类的交叉熵"></p><ul><li>证明：</li></ul><p>在二分类中我们通常将输出结果用sigmoid映射到区间$(0, 1)$，并将其作为该类的概率，由于只有两类，所以给定$x_i$求出类别为1或0的概率分别为：</p><script type="math/tex; mode=display">p(y_i = 1|x_i) = \hat{y_i} \\p(y_i = 0|x_i) = 1 - \hat{y_i}</script><p>合并成一个式子：</p><script type="math/tex; mode=display">p(y_i|x_i) = (\hat{y_i})^{y_i}(1 - \hat{y_i})^{1 - y_i}</script><p>由于各数据点独立同分布，则似然可以表示为：</p><script type="math/tex; mode=display">L(x, y) = \prod_{i=1}^{N}\left(\hat{y}_{i}\right)^{y_{i}}\left(1-\hat{y}_{i}\right)^{1-y_{i}}</script><p>取负对数：</p><script type="math/tex; mode=display">N L L(x, y)=J_{C E}=-\sum_{i=1}^{N}\left(y_{i} \log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right)\right)</script><h5 id="1-6-2-多分类"><a href="#1-6-2-多分类" class="headerlink" title="1.6.2 多分类"></a>1.6.2 多分类</h5><p>在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值$y_i$现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 Softmax 函数</p><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log \left(\hat{y}_{i}^{k}\right)</script><p>因为$y_i$是一个One-hot向量，所以还可以写为：</p><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N} y_{i}^{c_{i}} \log \left(\hat{y}_{i}^{c_{i}}\right)</script><p>其中$c_i$为样本$x_i$的目标类</p><ul><li>证明：</li></ul><p>对于一个样本，分类正确的概率为：</p><script type="math/tex; mode=display">p(y_i|x_i) = \prod_{k=1}^{K}\left(\hat{y}_{i}^{k}\right)^{y_{i}^{k}}</script><p>（其中$y_i^k和\hat{y_i}^k$为该向量的第k维）</p><p>因为所有样本相互，所有相乘再取负对数即可得到：</p><script type="math/tex; mode=display">N L L(x, y)=J_{C E}=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log \left(\hat{y}_{i}^{k}\right)</script><h3 id="1-7-合页损失（Hinge-Loss）"><a href="#1-7-合页损失（Hinge-Loss）" class="headerlink" title="1.7 合页损失（Hinge Loss）"></a>1.7 合页损失（Hinge Loss）</h3><ul><li>Hinge Loss也是一种二分类损失函数</li></ul><script type="math/tex; mode=display">J_{\text {hinge }}=\sum_{i=1}^{N} \max \left(0,1-\operatorname{sgn}\left(y_{i}\right) \hat{y}_{i}\right)</script><p>下图是$y$为正类， 即$sgn(y) = 1$时，不同输出的合页损失示意图：</p><p><img src="https://i.loli.net/2021/11/09/KngeQOwp54JZRMf.png" alt="image-20211109225642368"></p><ul><li>可以看到当$y$为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在$(0, 1)$区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。<strong>使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类</strong></li></ul><h1 id="2-评估模型的指标"><a href="#2-评估模型的指标" class="headerlink" title="2 评估模型的指标"></a>2 评估模型的指标</h1><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><img src="https://i.loli.net/2021/11/09/OCNnaMs2ceBQIJz.png" alt="image-20211109233859657"></p><h3 id="2-2-查准率和查全率"><a href="#2-2-查准率和查全率" class="headerlink" title="2.2 查准率和查全率"></a>2.2 查准率和查全率</h3><script type="math/tex; mode=display">查准率 P（Precision） = \frac{TP}{TP + FP}</script><script type="math/tex; mode=display">查全率 R（Recall） = \frac{TP}{TP + FN}</script><ul><li>查准率可以直观理解为所有预测为正项的样本中有多少是真正的正项</li><li>查全率可以直观理解为所有label是正项的样本中有多少被成功预测出来了</li><li>理想情况下，查准率和查全率两者都越高越好。<strong>然而事实上这两者在某些情况下是矛盾的，一般来说，查准率高时，查全率低；查确率低时，查全率高</strong></li></ul><h3 id="2-3-准确率和错误率"><a href="#2-3-准确率和错误率" class="headerlink" title="2.3 准确率和错误率"></a>2.3 准确率和错误率</h3><p>准确率：</p><script type="math/tex; mode=display">accuracy = \frac{TP + TF}{TP + TN + FP + FN}</script><ul><li>即有多少样本被分类正确</li></ul><p>而错误率：</p><script type="math/tex; mode=display">errorrate = 1 - accuracy</script><h3 id="2-4-P-R曲线"><a href="#2-4-P-R曲线" class="headerlink" title="2.4 P-R曲线"></a>2.4 P-R曲线</h3><p><img src="https://i.loli.net/2021/11/10/zV1Sj4HyYfD3G5e.png" alt="image-20211110000609300" style="zoom: 80%;" /></p><ul><li>P-R曲线直观地显示出学习器在样本总体上地查全率、查准率，在进行比较时，<strong>若一个学习器的P-R曲线被另一个学习器曲线“完全包住”，则可以断言后者的性能一定优于前者，如上图的B性能优于C，而A、B不一定</strong></li><li>平衡点（BEP）时$P = R$时的取值，如上图A的BEP为0.8，而如果基于BEP比较，可知A优于B</li></ul><h3 id="2-5-F函数"><a href="#2-5-F函数" class="headerlink" title="2.5 F函数"></a>2.5 F函数</h3><p>BEP还是过于简化了些，更常用得是F1度量，我们可以取P和R的调和平均：</p><script type="math/tex; mode=display">\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}</script><p>求得：</p><script type="math/tex; mode=display">F1 = \frac{2PR}{P + R}</script><p>但是在许多应用中我们对查准率和查全率得重视程度不同，所以可以给予P和R不同的权重，取P和R的加权调和平均：</p><script type="math/tex; mode=display">\frac{1}{F_{\beta}} = \frac{1}{1 + \beta ^2}(\frac{1}{P} + \frac{\beta ^2}{R})</script><p>求得：</p><script type="math/tex; mode=display">F_{\beta} = \frac{(1 + \beta ^2)PR}{\beta ^2P + R}</script><ul><li>$\beta &gt; 0$度量了查全率和查准率的相对重要性，$\beta = 1$退化为标准的F1，$\beta &gt; 1$查全率有更大影响，$\beta &lt; 1$查准率有更大影响</li></ul><h3 id="2-6-ROC与AUC"><a href="#2-6-ROC与AUC" class="headerlink" title="2.6 ROC与AUC"></a>2.6 ROC与AUC</h3><h5 id="2-6-1-基本概念"><a href="#2-6-1-基本概念" class="headerlink" title="2.6.1 基本概念"></a>2.6.1 基本概念</h5><ul><li><p>大多二分类问题是将输出的预测值与一个<strong>分类阈值（threshold）</strong>进行比较，若预测值大于阈值则为正类，反之则为负类</p></li><li><p>根据预测值，我们可将测试样本进行排序，根据预测值由大到小，“最可能”是正例的排在前面，“最不可能”是正例的排到后面。<strong>这样，分类过程就相当于以中间某个截断点（也就是分类阈值）将样本分为两部分，前一部分判定为正例，后一部分判定为反例</strong></p></li><li><p>在不同的任务中，我们我们可以根据任务需求采取不同的截断点。例如如更重视查准率，可以将截断点设置靠前，更注重查全率，可以将截断点靠后</p></li><li>而我们根据预测值进行排序后，按顺序将样本逐个作为正例进行预测，每次计算出<strong>真正例率（TPR）</strong>和<strong>假正例率（FPR）</strong>，以他们为横纵坐标就得到了<strong>ROC曲线</strong></li></ul><h5 id="2-6-2-ROC曲线"><a href="#2-6-2-ROC曲线" class="headerlink" title="2.6.2 ROC曲线"></a>2.6.2 ROC曲线</h5><ul><li>首先介绍真正例率和假正例率：</li></ul><script type="math/tex; mode=display">TPR = \frac{TP}{TP + FN} \\FPR = \frac{FP}{TN + FP}</script><ul><li>ROC曲线：</li></ul><p>首先将分类阈值设最大，则所有类都被分类为负类，TPR和FPR都为0，然后每次将分类阈值设为下一个样本点的预测值（按预测值由大到小进行排序），记录每次的TPR和FPR，组成ROC曲线</p><p><img src="https://i.loli.net/2021/11/10/TUMHuC3N9rX18iz.png" alt="image-20211110140633061"></p><p>但是现实中我们是基于有限个样本画的图，所以不会产生这么平滑的曲线，更多情况应该像下图：</p><p><img src="https://i.loli.net/2021/11/10/xG1TrPNfyimzq7Q.png" alt="image-20211110140853013"></p><ul><li>基于ROC的比较方法</li></ul><blockquote><p>如果一个学习器的ROC曲线完全被另一个学习器的”包住“，则后者性能优于前者</p><p>若两者的曲线交叉，则可以通过ROC曲线所包裹的面积进行判断，即<strong>AUC</strong></p></blockquote><h5 id="2-6-3-AUC"><a href="#2-6-3-AUC" class="headerlink" title="2.6.3 AUC"></a>2.6.3 AUC</h5><ul><li><strong>AUC就是ROC曲线下的面积</strong>，假定ROC曲线是由坐标为$\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right),\left(x<em>{3}, y</em>{3}\right), \cdots,\left(x<em>{m}, y</em>{m}\right)$的点按序连接而形成，则AUC为：</li></ul><script type="math/tex; mode=display">A U C=\frac{1}{2} \sum_{i=1}^{m-1}\left(x_{i+1}-x_{i}\right)\left(y_{i}+y_{i+1}\right)</script><ul><li><p>从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1&gt;p0的概率就等于AUC， 即<strong>AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性</strong></p></li><li><p><strong>所以AUC反应的是分类器对样本的排序能力</strong></p></li></ul><p>证明：</p><p>设所有正类的集合$X = { \hat{X_1}, \hat{X_2}, …, \hat{X_m}}$和负类的集合$Y = { \hat{Y_1}, \hat{Y_2}, …, \hat{Y_n}}$，其中是每个样本对应的预测值，设分类阈值为c，$F_X(x)$和$F_Y(y)$分别为X和Y的分布函数，则$TPR(c) = 1 - F_X(c)$，$FPR(c) = 1 - F_Y(c)$</p><p>设$t = FPR(c)$， 则$c = F_Y^{-1}(1 - t)$，则$ROC(t) = 1 - F_X(F_Y^{-1}(1 - t))$</p><p>则：</p><script type="math/tex; mode=display">AUC = \int_0^1ROC(t)dt \\=\int_0^1 [1 - F_X(F_Y^{-1}(1 - t))] dt \\=\int_0^1 [1 - F_X(F_Y^{-1}(t))] dt \\=\int_{-\infty}^{+\infty} [1 - F_X(y)] dF_Y(y) \\=\int_{-\infty}^{+\infty}P(X > y)f_Y(y)dy \\=\int_{-\infty}^{+\infty}P(X > y, Y = y)dy \\=P(X > Y)</script><h5 id="2-6-4-使用ROC和AUC的优点"><a href="#2-6-4-使用ROC和AUC的优点" class="headerlink" title="2.6.4 使用ROC和AUC的优点"></a>2.6.4 使用ROC和AUC的优点</h5><ul><li><strong>AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡（正负类样本不相同）的情况下，依然能够对分类器做出合理的评价</strong></li></ul><script type="math/tex; mode=display">TPR = P(\hat{Y} = 1 | Y = 1) \\FPR = P(\hat{Y} = 1 | Y = 0)</script><p><strong>由上式可得：无论Y的真实概率是多少， 都不会影响TPR和FPR</strong></p><p>而PR曲线更关注正例</p><ul><li>ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响，有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的查全率就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习基础总结</title>
    <link href="/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/"/>
    <url>/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="1-行业知识体系"><a href="#1-行业知识体系" class="headerlink" title="1 行业知识体系"></a>1 行业知识体系</h1><h3 id="1-1-机器学习算法"><a href="#1-1-机器学习算法" class="headerlink" title="1.1 机器学习算法"></a>1.1 机器学习算法</h3><p><img src="https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg" alt="qq_pic_merged_1635482805011"></p><h3 id="1-2-机器学习分类"><a href="#1-2-机器学习分类" class="headerlink" title="1.2 机器学习分类"></a>1.2 机器学习分类</h3><p><img src="https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg" alt="qq_pic_merged_1635482964705"></p><h3 id="1-3-问题领域"><a href="#1-3-问题领域" class="headerlink" title="1.3 问题领域"></a>1.3 问题领域</h3><script type="math/tex; mode=display">\begin{cases}\pmb{语言识别} \\\pmb{字符识别} \\\pmb{计算机视觉}（CV） \\\pmb{自然语言处理}（NLP）\\\pmb{知识推理}\\\pmb{自动控制}\\\pmb{游戏理论和人机对弈}\\\pmb{数据挖掘}\end{cases}</script><h1 id="2-线性回归"><a href="#2-线性回归" class="headerlink" title="2 线性回归"></a>2 线性回归</h1><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><img src="https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png" alt="image-20211029131504886"></p><ul><li><p>线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</p></li><li><p>线性回归输出是⼀个连续值，因此适⽤于回归问题</p></li></ul><h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><ul><li>均方差（mse）：</li></ul><script type="math/tex; mode=display">\ell^{(i)}(W, b) = \frac{(\hat{y}^{(i)} - y^{(i)})^2}{2}</script><p>其中$\hat{y}$为计算出的预测值，$y$为真实值（label）</p><h1 id="3-全连接层（稠密层）"><a href="#3-全连接层（稠密层）" class="headerlink" title="3  全连接层（稠密层）"></a>3  全连接层（稠密层）</h1><ul><li>输出层中的神经元和输⼊ 层中各个输⼊完全连接</li><li>计算完全依赖于输入层。</li></ul><h1 id="4-小批量随机梯度下降"><a href="#4-小批量随机梯度下降" class="headerlink" title="4  小批量随机梯度下降"></a>4  小批量随机梯度下降</h1><ul><li><p>在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\eta$）作为模型参数在本次迭代的减小量。</p></li><li><p>每次mini-batch梯度下降计算过程：</p></li></ul><script type="math/tex; mode=display">w_1 = w_1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial{\ell(...)}}{\partial{w_1}}</script><ul><li>注意学习率要取正数</li></ul><h1 id="5-Softmax分类"><a href="#5-Softmax分类" class="headerlink" title="5  Softmax分类"></a>5  Softmax分类</h1><h3 id="5-1-基本概念"><a href="#5-1-基本概念" class="headerlink" title="5.1 基本概念"></a>5.1 基本概念</h3><ul><li>softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数</li><li>由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为该类的概率，具体计算如下：</li></ul><p>假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:</p><script type="math/tex; mode=display">\hat{y}_1, \hat{y}_2, \hat{y}_3 = softmax(o_1, o_2, o_3)</script><p>其中</p><script type="math/tex; mode=display">\hat{y}_i = \frac{e^{o_i}}{\sum_{j = 1}^{3}e^{o_j}}</script><ul><li>容易看出$\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）</li><li>通常，我 们把预测概率最⼤的类别作为输出类别</li></ul><h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 损失函数</h3><ul><li>mse过于严格，因为并不需要所有的输出都和真实值相近</li><li>这里我们运用交叉熵损失函数（cross entropy）：</li></ul><script type="math/tex; mode=display">H(y^{(i)}, \hat{y}^{(i)}) = -\sum_{j = 1}^qy_j^{(i)}\log{\hat{y}^{(i)}}</script><p>q为输出元个数$\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值</p><h1 id="6-多层感知机（MLP）"><a href="#6-多层感知机（MLP）" class="headerlink" title="6 多层感知机（MLP）"></a>6 多层感知机（MLP）</h1><ul><li>多层感知机有一到多个隐藏层</li><li>多层感知机中的隐藏层和输出层都是全连接层</li><li>多层感知机具有激活函数，下面介绍常用激活函数</li></ul><h1 id="7-激活函数"><a href="#7-激活函数" class="headerlink" title="7 激活函数"></a>7 激活函数</h1><h3 id="7-1-Sigmoid函数"><a href="#7-1-Sigmoid函数" class="headerlink" title="7.1 Sigmoid函数"></a>7.1 Sigmoid函数</h3><script type="math/tex; mode=display">sigmoid(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">sigmoid'(x) = sigmoid(x)(1 - sigmoid(x))</script><p><img src="https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png" alt="image-20211030004640247"></p><ul><li>一般用在二分类的输出层中</li></ul><h3 id="7-2-tanh函数"><a href="#7-2-tanh函数" class="headerlink" title="7.2 tanh函数"></a>7.2 tanh函数</h3><script type="math/tex; mode=display">tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}</script><script type="math/tex; mode=display">tanh'(x) = 1 - tanh^2(x)</script><p><img src="https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png" alt="image-20211030005138300"></p><ul><li>tanh和sigmoid比较相似，但是性能一般要优于sigmoid</li></ul><h3 id="7-3-ReLU函数"><a href="#7-3-ReLU函数" class="headerlink" title="7.3 ReLU函数"></a>7.3 ReLU函数</h3><script type="math/tex; mode=display">ReLU(x) = max(0, x)</script><p><img src="https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png" alt="image-20211030005121490"></p><ul><li>十分常用</li></ul><h3 id="7-4-Leaky-ReLU函数"><a href="#7-4-Leaky-ReLU函数" class="headerlink" title="7.4 Leaky ReLU函数"></a>7.4 Leaky ReLU函数</h3><script type="math/tex; mode=display">Leaky\_Relu(x) = max(0.1 * x, x)</script><p><img src="https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png" alt="image-20211030005818507"></p><h1 id="8-训练误差和泛化误差"><a href="#8-训练误差和泛化误差" class="headerlink" title="8 训练误差和泛化误差"></a>8 训练误差和泛化误差</h1><h3 id="8-1-基本概念"><a href="#8-1-基本概念" class="headerlink" title="8.1 基本概念"></a>8.1 基本概念</h3><ul><li>训练误差：指模型在训练数据集上表现出的误差</li><li><p>泛化误差：指模型在任意⼀个测试数据样本上表 现出的误差的期望</p></li><li><p>我们通常假设训练数据集（训练题）和测试数据集（测试题）⾥的每⼀个样本都 是从同⼀个概率分布中相互独⽴地⽣成的</p></li></ul><h3 id="8-2-验证数据集"><a href="#8-2-验证数据集" class="headerlink" title="8.2 验证数据集"></a>8.2 验证数据集</h3><ul><li>在机器学习中，通常需要评估若⼲候选模型的表现并从中选择模型。这⼀过程称为模型选择 （model selection）。可供选择的候选模型可以是有着不同超参数的同类模型。</li><li>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使⽤⼀次。不可以使⽤测试数据选择模型，如调参。由于⽆法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留⼀部分在训练数据集和测试数据集以外的数据来进⾏模型选择。</li><li>我们可以从给定的训练集中随机选取⼀ 小部分作为验证集，而将剩余部分作为真正的训练集</li><li>对于总样本数量比较小的情况，一般训练集：验证集：测试集 = 6：2：1；如果数据量比较大，训练集：验证集：测试集 = 8：1：1</li></ul><h3 id="8-3-k折交叉验证"><a href="#8-3-k折交叉验证" class="headerlink" title="8.3 k折交叉验证"></a>8.3 k折交叉验证</h3><ul><li>当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法</li><li>即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个⼦数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。</li></ul><h3 id="8-4-欠拟合和过拟合"><a href="#8-4-欠拟合和过拟合" class="headerlink" title="8.4 欠拟合和过拟合"></a>8.4 欠拟合和过拟合</h3><ul><li>欠拟合：模型⽆法得到较低的训练误差</li><li><p>过拟合：是模型的训练误差远小于它在测试数据集上 的误差</p></li><li><p>如果模型的复杂度过低，很容易出现⽋拟合；如果模型复杂度过⾼，很容易出现过拟合</p></li><li><p>⼀般来说，如果训练数据集中样本 数过少，特别是⽐模型参数数量（按元素计）更少时，过拟合更容易发⽣。</p></li><li>泛化误差不会 随训练数据集⾥样本数量增加而增⼤</li></ul><h3 id="8-5-正则化"><a href="#8-5-正则化" class="headerlink" title="8.5 正则化"></a>8.5 正则化</h3><ul><li>正则化通过为模型损失函数添加惩罚项使学出 的模型参数值较小</li></ul><h4 id="8-5-1-L2正则化（权重衰减）"><a href="#8-5-1-L2正则化（权重衰减）" class="headerlink" title="8.5.1 L2正则化（权重衰减）"></a>8.5.1 L2正则化（权重衰减）</h4><ul><li>L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积</li><li>损失函数添加一个L2惩罚项：</li></ul><script type="math/tex; mode=display">\ell(...) + \frac{\lambda}{2}\begin{Vmatrix} W \end{Vmatrix}^2</script><p>求导后：（每个mini-batch）</p><script type="math/tex; mode=display">w1 = (1 - \eta\lambda)w1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial\ell^{(i)}(...)}{\partial{w1}}</script><p>其中超参数$\lambda$ &gt; 0。当权重参数均为0时，惩罚项最小。当$\lambda$较⼤时，惩罚项在损失函数中的⽐重较⼤，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作⽤。</p><ul><li>一般训练集中的损失函数要加惩罚项，测试集中不用</li></ul><h4 id="8-5-2-丢弃法（Dropout）"><a href="#8-5-2-丢弃法（Dropout）" class="headerlink" title="8.5.2 丢弃法（Dropout）"></a>8.5.2 丢弃法（Dropout）</h4><ul><li>对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：</li></ul><script type="math/tex; mode=display">h_i' = \frac{\xi_i}{1 - p}h_i</script><p>$\xi_i$为0和1的概率分别为p和1 - p</p><p>分母中的1- p是为了不改变其输⼊的期望值：</p><script type="math/tex; mode=display">由于E(\xi_i) = 1 - p \\所以E(h_i') = \frac{E(\xi_i)}{1 - p}h_i = h_i</script><ul><li><p>测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法</p></li><li><p>进行了Dropout的多层感知机：</p></li></ul><p><img src="https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png" alt="image-20211030141251677"></p><p>可以看到隐藏层中的$h_2$和$h_5$消失了</p><ul><li>每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点</li></ul><h1 id="9-正向传播和反向传播"><a href="#9-正向传播和反向传播" class="headerlink" title="9 正向传播和反向传播"></a>9 正向传播和反向传播</h1><h3 id="9-1-正向传播"><a href="#9-1-正向传播" class="headerlink" title="9.1 正向传播"></a>9.1 正向传播</h3><ul><li>正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）</li><li>中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等</li><li>正向传播的计算图：</li></ul><p><img src="https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png" alt="image-20211030142019713"></p><p>其中：</p><ol><li>左下⻆是输⼊，右上⻆是输出</li><li>⽅框代表变量，圆圈代表运算符</li><li>$J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项</li></ol><h3 id="9-2-反向传播"><a href="#9-2-反向传播" class="headerlink" title="9.2 反向传播"></a>9.2 反向传播</h3><ul><li>反向传播最重要的是通过链式法则求导，从后往前进行计算</li><li>反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传 播计算得到的</li></ul><h3 id="9-3-衰减和爆炸"><a href="#9-3-衰减和爆炸" class="headerlink" title="9.3 衰减和爆炸"></a>9.3 衰减和爆炸</h3><ul><li>如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：</li></ul><p>假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积</p><h3 id="9-4-随机初始化参数"><a href="#9-4-随机初始化参数" class="headerlink" title="9.4 随机初始化参数"></a>9.4 随机初始化参数</h3><ul><li>例如：</li></ul><p>假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。</p><ul><li><strong>在这种情况下，⽆论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作⽤</strong></li></ul><h3 id="9-5-Xavier随机初始化"><a href="#9-5-Xavier随机初始化" class="headerlink" title="9.5 Xavier随机初始化"></a>9.5 Xavier随机初始化</h3><ul><li>假设某全连接层的输⼊个数为a， 输出个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：</li></ul><script type="math/tex; mode=display">U(-\sqrt{\frac{6}{a + b}}, \sqrt{\frac{6}{a + b}})</script><ul><li>该设计主要为了每层输出的⽅差不该受该层输⼊个数影响，且每层梯度的⽅差也不该受该层输出个数影响</li></ul><h1 id="10-Block类"><a href="#10-Block类" class="headerlink" title="10 Block类"></a>10 Block类</h1><ul><li>Block类是⼀个模型构造类，我们可以继承它来定义我们想要的模型。</li><li>它的⼦类既可以是⼀个层（如Dense类），⼜可以是深度学习计算的⼀个模型，或者是模型的⼀个部分</li></ul><h1 id="11-模型参数的延后初始化"><a href="#11-模型参数的延后初始化" class="headerlink" title="11 模型参数的延后初始化"></a>11 模型参数的延后初始化</h1><h3 id="11-1-基本概念"><a href="#11-1-基本概念" class="headerlink" title="11.1 基本概念"></a>11.1 基本概念</h3><ul><li>如果我们在创建层时没有指定输入输出的个数，那么和该层相邻的参数的维度一开始我们是不知道的，只有我们将最开始的输入值输进去，这时系统才能推断出参数的维度，才能开始参数初始化。系统将这种真正的参数初始化延后到获得⾜够信息时才执⾏的⾏为叫作延后初始化（deferred initialization）</li><li>这个初始化只会在第⼀次前向计算时被调⽤</li></ul><h3 id="11-2-避免延后初始化"><a href="#11-2-避免延后初始化" class="headerlink" title="11.2 避免延后初始化"></a>11.2 避免延后初始化</h3><ul><li>可以额外做⼀次前向计算来迫使参数被真正地初始化</li><li>也可以在创建层的时候指定了它的输⼊个数</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Markdown &amp; Letax常用语法</title>
    <link href="/2021/10/05/%E8%AF%AD%E6%B3%95/"/>
    <url>/2021/10/05/%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="1、标题"><a href="#1、标题" class="headerlink" title="1、标题"></a>1、标题</h1><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>~<span class="hljs-number">6</span>级标题<br><span class="hljs-attribute">1</span>~<span class="hljs-number">6</span>个#加个空格<br></code></pre></td></tr></table></figure><h1 id="2、代码块"><a href="#2、代码块" class="headerlink" title="2、代码块"></a>2、代码块</h1><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">``` + 代码语言<br></code></pre></td></tr></table></figure><h1 id="3、字体"><a href="#3、字体" class="headerlink" title="3、字体"></a>3、字体</h1><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">加粗：<span class="hljs-strong">**文本**</span><br>删除线:~~文本~~<br>斜体：<span class="hljs-strong">*文本*</span><br></code></pre></td></tr></table></figure><p>加粗：<strong>文本</strong><br>删除线：<del>文本</del><br>斜体：<em>文本</em></p><h1 id="4、引用"><a href="#4、引用" class="headerlink" title="4、引用"></a>4、引用</h1><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css">&gt;<span class="hljs-selector-tag">A</span><br>&gt;&gt;<span class="hljs-selector-tag">A</span><br>&gt;&gt;&gt;<span class="hljs-selector-tag">A</span><br></code></pre></td></tr></table></figure><blockquote><p>A</p><blockquote><p>A</p><blockquote><p>A</p></blockquote></blockquote></blockquote><h1 id="5、分割线"><a href="#5、分割线" class="headerlink" title="5、分割线"></a>5、分割线</h1><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><hr><h1 id="6、图片插入"><a href="#6、图片插入" class="headerlink" title="6、图片插入"></a>6、图片插入</h1><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs less">!<span class="hljs-selector-attr">[名字]</span>(本地或网页链接)<br></code></pre></td></tr></table></figure><p><img src="" alt="图片"></p><h1 id="7、超链接"><a href="#7、超链接" class="headerlink" title="7、超链接"></a>7、超链接</h1><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clojure">[名字](链接)<br></code></pre></td></tr></table></figure><p><a href="www.baidu.com">百度</a></p><h1 id="8、列表"><a href="#8、列表" class="headerlink" title="8、列表"></a>8、列表</h1><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gcode">无序列表<br>- <span class="hljs-comment">(空格)</span> 内容<br>有序列表<br>数字.<span class="hljs-comment">(空格)</span>内容<br></code></pre></td></tr></table></figure><ul><li>无序</li></ul><ol><li>有序</li></ol><h1 id="9、表格"><a href="#9、表格" class="headerlink" title="9、表格"></a>9、表格</h1><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">干脆直接快捷键<br></code></pre></td></tr></table></figure><h1 id="10、Latex"><a href="#10、Latex" class="headerlink" title="10、Latex"></a>10、Latex</h1><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><div class="table-container"><table><thead><tr><th>Latex</th><th>对应形式</th><th>Latex</th><th>对应形式</th></tr></thead><tbody><tr><td>\alpha</td><td>$\alpha$</td><td>\Alpha</td><td>$\Alpha$</td></tr><tr><td>\beta</td><td>$\beta$</td><td>\Beta</td><td>$\Beta$</td></tr><tr><td>\gamma</td><td>$\gamma$</td><td>\Gamma</td><td>$\Gamma$</td></tr><tr><td>\delta</td><td>$\delta$</td><td>\Delta</td><td>$\Delta$</td></tr><tr><td>\epsilon</td><td>$\epsilon$</td><td>\Epsilon</td><td>$\Epsilon$</td></tr><tr><td>\zeta</td><td>$\zeta$</td><td>\Zeta</td><td>$\Zeta$</td></tr><tr><td>\eta</td><td>$\eta$</td><td>\Eta</td><td>$\Eta$</td></tr><tr><td>\theta</td><td>$\theta$</td><td>\Theta</td><td>$\Theta$</td></tr><tr><td>\lambda</td><td>$\lambda$</td><td>\Lambda</td><td>$\Lambda$</td></tr><tr><td>\mu</td><td>$\mu$</td><td>\Mu</td><td>$\Mu$</td></tr><tr><td>\nu</td><td>$\nu$</td><td>\Nu</td><td>$\Mu$</td></tr><tr><td>\xi</td><td>$\xi$</td><td>\Xi</td><td>$\Xi$</td></tr><tr><td>\pi</td><td>$\pi$</td><td>\Pi</td><td>$\Pi$</td></tr><tr><td>\rho</td><td>$\rho$</td><td>\Rho</td><td>$\Rho$</td></tr><tr><td>\sigma</td><td>$\sigma$</td><td>\Sigma</td><td>$\Sigma$</td></tr><tr><td>\varphi</td><td>$\varphi$</td><td>\Phi</td><td>$\Phi$</td></tr><tr><td>\chi</td><td>$\chi$</td><td>\Chi</td><td>$\Chi$</td></tr><tr><td>\psi</td><td>$\psi$</td><td>\Psi</td><td>$\Psi$</td></tr><tr><td>\omega</td><td>$\omega$</td><td>\Omega</td><td>$\Omega$</td></tr><tr><td>\ell</td><td>$\ell$</td><td>\varepsilon</td><td>$\varepsilon$</td></tr></tbody></table></div><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><div class="table-container"><table><thead><tr><th>解释</th><th>代码</th></tr></thead><tbody><tr><td>粗体</td><td>\pmb{…..}</td></tr><tr><td>单空格</td><td>\quad</td></tr><tr><td>双空格</td><td>\qquad</td></tr><tr><td>$\times$</td><td>\times</td></tr><tr><td>$\div$</td><td>\div</td></tr><tr><td>下标</td><td>_</td></tr><tr><td>上标</td><td>^</td></tr><tr><td>$\hat{a}$</td><td>\hat{a}</td></tr><tr><td>$\vec{a}$</td><td>\vec{a}</td></tr><tr><td>$\log_{32}{xy}$</td><td>\log_{32}{xy}</td></tr><tr><td>{ }</td><td>需要转义</td></tr><tr><td>$\sum_1^n$</td><td>\sum_1^n</td></tr><tr><td>$\prod_{k=1}^nk^2$</td><td>\prod_{k=1}^nk^2</td></tr><tr><td></td><td></td></tr><tr><td>$\int_a^b$</td><td>\int_a^b</td></tr><tr><td>$\iint$</td><td>\iint</td></tr><tr><td>$\iiint$</td><td>\iiint</td></tr><tr><td>$\infty$</td><td>\infty</td></tr><tr><td>$\lim_{x\to0}$</td><td>极限       \lim_{x\to0}</td></tr><tr><td>$f’(x)$</td><td>导数        f’(x)</td></tr><tr><td>$\int<em>{\theta=0}^{2\pi}    \int</em>{r=0}^R$</td><td>\int<em>{\theta=0}^{2\pi}    \int</em>{r=0}^R$</td></tr><tr><td>$\nabla$</td><td>\nabla</td></tr><tr><td>$\partial$</td><td>\partial</td></tr><tr><td></td><td></td></tr><tr><td>$\subset$</td><td>\subset</td></tr><tr><td>$\subseteq$</td><td>\subseteq</td></tr><tr><td>$\in$</td><td>\in</td></tr><tr><td>$\notin$</td><td>\notin</td></tr><tr><td>$\emptyset$</td><td>\emptyset</td></tr><tr><td>$\varnothing$</td><td>\varnothing</td></tr><tr><td>$\bigcup$</td><td>\bigcup</td></tr><tr><td>$\cup$</td><td>\cup</td></tr><tr><td>$\bigcap$</td><td>\bigcap</td></tr><tr><td>$\cap$</td><td>\cap</td></tr><tr><td></td><td></td></tr><tr><td>$\frac{a+1}{b+1}$</td><td>分数    \frac{a+1}{b+1}</td></tr><tr><td>$\sqrt{x^5}$</td><td>开方      \sqrt{x^5}</td></tr><tr><td>$\sqrt[3]{xy}$</td><td>开方       \sqrt[3]{xy}</td></tr><tr><td></td><td></td></tr><tr><td>$\le$</td><td>\le</td></tr><tr><td>$\geq$</td><td>\geq</td></tr><tr><td>$\neq$</td><td>\neq</td></tr><tr><td>$\approx$</td><td>\approx</td></tr><tr><td>$\equiv$</td><td>\equiv</td></tr><tr><td>$\pm$</td><td>\pm</td></tr><tr><td>$\mp$</td><td>\mp</td></tr><tr><td>$\mathbb{R}$</td><td>将字母改为黑板字体\mathbb{R}</td></tr></tbody></table></div><h3 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h3><script type="math/tex; mode=display">f(x) = \begin{cases}1 & x = 2\\2 & x > 2\\3 & x \leqslant 2\\\end{cases}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs livescript">$$<br>f(x) = <span class="hljs-string">\begin&#123;cases&#125;</span><br><span class="hljs-number">1</span> &amp; x = <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">2</span> &amp; x &gt; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; x <span class="hljs-string">\leqslant</span> <span class="hljs-number">2</span><span class="hljs-string">\\</span><br><span class="hljs-string">\end&#123;cases&#125;</span><br>$$<br></code></pre></td></tr></table></figure><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><script type="math/tex; mode=display">\begin{matrix}1 & 2 \\3 & 4 \end{matrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;matrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;matrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{pmatrix}1 & 2 \\3 & 4 \end{pmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;pmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;pmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;bmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;bmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Bmatrix}1 & 2 \\3 & 4 \end{Bmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;Bmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;Bmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{vmatrix}1 & 2 \\3 & 4 \end{vmatrix}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-string">\begin&#123;vmatrix&#125;</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br><span class="hljs-string">\end&#123;vmatrix&#125;</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Vmatrix}1 & 2 \\3 & 4 \end{Vmatrix}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-string">\begin&#123;Vmatrix&#125;</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br><span class="hljs-string">\end&#123;Vmatrix&#125;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
