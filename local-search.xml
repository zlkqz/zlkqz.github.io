<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>CNN基本概念</title>
    <link href="/2021/11/26/CNN/"/>
    <url>/2021/11/26/CNN/</url>
    
    <content type="html"><![CDATA[<h1 id="1-卷积层"><a href="#1-卷积层" class="headerlink" title="1 卷积层"></a>1 卷积层</h1><h3 id="1-1-互相关运算"><a href="#1-1-互相关运算" class="headerlink" title="1.1 互相关运算"></a>1.1 互相关运算</h3><ul><li><strong>在⼆维卷积层中，⼀个⼆维输⼊数组和⼀个⼆维核（kernel）数组通过互相 关运算输出⼀个⼆维数组</strong>，如下图：</li></ul><p><img src="https://i.loli.net/2021/11/24/8amf1EYbuU7JPKd.png" alt="image-20211124131936578"></p><p>卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依 次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘 并求和，得到输出数组中相应位置的元素</p><ul><li><strong>⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出</strong></li><li><strong>使用互相关运算做边缘检测：</strong></li></ul><p>比如我们把kernel设为[1, -1]，要识别的图像为单通道，只有黑白：<img src="https://i.loli.net/2021/11/24/Uro3neFEKigbCd2.png" alt="image-20211124132335469"></p><p>则进行互相关运算之后可以变为：<img src="https://i.loli.net/2021/11/24/kuH17Ef3p8Rbjdi.png" alt="image-20211124132405678"></p><p>由此我们可以看出边缘是在第2和第6列</p><ul><li><strong>卷积运算：</strong>其实就是把核数组左右翻转并 上下翻转，再与输⼊数组做互相关运算。但因为我们的参数都是学习出来的，而不是一开始设定好的，<strong>所以卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出</strong>，而我们一般使用的也是互相关运算</li></ul><h3 id="1-2-特征图和感受野"><a href="#1-2-特征图和感受野" class="headerlink" title="1.2 特征图和感受野"></a>1.2 特征图和感受野</h3><ul><li><strong>⼆维卷积层输出的⼆维数组可以看作输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）</strong></li><li><strong>影响元素x的前向计算的所有可能输⼊区域叫做x的感受野（receptive field）</strong>，用上图距离，阴影中19的感受野就是输入的四个阴影，如果输入的元素又是另一个卷积运算的输出，则我们还可以将19的感受野扩大</li><li>可⻅，我们可以通过更深的卷积神经⽹络使特征 图中单个元素的感受野变得更加⼴阔，从而捕捉输⼊上更⼤尺⼨的特征</li></ul><h3 id="1-3-填充和步幅"><a href="#1-3-填充和步幅" class="headerlink" title="1.3 填充和步幅"></a>1.3 填充和步幅</h3><ul><li><strong>填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）</strong>，如下图就是对3$\times$3数组进行填充，填充为1：</li></ul><p><img src="https://i.loli.net/2021/11/24/YE4p8vg31IaWbuK.png" alt="image-20211124133849360"></p><ul><li><p><strong>我们将每次滑动的⾏数和列数称为步幅（stride）</strong></p></li><li><p><strong>一般来说输出的高宽为：</strong></p><script type="math/tex; mode=display">[(n_h - k_h + p_h + s_h) / s_h] \times [(n_w - k_w + p_w + s_w) / s_w]</script><p>其中：$n为输入，k为核，p为填充，s为步幅$</p></li><li><p>步幅为1时，很多情况下，我们会设置$p_h = k_h−1$和$p_w = k_w −1$来使输⼊和输出具有相同的⾼和宽</p></li></ul><h3 id="1-4-多输入通道和多输出通道"><a href="#1-4-多输入通道和多输出通道" class="headerlink" title="1.4 多输入通道和多输出通道"></a>1.4 多输入通道和多输出通道</h3><ul><li>当输入通道 $c_i &gt; 1$ 时，我们为每个输入通道都分配一个核数组，则得到一个形状为$c_i \times k_h \times k_w$的卷积核，最后再将每个通道上的卷积结果相加，就得到输出，如下图：</li></ul><p><img src="https://i.loli.net/2021/11/24/iNGt96JXUlSAEKZ.png" alt="image-20211124140843742"></p><ul><li>可是按上面的方法，无论输入通道是多少，输出通道总为1，我们设输入、输出通道分别为$c_i, c_o$，如果我们想要多通道的输出，则可以给每个输出通道都创建一个形状为$c_i \times k_h \times k_w$的核，则可以得到形状为$c_o \times c_i \times k_h \times k_w$的卷积核</li></ul><h3 id="1-5-1-times-1卷积层"><a href="#1-5-1-times-1卷积层" class="headerlink" title="1.5 1$\times$1卷积层"></a>1.5 1$\times$1卷积层</h3><ul><li>卷积窗口形状为1 × 1的多通道卷积层。我们通常称之为1 × 1卷 积层，并将其中的卷积运算称为1 × 1卷积</li><li>因为使⽤了最小窗口，1 × 1卷积失去了卷积层可以 识别⾼和宽维度上相邻元素构成的模式的功能。<strong>实际上，1 × 1卷积的主要计算发⽣在通道维上， 1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度</strong></li><li><strong>假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那 么1 × 1卷积层的作⽤与全连接层等价</strong></li></ul><h3 id="1-6-卷积层相对于全连接层的优点"><a href="#1-6-卷积层相对于全连接层的优点" class="headerlink" title="1.6 卷积层相对于全连接层的优点"></a>1.6 卷积层相对于全连接层的优点</h3><ul><li>卷积层保留输⼊形状，使图像的像素在⾼和宽两个⽅向上的相关性均可能被有效识别</li><li>卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重复计算，同一输入通道使用同一卷积核，<strong>实现参数共享</strong>，从而<strong>避免参数过多</strong>。同时参数共享也具有物理意义，他使卷积层<strong>具有平移等特性</strong>，比如图像中有一只猫，那么无论他在图像的任何位置，我们都能提取到他的主要特征，将他识别为猫</li><li>对于全连接层，<strong>任意一对输入和输出之间都会产生交互</strong>，形成稠密的连接结构</li></ul><p><img src="https://i.loli.net/2021/11/25/BuGFLnhli2NJPCv.png" alt="image-20211125234024022"></p><p>而在卷积神经网络中，卷积核尺度远小于输入的维度，<strong>这样每个输出神经元仅与前一层部分神经元产生交互</strong></p><p><img src="C:\Users\LENOVO\AppData\Roaming\Typora\typora-user-images\image-20211125234143051.png" alt="image-20211125234143051"></p><p>我们将这种特性称为<strong>稀疏交互</strong>，这样我们可以将<strong>优化过程的时间复杂度减少好几个数量级，并且缓解过拟合</strong></p><p>稀疏交互的物理意义是<strong>许多现实中的数据都具有局部的特征结构，我们可以先学习局部的特征，然后再将局部的特征组合起来形成更复杂和抽象的特征</strong></p><h1 id="2-池化层"><a href="#2-池化层" class="headerlink" title="2 池化层"></a>2 池化层</h1><ul><li><strong>池化层的提出是为了缓解卷积层对位置的过度敏感性</strong>。不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均值。该运算也分别叫做<strong>最⼤池化</strong>或<strong>平均池化</strong></li><li><strong>池化层没有参数</strong></li><li>Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，这个妥协会越来越小，下面介绍几种常用的池化层</li></ul><h3 id="2-1-平均池化层（mean-pooling）"><a href="#2-1-平均池化层（mean-pooling）" class="headerlink" title="2.1 平均池化层（mean-pooling）"></a>2.1 平均池化层（mean-pooling）</h3><ul><li>即对邻域内特征点只求平均</li><li>优缺点：<strong>抑制邻域大小受限造成的估计值方差增大</strong>，能很好的保留背景，但容易使得图片变模糊</li></ul><h3 id="2-2-最大池化层（max-pooling）"><a href="#2-2-最大池化层（max-pooling）" class="headerlink" title="2.2 最大池化层（max-pooling）"></a>2.2 最大池化层（max-pooling）</h3><ul><li>即对邻域内特征点取最大</li><li>优缺点：<strong>抑制积层参数误差造成估计均值的偏移</strong>，能很好的保留纹理特征，一般现在都用max-pooling而很少用mean-pooling</li></ul><h3 id="2-3-全局平均池化（global-average-pooling）"><a href="#2-3-全局平均池化（global-average-pooling）" class="headerlink" title="2.3 全局平均池化（global average pooling）"></a>2.3 全局平均池化（global average pooling）</h3><ul><li>对每个通道中所有元素求平均并直接⽤于分类</li><li>优点：大幅度减少网络参数，理所当然的减少了过拟合现象</li></ul><h3 id="2-4-池化层的作用"><a href="#2-4-池化层的作用" class="headerlink" title="2.4 池化层的作用"></a>2.4 池化层的作用</h3><ol><li><strong>对特征进行压缩，保留主要特征的同时减少参数和计算量，减少模型复杂度，防止过拟合。</strong>用池化层做互相关运算，本身就能减少特征的个数。并且对于全局平均优化，如果我们要进行图像分类，我们需要使用参数很多的全连接层，最后再导入Softmax，而如果我们使用全局平均优化，则可以规避全连接庞大的参数量，减少过拟合</li><li><strong>实现不变性</strong>，包括平移不变性、旋转不变性和尺度不变性。</li></ol><h3 id="2-5-池化层的多通道"><a href="#2-5-池化层的多通道" class="headerlink" title="2.5 池化层的多通道"></a>2.5 池化层的多通道</h3><ul><li>和卷积层有区别， 在处理多通道输⼊数据时，<strong>池化层对每个输⼊通道分别池化，而不是像卷积层那样将各通道的输 ⼊按通道相加。这意味着池化层的输出通道数与输⼊通道数相等</strong></li></ul><h1 id="3-卷积神经网络（LeNet）"><a href="#3-卷积神经网络（LeNet）" class="headerlink" title="3 卷积神经网络（LeNet）"></a>3 卷积神经网络（LeNet）</h1><ul><li>分为卷积层块和全连接层块两个部分</li></ul><h3 id="3-1-卷积层块"><a href="#3-1-卷积层块" class="headerlink" title="3.1 卷积层块"></a>3.1 卷积层块</h3><ul><li>卷积层块⾥的基本单位是卷积层后接最⼤池化层，卷积层⽤来识别图像⾥的空间模式，如线条和 物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性</li><li>在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似</li><li>卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2</li></ul><h3 id="3-2-全连接层块"><a href="#3-2-全连接层块" class="headerlink" title="3.2 全连接层块"></a>3.2 全连接层块</h3><ul><li>当卷积层块的输出传⼊全连接层块时，全连接 层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输⼊形状将变成⼆维，其中第⼀维是小批量中的样本，第⼆维是每个样本变平后的向量表⽰，且向量⻓度为通道、⾼和宽的乘积</li><li>全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数</li></ul><p><img src="https://i.loli.net/2021/11/25/ijEI6yRpxeoQc7g.png" alt="image-20211125153420379"></p><h1 id="4-深度卷积神经网络（AlexNet）"><a href="#4-深度卷积神经网络（AlexNet）" class="headerlink" title="4 深度卷积神经网络（AlexNet）"></a>4 深度卷积神经网络（AlexNet）</h1><ul><li><p>相对于较小的LeNet，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层</p></li><li><p>AlexNet第⼀层中的卷积窗口形状是11 × 11。因为ImageNet中绝⼤多数图像的⾼和宽均 ⽐MNIST图像的⾼和宽⼤10倍以上，ImageNet图像的物体占⽤更多的像素，所以需要更⼤的卷积窗口来捕获物体。第⼆层中的卷积窗口形状减小到5 × 5，之后全采⽤3 × 3</p></li><li><p>第⼀、第⼆和第五个卷积层之后都使⽤了窗口形状为3 × 3、步幅为2的最⼤池化层</p></li><li><p>AlexNet使⽤的卷积通道数也⼤于LeNet中的卷积通道数数⼗倍，5层的输出通道数分别为96，256，384，384，256</p></li><li><p>紧接着最后⼀个卷积层的是两个输出个数为4096的全连接层</p></li><li><p>AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数</p></li><li><p>AlexNet使用了Dropout和图像增广</p></li><li><p>AlexNet相比于LeNet有更小的学习率</p><p><img src="https://i.loli.net/2021/11/25/r2R9AB6LtnCEUvh.png" alt="image-20211125153304927"></p></li></ul><h1 id="5-使用重复元素的网络（VGG）"><a href="#5-使用重复元素的网络（VGG）" class="headerlink" title="5 使用重复元素的网络（VGG）"></a>5 使用重复元素的网络（VGG）</h1><h3 id="5-1-VGG块"><a href="#5-1-VGG块" class="headerlink" title="5.1 VGG块"></a>5.1 VGG块</h3><ul><li>连续使⽤数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。可以指定卷积层的数量num_convs和输出 通道数num_channels</li></ul><p><img src="https://i.loli.net/2021/11/25/ckOUzpJsXayHPf3.png" alt="image-20211125153225097"></p><h3 id="5-2-VGG网络"><a href="#5-2-VGG网络" class="headerlink" title="5.2 VGG网络"></a>5.2 VGG网络</h3><ul><li>我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积层和3个全连接层，所以经常被称为VGG-11</li><li><p>之后的3个全连接层神经元个数分别为：4096，4096，10（最后一层为类别个数）</p></li><li><p>使用VGG后，每次输⼊的⾼和宽将减半，直到最终⾼和宽变成7后传⼊全连接层。与此同时，输出通道数每次翻倍，直到变成512。VGG这种⾼和宽减半以及通道翻倍的设计使多数卷积层都有相同的模型参数尺⼨和计算复杂度</p></li></ul><h1 id="6-网络中的网络（NiN）"><a href="#6-网络中的网络（NiN）" class="headerlink" title="6 网络中的网络（NiN）"></a>6 网络中的网络（NiN）</h1><ul><li>的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深</li><li>但是NiN有所不同，由于1$\times$1卷积层可以替代全连接层，所以NiN中就进行了替换：</li></ul><p><img src="https://i.loli.net/2021/11/25/NjfabGRcVe8ysZk.png" alt="image-20211125152432130"></p><h3 id="6-1-NiN块"><a href="#6-1-NiN块" class="headerlink" title="6.1 NiN块"></a>6.1 NiN块</h3><ul><li>NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 × 1卷积层串联而成。其中第 ⼀个卷积层的超参数可以⾃⾏设置，而第⼆和第三个卷积层的超参数⼀般是固定的</li><li>三个卷积层的通道数是相同的</li></ul><p><img src="https://i.loli.net/2021/11/25/HdYhK3qNntO5pIW.png" alt="image-20211125153051564"></p><h3 id="6-2-NiN模型"><a href="#6-2-NiN模型" class="headerlink" title="6.2 NiN模型"></a>6.2 NiN模型</h3><ul><li><p>NiN使⽤卷积窗口形状分别为11 × 11、5 × 5和3 × 3的卷积层，相应的输出通道数也与AlexNet中的⼀致。每个NiN块后接⼀ 个步幅为2、窗口形状为3 × 3的最⼤池化层</p></li><li><p>除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤<strong>全局平均池化层</strong>对每个通道中所有元素求平均并直接⽤于分类。<strong>这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合，然而，该设计有时会造成获得有效模型的训练时间的增加</strong></p></li></ul><p><img src="https://i.loli.net/2021/11/25/Ul21eaCDP7YhvyJ.png" alt="image-20211125154357683"></p><ul><li>NiN的学习率一般比AlexNet和VGG大</li></ul><h1 id="7-含并行连接的网络（GoogLeNet）"><a href="#7-含并行连接的网络（GoogLeNet）" class="headerlink" title="7 含并行连接的网络（GoogLeNet）"></a>7 含并行连接的网络（GoogLeNet）</h1><h3 id="7-1-Inception块"><a href="#7-1-Inception块" class="headerlink" title="7.1 Inception块"></a>7.1 Inception块</h3><p><img src="https://i.loli.net/2021/11/25/Q4sfSlhjntDaXFM.png" alt="image-20211125155525157"></p><ul><li><p>Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 × 3和5 × 5的卷积层来<strong>抽取不同空间尺⼨下的信息</strong></p></li><li><p>其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，<strong>以降低模型复杂度</strong>。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数</p></li><li><p>4条线路都使⽤了合适的填充来使<strong>输⼊与输出的⾼和宽⼀致</strong>。最后我们将每条线路的输出<strong>在通道维上连结</strong>，并输⼊接下来的层中去</p></li></ul><p><img src="https://i.loli.net/2021/11/25/UKanYfROdluCVA4.png" alt="image-20211125155928039"></p><h3 id="7-2-GoogLeNet模型"><a href="#7-2-GoogLeNet模型" class="headerlink" title="7.2 GoogLeNet模型"></a>7.2 GoogLeNet模型</h3><ul><li>在主体卷积部分中使⽤5个模块（block），每个模块之间使⽤步幅为2的3× 3最⼤池化层来减小输出⾼宽</li><li>第⼀模块使⽤⼀个64通道的7 × 7卷积层</li></ul><p><img src="https://i.loli.net/2021/11/25/GlgWw4M8aTIR5cq.png" alt="image-20211125160214306"></p><ul><li>第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层，然后是将通道增⼤3倍的3 × 3卷积层</li></ul><p><img src="https://i.loli.net/2021/11/25/8kEOFumjHlVGiew.png" alt="image-20211125160325185"></p><ul><li>第三模块串联2个完整的Inception块。第⼀个Inception块的输出通道数为64+128+32+32 = 256，其中4条线路的输出通道数⽐例为64 : 128 : 32 : 32 = 2 : 4 : 1 : 1。其中第⼆、第三条线路先分别将输⼊通道数减小⾄96/192 = 1/2和16/192 = 1/12后，再接上第⼆层卷积层。第⼆个Inception块 输出通道数增⾄128 + 192 + 96 + 64 = 480，每条线路的输出通道数之⽐为128 : 192 : 96 : 64 = 4 : 6 : 3 : 2。其中第⼆、第三条线路先分别将输⼊通道数减小⾄128/256 = 1/2和32/256 = 1/8</li></ul><p><img src="https://i.loli.net/2021/11/25/xswSZEGaPDNbmMJ.png" alt="image-20211125160529216"></p><ul><li>第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + 208 + 48 + 64 = 512、 160+224+64+64 = 512、128+256+64+64 = 512、112+288+64+64 = 528和256+320+128+128 = 832</li></ul><p><img src="https://i.loli.net/2021/11/25/ynwd6r7VTpAoEku.png" alt="image-20211125160605012"></p><ul><li>第五模块有输出通道数为256 + 320 + 128 + 128 = 832和384 + 384 + 128 + 128 = 1024的两个Inception块，<strong>然后再接上一个全局平均池化层</strong></li></ul><p><img src="https://i.loli.net/2021/11/25/edUp5mbZhLJv89j.png" alt="image-20211125161201342"></p><p><img src="https://i.loli.net/2021/11/25/xTDdPVAKZgcSfsu.png" alt="image-20211125161219115"></p><ul><li><strong>五个模块相连之后最后再接上一个全连接层，神经元个数为类别个数</strong></li></ul><h1 id="8-批量归一化"><a href="#8-批量归一化" class="headerlink" title="8 批量归一化"></a>8 批量归一化</h1><ul><li>我们一般在前向传播开始之前会对数据进行归一化，<strong>使不同特征之间具有可比性，并且更快收敛</strong></li><li>通常来说，数据标准化预处理对于浅层模型就⾜够有效了，<strong>但对深层神经网络来说，即使输⼊数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。</strong>这种计算数值的不稳定性通常令我们难以训练出有效的深度模型</li><li>而批量归一化则是对每一层的输出都做一次归一化，<strong>使均值永远为1，方差永远为0，从而不同层之间的相关性减弱</strong>，使整个神经⽹络在各层的中间输出的数值更稳定</li></ul><h3 id="8-1-批量归一化层"><a href="#8-1-批量归一化层" class="headerlink" title="8.1 批量归一化层"></a>8.1 批量归一化层</h3><ul><li><strong>通常，我们将批量归⼀化层置于全连接层中的仿射变换和激活函数之间</strong></li><li>首先要对小批量求均值和方差：</li></ul><script type="math/tex; mode=display">\begin{array}{c}\boldsymbol{\mu}_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} \boldsymbol{x}^{(i)} \\\boldsymbol{\sigma}_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}\right)^{2}\end{array}</script><p>得到的均值和方差是两个向量，维度为特征个数</p><ul><li>然后：</li></ul><script type="math/tex; mode=display">\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}}{\sqrt{\boldsymbol{\sigma}_{\mathcal{B}}^{2}+\epsilon}}</script><ul><li>$\epsilon$ 为一个很小的数，一般取$10^{-8}$ ，是为了保证分母大于0</li><li><strong>但是我们不确定是否一定是所有层都进行批量归一化才是最好的情况，所以我们要给予一个能变回归一化前的值的可能性，所以引入拉伸参数（scale）$\gamma$ 和  偏移参数（shift）$\beta$</strong></li></ul><script type="math/tex; mode=display">\boldsymbol{y}^{(i)} \leftarrow \gamma \odot \hat{\boldsymbol{x}}^{(i)}+\boldsymbol{\beta}</script><p>将$\gamma$ 和 $\beta$ 作为两个可变化参数，然后通过学习来决定拉伸多少和偏移多少</p><h3 id="8-2-对卷积层做批量归一化"><a href="#8-2-对卷积层做批量归一化" class="headerlink" title="8.2 对卷积层做批量归一化"></a>8.2 对卷积层做批量归一化</h3><ul><li>批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前</li><li><p>如果卷积计算输出多个通道，我们需要<strong>对这些通道的输出分别做批量归⼀化，且每个通道都拥有独立的拉伸和偏移参数</strong></p></li><li><p>设小批量中有m个样本。在单个通道上，假设卷积计算输出的⾼和宽分别为p和q。我们需要对该通道中m × p × q个元素同时做批量归⼀化</p></li></ul><h3 id="8-3-预测时的批量归一化"><a href="#8-3-预测时的批量归一化" class="headerlink" title="8.3 预测时的批量归一化"></a>8.3 预测时的批量归一化</h3><ul><li><strong>批量归一化在训练模式和预测模式的计算结果是不⼀样的</strong>。确定均值和方差时，单个样本的输出不应取决于批量归⼀化所需要的随机小批量中的均值和⽅差。⼀种常⽤的⽅法是<strong>通过移动平均（指数加权平均）估算整个训练数据集的样本均值和方差</strong>，并在预测时使⽤它们得到确定的输出</li></ul><h1 id="9-残差网络（ResNet）"><a href="#9-残差网络（ResNet）" class="headerlink" title="9 残差网络（ResNet）"></a>9 残差网络（ResNet）</h1><h3 id="9-1-残差块"><a href="#9-1-残差块" class="headerlink" title="9.1 残差块"></a>9.1 残差块</h3><p><img src="https://i.loli.net/2021/11/25/zADLUxHm7wS8Z9Q.png" alt="image-20211125221657826"></p><ul><li>左图是一般形式的映射，右图为残差映射，是将输入x加权-&gt;激活-&gt;再加权后，再和原输入x相加，再送入激活函数</li><li><p>这样的结构中，输⼊可通过跨层的数据线路更快地向前传播</p></li><li><p>残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接⼀个批量归⼀化层和ReLU激活函数。然后我们<strong>将输⼊跳过这2个卷积运算后直接加在最后的ReLU激活函数前</strong></p></li><li>这样的设计要求2个卷积层的输出与输⼊形状⼀样，从而可以相加。如果想改变通道数，就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算</li></ul><p><img src="https://i.loli.net/2021/11/25/1FIzbxkRYLonM6A.png" alt="image-20211125222235166"></p><p>我们在这个类中引入use_1x1conv变量，若为False则不引入额外的1x1卷积层，反之则引入。可以看出，如果不引入1x1卷积层，输出形状是等于输入形状的，输入输出可以直接进行相加</p><h3 id="9-2-ResNet模型"><a href="#9-2-ResNet模型" class="headerlink" title="9.2 ResNet模型"></a>9.2 ResNet模型</h3><ul><li>ResNet的前两层跟之前介绍的GoogLeNet中的⼀样：在输出通道数为64、步幅为2的7 × 7卷积层后接步幅为2的3 × 3的最⼤池化层。不同之处在于<strong>ResNet每个卷积层后增加的批量归⼀化层</strong></li></ul><p><img src="https://i.loli.net/2021/11/25/baytmMlOCX9G2IJ.png" alt="image-20211125223112124"></p><ul><li><strong>后接4个由残差块组成的模块，每个模块使⽤若⼲个同样输出通道数的残差块</strong></li><li>第⼀个模块的通道数同输⼊通道数⼀致。由于之前已经使⽤了步幅为2的最⼤池化层，所以⽆须减小⾼和宽。<strong>之后的每个模块在第⼀个残差块⾥将上⼀个模块的通道数翻倍，并将⾼和宽减半</strong></li></ul><p><img src="https://i.loli.net/2021/11/25/jqLBfVmvpwZUcNW.png" alt="image-20211125223331860"></p><ul><li>接着我们为ResNet加⼊所有残差块。这⾥每个模块使⽤2个残差块</li></ul><p><img src="https://i.loli.net/2021/11/25/BWNbRwVdM3xy81z.png" alt="image-20211125223812172"></p><ul><li>最后，与GoogLeNet⼀样，加⼊全局平均池化层后接上全连接层输出</li></ul><p><img src="https://i.loli.net/2021/11/25/9aypGjvX1ESYtrM.png" alt="image-20211125223902681"></p><p>其中10为类别个数</p><h3 id="9-3-ResNet的作用"><a href="#9-3-ResNet的作用" class="headerlink" title="9.3 ResNet的作用"></a>9.3 ResNet的作用</h3><ul><li>在深度神经网络中，如果我们进行反向传播，那么由链式求导法则可知，我们会<strong>涉及到非常多参数和导数的连乘</strong>，这时误差很容易产生消失或膨胀，通过实验也可以发现，层数更深的神经网络结果反而没有浅层的好</li></ul><p><img src="C:\Users\LENOVO\AppData\Roaming\Typora\typora-user-images\image-20211125235804507.png" alt="image-20211125235804507"></p><p>这种结果很大程度归结于深度神经网络的<strong>梯度消失问题</strong></p><ul><li>而ResNet的提出就是为了解决梯度消失的问题，<strong>既然离输入近的神经网络层较难训练，那么我们可以将他短接到更接近输出的层</strong>，并且这种方法并不会对结果产生影响，假设输入为X，我们在一般网络中想要拟合f(X)，放到残差网络中就只需要拟合f(X) - X 即可</li></ul><h1 id="10-稠密连接网络（DenseNet）"><a href="#10-稠密连接网络（DenseNet）" class="headerlink" title="10 稠密连接网络（DenseNet）"></a>10 稠密连接网络（DenseNet）</h1><p><img src="https://i.loli.net/2021/11/25/NTajCiqAydPvoVt.png" alt="image-20211125224307704"></p><ul><li>DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是<strong>在通道维上连结</strong></li><li>DenseNet的主要构建模块是<strong>稠密块（dense block）</strong>和<strong>过渡层（transition layer）</strong>。前者定义了输 ⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤</li></ul><h3 id="10-1-稠密块"><a href="#10-1-稠密块" class="headerlink" title="10.1 稠密块"></a>10.1 稠密块</h3><ul><li>我们将批量归⼀化、激活和卷积组合到一起形成一种块：</li></ul><p><img src="https://i.loli.net/2021/11/25/hfHF7BLySM1swZG.png" alt="image-20211125225431518"></p><ul><li><strong>稠密块由多个conv_block组成，每块使⽤相同的输出通道数。但在前向计算时，我们将每块的输⼊和输出在通道维上连结</strong></li></ul><p><img src="https://i.loli.net/2021/11/25/CtYie1P3jAEX9co.png" alt="image-20211125225517509"></p><p><img src="https://i.loli.net/2021/11/25/2CmewFoEItkvAyB.png" alt="image-20211125225537190"></p><h3 id="10-2-过渡层"><a href="#10-2-过渡层" class="headerlink" title="10.2 过渡层"></a>10.2 过渡层</h3><ul><li>由于每个稠密块都会带来通道数的增加，使⽤过多则会带来过于复杂的模型。过渡层⽤来控制模型复杂度。它<strong>通过1 × 1卷积层来减小通道数，并使⽤步幅为2的平均池化层减半高和宽</strong></li></ul><p><img src="https://i.loli.net/2021/11/25/NzEnocuAblhUwZ4.png" alt="image-20211125225807921"></p><h3 id="10-3-DenseNet模型"><a href="#10-3-DenseNet模型" class="headerlink" title="10.3 DenseNet模型"></a>10.3 DenseNet模型</h3><ul><li>DenseNet⾸先使⽤同ResNet⼀样的单卷积层和最⼤池化层</li></ul><p><img src="https://i.loli.net/2021/11/25/DqcGv6FtMioNez1.png" alt="image-20211125230039527"></p><ul><li><p>接着使用4个稠密块，同ResNet⼀样，我们可以设置每个稠密块使⽤多少个卷积层。这⾥我们设成4，从而与上⼀节的ResNet-18保持⼀致。稠密块⾥的卷积层通道数（即增⻓率）设为32，所以每个稠密块将增加128个通道</p></li><li><p>在稠密块之间我们使用过渡层来减半高宽，并减半通道数</p></li></ul><p><img src="https://i.loli.net/2021/11/25/WZ71mSLCr3EyPUw.png" alt="image-20211125230409283"></p><p>注意最后一层是不使用过渡层进行减半的</p><ul><li>最后再和ResNet一样，接上全局平均池化层和全连接层来输出</li></ul><p><img src="https://i.loli.net/2021/11/25/F9YakIDjQrzefnq.png" alt="image-20211125230633074"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>欠拟合和过拟合以及正则化</title>
    <link href="/2021/11/12/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2021/11/12/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="1-什么是过拟合和欠拟合"><a href="#1-什么是过拟合和欠拟合" class="headerlink" title="1 什么是过拟合和欠拟合"></a>1 什么是过拟合和欠拟合</h1><p>我们知道进行反向传播的目的是让学习器去拟合数据，而当拟合程度不够时就称为<strong>欠拟合</strong>，拟合程度过于高时则称为<strong>过拟合</strong></p><p>我们对欠拟合和过拟合的判断可以根据<strong>训练误差</strong>和<strong>泛化误差</strong>，具体可看<a href="https://zlkqz.top/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/#8-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">我的前一篇帖子</a></p><h1 id="2-过拟合和欠拟合是怎么发生的"><a href="#2-过拟合和欠拟合是怎么发生的" class="headerlink" title="2 过拟合和欠拟合是怎么发生的"></a>2 过拟合和欠拟合是怎么发生的</h1><p>用下图可以很好的解释：</p><p><img src="https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png" alt="image-20211111215618245" style="zoom:150%;" /></p><h3 id="2-1-欠拟合"><a href="#2-1-欠拟合" class="headerlink" title="2.1 欠拟合"></a>2.1 欠拟合</h3><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法</p><h3 id="2-2-过拟合"><a href="#2-2-过拟合" class="headerlink" title="2.2 过拟合"></a>2.2 过拟合</h3><h5 id="2-2-1-基本概念"><a href="#2-2-1-基本概念" class="headerlink" title="2.2.1 基本概念"></a>2.2.1 基本概念</h5><p>过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<strong>模型在训练集上表现很好，但在测试集上却表现很差</strong>。模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，<strong>泛化能力差</strong></p><h5 id="2-2-2-为什么会出现过拟合现象"><a href="#2-2-2-为什么会出现过拟合现象" class="headerlink" title="2.2.2 为什么会出现过拟合现象"></a>2.2.2 为什么会出现过拟合现象</h5><ul><li><strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型</li><li><strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系</li><li><strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素</li></ul><h1 id="3-如何防止过拟合"><a href="#3-如何防止过拟合" class="headerlink" title="3 如何防止过拟合"></a>3 如何防止过拟合</h1><h3 id="3-1-获取和使用更多的数据（数据集增强）"><a href="#3-1-获取和使用更多的数据（数据集增强）" class="headerlink" title="3.1 获取和使用更多的数据（数据集增强）"></a>3.1 <strong>获取和使用更多的数据（数据集增强）</strong></h3><ul><li><strong>更多解决过拟合的根本性方法</strong></li><li>但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是<strong>创建“假数据”并添加到训练集中——数据集增强</strong>。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力</li></ul><p>我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。</p><h3 id="3-2-采用合适的模型（控制模型的复杂度）"><a href="#3-2-采用合适的模型（控制模型的复杂度）" class="headerlink" title="3.2 采用合适的模型（控制模型的复杂度）"></a>3.2 采用合适的模型（控制模型的复杂度）</h3><ul><li><p>过于复杂的模型会带来过拟合问题。对于模型的设计，目前公认的一个深度学习规律”deeper is better”。国内外各种大牛通过实验和竞赛发现，对于CNN来说，层数越多效果越好，但是也更容易产生过拟合，并且计算所耗费的时间也越长。</p></li><li><p>根据<strong>奥卡姆剃刀法则</strong>：在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。对于模型的设计而言，我们应该<strong>选择简单、合适的模型解决复杂的问题</strong></p></li></ul><h3 id="3-3-降低特征的数量"><a href="#3-3-降低特征的数量" class="headerlink" title="3.3 降低特征的数量"></a>3.3 降低特征的数量</h3><ul><li>对于一些特征工程而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题</li></ul><h3 id="3-4-正则化"><a href="#3-4-正则化" class="headerlink" title="3.4 正则化"></a>3.4 正则化</h3><p>常用正则化方法可看：<a href="https://zlkqz.top/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/#8-5-%E6%AD%A3%E5%88%99%E5%8C%96">正则化</a></p><h3 id="3-5-Early-stopping（提前终止）"><a href="#3-5-Early-stopping（提前终止）" class="headerlink" title="3.5 Early stopping（提前终止）"></a>3.5 Early stopping（提前终止）</h3><ul><li><strong>Early stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合</strong></li><li>为了获得性能良好的神经网络，训练过程中可能会经过很多次epoch（遍历整个数据集的次数，一次为一个epoch）。如果epoch数量太少，网络有可能发生欠拟合；如果epoch数量太多，则有可能发生过拟合</li><li>Early stopping旨在解决epoch数量需要手动设置的问题。具体做法：每个epoch（或每N个epoch）结束后，在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练，将停止之后的权重作为网络的最终参数</li><li><strong>缺点：</strong></li></ul><p><strong>没有采取不同的方式来解决优化损失函数和过拟合这两个问题</strong>，而是用一种方法同时解决两个问题 ，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化损失函数，你可能会发现损失函数的值不够小，同时你又不希望过拟合</p><h1 id="4-L1正则化防止过拟合的原理"><a href="#4-L1正则化防止过拟合的原理" class="headerlink" title="4 L1正则化防止过拟合的原理"></a>4 L1正则化防止过拟合的原理</h1><h3 id="4-1-产生原因"><a href="#4-1-产生原因" class="headerlink" title="4.1 产生原因"></a>4.1 产生原因</h3><ul><li>我们知道过拟合是由于<strong>记住了不适用于测试集的训练集性质或特点</strong>，没有理解数据背后的规律，导致泛化能力差，所以我们所要做的就是<strong>减少某些特征的重要性</strong>，并且<strong>降低模型复杂度</strong>，<strong>减少不同神经元间的相关性</strong></li><li>过拟合发生在模型完美拟合训练数据，对新的数据效果不好</li></ul><p><img src="https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png" alt="image-20211111215618245" style="zoom:150%;" /></p><h3 id="4-2-奥坎姆剃刀理论"><a href="#4-2-奥坎姆剃刀理论" class="headerlink" title="4.2 奥坎姆剃刀理论"></a>4.2 奥坎姆剃刀理论</h3><ul><li><strong>优先选择拟合数据的最简单的假设。 简单的模型才是最好的</strong>。通俗来讲就是当我们有了足够低的训练误差后，尽量选择简单的模型，如上图的情况</li></ul><h3 id="4-3-减少特征的重要性"><a href="#4-3-减少特征的重要性" class="headerlink" title="4.3 减少特征的重要性"></a>4.3 减少特征的重要性</h3><ul><li>用$L_2$正则化举例，其中一个参数$w_1$的更新：</li></ul><script type="math/tex; mode=display">w1 = (1 - \eta\lambda)w1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial\ell^{(i)}(...)}{\partial{w1}}</script><p>我们可以看到$L_2$正则化主要是通过设置一个<strong>权重的惩罚项</strong>，使权重不会过大，<strong>而减少权重就起到了减少某些特征的重要性的作用</strong>，<strong>使用降低权重值对结果的影响可以减小网络加深对训练准确度降低而产生的影响</strong>，惩罚项让$w_1$往0靠近，其实就是减少了模型的复杂度</p><h3 id="4-4-降低模型复杂度"><a href="#4-4-降低模型复杂度" class="headerlink" title="4.4 降低模型复杂度"></a>4.4 降低模型复杂度</h3><ul><li><strong>当模型复杂度过高的时候，拟合函数的系数往往非常大，需要顾忌每一个点，最终形成的拟合函数波动很大</strong>，如上图过拟合情况。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，所以才需要惩罚项来降低</li></ul><h3 id="4-5-减少神经元间的相关性"><a href="#4-5-减少神经元间的相关性" class="headerlink" title="4.5 减少神经元间的相关性"></a>4.5 减少神经元间的相关性</h3><ul><li><strong>我们可以通过减少神经元之间的相关性而降低模型的复杂度</strong>，用Dropout来举例：</li></ul><p><strong>Dropout指把一些神经元进行暂时的消去（具体方法就是把该神经元相关的权值设为0）</strong>，然后再进行正向传播和反向传播，当我们过拟合的时候，往往是因为要顾及每一个点，最终造成拟合函数的波动很大，<strong>而在把一些神经元进行消去后，这样我们就减少了神经元之间的相关性，就不需要因为顾及所有而产生过大的波动</strong></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>三种梯度下降的方法</title>
    <link href="/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<ul><li>梯度下降法作为机器学习中较常使用的优化算法，其有着3种不同的形式：<strong>批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）、小批量梯度下降（Mini-Batch Gradient Descent）</strong></li></ul><h1 id="1-批量梯度下降（BGD）"><a href="#1-批量梯度下降（BGD）" class="headerlink" title="1 批量梯度下降（BGD）"></a>1 批量梯度下降（BGD）</h1><p>使用整个训练集的优化算法被称为<strong>批量</strong>(batch)或<strong>确定性</strong>(deterministic)梯度算法，因为它们会<strong>在一个大批量中同时处理所有样本</strong></p><p><strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新</p><ul><li><strong>优点：</strong></li></ul><ol><li>在训练过程中，使用固定的学习率，不必担心学习率衰退现象的出现</li><li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向</li><li>一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行</li></ol><ul><li><strong>缺点：</strong></li></ul><ol><li><strong>尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了</strong></li><li>不能投入新数据实时更新模型</li><li>对非凸函数可能只能收敛到局部最小点，而非全局最小点</li></ol><p><img src="https://i.loli.net/2021/11/11/mCkfeDIPyh3a5iY.png" alt="BGD"></p><h1 id="2-随机梯度下降（SGD）"><a href="#2-随机梯度下降（SGD）" class="headerlink" title="2 随机梯度下降（SGD）"></a>2 随机梯度下降（SGD）</h1><p><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是在<strong>每次迭代时</strong>使用<strong>一个样本</strong>来对参数进行更新（mini-batch size =1）</p><ul><li><strong>优点：</strong></li></ul><ol><li>在学习过程中加入了噪声，提高了泛化误差</li><li><strong>噪声造成的扰动可能可以使其脱离鞍点</strong></li><li>SGD一次只遍历一个样本就可以进行更新 ，所以收敛很快，并且可以新增样本</li></ol><ul><li><strong>缺点：</strong></li></ul><ol><li><strong>不收敛，在最小值附近波动</strong></li><li><strong>不能在一个样本中使用并行化计算，学习过程变得很慢</strong></li><li>单个样本并不能代表全体样本的趋势，所以学习过程可能变得特别的曲折，<strong>虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的</strong>，如下图</li></ol><p><img src="https://i.loli.net/2021/11/11/GLBkox67lvnmsOA.png" alt="image-20211111201918007" style="zoom: 67%;" /></p><h1 id="3-Mini-batch梯度下降"><a href="#3-Mini-batch梯度下降" class="headerlink" title="3 Mini-batch梯度下降"></a>3 Mini-batch梯度下降</h1><p>大多数用于深度学习的梯度下降算法介于以上两者之间，<strong>使用一个以上而又不是全部的训练样本</strong></p><ul><li><p>在一次取样本的时候我们需要在所有样本中<strong>随机</strong>取batch-size个样本</p></li><li><p><strong>优点：</strong></p></li></ul><ol><li>收敛速度比BGD快，因为只遍历部分样例就可执行更新</li><li>随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例</li><li>每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果</li><li>因为有噪音（但是噪音比SGD小），所以能<strong>脱离鞍点</strong></li></ol><ul><li><strong>缺点：</strong></li></ul><ol><li>在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛</li><li>学习过程会有更多的振荡，为更接近最小值，需要增加<strong>学习率衰减项</strong>，以降低学习率，避免过度振荡</li></ol><ul><li><strong>小批量大小</strong>（mini-batch size）的选择：</li></ul><ol><li>更大的批量会计算更精确的梯度，但是回报却是小于线性的</li><li>极小的批量通常难以充分利用多核结构。当批量低于某个数值时，计算时间不会减少</li><li>批量处理中的所有样本可以并行处理，<strong>内存消耗和批量大小会成正比</strong>。对于很多硬件设备，这是批量大小的限制因素</li><li>在使用<strong>GPU</strong>时，通常使用<strong>2的幂数作为批量大小</strong>可以获得更少的运行时间。一般，2的幂数取值范围是<strong>32~256</strong>。16有时在尝试大模型时使用</li></ol><p>使用三种梯度下降的收敛过程：</p><p><img src="https://i.loli.net/2021/11/11/paxvbFh9fRiDKNg.png" alt="image-20211111204100788" style="zoom: 80%;" /></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>激活函数的作用和比较</title>
    <link href="/2021/11/11/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E6%AF%94%E8%BE%83/"/>
    <url>/2021/11/11/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E6%AF%94%E8%BE%83/</url>
    
    <content type="html"><![CDATA[<h1 id="1-线性结构"><a href="#1-线性结构" class="headerlink" title="1 线性结构"></a>1 线性结构</h1><p>如果满足：</p><script type="math/tex; mode=display">y = wx + b</script><p>则可称y、x间具有线性关系</p><p>而对于神经网络，相邻两层之间的输出之间满足：</p><script type="math/tex; mode=display">X^{[l]} = w^{[l]}X^{[l - 1]} + b^{[l]}</script><p>则可称其满足线性结构</p><h1 id="2-激活函数的作用"><a href="#2-激活函数的作用" class="headerlink" title="2 激活函数的作用"></a>2 激活函数的作用</h1><ul><li>我们先假设不用激活函数，假设神经网络有3层，每层的权重为$w^{[l]}$，偏差都为$b^{[l]}$，输入为$X$，输出为$Y$，则从头到尾的计算为：</li></ul><script type="math/tex; mode=display">Y = w^{[3]}(w^{[2]}(w^{[1]}X + b^{[1]}) + b^{[2]}) + b^{[3]}</script><p>就相当于做一次线性运算：</p><script type="math/tex; mode=display">Y = w'X + b'</script><p>则从头到尾都是线性结构，这在某些情况是适用的，但是大多数时候输入和输出的关系是非线性的，<strong>这时候我们就需要引入非线性因素，即激活函数，来使得神经网络有足够的capacity来抓取复杂的pattern</strong></p><p>所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。</p><ul><li><strong>能用线性拟合的情况：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/NdbYFTOkwyBj7xh.png" alt="image-20211111115632781" style="zoom:50%;" /></p><ul><li><strong>无法用线性拟合的情况：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/Zvo5O17LesqBbVF.png" alt="image-20211111115745462" style="zoom:50%;" /></p><h1 id="3-常用激活函数的优缺点"><a href="#3-常用激活函数的优缺点" class="headerlink" title="3 常用激活函数的优缺点"></a>3 常用激活函数的优缺点</h1><h3 id="3-1-Sigmoid函数"><a href="#3-1-Sigmoid函数" class="headerlink" title="3.1 Sigmoid函数"></a>3.1 Sigmoid函数</h3><ol><li><p>Sigmoid具有一个很好的特性就是能将输入值映射到$(0, 1)$，便于我们将值转化为概率，并且sigmoid平滑，便于求导</p></li><li><p>但sigmoid还有三大缺点：</p></li></ol><ul><li><strong>Gradient Vanishing：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/TWxNvl6CrD4kuAU.png" alt="image-20211111155149051"></p><p><strong>由图可以看出，在输入值较大或者较小时，其导数是趋于0，而在梯度下降时，其值就基本不会被更新</strong></p><ul><li><strong>函数输出并不是zero-centered</strong></li></ul><p>我们以一个二维的情况举例：</p><script type="math/tex; mode=display">f(\vec{x} ; \vec{w}, b)=f\left(w_{0} x_{0}+w_{1} x_{1}+b\right)</script><p>现在假设，参数 $w_0, w_1$ 的最优解 $w_0^∗, w_1^∗$ 满足条件:</p><script type="math/tex; mode=display">\left\{\begin{array}{l}w_{0}<w_{0}^{*} \\w_{1} \geqslant w_{1}^{*}\end{array}\right.</script><p>所以我们现在就是要让$w_0$变大，$w_1$变小，而：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_0} = x_0 \frac{\partial L}{\partial f} \\\frac{\partial L}{\partial w_1} = x_1 \frac{\partial L}{\partial f}</script><p>由于上一层sigmoid输出的$x_0, x_1$为正值，所以$w_0, w_1$的梯度不可能符号相同，而我们要逼近最小值点，只能走下图红色线路：</p><p><img src="https://i.loli.net/2021/11/11/IXqb8Brk9KOH7WG.png" alt="image-20211111160220315" style="zoom:50%;" /></p><p>而显然绿色线路才是最快的，所以这会<strong>影响梯度下降的速度</strong></p><ul><li><strong>幂运算相对来说比较耗时</strong></li></ul><h3 id="3-2-Tanh函数"><a href="#3-2-Tanh函数" class="headerlink" title="3.2 Tanh函数"></a>3.2 Tanh函数</h3><ul><li>tanh一般优于sigmoid，就是因为tanh是zero-centered，但是tanh仍然没有解决sigmoid的其他两个问题</li></ul><h3 id="3-3-ReLu函数"><a href="#3-3-ReLu函数" class="headerlink" title="3.3 ReLu函数"></a>3.3 ReLu函数</h3><ol><li>优点：</li></ol><ul><li>解决了gradient vanishing问题 (在正区间)</li><li>计算速度非常快，只需要判断输入是否大于0</li><li>收敛速度快</li></ul><ol><li>缺点：</li></ol><ul><li>ReLU的输出不是zero-centered</li><li><strong>Dead ReLU Problem：</strong></li></ul><p>指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。</p><p>在x&lt;0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应</p><p><strong>产生原因：</strong></p><ol><li><p>参数初始化问题（比较少见）</p></li><li><p>learning rate太高导致在训练过程中参数更新太大</p></li></ol><p>而为了防止这种情况，我们可以使用改进的ReLu函数，如Leaky ReLu：</p><script type="math/tex; mode=display">f(x) = max(0.01x, x)</script><p>但是大部分时候我们还是使用ReLu，很多时候LReLu的表现和ReLu其实相差不多，只有神经元大量坏死，ReLu表现不好的时候，我们才会考虑使用LReLu，甚至更复杂的PReLu和ELU</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>常用损失函数和评估模型的指标</title>
    <link href="/2021/11/11/%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8C%87%E6%A0%87/"/>
    <url>/2021/11/11/%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h1 id="1-常用损失函数"><a href="#1-常用损失函数" class="headerlink" title="1 常用损失函数"></a>1 常用损失函数</h1><h3 id="1-1-0-1损失函数"><a href="#1-1-0-1损失函数" class="headerlink" title="1.1 0-1损失函数"></a>1.1 0-1损失函数</h3><script type="math/tex; mode=display">L(y, \hat{y}) = \begin{cases}1 & y\neq \hat{y}\\0 & y = \hat{y}\end{cases}</script><ul><li>0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用</li><li>感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足$|y - \hat{y}| &lt; T$时认为相等，即：</li></ul><script type="math/tex; mode=display">L(y, \hat{y}) = \begin{cases}1 & |y - \hat{y}| \geq T\\0 & |y - \hat{y}| < T\end{cases}</script><h3 id="1-2-均方差损失函数（MSE）"><a href="#1-2-均方差损失函数（MSE）" class="headerlink" title="1.2 均方差损失函数（MSE）"></a>1.2 均方差损失函数（MSE）</h3><script type="math/tex; mode=display">J_{MSE} = \frac{1}{N} \sum_{i = 1}^N(y_i - \hat{y_i})^2</script><ul><li>也称L2 Loss</li></ul><h5 id="1-2-1-证明"><a href="#1-2-1-证明" class="headerlink" title="1.2.1 证明"></a>1.2.1 证明</h5><p>假设预测值和真实值的误差服从标准正态分布，则给定一个$x_i$输出真实值$y_i$的概率为：</p><script type="math/tex; mode=display">p(y_i|x_i) = \frac{1}{\sqrt{2\pi}}exp(-\frac{(y_i - \hat{y_i})^2}{2})</script><p>其实就是极大似然估计，我们要寻找一组参数，使$p(y_i|x_i)$最大</p><p>进一步对所有样本，由于他们相互独立，所以所有样本都正好取到真实值$y$的概率为：</p><script type="math/tex; mode=display">L(x, y) = \prod_{i = 1}^N\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_i - \hat{y_i})^2}{2})</script><p>现在我们就要使$L(x, y)$最大，为了方便计算，我们取对数：</p><script type="math/tex; mode=display">LL(x, y) = log(L(x, y)) = -\frac{N}{2}log2\pi - \frac{1}{2}\sum_{i = 1}^N(y_i - \hat{y_i})^2</script><p>把第一项无关项去掉，再取负：</p><script type="math/tex; mode=display">NLL(x, y) = \frac{1}{2}\sum_{i = 1}^N(y_i - \hat{y_i})^2</script><p>即得到均方差形式</p><h5 id="1-2-2-为什么可以用极大似然"><a href="#1-2-2-为什么可以用极大似然" class="headerlink" title="1.2.2 为什么可以用极大似然"></a>1.2.2 为什么可以用极大似然</h5><p><strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，拟合数据最好的情况就是所有的$y_i = \hat{y_i}$，即每个样本的$p(y_i|x_i)$取最大，即$L(x, y)$取最大，由于对数运算不改变单调性，并且最后取了个负值，所以即$NLL(x, y)$取最小</strong></p><h3 id="1-3-平均绝对误差损失（MAE）"><a href="#1-3-平均绝对误差损失（MAE）" class="headerlink" title="1.3 平均绝对误差损失（MAE）"></a>1.3 平均绝对误差损失（MAE）</h3><script type="math/tex; mode=display">J_{MAE} = \frac{1}{N}\sum_{i = 1}^N|y_i - \hat{y_i}|</script><ul><li>也称L1 Loss</li></ul><h5 id="1-3-1-拉普拉斯分布"><a href="#1-3-1-拉普拉斯分布" class="headerlink" title="1.3.1 拉普拉斯分布"></a>1.3.1 拉普拉斯分布</h5><script type="math/tex; mode=display">f(x|\mu, b) = \frac{1}{2b}exp(-\frac{|x - \mu|}{b})</script><p>期望值：$\mu$             方差：$2b^2$</p><p><img src="https://i.loli.net/2021/11/08/yWY3hI4ioKpADRF.png" alt="Laplace_distribution_pdf" style="zoom: 25%;" /></p><h5 id="1-3-2-证明"><a href="#1-3-2-证明" class="headerlink" title="1.3.2 证明"></a>1.3.2 证明</h5><p>假设预测值和真实值的误差服从拉普拉斯分布（$\mu = 0, b = 1$）</p><script type="math/tex; mode=display">p(y_i | x_i) = \frac{1}{2}exp(-{|y_i - \hat{y_i}|})</script><p>剩余证明和上述MSE证明过程一样</p><h5 id="1-3-3-MSE和MAE的区别："><a href="#1-3-3-MSE和MAE的区别：" class="headerlink" title="1.3.3 MSE和MAE的区别："></a>1.3.3 MSE和MAE的区别：</h5><ul><li><strong>MSE 损失相比 MAE 通常可以更快地收敛</strong></li></ul><p>关于$\hat{y_i}$求导时，MSE为$-(y_i - \hat{y_i})$，MAE为$\pm1$，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的（当然也可以通过调整学习率缓解这个问题，但总体来说还是MAE更快）。</p><ul><li><strong>MAE对于离群值更加健壮，即更加不易受到离群值影响</strong></li></ul><ol><li>由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失</li><li>MSE 假设了误差服从正态分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于离群值更加健壮</li></ol><h5 id="1-3-4-MSE和MAE的收敛"><a href="#1-3-4-MSE和MAE的收敛" class="headerlink" title="1.3.4 MSE和MAE的收敛"></a>1.3.4 MSE和MAE的收敛</h5><ul><li>MSE收敛于均值</li></ul><p>将$\hat{y_i}$设为变量$t$：</p><script type="math/tex; mode=display">J_{MSE} = \frac{1}{N}\sum_{i = 1}^N(t - y_i)^2</script><p>关于$t$求导：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial t} = \frac{2}{N}\sum_{i = 1}^N(t - y_i) = 0</script><p>求得：</p><script type="math/tex; mode=display">t = \frac{1}{N}\sum_{i = 1}^Ny_i = E(y)</script><ul><li>MAE收敛于中值</li></ul><p>将$\hat{y_i}$设为变量$t$：</p><script type="math/tex; mode=display">J_{MAE} = \frac{1}{N}\sum_{i = 1}^N|t - y_i|</script><p>关于$t$求导：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial t} = \frac{1}{N}\sum_{i = 1}^Nsgn(t - y_i) = 0</script><p>显然在该种情况下应该取$t$为中值</p><h3 id="1-4-Huber-Loss"><a href="#1-4-Huber-Loss" class="headerlink" title="1.4 Huber Loss"></a>1.4 Huber Loss</h3><ul><li>上面介绍了MSE和MAE，他们各有各的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，而Huber Loss则是将两者结合起来，原理很简单，就是误差接近 0 时使用 MSE，误差较大时使用 MAE：</li></ul><script type="math/tex; mode=display">J_{huber} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right| \leq \delta} \frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}+\mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right|>\delta}\left(\delta\left|y_{i}-\hat{y}_{i}\right|-\frac{1}{2} \delta^{2}\right)</script><ul><li>前半部分是MSE部分，后半部分是MAE部分，超参数$\delta$为两个部分的连接处</li><li>MAE部分为$\delta |y_i - \hat{y_i}| - \frac{1}{2}\delta ^2$是为了在$|y_i - \hat{y_i}| = \delta$ 端点处连续可导</li></ul><p><img src="https://i.loli.net/2021/11/09/e6FQMBY42PDGxtI.png" alt="超参数为1的Huber Loss"></p><ul><li>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个$\delta$超参数。</li></ul><h3 id="1-5-分位数损失（Quantile-Loss）"><a href="#1-5-分位数损失（Quantile-Loss）" class="headerlink" title="1.5 分位数损失（Quantile Loss）"></a>1.5 分位数损失（Quantile Loss）</h3><script type="math/tex; mode=display">J_{\text {quant }}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\hat{y}_{i} \geq y_{i}}(1-r)\left|y_{i}-\hat{y}_{i}\right|+\mathbb{I}_{\hat{y}_{i}<y_{i}} r\left|y_{i}-\hat{y}_{i}\right|</script><ul><li>这是一个分段函数，这个损失函数是一个分段的函数 ，将$\hat{y_i} \geq y_i$（高估） 和$\hat{y_i} &lt; y_i$（低估) 两种情况分开来，并分别给予不同的系数</li><li>分位数损失实现了分别用不同的系数（r和1-r）控制高估和低估的损失</li><li>特别地，当$r = 0.5$时分位数损失退化为 MAE 损失</li></ul><p><img src="https://i.loli.net/2021/11/09/BvpTeXqWFYRKoMU.png" alt="Quantile Loss"></p><h3 id="1-6-交叉熵损失（Cross-Entropy-Loss）"><a href="#1-6-交叉熵损失（Cross-Entropy-Loss）" class="headerlink" title="1.6 交叉熵损失（Cross Entropy Loss）"></a>1.6 交叉熵损失（Cross Entropy Loss）</h3><ul><li>上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 </li></ul><h5 id="1-6-1-二分类"><a href="#1-6-1-二分类" class="headerlink" title="1.6.1 二分类"></a>1.6.1 二分类</h5><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N}\left(y_{i} \log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right)\right)</script><p><img src="https://i.loli.net/2021/11/09/hPt1nITJ624Llfp.png" alt="二分类的交叉熵"></p><ul><li>证明：</li></ul><p>在二分类中我们通常将输出结果用sigmoid映射到区间$(0, 1)$，并将其作为该类的概率，由于只有两类，所以给定$x_i$求出类别为1或0的概率分别为：</p><script type="math/tex; mode=display">p(y_i = 1|x_i) = \hat{y_i} \\p(y_i = 0|x_i) = 1 - \hat{y_i}</script><p>合并成一个式子：</p><script type="math/tex; mode=display">p(y_i|x_i) = (\hat{y_i})^{y_i}(1 - \hat{y_i})^{1 - y_i}</script><p>由于各数据点独立同分布，则似然可以表示为：</p><script type="math/tex; mode=display">L(x, y) = \prod_{i=1}^{N}\left(\hat{y}_{i}\right)^{y_{i}}\left(1-\hat{y}_{i}\right)^{1-y_{i}}</script><p>取负对数：</p><script type="math/tex; mode=display">N L L(x, y)=J_{C E}=-\sum_{i=1}^{N}\left(y_{i} \log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right)\right)</script><h5 id="1-6-2-多分类"><a href="#1-6-2-多分类" class="headerlink" title="1.6.2 多分类"></a>1.6.2 多分类</h5><p>在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值$y_i$现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 Softmax 函数</p><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log \left(\hat{y}_{i}^{k}\right)</script><p>因为$y_i$是一个One-hot向量，所以还可以写为：</p><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N} y_{i}^{c_{i}} \log \left(\hat{y}_{i}^{c_{i}}\right)</script><p>其中$c_i$为样本$x_i$的目标类</p><ul><li>证明：</li></ul><p>对于一个样本，分类正确的概率为：</p><script type="math/tex; mode=display">p(y_i|x_i) = \prod_{k=1}^{K}\left(\hat{y}_{i}^{k}\right)^{y_{i}^{k}}</script><p>（其中$y_i^k和\hat{y_i}^k$为该向量的第k维）</p><p>因为所有样本相互，所有相乘再取负对数即可得到：</p><script type="math/tex; mode=display">N L L(x, y)=J_{C E}=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log \left(\hat{y}_{i}^{k}\right)</script><h3 id="1-7-合页损失（Hinge-Loss）"><a href="#1-7-合页损失（Hinge-Loss）" class="headerlink" title="1.7 合页损失（Hinge Loss）"></a>1.7 合页损失（Hinge Loss）</h3><ul><li>Hinge Loss也是一种二分类损失函数</li></ul><script type="math/tex; mode=display">J_{\text {hinge }}=\sum_{i=1}^{N} \max \left(0,1-\operatorname{sgn}\left(y_{i}\right) \hat{y}_{i}\right)</script><p>下图是$y$为正类， 即$sgn(y) = 1$时，不同输出的合页损失示意图：</p><p><img src="https://i.loli.net/2021/11/09/KngeQOwp54JZRMf.png" alt="image-20211109225642368"></p><ul><li>可以看到当$y$为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在$(0, 1)$区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。<strong>使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类</strong></li></ul><h1 id="2-评估模型的指标"><a href="#2-评估模型的指标" class="headerlink" title="2 评估模型的指标"></a>2 评估模型的指标</h1><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><img src="https://i.loli.net/2021/11/09/OCNnaMs2ceBQIJz.png" alt="image-20211109233859657"></p><h3 id="2-2-查准率和查全率"><a href="#2-2-查准率和查全率" class="headerlink" title="2.2 查准率和查全率"></a>2.2 查准率和查全率</h3><script type="math/tex; mode=display">查准率 P（Precision） = \frac{TP}{TP + FP}</script><script type="math/tex; mode=display">查全率 R（Recall） = \frac{TP}{TP + FN}</script><ul><li>查准率可以直观理解为所有预测为正项的样本中有多少是真正的正项</li><li>查全率可以直观理解为所有label是正项的样本中有多少被成功预测出来了</li><li>理想情况下，查准率和查全率两者都越高越好。<strong>然而事实上这两者在某些情况下是矛盾的，一般来说，查准率高时，查全率低；查确率低时，查全率高</strong></li></ul><h3 id="2-3-准确率和错误率"><a href="#2-3-准确率和错误率" class="headerlink" title="2.3 准确率和错误率"></a>2.3 准确率和错误率</h3><p>准确率：</p><script type="math/tex; mode=display">accuracy = \frac{TP + TF}{TP + TN + FP + FN}</script><ul><li>即有多少样本被分类正确</li></ul><p>而错误率：</p><script type="math/tex; mode=display">errorrate = 1 - accuracy</script><h3 id="2-4-P-R曲线"><a href="#2-4-P-R曲线" class="headerlink" title="2.4 P-R曲线"></a>2.4 P-R曲线</h3><p><img src="https://i.loli.net/2021/11/10/zV1Sj4HyYfD3G5e.png" alt="image-20211110000609300" style="zoom: 80%;" /></p><ul><li>P-R曲线直观地显示出学习器在样本总体上地查全率、查准率，在进行比较时，<strong>若一个学习器的P-R曲线被另一个学习器曲线“完全包住”，则可以断言后者的性能一定优于前者，如上图的B性能优于C，而A、B不一定</strong></li><li>平衡点（BEP）时$P = R$时的取值，如上图A的BEP为0.8，而如果基于BEP比较，可知A优于B</li></ul><h3 id="2-5-F函数"><a href="#2-5-F函数" class="headerlink" title="2.5 F函数"></a>2.5 F函数</h3><p>BEP还是过于简化了些，更常用得是F1度量，我们可以取P和R的调和平均：</p><script type="math/tex; mode=display">\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}</script><p>求得：</p><script type="math/tex; mode=display">F1 = \frac{2PR}{P + R}</script><p>但是在许多应用中我们对查准率和查全率得重视程度不同，所以可以给予P和R不同的权重，取P和R的加权调和平均：</p><script type="math/tex; mode=display">\frac{1}{F_{\beta}} = \frac{1}{1 + \beta ^2}(\frac{1}{P} + \frac{\beta ^2}{R})</script><p>求得：</p><script type="math/tex; mode=display">F_{\beta} = \frac{(1 + \beta ^2)PR}{\beta ^2P + R}</script><ul><li>$\beta &gt; 0$度量了查全率和查准率的相对重要性，$\beta = 1$退化为标准的F1，$\beta &gt; 1$查全率有更大影响，$\beta &lt; 1$查准率有更大影响</li></ul><h3 id="2-6-ROC与AUC"><a href="#2-6-ROC与AUC" class="headerlink" title="2.6 ROC与AUC"></a>2.6 ROC与AUC</h3><h5 id="2-6-1-基本概念"><a href="#2-6-1-基本概念" class="headerlink" title="2.6.1 基本概念"></a>2.6.1 基本概念</h5><ul><li><p>大多二分类问题是将输出的预测值与一个<strong>分类阈值（threshold）</strong>进行比较，若预测值大于阈值则为正类，反之则为负类</p></li><li><p>根据预测值，我们可将测试样本进行排序，根据预测值由大到小，“最可能”是正例的排在前面，“最不可能”是正例的排到后面。<strong>这样，分类过程就相当于以中间某个截断点（也就是分类阈值）将样本分为两部分，前一部分判定为正例，后一部分判定为反例</strong></p></li><li><p>在不同的任务中，我们我们可以根据任务需求采取不同的截断点。例如如更重视查准率，可以将截断点设置靠前，更注重查全率，可以将截断点靠后</p></li><li>而我们根据预测值进行排序后，按顺序将样本逐个作为正例进行预测，每次计算出<strong>真正例率（TPR）</strong>和<strong>假正例率（FPR）</strong>，以他们为横纵坐标就得到了<strong>ROC曲线</strong></li></ul><h5 id="2-6-2-ROC曲线"><a href="#2-6-2-ROC曲线" class="headerlink" title="2.6.2 ROC曲线"></a>2.6.2 ROC曲线</h5><ul><li>首先介绍真正例率和假正例率：</li></ul><script type="math/tex; mode=display">TPR = \frac{TP}{TP + FN} \\FPR = \frac{FP}{TN + FP}</script><ul><li>ROC曲线：</li></ul><p>首先将分类阈值设最大，则所有类都被分类为负类，TPR和FPR都为0，然后每次将分类阈值设为下一个样本点的预测值（按预测值由大到小进行排序），记录每次的TPR和FPR，组成ROC曲线</p><p><img src="https://i.loli.net/2021/11/10/TUMHuC3N9rX18iz.png" alt="image-20211110140633061"></p><p>但是现实中我们是基于有限个样本画的图，所以不会产生这么平滑的曲线，更多情况应该像下图：</p><p><img src="https://i.loli.net/2021/11/10/xG1TrPNfyimzq7Q.png" alt="image-20211110140853013"></p><ul><li>基于ROC的比较方法</li></ul><blockquote><p>如果一个学习器的ROC曲线完全被另一个学习器的”包住“，则后者性能优于前者</p><p>若两者的曲线交叉，则可以通过ROC曲线所包裹的面积进行判断，即<strong>AUC</strong></p></blockquote><h5 id="2-6-3-AUC"><a href="#2-6-3-AUC" class="headerlink" title="2.6.3 AUC"></a>2.6.3 AUC</h5><ul><li><strong>AUC就是ROC曲线下的面积</strong>，假定ROC曲线是由坐标为$\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right),\left(x<em>{3}, y</em>{3}\right), \cdots,\left(x<em>{m}, y</em>{m}\right)$的点按序连接而形成，则AUC为：</li></ul><script type="math/tex; mode=display">A U C=\frac{1}{2} \sum_{i=1}^{m-1}\left(x_{i+1}-x_{i}\right)\left(y_{i}+y_{i+1}\right)</script><ul><li><p>从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1&gt;p0的概率就等于AUC， 即<strong>AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性</strong></p></li><li><p><strong>所以AUC反应的是分类器对样本的排序能力</strong></p></li></ul><p>证明：</p><p>设所有正类的集合$X = { \hat{X_1}, \hat{X_2}, …, \hat{X_m}}$和负类的集合$Y = { \hat{Y_1}, \hat{Y_2}, …, \hat{Y_n}}$，其中是每个样本对应的预测值，设分类阈值为c，$F_X(x)$和$F_Y(y)$分别为X和Y的分布函数，则$TPR(c) = 1 - F_X(c)$，$FPR(c) = 1 - F_Y(c)$</p><p>设$t = FPR(c)$， 则$c = F_Y^{-1}(1 - t)$，则$ROC(t) = 1 - F_X(F_Y^{-1}(1 - t))$</p><p>则：</p><script type="math/tex; mode=display">AUC = \int_0^1ROC(t)dt \\=\int_0^1 [1 - F_X(F_Y^{-1}(1 - t))] dt \\=\int_0^1 [1 - F_X(F_Y^{-1}(t))] dt \\=\int_{-\infty}^{+\infty} [1 - F_X(y)] dF_Y(y) \\=\int_{-\infty}^{+\infty}P(X > y)f_Y(y)dy \\=\int_{-\infty}^{+\infty}P(X > y, Y = y)dy \\=P(X > Y)</script><h5 id="2-6-4-使用ROC和AUC的优点"><a href="#2-6-4-使用ROC和AUC的优点" class="headerlink" title="2.6.4 使用ROC和AUC的优点"></a>2.6.4 使用ROC和AUC的优点</h5><ul><li><strong>AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡（正负类样本不相同）的情况下，依然能够对分类器做出合理的评价</strong></li></ul><script type="math/tex; mode=display">TPR = P(\hat{Y} = 1 | Y = 1) \\FPR = P(\hat{Y} = 1 | Y = 0)</script><p><strong>由上式可得：无论Y的真实概率是多少， 都不会影响TPR和FPR</strong></p><p>而PR曲线更关注正例</p><ul><li>ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响，有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的查全率就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习基础总结</title>
    <link href="/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/"/>
    <url>/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="1-行业知识体系"><a href="#1-行业知识体系" class="headerlink" title="1 行业知识体系"></a>1 行业知识体系</h1><h3 id="1-1-机器学习算法"><a href="#1-1-机器学习算法" class="headerlink" title="1.1 机器学习算法"></a>1.1 机器学习算法</h3><p><img src="https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg" alt="qq_pic_merged_1635482805011"></p><h3 id="1-2-机器学习分类"><a href="#1-2-机器学习分类" class="headerlink" title="1.2 机器学习分类"></a>1.2 机器学习分类</h3><p><img src="https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg" alt="qq_pic_merged_1635482964705"></p><h3 id="1-3-问题领域"><a href="#1-3-问题领域" class="headerlink" title="1.3 问题领域"></a>1.3 问题领域</h3><script type="math/tex; mode=display">\begin{cases}\pmb{语言识别} \\\pmb{字符识别} \\\pmb{计算机视觉}（CV） \\\pmb{自然语言处理}（NLP）\\\pmb{知识推理}\\\pmb{自动控制}\\\pmb{游戏理论和人机对弈}\\\pmb{数据挖掘}\end{cases}</script><h1 id="2-线性回归"><a href="#2-线性回归" class="headerlink" title="2 线性回归"></a>2 线性回归</h1><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><img src="https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png" alt="image-20211029131504886"></p><ul><li><p>线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</p></li><li><p>线性回归输出是⼀个连续值，因此适⽤于回归问题</p></li></ul><h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><ul><li>均方差（mse）：</li></ul><script type="math/tex; mode=display">\ell^{(i)}(W, b) = \frac{(\hat{y}^{(i)} - y^{(i)})^2}{2}</script><p>其中$\hat{y}$为计算出的预测值，$y$为真实值（label）</p><h1 id="3-全连接层（稠密层）"><a href="#3-全连接层（稠密层）" class="headerlink" title="3  全连接层（稠密层）"></a>3  全连接层（稠密层）</h1><ul><li>输出层中的神经元和输⼊ 层中各个输⼊完全连接</li><li>计算完全依赖于输入层。</li></ul><h1 id="4-小批量随机梯度下降"><a href="#4-小批量随机梯度下降" class="headerlink" title="4  小批量随机梯度下降"></a>4  小批量随机梯度下降</h1><ul><li><p>在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\eta$）作为模型参数在本次迭代的减小量。</p></li><li><p>每次mini-batch梯度下降计算过程：</p></li></ul><script type="math/tex; mode=display">w_1 = w_1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial{\ell(...)}}{\partial{w_1}}</script><ul><li>注意学习率要取正数</li></ul><h1 id="5-Softmax分类"><a href="#5-Softmax分类" class="headerlink" title="5  Softmax分类"></a>5  Softmax分类</h1><h3 id="5-1-基本概念"><a href="#5-1-基本概念" class="headerlink" title="5.1 基本概念"></a>5.1 基本概念</h3><ul><li>softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数</li><li>由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为该类的概率，具体计算如下：</li></ul><p>假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:</p><script type="math/tex; mode=display">\hat{y}_1, \hat{y}_2, \hat{y}_3 = softmax(o_1, o_2, o_3)</script><p>其中</p><script type="math/tex; mode=display">\hat{y}_i = \frac{e^{o_i}}{\sum_{j = 1}^{3}e^{o_j}}</script><ul><li>容易看出$\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）</li><li>通常，我 们把预测概率最⼤的类别作为输出类别</li></ul><h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 损失函数</h3><ul><li>mse过于严格，因为并不需要所有的输出都和真实值相近</li><li>这里我们运用交叉熵损失函数（cross entropy）：</li></ul><script type="math/tex; mode=display">H(y^{(i)}, \hat{y}^{(i)}) = -\sum_{j = 1}^qy_j^{(i)}\log{\hat{y}^{(i)}}</script><p>q为输出元个数$\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值</p><h1 id="6-多层感知机（MLP）"><a href="#6-多层感知机（MLP）" class="headerlink" title="6 多层感知机（MLP）"></a>6 多层感知机（MLP）</h1><ul><li>多层感知机有一到多个隐藏层</li><li>多层感知机中的隐藏层和输出层都是全连接层</li><li>多层感知机具有激活函数，下面介绍常用激活函数</li></ul><h1 id="7-激活函数"><a href="#7-激活函数" class="headerlink" title="7 激活函数"></a>7 激活函数</h1><h3 id="7-1-Sigmoid函数"><a href="#7-1-Sigmoid函数" class="headerlink" title="7.1 Sigmoid函数"></a>7.1 Sigmoid函数</h3><script type="math/tex; mode=display">sigmoid(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">sigmoid'(x) = sigmoid(x)(1 - sigmoid(x))</script><p><img src="https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png" alt="image-20211030004640247"></p><ul><li>一般用在二分类的输出层中</li></ul><h3 id="7-2-tanh函数"><a href="#7-2-tanh函数" class="headerlink" title="7.2 tanh函数"></a>7.2 tanh函数</h3><script type="math/tex; mode=display">tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}</script><script type="math/tex; mode=display">tanh'(x) = 1 - tanh^2(x)</script><p><img src="https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png" alt="image-20211030005138300"></p><ul><li>tanh和sigmoid比较相似，但是性能一般要优于sigmoid</li></ul><h3 id="7-3-ReLU函数"><a href="#7-3-ReLU函数" class="headerlink" title="7.3 ReLU函数"></a>7.3 ReLU函数</h3><script type="math/tex; mode=display">ReLU(x) = max(0, x)</script><p><img src="https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png" alt="image-20211030005121490"></p><ul><li>十分常用</li></ul><h3 id="7-4-Leaky-ReLU函数"><a href="#7-4-Leaky-ReLU函数" class="headerlink" title="7.4 Leaky ReLU函数"></a>7.4 Leaky ReLU函数</h3><script type="math/tex; mode=display">Leaky\_Relu(x) = max(0.1 * x, x)</script><p><img src="https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png" alt="image-20211030005818507"></p><h1 id="8-训练误差和泛化误差"><a href="#8-训练误差和泛化误差" class="headerlink" title="8 训练误差和泛化误差"></a>8 训练误差和泛化误差</h1><h3 id="8-1-基本概念"><a href="#8-1-基本概念" class="headerlink" title="8.1 基本概念"></a>8.1 基本概念</h3><ul><li>训练误差：指模型在训练数据集上表现出的误差</li><li><p>泛化误差：指模型在任意⼀个测试数据样本上表 现出的误差的期望</p></li><li><p>我们通常假设训练数据集（训练题）和测试数据集（测试题）⾥的每⼀个样本都 是从同⼀个概率分布中相互独⽴地⽣成的</p></li></ul><h3 id="8-2-验证数据集"><a href="#8-2-验证数据集" class="headerlink" title="8.2 验证数据集"></a>8.2 验证数据集</h3><ul><li>在机器学习中，通常需要评估若⼲候选模型的表现并从中选择模型。这⼀过程称为模型选择 （model selection）。可供选择的候选模型可以是有着不同超参数的同类模型。</li><li>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使⽤⼀次。不可以使⽤测试数据选择模型，如调参。由于⽆法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留⼀部分在训练数据集和测试数据集以外的数据来进⾏模型选择。</li><li>我们可以从给定的训练集中随机选取⼀ 小部分作为验证集，而将剩余部分作为真正的训练集</li><li>对于总样本数量比较小的情况，一般训练集：验证集：测试集 = 6：2：1；如果数据量比较大，训练集：验证集：测试集 = 8：1：1</li></ul><h3 id="8-3-k折交叉验证"><a href="#8-3-k折交叉验证" class="headerlink" title="8.3 k折交叉验证"></a>8.3 k折交叉验证</h3><ul><li>当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法</li><li>即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个⼦数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。</li></ul><h3 id="8-4-欠拟合和过拟合"><a href="#8-4-欠拟合和过拟合" class="headerlink" title="8.4 欠拟合和过拟合"></a>8.4 欠拟合和过拟合</h3><ul><li>欠拟合：模型⽆法得到较低的训练误差</li><li><p>过拟合：是模型的训练误差远小于它在测试数据集上 的误差</p></li><li><p>如果模型的复杂度过低，很容易出现⽋拟合；如果模型复杂度过⾼，很容易出现过拟合</p></li><li><p>⼀般来说，如果训练数据集中样本 数过少，特别是⽐模型参数数量（按元素计）更少时，过拟合更容易发⽣。</p></li><li>泛化误差不会 随训练数据集⾥样本数量增加而增⼤</li></ul><h3 id="8-5-正则化"><a href="#8-5-正则化" class="headerlink" title="8.5 正则化"></a>8.5 正则化</h3><ul><li>正则化通过为模型损失函数添加惩罚项使学出 的模型参数值较小</li></ul><h4 id="8-5-1-L2正则化（权重衰减）"><a href="#8-5-1-L2正则化（权重衰减）" class="headerlink" title="8.5.1 L2正则化（权重衰减）"></a>8.5.1 L2正则化（权重衰减）</h4><ul><li>L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积</li><li>损失函数添加一个L2惩罚项：</li></ul><script type="math/tex; mode=display">\ell(...) + \frac{\lambda}{2}\begin{Vmatrix} W \end{Vmatrix}^2</script><p>求导后：（每个mini-batch）</p><script type="math/tex; mode=display">w1 = (1 - \eta\lambda)w1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial\ell^{(i)}(...)}{\partial{w1}}</script><p>其中超参数$\lambda$ &gt; 0。当权重参数均为0时，惩罚项最小。当$\lambda$较⼤时，惩罚项在损失函数中的⽐重较⼤，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作⽤。</p><ul><li>一般训练集中的损失函数要加惩罚项，测试集中不用</li></ul><h4 id="8-5-2-丢弃法（Dropout）"><a href="#8-5-2-丢弃法（Dropout）" class="headerlink" title="8.5.2 丢弃法（Dropout）"></a>8.5.2 丢弃法（Dropout）</h4><ul><li>对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：</li></ul><script type="math/tex; mode=display">h_i' = \frac{\xi_i}{1 - p}h_i</script><p>$\xi_i$为0和1的概率分别为p和1 - p</p><p>分母中的1- p是为了不改变其输⼊的期望值：</p><script type="math/tex; mode=display">由于E(\xi_i) = 1 - p \\所以E(h_i') = \frac{E(\xi_i)}{1 - p}h_i = h_i</script><ul><li><p>测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法</p></li><li><p>进行了Dropout的多层感知机：</p></li></ul><p><img src="https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png" alt="image-20211030141251677"></p><p>可以看到隐藏层中的$h_2$和$h_5$消失了</p><ul><li>每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点</li></ul><h1 id="9-正向传播和反向传播"><a href="#9-正向传播和反向传播" class="headerlink" title="9 正向传播和反向传播"></a>9 正向传播和反向传播</h1><h3 id="9-1-正向传播"><a href="#9-1-正向传播" class="headerlink" title="9.1 正向传播"></a>9.1 正向传播</h3><ul><li>正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）</li><li>中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等</li><li>正向传播的计算图：</li></ul><p><img src="https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png" alt="image-20211030142019713"></p><p>其中：</p><ol><li>左下⻆是输⼊，右上⻆是输出</li><li>⽅框代表变量，圆圈代表运算符</li><li>$J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项</li></ol><h3 id="9-2-反向传播"><a href="#9-2-反向传播" class="headerlink" title="9.2 反向传播"></a>9.2 反向传播</h3><ul><li>反向传播最重要的是通过链式法则求导，从后往前进行计算</li><li>反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传 播计算得到的</li></ul><h3 id="9-3-衰减和爆炸"><a href="#9-3-衰减和爆炸" class="headerlink" title="9.3 衰减和爆炸"></a>9.3 衰减和爆炸</h3><ul><li>如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：</li></ul><p>假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积</p><h3 id="9-4-随机初始化参数"><a href="#9-4-随机初始化参数" class="headerlink" title="9.4 随机初始化参数"></a>9.4 随机初始化参数</h3><ul><li>例如：</li></ul><p>假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。</p><ul><li><strong>在这种情况下，⽆论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作⽤</strong></li></ul><h3 id="9-5-Xavier随机初始化"><a href="#9-5-Xavier随机初始化" class="headerlink" title="9.5 Xavier随机初始化"></a>9.5 Xavier随机初始化</h3><ul><li>假设某全连接层的输⼊个数为a， 输出个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：</li></ul><script type="math/tex; mode=display">U(-\sqrt{\frac{6}{a + b}}, \sqrt{\frac{6}{a + b}})</script><ul><li>该设计主要为了每层输出的⽅差不该受该层输⼊个数影响，且每层梯度的⽅差也不该受该层输出个数影响</li></ul><h1 id="10-Block类"><a href="#10-Block类" class="headerlink" title="10 Block类"></a>10 Block类</h1><ul><li>Block类是⼀个模型构造类，我们可以继承它来定义我们想要的模型。</li><li>它的⼦类既可以是⼀个层（如Dense类），⼜可以是深度学习计算的⼀个模型，或者是模型的⼀个部分</li></ul><h1 id="11-模型参数的延后初始化"><a href="#11-模型参数的延后初始化" class="headerlink" title="11 模型参数的延后初始化"></a>11 模型参数的延后初始化</h1><h3 id="11-1-基本概念"><a href="#11-1-基本概念" class="headerlink" title="11.1 基本概念"></a>11.1 基本概念</h3><ul><li>如果我们在创建层时没有指定输入输出的个数，那么和该层相邻的参数的维度一开始我们是不知道的，只有我们将最开始的输入值输进去，这时系统才能推断出参数的维度，才能开始参数初始化。系统将这种真正的参数初始化延后到获得⾜够信息时才执⾏的⾏为叫作延后初始化（deferred initialization）</li><li>这个初始化只会在第⼀次前向计算时被调⽤</li></ul><h3 id="11-2-避免延后初始化"><a href="#11-2-避免延后初始化" class="headerlink" title="11.2 避免延后初始化"></a>11.2 避免延后初始化</h3><ul><li>可以额外做⼀次前向计算来迫使参数被真正地初始化</li><li>也可以在创建层的时候指定了它的输⼊个数</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Markdown &amp; Letax常用语法</title>
    <link href="/2021/10/05/%E8%AF%AD%E6%B3%95/"/>
    <url>/2021/10/05/%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="1、标题"><a href="#1、标题" class="headerlink" title="1、标题"></a>1、标题</h1><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>~<span class="hljs-number">6</span>级标题<br><span class="hljs-attribute">1</span>~<span class="hljs-number">6</span>个#加个空格<br></code></pre></td></tr></table></figure><h1 id="2、代码块"><a href="#2、代码块" class="headerlink" title="2、代码块"></a>2、代码块</h1><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">``` + 代码语言<br></code></pre></td></tr></table></figure><h1 id="3、字体"><a href="#3、字体" class="headerlink" title="3、字体"></a>3、字体</h1><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">加粗：<span class="hljs-strong">**文本**</span><br>删除线:~~文本~~<br>斜体：<span class="hljs-strong">*文本*</span><br></code></pre></td></tr></table></figure><p>加粗：<strong>文本</strong><br>删除线：<del>文本</del><br>斜体：<em>文本</em></p><h1 id="4、引用"><a href="#4、引用" class="headerlink" title="4、引用"></a>4、引用</h1><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css">&gt;<span class="hljs-selector-tag">A</span><br>&gt;&gt;<span class="hljs-selector-tag">A</span><br>&gt;&gt;&gt;<span class="hljs-selector-tag">A</span><br></code></pre></td></tr></table></figure><blockquote><p>A</p><blockquote><p>A</p><blockquote><p>A</p></blockquote></blockquote></blockquote><h1 id="5、分割线"><a href="#5、分割线" class="headerlink" title="5、分割线"></a>5、分割线</h1><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><hr><h1 id="6、图片插入"><a href="#6、图片插入" class="headerlink" title="6、图片插入"></a>6、图片插入</h1><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs less">!<span class="hljs-selector-attr">[名字]</span>(本地或网页链接)<br></code></pre></td></tr></table></figure><p><img src="" alt="图片"></p><h1 id="7、超链接"><a href="#7、超链接" class="headerlink" title="7、超链接"></a>7、超链接</h1><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clojure">[名字](链接)<br></code></pre></td></tr></table></figure><p><a href="www.baidu.com">百度</a></p><h1 id="8、列表"><a href="#8、列表" class="headerlink" title="8、列表"></a>8、列表</h1><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gcode">无序列表<br>- <span class="hljs-comment">(空格)</span> 内容<br>有序列表<br>数字.<span class="hljs-comment">(空格)</span>内容<br></code></pre></td></tr></table></figure><ul><li>无序</li></ul><ol><li>有序</li></ol><h1 id="9、表格"><a href="#9、表格" class="headerlink" title="9、表格"></a>9、表格</h1><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">干脆直接快捷键<br></code></pre></td></tr></table></figure><h1 id="10、Latex"><a href="#10、Latex" class="headerlink" title="10、Latex"></a>10、Latex</h1><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><div class="table-container"><table><thead><tr><th>Latex</th><th>对应形式</th><th>Latex</th><th>对应形式</th></tr></thead><tbody><tr><td>\alpha</td><td>$\alpha$</td><td>\Alpha</td><td>$\Alpha$</td></tr><tr><td>\beta</td><td>$\beta$</td><td>\Beta</td><td>$\Beta$</td></tr><tr><td>\gamma</td><td>$\gamma$</td><td>\Gamma</td><td>$\Gamma$</td></tr><tr><td>\delta</td><td>$\delta$</td><td>\Delta</td><td>$\Delta$</td></tr><tr><td>\epsilon</td><td>$\epsilon$</td><td>\Epsilon</td><td>$\Epsilon$</td></tr><tr><td>\zeta</td><td>$\zeta$</td><td>\Zeta</td><td>$\Zeta$</td></tr><tr><td>\eta</td><td>$\eta$</td><td>\Eta</td><td>$\Eta$</td></tr><tr><td>\theta</td><td>$\theta$</td><td>\Theta</td><td>$\Theta$</td></tr><tr><td>\lambda</td><td>$\lambda$</td><td>\Lambda</td><td>$\Lambda$</td></tr><tr><td>\mu</td><td>$\mu$</td><td>\Mu</td><td>$\Mu$</td></tr><tr><td>\nu</td><td>$\nu$</td><td>\Nu</td><td>$\Mu$</td></tr><tr><td>\xi</td><td>$\xi$</td><td>\Xi</td><td>$\Xi$</td></tr><tr><td>\pi</td><td>$\pi$</td><td>\Pi</td><td>$\Pi$</td></tr><tr><td>\rho</td><td>$\rho$</td><td>\Rho</td><td>$\Rho$</td></tr><tr><td>\sigma</td><td>$\sigma$</td><td>\Sigma</td><td>$\Sigma$</td></tr><tr><td>\varphi</td><td>$\varphi$</td><td>\Phi</td><td>$\Phi$</td></tr><tr><td>\chi</td><td>$\chi$</td><td>\Chi</td><td>$\Chi$</td></tr><tr><td>\psi</td><td>$\psi$</td><td>\Psi</td><td>$\Psi$</td></tr><tr><td>\omega</td><td>$\omega$</td><td>\Omega</td><td>$\Omega$</td></tr><tr><td>\ell</td><td>$\ell$</td><td>\varepsilon</td><td>$\varepsilon$</td></tr></tbody></table></div><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><div class="table-container"><table><thead><tr><th>解释</th><th>代码</th></tr></thead><tbody><tr><td>粗体</td><td>\pmb{…..}</td></tr><tr><td>单空格</td><td>\quad</td></tr><tr><td>双空格</td><td>\qquad</td></tr><tr><td>$\times$</td><td>\times</td></tr><tr><td>$\div$</td><td>\div</td></tr><tr><td>下标</td><td>_</td></tr><tr><td>上标</td><td>^</td></tr><tr><td>$\hat{a}$</td><td>\hat{a}</td></tr><tr><td>$\vec{a}$</td><td>\vec{a}</td></tr><tr><td>$\log_{32}{xy}$</td><td>\log_{32}{xy}</td></tr><tr><td>{ }</td><td>需要转义</td></tr><tr><td>$\sum_1^n$</td><td>\sum_1^n</td></tr><tr><td>$\prod_{k=1}^nk^2$</td><td>\prod_{k=1}^nk^2</td></tr><tr><td></td><td></td></tr><tr><td>$\int_a^b$</td><td>\int_a^b</td></tr><tr><td>$\iint$</td><td>\iint</td></tr><tr><td>$\iiint$</td><td>\iiint</td></tr><tr><td>$\infty$</td><td>\infty</td></tr><tr><td>$\lim_{x\to0}$</td><td>极限       \lim_{x\to0}</td></tr><tr><td>$f’(x)$</td><td>导数        f’(x)</td></tr><tr><td>$\int<em>{\theta=0}^{2\pi}    \int</em>{r=0}^R$</td><td>\int<em>{\theta=0}^{2\pi}    \int</em>{r=0}^R$</td></tr><tr><td>$\nabla$</td><td>\nabla</td></tr><tr><td>$\partial$</td><td>\partial</td></tr><tr><td></td><td></td></tr><tr><td>$\subset$</td><td>\subset</td></tr><tr><td>$\subseteq$</td><td>\subseteq</td></tr><tr><td>$\in$</td><td>\in</td></tr><tr><td>$\notin$</td><td>\notin</td></tr><tr><td>$\emptyset$</td><td>\emptyset</td></tr><tr><td>$\varnothing$</td><td>\varnothing</td></tr><tr><td>$\bigcup$</td><td>\bigcup</td></tr><tr><td>$\cup$</td><td>\cup</td></tr><tr><td>$\bigcap$</td><td>\bigcap</td></tr><tr><td>$\cap$</td><td>\cap</td></tr><tr><td></td><td></td></tr><tr><td>$\frac{a+1}{b+1}$</td><td>分数    \frac{a+1}{b+1}</td></tr><tr><td>$\sqrt{x^5}$</td><td>开方      \sqrt{x^5}</td></tr><tr><td>$\sqrt[3]{xy}$</td><td>开方       \sqrt[3]{xy}</td></tr><tr><td></td><td></td></tr><tr><td>$\le$</td><td>\le</td></tr><tr><td>$\geq$</td><td>\geq</td></tr><tr><td>$\neq$</td><td>\neq</td></tr><tr><td>$\approx$</td><td>\approx</td></tr><tr><td>$\equiv$</td><td>\equiv</td></tr><tr><td>$\pm$</td><td>\pm</td></tr><tr><td>$\mp$</td><td>\mp</td></tr><tr><td>$\mathbb{R}$</td><td>将字母改为黑板字体\mathbb{R}</td></tr><tr><td>$\mathcal{B}$</td><td>\mathcal{B}</td></tr><tr><td>$\leftarrow$</td><td>\leftarrow</td></tr><tr><td>$\rightarrow$</td><td>\rightarrow</td></tr></tbody></table></div><h3 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h3><script type="math/tex; mode=display">f(x) = \begin{cases}1 & x = 2\\2 & x > 2\\3 & x \leqslant 2\\\end{cases}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs livescript">$$<br>f(x) = <span class="hljs-string">\begin&#123;cases&#125;</span><br><span class="hljs-number">1</span> &amp; x = <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">2</span> &amp; x &gt; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; x <span class="hljs-string">\leqslant</span> <span class="hljs-number">2</span><span class="hljs-string">\\</span><br><span class="hljs-string">\end&#123;cases&#125;</span><br>$$<br></code></pre></td></tr></table></figure><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><script type="math/tex; mode=display">\begin{matrix}1 & 2 \\3 & 4 \end{matrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;matrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;matrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{pmatrix}1 & 2 \\3 & 4 \end{pmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;pmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;pmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;bmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;bmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Bmatrix}1 & 2 \\3 & 4 \end{Bmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;Bmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;Bmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{vmatrix}1 & 2 \\3 & 4 \end{vmatrix}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-string">\begin&#123;vmatrix&#125;</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br><span class="hljs-string">\end&#123;vmatrix&#125;</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Vmatrix}1 & 2 \\3 & 4 \end{Vmatrix}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-string">\begin&#123;Vmatrix&#125;</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br><span class="hljs-string">\end&#123;Vmatrix&#125;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
