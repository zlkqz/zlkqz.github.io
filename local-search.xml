<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>深度学习基础总结</title>
    <link href="/2021/10/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/"/>
    <url>/2021/10/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="1-行业知识体系"><a href="#1-行业知识体系" class="headerlink" title="1 行业知识体系"></a>1 行业知识体系</h1><h3 id="1-1-机器学习算法"><a href="#1-1-机器学习算法" class="headerlink" title="1.1 机器学习算法"></a>1.1 机器学习算法</h3><p><img src="https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg" alt="qq_pic_merged_1635482805011"></p><h3 id="1-2-机器学习分类"><a href="#1-2-机器学习分类" class="headerlink" title="1.2 机器学习分类"></a>1.2 机器学习分类</h3><p><img src="https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg" alt="qq_pic_merged_1635482964705"></p><h3 id="1-3-问题领域"><a href="#1-3-问题领域" class="headerlink" title="1.3 问题领域"></a>1.3 问题领域</h3><script type="math/tex; mode=display">\begin{cases}\pmb{语言识别} \\\pmb{字符识别} \\\pmb{计算机视觉}（CV） \\\pmb{自然语言处理}（NLP）\\\pmb{知识推理}\\\pmb{自动控制}\\\pmb{游戏理论和人机对弈}\\\pmb{数据挖掘}\end{cases}</script><h1 id="2-线性回归"><a href="#2-线性回归" class="headerlink" title="2 线性回归"></a>2 线性回归</h1><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><img src="https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png" alt="image-20211029131504886"></p><ul><li><p>线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</p></li><li><p>线性回归输出是⼀个连续值，因此适⽤于回归问题</p></li></ul><h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><ul><li>均方差（mse）：</li></ul><script type="math/tex; mode=display">\ell^{(i)}(W, b) = \frac{(\hat{y}^{(i)} - y^{(i)})^2}{2}</script><p>其中$\hat{y}$为计算出的预测值，$y$为真实值（label）</p><h1 id="3-全连接层（稠密层）"><a href="#3-全连接层（稠密层）" class="headerlink" title="3  全连接层（稠密层）"></a>3  全连接层（稠密层）</h1><ul><li>输出层中的神经元和输⼊ 层中各个输⼊完全连接</li><li>计算完全依赖于输入层。</li></ul><h1 id="4-小批量随机梯度下降"><a href="#4-小批量随机梯度下降" class="headerlink" title="4  小批量随机梯度下降"></a>4  小批量随机梯度下降</h1><ul><li><p>在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\eta$）作为模型参数在本次迭代的减小量。</p></li><li><p>每次mini-batch梯度下降计算过程：</p></li></ul><script type="math/tex; mode=display">w_1 = w_1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial{\ell(...)}}{\partial{w_1}}</script><ul><li>注意学习率要取正数</li></ul><h1 id="5-Softmax回归"><a href="#5-Softmax回归" class="headerlink" title="5  Softmax回归"></a>5  Softmax回归</h1><h3 id="5-1-基本概念"><a href="#5-1-基本概念" class="headerlink" title="5.1 基本概念"></a>5.1 基本概念</h3><ul><li>softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数</li><li>由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为置信度（也就是为该类的概率），具体计算如下：</li></ul><p>假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:</p><script type="math/tex; mode=display">\hat{y}_1, \hat{y}_2, \hat{y}_3 = softmax(o_1, o_2, o_3)</script><p>其中</p><script type="math/tex; mode=display">\hat{y}_i = \frac{e^{o_i}}{\sum_{j = 1}^{3}e^{o_j}}</script><ul><li>容易看出$\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）</li><li>通常，我 们把预测概率最⼤的类别作为输出类别</li></ul><h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 损失函数</h3><ul><li>mse过于严格，因为并不需要所有的输出都和真实值相近</li><li>这里我们运用交叉熵损失函数（cross entropy）：</li></ul><script type="math/tex; mode=display">H(y^{(i)}, \hat{y}^{(i)}) = -\sum_{j = 1}^qy_j^{(i)}\log{\hat{y}^{(i)}}</script><p>q为输出元个数$\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值</p><h1 id="6-多层感知机（MLP）"><a href="#6-多层感知机（MLP）" class="headerlink" title="6 多层感知机（MLP）"></a>6 多层感知机（MLP）</h1><ul><li>多层感知机有一到多个隐藏层</li><li>多层感知机中的隐藏层和输出层都是全连接层</li><li>多层感知机具有激活函数，下面介绍常用激活函数</li></ul><h1 id="7-激活函数"><a href="#7-激活函数" class="headerlink" title="7 激活函数"></a>7 激活函数</h1><h3 id="7-1-Sigmoid函数"><a href="#7-1-Sigmoid函数" class="headerlink" title="7.1 Sigmoid函数"></a>7.1 Sigmoid函数</h3><script type="math/tex; mode=display">sigmoid(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">sigmoid'(x) = sigmoid(x)(1 - sigmoid(x))</script><p><img src="https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png" alt="image-20211030004640247"></p><ul><li>一般用在二分类的输出层中</li></ul><h3 id="7-2-tanh函数"><a href="#7-2-tanh函数" class="headerlink" title="7.2 tanh函数"></a>7.2 tanh函数</h3><script type="math/tex; mode=display">tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}</script><script type="math/tex; mode=display">tanh'(x) = 1 - tanh^2(x)</script><p><img src="https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png" alt="image-20211030005138300"></p><ul><li>tanh和sigmoid比较相似，但是性能一般要优于sigmoid</li></ul><h3 id="7-3-ReLU函数"><a href="#7-3-ReLU函数" class="headerlink" title="7.3 ReLU函数"></a>7.3 ReLU函数</h3><script type="math/tex; mode=display">ReLU(x) = max(0, x)</script><p><img src="https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png" alt="image-20211030005121490"></p><ul><li>十分常用</li></ul><h3 id="7-4-Leaky-ReLU函数"><a href="#7-4-Leaky-ReLU函数" class="headerlink" title="7.4 Leaky ReLU函数"></a>7.4 Leaky ReLU函数</h3><script type="math/tex; mode=display">Leaky\_Relu(x) = max(0.1 * x, x)</script><p><img src="https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png" alt="image-20211030005818507"></p><h1 id="8-训练误差和泛化误差"><a href="#8-训练误差和泛化误差" class="headerlink" title="8 训练误差和泛化误差"></a>8 训练误差和泛化误差</h1><h3 id="8-1-基本概念"><a href="#8-1-基本概念" class="headerlink" title="8.1 基本概念"></a>8.1 基本概念</h3><ul><li>训练误差：指模型在训练数据集上表现出的误差</li><li><p>泛化误差：指模型在任意⼀个测试数据样本上表 现出的误差的期望</p></li><li><p>我们通常假设训练数据集（训练题）和测试数据集（测试题）⾥的每⼀个样本都 是从同⼀个概率分布中相互独⽴地⽣成的</p></li></ul><h3 id="8-2-验证数据集"><a href="#8-2-验证数据集" class="headerlink" title="8.2 验证数据集"></a>8.2 验证数据集</h3><ul><li>在机器学习中，通常需要评估若⼲候选模型的表现并从中选择模型。这⼀过程称为模型选择 （model selection）。可供选择的候选模型可以是有着不同超参数的同类模型。</li><li>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使⽤⼀次。不可以使⽤测试数据选择模型，如调参。由于⽆法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留⼀部分在训练数据集和测试数据集以外的数据来进⾏模型选择。</li><li>我们可以从给定的训练集中随机选取⼀ 小部分作为验证集，而将剩余部分作为真正的训练集</li><li>对于总样本数量比较小的情况，一般训练集：验证集：测试集 = 6：2：1；如果数据量比较大，如上百万，那么训练集：验证集：测试集 = 98：1：1</li></ul><h3 id="8-3-k折交叉验证"><a href="#8-3-k折交叉验证" class="headerlink" title="8.3 k折交叉验证"></a>8.3 k折交叉验证</h3><ul><li>当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法</li><li>即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个⼦数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。</li></ul><h3 id="8-4-欠拟合和过拟合"><a href="#8-4-欠拟合和过拟合" class="headerlink" title="8.4 欠拟合和过拟合"></a>8.4 欠拟合和过拟合</h3><ul><li>欠拟合：模型⽆法得到较低的训练误差</li><li><p>过拟合：是模型的训练误差远小于它在测试数据集上 的误差</p></li><li><p>如果模型的复杂度过低，很容易出现⽋拟合；如果模型复杂度过⾼，很容易出现过拟合</p></li><li><p>⼀般来说，如果训练数据集中样本 数过少，特别是⽐模型参数数量（按元素计）更少时，过拟合更容易发⽣。</p></li><li>泛化误差不会 随训练数据集⾥样本数量增加而增⼤</li></ul><h3 id="8-5-正则化"><a href="#8-5-正则化" class="headerlink" title="8.5 正则化"></a>8.5 正则化</h3><ul><li>正则化通过为模型损失函数添加惩罚项使学出 的模型参数值较小</li></ul><h4 id="8-5-1-L2正则化（权重衰减）"><a href="#8-5-1-L2正则化（权重衰减）" class="headerlink" title="8.5.1 L2正则化（权重衰减）"></a>8.5.1 L2正则化（权重衰减）</h4><ul><li>L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积</li><li>损失函数添加一个L2惩罚项：</li></ul><script type="math/tex; mode=display">\ell(...) + \frac{\lambda}{2}\begin{Vmatrix} W \end{Vmatrix}^2</script><p>求导后：（每个mini-batch）</p><script type="math/tex; mode=display">w1 = (1 - \eta\lambda)w1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial\ell^{(i)}(...)}{\partial{w1}}</script><p>其中超参数$\lambda$ &gt; 0。当权重参数均为0时，惩罚项最小。当$\lambda$较⼤时，惩罚项在损失函数中的⽐重较⼤，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作⽤。</p><ul><li>一般训练集中的损失函数要加惩罚项，测试集中不用</li></ul><h4 id="8-5-2-丢弃法（Dropout）"><a href="#8-5-2-丢弃法（Dropout）" class="headerlink" title="8.5.2 丢弃法（Dropout）"></a>8.5.2 丢弃法（Dropout）</h4><ul><li>对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：</li></ul><script type="math/tex; mode=display">h_i' = \frac{\xi_i}{1 - p}h_i</script><p>$\xi_i$为0和1的概率分别为p和1 - p</p><p>分母中的1- p是为了不改变其输⼊的期望值：</p><script type="math/tex; mode=display">由于E(\xi_i) = 1 - p \\所以E(h_i') = \frac{E(\xi_i)}{1 - p}h_i = h_i</script><ul><li><p>测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法</p></li><li><p>进行了Dropout的多层感知机：</p></li></ul><p><img src="https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png" alt="image-20211030141251677"></p><p>可以看到隐藏层中的$h_2$和$h_5$消失了</p><ul><li>每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点</li></ul><h1 id="9-正向传播和反向传播"><a href="#9-正向传播和反向传播" class="headerlink" title="9 正向传播和反向传播"></a>9 正向传播和反向传播</h1><h3 id="9-1-正向传播"><a href="#9-1-正向传播" class="headerlink" title="9.1 正向传播"></a>9.1 正向传播</h3><ul><li>正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）</li><li>中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等</li><li>正向传播的计算图：</li></ul><p><img src="https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png" alt="image-20211030142019713"></p><p>其中：</p><ol><li>左下⻆是输⼊，右上⻆是输出</li><li>⽅框代表变量，圆圈代表运算符</li><li>$J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项</li></ol><h3 id="9-2-反向传播"><a href="#9-2-反向传播" class="headerlink" title="9.2 反向传播"></a>9.2 反向传播</h3><ul><li>反向传播最重要的是通过链式法则求导，从后往前进行计算</li><li>反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传 播计算得到的</li></ul><h3 id="9-3-衰减和爆炸"><a href="#9-3-衰减和爆炸" class="headerlink" title="9.3 衰减和爆炸"></a>9.3 衰减和爆炸</h3><ul><li>如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：</li></ul><p>假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积</p><h3 id="9-4-随机初始化参数"><a href="#9-4-随机初始化参数" class="headerlink" title="9.4 随机初始化参数"></a>9.4 随机初始化参数</h3><ul><li>例如：</li></ul><p>假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。</p><ul><li><strong>在这种情况下，⽆论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作⽤</strong></li></ul><h3 id="9-5-Xavier随机初始化"><a href="#9-5-Xavier随机初始化" class="headerlink" title="9.5 Xavier随机初始化"></a>9.5 Xavier随机初始化</h3><ul><li>假设某全连接层的输⼊个数为a， 输出个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：</li></ul><script type="math/tex; mode=display">U(-\sqrt{\frac{6}{a + b}}, \sqrt{\frac{6}{a + b}})</script><ul><li>该设计主要为了每层输出的⽅差不该受该层输⼊个数影响，且每层梯度的⽅差也不该受该层输出个数影响</li></ul><h1 id="10-Block类"><a href="#10-Block类" class="headerlink" title="10 Block类"></a>10 Block类</h1><ul><li>Block类是⼀个模型构造类，我们可以继承它来定义我们想要的模型。</li><li>它的⼦类既可以是⼀个层（如Dense类），⼜可以是深度学习计算的⼀个模型，或者是模型的⼀个部分</li></ul><h1 id="11-模型参数的延后初始化"><a href="#11-模型参数的延后初始化" class="headerlink" title="11 模型参数的延后初始化"></a>11 模型参数的延后初始化</h1><h3 id="11-1-基本概念"><a href="#11-1-基本概念" class="headerlink" title="11.1 基本概念"></a>11.1 基本概念</h3><ul><li>如果我们在创建层时没有指定输入输出的个数，那么和该层相邻的参数的维度一开始我们是不知道的，只有我们将最开始的输入值输进去，这时系统才能推断出参数的维度，才能开始参数初始化。系统将这种真正的参数初始化延后到获得⾜够信息时才执⾏的⾏为叫作延后初始化（deferred initialization）</li><li>这个初始化只会在第⼀次前向计算时被调⽤</li></ul><h3 id="11-2-避免延后初始化"><a href="#11-2-避免延后初始化" class="headerlink" title="11.2 避免延后初始化"></a>11.2 避免延后初始化</h3><ul><li>可以额外做⼀次前向计算来迫使参数被真正地初始化</li><li>也可以在创建层的时候指定了它的输⼊个数</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Markdown &amp; Letax常用语法</title>
    <link href="/2021/10/05/%E8%AF%AD%E6%B3%95/"/>
    <url>/2021/10/05/%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="1、标题"><a href="#1、标题" class="headerlink" title="1、标题"></a>1、标题</h1><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>~<span class="hljs-number">6</span>级标题<br><span class="hljs-attribute">1</span>~<span class="hljs-number">6</span>个#加个空格<br></code></pre></td></tr></table></figure><h1 id="2、代码块"><a href="#2、代码块" class="headerlink" title="2、代码块"></a>2、代码块</h1><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">``` + 代码语言<br></code></pre></td></tr></table></figure><h1 id="3、字体"><a href="#3、字体" class="headerlink" title="3、字体"></a>3、字体</h1><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">加粗：<span class="hljs-strong">**文本**</span><br>删除线:~~文本~~<br>斜体：<span class="hljs-strong">*文本*</span><br></code></pre></td></tr></table></figure><p>加粗：<strong>文本</strong><br>删除线：<del>文本</del><br>斜体：<em>文本</em></p><h1 id="4、引用"><a href="#4、引用" class="headerlink" title="4、引用"></a>4、引用</h1><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css">&gt;<span class="hljs-selector-tag">A</span><br>&gt;&gt;<span class="hljs-selector-tag">A</span><br>&gt;&gt;&gt;<span class="hljs-selector-tag">A</span><br></code></pre></td></tr></table></figure><blockquote><p>A</p><blockquote><p>A</p><blockquote><p>A</p></blockquote></blockquote></blockquote><h1 id="5、分割线"><a href="#5、分割线" class="headerlink" title="5、分割线"></a>5、分割线</h1><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><hr><h1 id="6、图片插入"><a href="#6、图片插入" class="headerlink" title="6、图片插入"></a>6、图片插入</h1><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs less">!<span class="hljs-selector-attr">[名字]</span>(本地或网页链接)<br></code></pre></td></tr></table></figure><p><img src="" alt="图片"></p><h1 id="7、超链接"><a href="#7、超链接" class="headerlink" title="7、超链接"></a>7、超链接</h1><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clojure">[名字](链接)<br></code></pre></td></tr></table></figure><p><a href="www.baidu.com">百度</a></p><h1 id="8、列表"><a href="#8、列表" class="headerlink" title="8、列表"></a>8、列表</h1><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gcode">无序列表<br>- <span class="hljs-comment">(空格)</span> 内容<br>有序列表<br>数字.<span class="hljs-comment">(空格)</span>内容<br></code></pre></td></tr></table></figure><ul><li>无序</li></ul><ol><li>有序</li></ol><h1 id="9、表格"><a href="#9、表格" class="headerlink" title="9、表格"></a>9、表格</h1><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">干脆直接快捷键<br></code></pre></td></tr></table></figure><h1 id="10、Latex"><a href="#10、Latex" class="headerlink" title="10、Latex"></a>10、Latex</h1><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><div class="table-container"><table><thead><tr><th>Latex</th><th>对应形式</th><th>Latex</th><th>对应形式</th></tr></thead><tbody><tr><td>\alpha</td><td>$\alpha$</td><td>\Alpha</td><td>$\Alpha$</td></tr><tr><td>\beta</td><td>$\beta$</td><td>\Beta</td><td>$\Beta$</td></tr><tr><td>\gamma</td><td>$\gamma$</td><td>\Gamma</td><td>$\Gamma$</td></tr><tr><td>\delta</td><td>$\delta$</td><td>\Delta</td><td>$\Delta$</td></tr><tr><td>\epsilon</td><td>$\epsilon$</td><td>\Epsilon</td><td>$\Epsilon$</td></tr><tr><td>\zeta</td><td>$\zeta$</td><td>\Zeta</td><td>$\Zeta$</td></tr><tr><td>\eta</td><td>$\eta$</td><td>\Eta</td><td>$\Eta$</td></tr><tr><td>\theta</td><td>$\theta$</td><td>\Theta</td><td>$\Theta$</td></tr><tr><td>\lambda</td><td>$\lambda$</td><td>\Lambda</td><td>$\Lambda$</td></tr><tr><td>\mu</td><td>$\mu$</td><td>\Mu</td><td>$\Mu$</td></tr><tr><td>\nu</td><td>$\nu$</td><td>\Nu</td><td>$\Mu$</td></tr><tr><td>\xi</td><td>$\xi$</td><td>\Xi</td><td>$\Xi$</td></tr><tr><td>\pi</td><td>$\pi$</td><td>\Pi</td><td>$\Pi$</td></tr><tr><td>\rho</td><td>$\rho$</td><td>\Rho</td><td>$\Rho$</td></tr><tr><td>\sigma</td><td>$\sigma$</td><td>\Sigma</td><td>$\Sigma$</td></tr><tr><td>\varphi</td><td>$\varphi$</td><td>\Phi</td><td>$\Phi$</td></tr><tr><td>\chi</td><td>$\chi$</td><td>\Chi</td><td>$\Chi$</td></tr><tr><td>\psi</td><td>$\psi$</td><td>\Psi</td><td>$\Psi$</td></tr><tr><td>\omega</td><td>$\omega$</td><td>\Omega</td><td>$\Omega$</td></tr><tr><td>\ell</td><td>$\ell$</td><td>\varepsilon</td><td>$\varepsilon$</td></tr></tbody></table></div><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><div class="table-container"><table><thead><tr><th>解释</th><th>代码</th></tr></thead><tbody><tr><td>粗体</td><td>\pmb{…..}</td></tr><tr><td>单空格</td><td>\quad</td></tr><tr><td>双空格</td><td>\qquad</td></tr><tr><td>$\times$</td><td>\times</td></tr><tr><td>$\div$</td><td>\div</td></tr><tr><td>下标</td><td>_</td></tr><tr><td>上标</td><td>^</td></tr><tr><td>$\hat{a}$</td><td>\hat{a}</td></tr><tr><td>$\vec{a}$</td><td>\vec{a}</td></tr><tr><td>$\log_{32}{xy}$</td><td>\log_{32}{xy}</td></tr><tr><td>{ }</td><td>需要转义</td></tr><tr><td>$\sum_1^n$</td><td>\sum_1^n</td></tr><tr><td>$\prod_{k=1}^nk^2$</td><td>\prod_{k=1}^nk^2</td></tr><tr><td></td><td></td></tr><tr><td>$\int_a^b$</td><td>\int_a^b</td></tr><tr><td>$\iint$</td><td>\iint</td></tr><tr><td>$\iiint$</td><td>\iiint</td></tr><tr><td>$\infty$</td><td>\infty</td></tr><tr><td>$\lim_{x\to0}$</td><td>极限       \lim_{x\to0}</td></tr><tr><td>$f’(x)$</td><td>导数        f’(x)</td></tr><tr><td>$\int<em>{\theta=0}^{2\pi}    \int</em>{r=0}^R$</td><td>\int<em>{\theta=0}^{2\pi}    \int</em>{r=0}^R$</td></tr><tr><td>$\nabla$</td><td>\nabla</td></tr><tr><td>$\partial$</td><td>\partial</td></tr><tr><td></td><td></td></tr><tr><td>$\subset$</td><td>\subset</td></tr><tr><td>$\subseteq$</td><td>\subseteq</td></tr><tr><td>$\in$</td><td>\in</td></tr><tr><td>$\notin$</td><td>\notin</td></tr><tr><td>$\emptyset$</td><td>\emptyset</td></tr><tr><td>$\varnothing$</td><td>\varnothing</td></tr><tr><td>$\bigcup$</td><td>\bigcup</td></tr><tr><td>$\cup$</td><td>\cup</td></tr><tr><td>$\bigcap$</td><td>\bigcap</td></tr><tr><td>$\cap$</td><td>\cap</td></tr><tr><td></td><td></td></tr><tr><td>$\frac{a+1}{b+1}$</td><td>分数    \frac{a+1}{b+1}</td></tr><tr><td>$\sqrt{x^5}$</td><td>开方      \sqrt{x^5}</td></tr><tr><td>$\sqrt[3]{xy}$</td><td>开方       \sqrt[3]{xy}</td></tr><tr><td></td><td></td></tr><tr><td>$\le$</td><td>\le</td></tr><tr><td>$\geq$</td><td>\geq</td></tr><tr><td>$\neq$</td><td>\neq</td></tr><tr><td>$\approx$</td><td>\approx</td></tr><tr><td>$\equiv$</td><td>\equiv</td></tr><tr><td>$\pm$</td><td>\pm</td></tr><tr><td>$\mp$</td><td>\mp</td></tr><tr><td>$\mathbb{R}$</td><td>将字母改为黑板字体\mathbb{R}</td></tr></tbody></table></div><h3 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h3><script type="math/tex; mode=display">f(x) = \begin{cases}1 & x = 2\\2 & x > 2\\3 & x \leqslant 2\\\end{cases}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs livescript">$$<br>f(x) = <span class="hljs-string">\begin&#123;cases&#125;</span><br><span class="hljs-number">1</span> &amp; x = <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">2</span> &amp; x &gt; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; x <span class="hljs-string">\leqslant</span> <span class="hljs-number">2</span><span class="hljs-string">\\</span><br><span class="hljs-string">\end&#123;cases&#125;</span><br>$$<br></code></pre></td></tr></table></figure><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><script type="math/tex; mode=display">\begin{matrix}1 & 2 \\3 & 4 \end{matrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;matrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;matrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{pmatrix}1 & 2 \\3 & 4 \end{pmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;pmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;pmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;bmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;bmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Bmatrix}1 & 2 \\3 & 4 \end{Bmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;Bmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;Bmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{vmatrix}1 & 2 \\3 & 4 \end{vmatrix}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-string">\begin&#123;vmatrix&#125;</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br><span class="hljs-string">\end&#123;vmatrix&#125;</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Vmatrix}1 & 2 \\3 & 4 \end{Vmatrix}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-string">\begin&#123;Vmatrix&#125;</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br><span class="hljs-string">\end&#123;Vmatrix&#125;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
