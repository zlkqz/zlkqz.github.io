<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>BERT总结</title>
    <link href="/2022/07/02/Bert%E6%80%BB%E7%BB%93/"/>
    <url>/2022/07/02/Bert%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<ul><li><p>BERT是一种通过<strong>在预训练时使用无监督</strong>方法能在<strong>每一层</strong>实现<strong>双向</strong>表征的语言模型，并且使用微调的方法，在<strong>具体的下游任务时不需要task-specific architecture</strong>，只需要添加一两层和少部分参数，十分易于迁移</p></li><li><p>BERT带来的主要提升是解决了双向性的问题。如OpenAI GPT使用的是left-to-right（LTR）Transformer结构，失去了双向性。又比如ELMo使用简单的将left-to-right（LTR）的LSTM和right-to-left（RTL）的LSTM在最后简单的连结来实现双向性，而BERT能在每一层都实现双向，并且相比于在最后简单的连结更具有直观性和可解释性。</p></li></ul><h1 id="1-BERT的结构"><a href="#1-BERT的结构" class="headerlink" title="1 BERT的结构"></a>1 BERT的结构</h1><h3 id="1-1-结构和规模"><a href="#1-1-结构和规模" class="headerlink" title="1.1 结构和规模"></a>1.1 结构和规模</h3><ul><li>BERT的结构十分简单，就是由<strong>多个Transformer的encoder组合而成</strong></li><li><p>我们将encoder的数量设为L，隐藏层的单元数设为H，自注意力头的个数设为A，则BERT可分为<script type="math/tex">BERT_{BASE}</script>（L=12，H=768，A=12，总参数量=110M  ）和<script type="math/tex">BERT_{LARGE}</script>（L=24，H=1024，A=16，总参数量=340M）两个版本</p></li><li><p><script type="math/tex">BERT_{LARGE}</script>在几乎所有的任务上都是优于<script type="math/tex">BERT_{BASE}</script>的，特别是特别小的数据集上</p></li></ul><h3 id="1-2-BERT的输入输出"><a href="#1-2-BERT的输入输出" class="headerlink" title="1.2 BERT的输入输出"></a>1.2 BERT的输入输出</h3><ul><li><p>BERT使用WordPiece embeddings  </p></li><li><p>BERT的<strong>输入可以是一个句子也可以是两个句子</strong>，每个输入的<strong>最开始都需要加一个[CLS] token</strong>，如果输入包含两个句子（sentence A and sentence B），则<strong>中间需要加入一个[SEP] token来做分隔</strong></p></li><li><strong>总的输入为</strong>：对应的token embedding+segment embedding+position embedding的总和：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701114959234.png" alt="image-20220701114959234"></p><p>其中segment embedding是用以区分sentence A和sentence B（<strong>第一个句子的segment embedding都是0，第二个的都是1</strong>），而position embedding和Transformer中的不一样，Transformer是采用三角函数，而<strong>BERT采用learned position embedding</strong></p><ul><li>输入输出的形式大致如下：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701115457939.png" alt="image-20220701115457939"></p><p><strong>其中C为[CLS]对应的最终的embedding，在分类任务时作为整个序列的总表征（但是C在微调之前是没有具体意义的向量，因为他是通过NSP预训练出来的）</strong>。<script type="math/tex">T_i</script>为第<script type="math/tex">i</script>个token所对应的embedding</p><h1 id="2-BERT的Pre-training"><a href="#2-BERT的Pre-training" class="headerlink" title="2 BERT的Pre-training"></a>2 BERT的Pre-training</h1><ul><li>BERT的Pre-training可分为MLM和NSP，分别对应token级的任务和sentence级的任务</li><li>Pre-training采用的是<strong>无监督的方法</strong></li><li>在Pre-training数据的选择上，使用document-level corpus要优于shuffled sentence-level corpus</li></ul><h3 id="2-1-Masked-Language-Model（MLM）"><a href="#2-1-Masked-Language-Model（MLM）" class="headerlink" title="2.1 Masked Language Model（MLM）"></a>2.1 Masked Language Model（MLM）</h3><h4 id="2-1-1-MLM的输入"><a href="#2-1-1-MLM的输入" class="headerlink" title="2.1.1 MLM的输入"></a>2.1.1 MLM的输入</h4><ul><li>每个输入的sequence会<strong>随机mask掉15%的token</strong>，并且在最后预测mask掉的地方是什么词（通过将该token最后对应的embedding送入softmax层并采用交叉熵损失，分类个数为整个词典的token数）</li><li><strong>其中mask的策略为</strong>，对于一个要mask的token：</li></ul><ol><li>80%的概率变为[MASK]</li><li>10%的概率变为随机词</li><li>10%的概率不变</li></ol><p>举个栗子：</p><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701122513870.png" alt="image-20220701122513870" style="zoom:80%;" /></p><h4 id="2-2-采用此mask策略的原因"><a href="#2-2-采用此mask策略的原因" class="headerlink" title="2.2 采用此mask策略的原因"></a>2.2 采用此mask策略的原因</h4><ul><li>预训练时是有[MASK]的，但是微调时是没有的，那么<strong>微调时模型就只能根据其他token的信息和语序结构来预测当前词，而无法利用到这个词本身的信息</strong>（因为它们从未出现在训练过程中，等于模型从未接触到它们的信息，等于整个语义空间损失了部分信息），所以会<strong>产生预训练和微调的mismatch</strong></li><li>而保留下来的信息<strong>如果全部使用原始token，那么模型在预训练的时候可能会偷懒，直接照抄当前的token</strong>，所以需要随机换成其他词，会让模型不能去死记硬背当前的token，而去<strong>尽力学习单词周边的语义表达和远距离的信息依赖</strong>，尝试建模完整的语言信息</li><li>但是随机替换不能太多，要不然肯定会对模型产生误导，以下是经过多次实验的数据：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123511287.png" alt="image-20220701123511287"></p><p>可以看到只使用随即替换，会对结果产生极大的影响</p><h4 id="2-3-MLM的问题"><a href="#2-3-MLM的问题" class="headerlink" title="2.3 MLM的问题"></a>2.3 MLM的问题</h4><ul><li>由于MLM每次只mask掉15%的词，所以只预测15%的词，所以需要更多的steps才能收敛，以下是MLM和LTR模型的对比：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123837335.png" alt="image-20220701123837335"></p><p>可以看到MLM收敛速度更慢，需要更多的steps，但是所获得的改进远大于所增加的成本，所以问题不大</p><h3 id="2-2-Next-Sentence-Predictoin（NSP）"><a href="#2-2-Next-Sentence-Predictoin（NSP）" class="headerlink" title="2.2 Next Sentence Predictoin（NSP）"></a>2.2 Next Sentence Predictoin（NSP）</h3><ul><li>NSP的输入为两个句子，有50%的概率sentence B是sentence A的下一句，有50%的概率不是，举个栗子：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701124711635.png" alt="image-20220701124711635" style="zoom:90%;" /></p><ul><li>在最后使用C向量送入下一层，判断为IsNext or NotNext</li></ul><h3 id="2-3-Pre-training的细节"><a href="#2-3-Pre-training的细节" class="headerlink" title="2.3 Pre-training的细节"></a>2.3 Pre-training的细节</h3><ul><li><p><strong>优化：</strong></p><blockquote><ol><li>Adam（learning rate = <script type="math/tex">10^{-4}</script>，<script type="math/tex">\beta_1 = 0.9</script>， <script type="math/tex">\beta_2 = 0.999</script>）</li><li>learning rate在前10000个steps采用warmup，并且还应用了线性衰减</li><li>0.01的L2权重衰减和0.1的Dropout</li><li>batch size = 256 sequences / batch</li></ol></blockquote></li><li><p>激活函数采用gelu而非relu</p></li><li><p>损失为MLM的最大似然和NSP的最大似然的和</p></li><li><p>由于attention是随序列长度进行平方增长的，所以为了提高预训练速度，在实验时，<strong>先在90%的steps应用应用128的序列长，然后在剩下的10%的steps中改为512序列长度，来学习position embedding</strong></p></li></ul><h1 id="3-BERT的Fine-tuning"><a href="#3-BERT的Fine-tuning" class="headerlink" title="3 BERT的Fine-tuning"></a>3 BERT的Fine-tuning</h1><h3 id="3-1-Fine-tuning的一般做法"><a href="#3-1-Fine-tuning的一般做法" class="headerlink" title="3.1 Fine-tuning的一般做法"></a>3.1 Fine-tuning的一般做法</h3><ul><li>都是在最后加上一两层，来进行微调。对于Transformer Encoder的输出：</li></ul><ol><li>如果是token级的下游任务，如sequence tagging和question answering，是直接将对应的token输出的embedding送入下一层。</li><li>如果是sentence级的下游任务，如sentiment analysis，需要将[CLS]对应的输出，也就是C，送入下一层用以分类</li></ol><h3 id="3-2-Fine-tuning的细节"><a href="#3-2-Fine-tuning的细节" class="headerlink" title="3.2 Fine-tuning的细节"></a>3.2 Fine-tuning的细节</h3><ul><li>大多数超参数和pre-training时是一样的，除了batch size、learning rate和epochs</li><li>dropout的概率还是保持为0.1</li><li>在实验中发现，以下几个超参的选择，适用于大多数的任务：</li></ul><blockquote><p><strong>Batch size：16， 32</strong></p><p><strong>Learning rate (Adam)：<script type="math/tex">5 \times 10^{-5}, 3 \times 10^{-5}, 2 \times 10^{-5}</script>  </strong></p><p><strong>Number of epochs：2，3，4  </strong></p></blockquote><ul><li>并且还发现大数据集相比小数据集对于超参的选择是不那么敏感的</li></ul><h1 id="4-BERT实践"><a href="#4-BERT实践" class="headerlink" title="4 BERT实践"></a>4 BERT实践</h1><ul><li>下面介绍BERT在各种下游任务上的表现：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701233158369.png" alt="image-20220701233158369" style="zoom:70%;" /></p><h3 id="4-1-GLUE"><a href="#4-1-GLUE" class="headerlink" title="4.1 GLUE"></a>4.1 GLUE</h3><ul><li>GLUE全称为The General Language Understanding Evaluation，包含了各种而样的自然语言理解任务</li><li>在BERT中我们只添加了一个多分类输出层，将[CLS]对用的输出C，送入该层，再使用softmax，计算损失</li><li><p>采用的超参：batch size = 32，epochs = 3，learning rate =  <script type="math/tex">5 \times 10^{-5}，4 \times 10^{-5}，3 \times 10^{-5}，2 \times 10^{-5}</script></p></li><li><p>在微调时，<strong><script type="math/tex">BERT_{LARGE}</script>在小数据集上的结果是不稳定的</strong>，所以采取了<strong>多次随机重启</strong>（不一样的数据重洗和分类层的参数初始化），并且选择了在验证集上结果最好的模型</p></li></ul><h3 id="4-2-SQuAD"><a href="#4-2-SQuAD" class="headerlink" title="4.2 SQuAD"></a>4.2 SQuAD</h3><ul><li>SQuAD全称The Stanford Question Answering Dataset，收录了100k的QA对，其中每个Query的Answer是在对应的Passage中的一段连续文本（answer span）</li></ul><h4 id="4-2-1-SQuAD-v1-1"><a href="#4-2-1-SQuAD-v1-1" class="headerlink" title="4.2.1 SQuAD v1.1"></a>4.2.1 SQuAD v1.1</h4><ul><li>首先设<script type="math/tex">S \in R^H</script>和<script type="math/tex">E \in R^H</script>分别为answer span中的第一个词和最后一个词的embedding</li><li>那么<script type="math/tex">word_i</script>作为第一个词的概率，可以使用点积+softmax的求得：（其中<script type="math/tex">T_i</script>是<script type="math/tex">word_i</script>对应的output）</li></ul><script type="math/tex; mode=display">P_{i}=\frac{e^{S \cdot T_{i}}}{\sum_{j} e^{S \cdot T_{j}}}</script><p>将<script type="math/tex">word_i</script>作为最后一个词的概率也是一样的算法，只是把S替换成E</p><ul><li><p>在训练时的损失为正确的开始和结束位置的最大似然</p></li><li><p>在预测时，每个候选位置，即将<script type="math/tex">word_i</script>到<script type="math/tex">word_j</script>作为answer的score为：</p></li></ul><script type="math/tex; mode=display">S \cdot T_i + E \cdot T_i \quad (j \geq i)</script><p>然后取最大score的侯选位置作为输出</p><ul><li><p>超参：batch size = 32，epochs = 3，learning rate = <script type="math/tex">5 \times 10^{-5}</script></p></li><li><p>在具体实验中，应用于SQuAD数据集上前，先在TriviaQA上微调，进行适当的数据增强</p></li></ul><h4 id="4-2-2-SQuAD-v2-0"><a href="#4-2-2-SQuAD-v2-0" class="headerlink" title="4.2.2 SQuAD v2.0"></a>4.2.2 SQuAD v2.0</h4><ul><li>SQuAD v2.0相对于SQuAD v1.1增加了一个No Answer的输出，因为一个问题的答案并不总是出现在passage中的，No Answer的的具体形式为start和end都是[CLS]的answer span，预测为No Answer的score为：</li></ul><script type="math/tex; mode=display">s_{null} = S \cdot C + E \cdot C</script><p>当满足下式时，则不预测为No Answer：</p><script type="math/tex; mode=display">\hat{s_{i, j}}>s_{\mathrm{null}}+\tau</script><p>其中<script type="math/tex">\hat{s_{i, j}}=\max _{j \geq i} S \cdot T_{i}+E \cdot T_{j}</script>，而<script type="math/tex">\tau</script>是通过实验所得，使在验证集上获得最大的F1</p><ul><li>在本次实验中并未使用TriviaQA data set</li><li>超参：batch size = 48，epochs = 2，learning rate = <script type="math/tex">5 \times 10^{-5}</script></li></ul><h3 id="4-3-SWAG"><a href="#4-3-SWAG" class="headerlink" title="4.3 SWAG"></a>4.3 SWAG</h3><ul><li>全称The Situations With Adversarial Generations，用于常识推断任务，具体任务是给定一个sentence，然后需要在4个选择中选出最合适的答案</li><li>任务可建模为：每次有4个输入序列，每个输出是给定的sentence+4个可能的选择之一，最后得到C向量，再加一层全连接层，用sotfmax计算概率</li><li>超参：batch size = 16，epochs = 3，learning rate = <script type="math/tex">2 \times 10^{-5}</script></li></ul><h1 id="5-BERT和其他模型的对比"><a href="#5-BERT和其他模型的对比" class="headerlink" title="5 BERT和其他模型的对比"></a>5 BERT和其他模型的对比</h1><ul><li>实验进行了ELMo，OpenAI GPT和BERT之间的对比</li><li>首先介绍大致做法和结构：</li></ul><blockquote><ol><li>BERT使用双向Transformer，OpenAI GPT使用LTR Transformer，而ELMo使用LTR和RTL的LSTM在最后的简单连结</li><li>BERT和OpenAI GPT使用fine-tuning approaches，而ELMo使用feature-based approach</li></ol><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220702121317344.png" alt="image-20220702121317344" style="zoom:80%;" /></p></blockquote><ul><li>另外，ELMo使用在最后将LTR和TRL简单的连结，有以下缺点：</li></ul><blockquote><ol><li>两倍的工作量</li><li>对于有些任务是不直观的，如QA</li><li>BERT在每层都可以实现双向，而ELMo只会在最后连结</li></ol></blockquote><h1 id="6-BERT的变体"><a href="#6-BERT的变体" class="headerlink" title="6 BERT的变体"></a>6 BERT的变体</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Transformer总结</title>
    <link href="/2022/05/16/Transformer%E6%80%BB%E7%BB%93/"/>
    <url>/2022/05/16/Transformer%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<ul><li>Transformer摒弃了传统的CNN/RNN模型，而是用纯注意力机制， <strong>相对于RNN，实现了并行化，并且消除了memory对于距离的依赖性（无法掌握过长的距离）。</strong></li></ul><h1 id="1-注意力机制"><a href="#1-注意力机制" class="headerlink" title="1 注意力机制"></a>1 注意力机制</h1><ul><li><strong>注意力机制中分别有key、query、value，通过key、query之间的相似度，计算得到每个value对应的权值，再对所有value加权求和，得到一整个序列的表征。其中对于自己本身的注意力机制称为self-attention（自注意力机制），即key=value</strong></li></ul><h3 id="1-1-Scaled-Dot-Product-Attention-点积"><a href="#1-1-Scaled-Dot-Product-Attention-点积" class="headerlink" title="1.1 Scaled Dot-Product Attention(点积)"></a>1.1 Scaled Dot-Product Attention(点积)</h3><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112222285.png" alt="image-20220512112222285" style="zoom: 37%;" /></p><ul><li>在计算时，我们是将query、key、value（分别为<script type="math/tex">d_k, d_k, d_v</script>维）打包成Q，K<script type="math/tex">\in R^{N \times d_k}</script>，V<script type="math/tex">\in R^{N \times d_v}</script>，具体做法是：</li></ul><blockquote><p>将送进来的输入input<script type="math/tex">\in R^{N \times d_{model} }</script>（其中<script type="math/tex">d_{model}</script>为embedding的维度，且q、k、v三者的input可能各自不同），input分别乘<script type="math/tex">W^Q、W^K \in R^{d_{model} \times d_k}</script>，<script type="math/tex">W^V \in R^{ {d_{model} \times d_v} }</script>即可得到Q、K、V</p></blockquote><ul><li>在计算权值时，将Q、K相乘，再除以<script type="math/tex">\sqrt{d_k}</script>，再softmax得到权值。<strong>除以<script type="math/tex">\sqrt{d_k}</script>的原因</strong>：</li></ul><blockquote><p><strong>维度过大会使Q、K相乘的结果过大，容易把softmax的区域推向梯度极小的区域。并且实验证明在<script type="math/tex">d_k</script>较小时，其实除不除效果差不多</strong></p></blockquote><ul><li>得到权重后再和V相乘，总过程为：</li></ul><script type="math/tex; mode=display">\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T} }{\sqrt{d_{k} }}\right) V</script><ul><li>还有一种较常用的注意力机制叫Additive attention， 是使用一个单隐藏层的全连接网络计算权重，两者效果差不多，<strong>但是dot-product会快得多</strong></li></ul><h3 id="1-2-Multi-Head-Attention-多头注意力机制"><a href="#1-2-Multi-Head-Attention-多头注意力机制" class="headerlink" title="1.2 Multi-Head Attention(多头注意力机制)"></a>1.2 Multi-Head Attention(多头注意力机制)</h3><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112235945.png" alt="image-20220512112235945" style="zoom:35%;" /></p><ul><li>多头注意力机制本质上就是<strong>做多次Scaled Dot-Product Attention</strong></li><li>具体做法是：</li></ul><blockquote><p><strong>重复做h次Scaled Dot-Product Attention（每次的W权重矩阵分别独立），将每次得到的结果<script type="math/tex">Z \in R^{N \times d_v}</script>在第二维连结，形状变为<script type="math/tex">R^{N \times hd_v}</script>，再乘一个<script type="math/tex">W^O \in R^{hd_v \times d_{model} }</script>，即可得到形状为<script type="math/tex">R^{N \times d_{model} }</script>的最终结果</strong></p></blockquote><ul><li>总过程为：</li></ul><script type="math/tex; mode=display">\begin{aligned}\operatorname{MultiHead}(Q, K, V) &=\operatorname{Concat}\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h} }\right) W^{O} \\\text { where head } &=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)\end{aligned}</script><ul><li><p>在base模型中取的<script type="math/tex">h = 8</script>，且<script type="math/tex">d_k = d_v = d_{model}/h = 64</script></p></li><li><p><strong>多头注意力的好处：类似于CNN中的通道，能提取到不同子空间下的特征。多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。</strong>如果单纯使用单注意力头+平均化，会抑制这一点</p></li></ul><blockquote><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  </p></blockquote><h3 id="1-3-自注意力机制的好处"><a href="#1-3-自注意力机制的好处" class="headerlink" title="1.3 自注意力机制的好处"></a>1.3 自注意力机制的好处</h3><ul><li><p>自注意力机制最大的好处肯定是实现了<strong>并行化，加快了训练速度</strong>。并且得到的结果相比于其他方法（如全局平均池化），<strong>更具有解释性</strong>，self-attention是可以退化成平均的，所以结果肯定好于平均。</p></li><li><p>论文从每层的总计算复杂度、可并行化的计算数量（用顺序操作的最小量来衡量）、长距离依赖的距离三个方面进行了对比：</p></li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512114401561.png" alt="image-20220512114401561"></p><ul><li><strong>并且单个注意力头不仅清楚地学习执行不同的任务，而且许多似乎表现出与句子的句法和语义结构相关的行为</strong></li></ul><blockquote><p>Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic<br>and semantic structure of the sentences  </p></blockquote><h1 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2 模型结构"></a>2 模型结构</h1><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512121448150.png" alt="image-20220512121448150" style="zoom:28%;" /></p><ul><li>图中多头注意力层的输入从左到右依次为V、K、Q</li></ul><h3 id="2-1-Encoder和Decoder"><a href="#2-1-Encoder和Decoder" class="headerlink" title="2.1 Encoder和Decoder"></a>2.1 Encoder和Decoder</h3><ul><li>Encoder由N=6个相同的块组成，每个块有两个子层，一个多头注意力层，一个全连接层，子层输入输出都是都是<script type="math/tex">R^{N \times d_{model} }</script>，其中N为时间步个数，也就是词数。<strong>并且在每个子层都有一个残差结构+LayerNorm</strong>，先残差后LayerNorm：</li></ul><script type="math/tex; mode=display">LayerNorm(x + Sublayer(x))</script><ul><li>Decoder同样是由N=6个相同的块组成，每个块有3个子层，有两个和Encoder中一模一样。增加了一个带Mask的多头注意力层。<strong>Decoder最开始的输入在训练时和预测时不一样</strong>，在<strong>训练时是把所有的翻译结果一次性输入</strong>，并行化提高速度。而<strong>预测时是类似于RNN一样的串行方式</strong>，第一次给Decoder输入句子的开始符号，然后得到第一个翻译结果，再将第一个翻译结果当作输入送入Decoder。总结来说就是：<strong>每次Decoder的输入为之前所有时间步的结果</strong>。而在训练时，是一次导入所有结果，所以需要<strong>Mask掉未来时间步的翻译结果</strong>。</li></ul><h3 id="2-2-多头注意力层"><a href="#2-2-多头注意力层" class="headerlink" title="2.2 多头注意力层"></a>2.2 多头注意力层</h3><ul><li><p>进行的操作其实就是上文提到的多头注意力机制：<strong>将输入分别乘一个矩阵W，转换成Q、K、V，再计算权重并加权平均，得到Z。将上述过程进行h次，每次使用的是相互独立的W，再将Z连结，再乘一个权重矩阵，得到最终结果。</strong></p></li><li><p>需要注意的是Decoder中的Masked Multi-Head Attention。我们在<strong>预测时，肯定是无法知道未来的信息的（也就是之后时间步的输出），但是在训练时我们是将翻译结果一次性使用一个矩阵导入的</strong>。所以为了保持一致性，我们需要在<strong>训练时屏蔽掉未来的信息，即当前时间步t的输出只取决于t-1及其之前的时间步。</strong></p></li><li>下方为一个Attention Map，每个单元代表该行对四个列对应的权值。如第一行代表”I”分别对”I”、”have”、”a”、”dream”的权值。</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/64.jpg" alt="64" style="zoom:50%;" /></p><p>显然在通过”I”预测”have”时，是不知道后面的”have”、”a”、”dream”的，所以需要通过Mask屏蔽掉未来的信息，其他时间步的时候类似：</p><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/56.jpg" alt="56" style="zoom:50%;" /></p><p>上图就是经过Mask后的Attention Map，将每个时间步未来的信息进行了屏蔽，具体的做法是：<strong>在计算V的权重时，softmax之前将对应的值设为<script type="math/tex">-\infty</script></strong></p><h3 id="2-3-全连接层"><a href="#2-3-全连接层" class="headerlink" title="2.3 全连接层"></a>2.3 全连接层</h3><ul><li>每个全连接子层有两个层，进行的运算为：</li></ul><script type="math/tex; mode=display">\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><p>第二层是没有激活函数的</p><ul><li>层的输入为<script type="math/tex">d_{model} = 512</script>，经过第一层变为<script type="math/tex">d_{ff} = 2048</script>，经过第二层又变为512维。</li></ul><h3 id="2-4-Layer-Normalization"><a href="#2-4-Layer-Normalization" class="headerlink" title="2.4 Layer Normalization"></a>2.4 Layer Normalization</h3><ul><li><strong>BN是对一个batch-size样本内的每个特征做归一化，LN是对每个样本的所有特征做归一化</strong>：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512222449616.png" alt="image-20220512222449616" style="zoom:50%;" /></p><ul><li><strong>选用LN而弃用BN的原因：</strong>BN需要较大的batch-size来保证对于期望、方差的统计的可靠性，对于CNN来说则好统计。但是在天然变长的RNN中，如果选用BN需要<strong>对每个时序的状态进行统计</strong>，这会导致在偏长的序列中，<strong>靠后的时间步的统计量不足</strong>。相比之下使用LN则不会有这种限制。</li><li>对于 CNN 图像类任务，每个卷积核可以看做特定的特征抽取器，对其输出做统计是有理可循的；对于 RNN 序列类任务，<strong>统计特定时序每个隐层的输出，毫无道理可言——序列中的绝对位置并没有什么显著的相关性</strong>。相反，同一样本同一时序同一层内，不同神经元节点处理的是相同的输入，在它们的输出间做统计合理得多。</li></ul><h3 id="2-5-词嵌入"><a href="#2-5-词嵌入" class="headerlink" title="2.5 词嵌入"></a>2.5 词嵌入</h3><ul><li>Transformer中的embedding是训练出来的，所以总的结构类似于跳字模型或者连续词袋模型，具体可看<a href="https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#1-2-%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B%EF%BC%88skip-gram%EF%BC%89">跳字模型</a>中的具体实现，简单来说就是：一个单隐藏层的全连接网络，输入one-hot向量，乘一个V矩阵，得到隐藏层值，再乘一个U矩阵，得到输入层值，再softmax计算概率最后梯度下降。<strong>而Decoder的前后就是分别为乘V和乘U两个操作，分别称为embedding转换和pre-softmax linear transformation</strong></li><li>在一般的词嵌入模型当中，U、V矩阵一般是两个不同的矩阵，而Transformer中使用了<strong>Weight Tying</strong>，即U、V使用同一矩阵<strong>（注意只是共用权重矩阵，偏差还是相互独立的）</strong></li><li>one-hot向量和对U的操作是“指定抽取”，即取出某个单词的向量行；pre-softmax对V的操作是“逐个点积”，对隐层的输出，依次计算其和每个单词向量行的变换结果。虽然具体的操作不同，<strong>但在本质上，U和V都是对任一的单词进行向量化表示，然后按词表序stack起来。因此，两个权重矩阵在语义上是相通的</strong>。</li><li>也是由于上面两种操作方式的不同，<strong>U在反向传播中不如V训练得充分</strong>，将两者绑定在一起缓和了这一问题，可以训练得到质量更高的新矩阵。并且<strong>Weight Tying 可以显著减小模型的参数量</strong>。</li><li>在embdding层中，<strong>为了让embedding层的权重值不至于过小，乘以<script type="math/tex">\sqrt{d_{model} }</script>后与位置编码的值差不多，可以保护原有向量空间不被破坏</strong>。</li></ul><h3 id="2-6-Positional-Encode"><a href="#2-6-Positional-Encode" class="headerlink" title="2.6 Positional Encode"></a>2.6 Positional Encode</h3><ul><li>由于模型摒弃了RNN结构，所以<strong>无法获得序列的位置信息</strong>，而为了获得这种位置信息我们需要引入Positional Embedding来表示位置信息。</li><li>Positional Embedding的维度同样是<script type="math/tex">d_{model}</script>，并且在一开始的时候和Embedding进行相加，具体表示为：</li></ul><script type="math/tex; mode=display">\begin{aligned}P E_{(p o s, 2 i)} &=\sin \left(p o s / 10000^{2 i / d_{\text {model } }}\right) \\P E_{(p o s, 2 i+1)} &=\cos \left(p o s / 10000^{2 i / d_{\text {model } }}\right)\end{aligned}</script><p>其中pos代表第几个序列位置（最大值为规定的最长序列长度），i代表第几个维度（最大值为<script type="math/tex">d_{mdoel} / 2</script>）</p><ul><li>以上公式不仅能很好的表示单词的绝对位置，还能表示出相对位置：<strong>相隔 k 个词的两个位置 pos 和 pos+k 的位置编码是由 k 的位置编码定义的一个线性变换</strong>：</li></ul><script type="math/tex; mode=display">\begin{array}{c}P E(p o s+k, 2 i)=P E(p o s, 2 i) P E(k, 2 i+1)+P E(p o s, 2 i+1) P E(k, 2 i) \\P E(p o s+k, 2 i+1)=P E(p o s, 2 i+1) P E(k, 2 i+1)-P E(p o s, 2 i) P E(k, 2 i)\end{array}</script><ul><li>采用正弦方式和学习方式position embedding结果几乎一样。但采用正弦，因为<strong>能让模型推断出比训练期间遇到的序列长度更长的序列长度</strong></li></ul><h1 id="3-模型训练"><a href="#3-模型训练" class="headerlink" title="3 模型训练"></a>3 模型训练</h1><h3 id="3-1-Optimizer-amp-amp-learning-rate"><a href="#3-1-Optimizer-amp-amp-learning-rate" class="headerlink" title="3.1 Optimizer &amp;&amp; learning rate"></a>3.1 Optimizer &amp;&amp; learning rate</h3><ul><li><p>采用Adam优化器，参数都是模型参数：<script type="math/tex">\beta_1=0.9,\beta_2=0.98,\epsilon=10^{-9}</script></p></li><li><p>Transformer 的学习率更新公式叫作“<strong>noam</strong>”，它将 warmup 和 decay 两个部分组合在一起，总体趋势是<strong>先增加后减小</strong>，具体公式为：</p></li></ul><script type="math/tex; mode=display">\text { lrate }=d_{\text {model } }^{-0.5} \cdot \min \left(\text { step }_{-} \text {num }^{-0.5}, \text { step_ }_{-} \text {num } \cdot \text { warmup_steps }^{-1.5}\right)</script><ul><li>公式实际上是一个以warmup_steps为分界点的分段函数。该点之前是warmup部分，采用线性函数的形式，且warmup_steps越大，斜率越小。该点之后是decay部分，采用负幂的衰减形式，衰减速度先快后慢：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-7c98b2c7ca4467ab770da064bb2b58ba_720w.jpg" alt="v2-7c98b2c7ca4467ab770da064bb2b58ba_720w"  /></p><ul><li><strong>设置warmup的原因：</strong>在CV领域中常常这样做，在《Deep Residual Learning for Image Recognition》中，作者训练110层的超深网络是就用过类似策略：</li></ul><blockquote><p>In this case, we find that <strong>the initial learning rate of 0.1 is slightly too large to start converging</strong>. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.</p></blockquote><p>对于 Transformer 这样的大型网络，<strong>在训练初始阶段，模型尚不稳定，较大的学习率会增加收敛难度</strong>。因此，使用较小的学习率进行 warmup，等 loss 下降到一定程度后，再恢复回常规学习率。</p><h3 id="3-2-Dropout"><a href="#3-2-Dropout" class="headerlink" title="3.2 Dropout"></a>3.2 Dropout</h3><ul><li>在每个子块中，输出结果加入到残差结构和layer normalization之前，进行Dropout</li><li>并且还在Encoder和Decoder最开始的两种embedding相加的时候，使用了Dropout</li><li>Dropout的概率均为0.1</li></ul><h3 id="3-3-Label-Smoothing"><a href="#3-3-Label-Smoothing" class="headerlink" title="3.3 Label Smoothing"></a>3.3 Label Smoothing</h3><ul><li>为了不要对正确类别”too confident”（防止过拟合），Transformer中还使用了Label Smoothing。这种方法<strong>会增大困惑度（perplexity），但是可以提高accuracy和BLEU</strong>。</li><li>假设目标类别为y，任意类别为k，ground-truth 分布为q(k)，模型预测分布为p(k)。 显然，当k=y时，q(k)=1。当k<script type="math/tex">\neq</script>y时，q(k)=0。<strong>LSR（Label Smoothing Regularization）为了让模型的输出不要过于贴合单点分布，选择在gound-truth中加入噪声</strong>。即削弱y的概率，并整体叠加一个独立于训练样例的均匀分布u(k)：</li></ul><script type="math/tex; mode=display">q^{\prime}(k)=(1-\epsilon) q(k)+\epsilon u(k)=(1-\epsilon) q(k)+\epsilon / K</script><p>其中K为softmax的类别数，拆开来看就是：</p><script type="math/tex; mode=display">\begin{array}{ll}q^{\prime}(k)=1-\epsilon+\epsilon / K, & k=y \\q^{\prime}(k)=\epsilon / K, & k \neq y\end{array}</script><p>所有类别的概率和仍然是归一的。说白了就是把最高点砍掉一点，多出来的概率平均分给所有人。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>NLP基础</title>
    <link href="/2022/02/20/NLP%E5%9F%BA%E7%A1%80/"/>
    <url>/2022/02/20/NLP%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="1-词嵌入（word-embedding）"><a href="#1-词嵌入（word-embedding）" class="headerlink" title="1 词嵌入（word embedding）"></a>1 词嵌入（word embedding）</h1><ul><li>词向量是⽤来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）</li></ul><h3 id="1-1-词嵌入相比于one-hot向量的优点"><a href="#1-1-词嵌入相比于one-hot向量的优点" class="headerlink" title="1.1 词嵌入相比于one-hot向量的优点"></a>1.1 词嵌入相比于one-hot向量的优点</h3><ul><li>虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，<strong>one-hot词 向量⽆法准确表达不同词之间的相似度</strong>，如我们常常使⽤的余弦相似度。对于向量<script type="math/tex">x, y \in R^d</script>，它 们的余弦相似度是它们之间夹⻆的余弦值：</li></ul><script type="math/tex; mode=display">\frac{\boldsymbol{x}^{\top} \boldsymbol{y}}{\|\boldsymbol{x}\|\|\boldsymbol{y}\|} \in[-1,1]</script><p>由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。</p><p>word2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表示成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。word2vec⼯具包含了两个模型，即<strong>跳字模型（skip-gram）</strong> 和<strong>连续词袋模型（continuous bag of words，CBOW）</strong></p><h3 id="1-2-跳字模型（skip-gram）"><a href="#1-2-跳字模型（skip-gram）" class="headerlink" title="1.2 跳字模型（skip-gram）"></a>1.2 跳字模型（skip-gram）</h3><ul><li>跳字模型假设基于某个词来生成它在⽂本序列周围的词</li><li>举个例子，假设⽂本序列是 “the” “man” “loves” “his” “son”。以“loves”作为中⼼词，设背景窗口大小为2。如下图所示，跳字模型所关⼼的是，给定中⼼词“loves”，生成与它距离不超过2个词的背景词 “the” “man” “his” “son”的条件概率，即：</li></ul><script type="math/tex; mode=display">P(the, man, his, son | loves)</script><p>假设给定中⼼词的情况下，背景词的生成是相互独⽴的，那么上式可以改写成：</p><script type="math/tex; mode=display">P(the| loves) \times P(man | loves) \times P(his | loves) \times P(son | loves)</script><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125151903218.png" alt="image-20220125151903218" style="zoom: 80%;" /></p><ul><li>在跳字模型中，<strong>每个词被表示成两个<script type="math/tex">d</script>维向量</strong>，⽤来计算条件概率。假设这个词在词典中索引为<script type="math/tex">i</script>， 当它为中心词时向量表示为<script type="math/tex">v_i \in R^d</script>，而为背景词时向量表示为<script type="math/tex">u_i \in R^d</script>。设中⼼词<script type="math/tex">w_c</script>在词典中索引为<script type="math/tex">c</script>，背景词<script type="math/tex">w_o</script>在词典中索引为<script type="math/tex">o</script>，<strong>给定中⼼词<script type="math/tex">w_c</script>生成背景词<script type="math/tex">w_o</script>的条件概率可以通过对向量内积做softmax运算而得到：</strong></li></ul><script type="math/tex; mode=display">P\left(w_{o} \mid w_{c}\right)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)},</script><p>其中词典索引集<script type="math/tex">V = \{0, 1, . . . , |V|−1\}</script></p><ul><li>假设给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type="math/tex">w^{(t)}</script>。 假设给定中⼼词的情况下背景词的生成相互独⽴，当背景窗口大小为m时，跳字模型的<strong>似然函数</strong>即给定任⼀中⼼词生成所有背景词的概率：</li></ul><script type="math/tex; mode=display">\prod_{t=1}^{T} \prod_{-m \leq j \leq m, j \neq 0} P\left(w^{(t+j)} \mid w^{(t)}\right),</script><p>这⾥小于1或大于T的时间步可以被忽略</p><ul><li><strong>跳字模型的训练：</strong></li></ul><blockquote><ul><li><strong>跳字模型的参数是每个词所对应的中心词向量和背景词向量</strong></li><li>先把把最大似然函数取负对数：</li></ul><script type="math/tex; mode=display">-\sum_{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} \log P\left(w^{(t+j)} \mid w^{(t)}\right)</script><p>如果使⽤随机梯度下降，那么在每⼀次迭代⾥我们随机采样⼀个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数</p><ul><li>由Softmax的运算结果可以得到：</li></ul><script type="math/tex; mode=display">\log P\left(w_{o} \mid w_{c}\right)=\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}-\log \left(\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)\right)</script><p>那么我们可以求得上式关于各中心词向量和背景词向量的梯度，比如关于<script type="math/tex">v_c</script>的梯度：</p><script type="math/tex; mode=display">\frac{\partial \log P\left(w_{o} \mid w_{c}\right)}{\partial v_{c}}=\boldsymbol{u}_{o}-\sum_{j \in \mathcal{V}} P\left(w_{j} \mid w_{c}\right) \boldsymbol{u}_{j} .</script><p>训练结束后，对于词典中的任⼀索引为<script type="math/tex">i</script>的词，我们均得到该词作为中⼼词和背景词的两组词向量<script type="math/tex">v_i</script>和<script type="math/tex">u_i</script>。在⾃然语⾔处理应⽤中，<strong>⼀般使⽤跳字模型的中心词向量作为词的表征向量</strong></p></blockquote><ul><li><strong>跳字模型的具体实现：</strong></li></ul><blockquote><ul><li>可以用一个input-hidden-output的三层神经网络来建模上面的skip-model：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526195633208.jpg" alt="在这里插入图片描述" style="zoom: 50%;" /></p><ul><li><p>输入的表示：输入层中每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1,其余维度的值均为0</p></li><li><p>网络中传播的前向过程：输出层向量的值可以通过<strong>隐含层向量（K维，即每一个词向量的维度）</strong>，以及连接隐藏层和输出层之间的<strong>KxN维权重矩阵</strong>计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算每一个单词的生成概率。</p></li><li><strong>input层和hidden层之间的N<script type="math/tex">\times</script>K权重矩阵即N个词的中心词向量，从input层到hidden层的运算即对中心词向量的提取，具体见下图。而hidden层和output层之间的K<script type="math/tex">\times</script>N权重矩阵即N个词的背景词向量，hidden层到output层的运算即实现<script type="math/tex">u^T_ov_c</script>，然后再在最后做一个softmax</strong></li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526202027163.jpg" alt="在这里插入图片描述" style="zoom: 67%;" /></p><p>左边矩阵是input，右边矩阵是input到hidden的权重矩阵，即N个词的中心词向量，由于输入是ont-hot向量，所以运算结果其实就是对中心词向量的一个提取</p><ul><li>在代码实现中是直接使用一个<strong>嵌入层（Embedding层）</strong>，此层的权值形状为<strong>词典大小<script type="math/tex">\times</script>每个词的维度</strong>，可以直接将中心词和背景词对应的one-hot向量转化为词向量。以skip-gram举例，中心词的one-hot向量形状为(批量大小，1)，背景词为(批量大小，词典大小)，两者皆通过嵌入层转化成词向量后形状分别为(批量大小，1，每个词维度)、(批量大小，词典大小，每个词维度)。然后通过<strong>小批量乘法</strong>实现中心词向量和背景词向量的相乘<script type="math/tex">u^T_ov_c</script>，得到形状为(批量大小，词典大小)的结果，然后进行Softmax</li><li>上面的背景词one-hot向量第二维度不一定要为词典大小，如果我们采用<strong>负采样</strong>，则应该改为<strong>正负样本和的大小</strong>，具体可看下方负采样原理</li><li>现在我们来解释一下<strong>小批量乘法</strong>，假设第⼀个小批量包含n个形状为<script type="math/tex">a \times b</script>的矩阵<script type="math/tex">X_1, . . . , X_n</script>，第⼆个小批量包含n个形状为<script type="math/tex">b \times c</script>的矩阵<script type="math/tex">Y_1, . . . , Y_n</script>。 这两个小批量的矩阵乘法输出为n个形状为<script type="math/tex">a \times c</script>的矩阵<script type="math/tex">X_1Y_1, . . . , X_nY_n</script></li></ul></blockquote><h3 id="1-3-连续词袋模型（CBOW）"><a href="#1-3-连续词袋模型（CBOW）" class="headerlink" title="1.3 连续词袋模型（CBOW）"></a>1.3 连续词袋模型（CBOW）</h3><ul><li><p>连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，<strong>连续词袋模型假设基于某中⼼词在 ⽂本序列前后的背景词来生成该中心词</strong></p></li><li><p>如下图，在同样的⽂本序列“the” “man” “loves” “his” “son” ⾥，以“loves”作为中⼼词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词 “the” “man” “his” “son”生成中心词“loves”的条件概率，也就是：</p></li></ul><script type="math/tex; mode=display">P(loves|the,man,his,son)</script><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125172417014.png" alt="image-20220125172417014" style="zoom:80%;" /></p><ul><li><p>因为连续词袋模型的背景词有多个，我们<strong>将这些背景词向量取平均</strong>，然后使⽤和跳字模型⼀样的 ⽅法来计算条件概率</p><p>设<script type="math/tex">v_i \in R^d</script>和<script type="math/tex">u_i \in R^d</script>分别表示词典中索引为<script type="math/tex">i</script>的词作为背景词和中⼼词的向 量（<strong>注意符号的含义与跳字模型中的相反</strong>）。设中⼼词<script type="math/tex">w_c</script>在词典中索引为<script type="math/tex">c</script>，背景词<script type="math/tex">w_{o1} , . . . , w_{o2m}</script>在 词典中索引为<script type="math/tex">o_1, . . . , o_{2m}</script>，那么给定背景词生成中⼼词的条件概率：</p><script type="math/tex; mode=display">.P\left(w_{c} \mid w_{o_{1}}, \ldots, w_{o_{2 m}}\right)=\frac{\exp \left(\frac{1}{2 m} \boldsymbol{u}_{c}^{\top}\left(\boldsymbol{v}_{o_{1}}+\ldots+\boldsymbol{v}_{o_{2 m}}\right)\right)}{\sum_{i \in \mathcal{V}} \exp \left(\frac{1}{2 m} \boldsymbol{u}_{i}^{\top}\left(\boldsymbol{v}_{o_{1}}+\ldots+\boldsymbol{v}_{o_{2 m}}\right)\right)}</script><p>设<script type="math/tex">W_o = \{w_{o1} , . . . , w_{o2m}\}</script>，<script type="math/tex">\overline{\boldsymbol{v}}_{o}=\left(\boldsymbol{v}_{o_{1}}+\ldots+\boldsymbol{v}_{o_{2 m}}\right) /(2 m)</script>，可将上式简化为：</p><script type="math/tex; mode=display">P\left(w_{c} \mid \mathcal{W}_{o}\right)=\frac{\exp \left(\boldsymbol{u}_{c}^{\top} \overline{\boldsymbol{v}}_{o}\right)}{\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \overline{\boldsymbol{v}}_{o}\right)}</script></li><li><p>给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type="math/tex">w^{(t)}</script>，背景窗口大小为m。连续词袋模型的似然函数是由背景词生成任⼀中⼼词的概率：</p></li></ul><script type="math/tex; mode=display">\prod_{t=1}^{T} P\left(w^{(t)} \mid w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)}\right)</script><ul><li>接着再像跳字模型一样求似然函数的负对数，然后再求关于背景向量和中心向量的梯度。<strong>同跳字模型不⼀样的⼀点在于，我们⼀般使⽤连续词袋模型的背景词向量作为词的表征向量</strong></li></ul><ul><li><strong>跳字模型更适合大型语料库，CBOW更适合小型的语料库</strong></li></ul><h3 id="1-4-近似训练"><a href="#1-4-近似训练" class="headerlink" title="1.4 近似训练"></a>1.4 近似训练</h3><ul><li>无论是跳字模型还是连续词袋模型，在求梯度的时候，由于条件概率使⽤了softmax运 算，每⼀步的梯度计算都包含词典大小数⽬的项的累加。对于含⼏⼗万或上百万词的较大词典， 每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练⽅法，即<strong>负采样（negative sampling）</strong>或<strong>层序softmax（hierarchical softmax）</strong></li></ul><h5 id="1-4-1-高频词抽样"><a href="#1-4-1-高频词抽样" class="headerlink" title="1.4.1 高频词抽样"></a>1.4.1 高频词抽样</h5><ul><li>首先介绍一下对高频词进行抽样（也称⼆次采样(subsampling)）。对于文本中的高频词，比如”the”，不进行抽样会带来两个问题：</li></ul><blockquote><ol><li>当我们得到成对的单词训练样本时，<strong>(“fox”, “the”) 这样的训练样本并不会给我们提供关于“fox”更多的语义信息</strong>，因为“the”在每个单词的上下文中几乎都会出现</li><li>由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数</li></ol></blockquote><ul><li>Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：<strong>对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关，词频越高被删除的概率就越大</strong></li></ul><h5 id="1-4-2-负采样（negative-sampling）"><a href="#1-4-2-负采样（negative-sampling）" class="headerlink" title="1.4.2 负采样（negative sampling）"></a>1.4.2 负采样（negative sampling）</h5><ul><li><strong>负采样（negative sampling）</strong>解决了因为词典大小过大的计算开销问题。不同于原本每个训练样本更新所有的权重，<strong>负采样每次让一个训练样本仅仅更新一小部分的权重</strong>，这样就会降低梯度下降过程中的计算量。</li><li>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们<strong>期望输出为0的神经元结点</strong>所对应的单词我们称为<strong>“negative” word</strong>。</li><li><p>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。<strong>（在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。）</strong></p></li><li><p><strong>每个单词被选为“negative words”的概率计算公式与其出现的频次有关，词频越高，被选上的概率就越高</strong>，论文中给的公式是：</p></li></ul><script type="math/tex; mode=display">P\left(w_{i}\right)=\frac{f\left(w_{i}\right)^{3 / 4}}{\sum_{j=0}^{n}\left(f\left(w_{j}\right)^{3 / 4}\right)}</script><ul><li><strong>负采样概述：</strong></li></ul><blockquote><ul><li>负采样修改了原来的⽬标函数。给定中⼼词<script type="math/tex">w_c</script>的⼀个背景窗口，我们把背景词<script type="math/tex">w_o</script>出现在该背景窗口看作⼀个事件，并将该事件的概率计算为：</li></ul><script type="math/tex; mode=display">P\left(D=1 \mid w_{c}, w_{o}\right)=\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)</script><ul><li>我们先考虑最大化⽂本序列中所有该事件的联合概率来训练词向量。具体来说，给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type="math/tex">w^{(t)}</script>且背景窗口大小为m，考虑最大化联合概率：</li></ul><script type="math/tex; mode=display">\prod_{t=1}^{T} \prod_{-m \leq j \leq m, j \neq 0} P\left(D=1 \mid w^{(t)}, w^{(t+j)}\right)</script><p>然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为⽆穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫⽆意义。<strong>负采样通过采样并添加负类样本使⽬标函数更有意义。</strong></p><ul><li>在采集了K个负样本后，我们可以将所求概率近似为：</li></ul><script type="math/tex; mode=display">P\left(w^{(t+j)} \mid w^{(t)}\right)=P\left(D=1 \mid w^{(t)}, w^{(t+j)}\right) \prod_{k=1, w_{k} \sim P(w)}^{K} P\left(D=0 \mid w^{(t)}, w_{k}\right)</script><ul><li>取负对数后变成了：</li></ul><script type="math/tex; mode=display">-\log P\left(w^{(t+j)} \mid w^{(t)}\right)=-\log \sigma\left(\boldsymbol{u}_{i_{t+j}}^{\top} v_{i_{t}}\right)-\sum_{k=1, w_{k} \sim P(w)}^{K} \log \sigma\left(-\boldsymbol{u}_{h_{k}}^{\top} v_{i_{t}}\right)</script><p>现在，<strong>训练中每⼀步的梯度计算开销不再与词典大小相关，而与K线性相关</strong>。当K取较小的常数时，负采样在每⼀步的梯度计算开销较小。</p></blockquote><ul><li><strong>负采样的具体实现：</strong></li></ul><blockquote><ul><li><p>在实际应用中，我们并没有严格的将不在窗口中的词定义为负样本，而是根据词频直接随机取负样本，这样的实验结果是更优的。</p></li><li><p><strong>设词典大小为<script type="math/tex">\mathcal{V}</script>，将一根长为1的线段分为<script type="math/tex">\mathcal{V}</script>段，每段分别对应一个词，每个词对应的长度就是上面所写的每个词对应的频率<script type="math/tex">P(w_i)</script>，然后取一个M（M&gt;&gt;<script type="math/tex">\mathcal{V}</script>），论文中M取的是<script type="math/tex">10^8</script>，将长度为1的线段均分为M份（如下图），然后取K个负样本即在该线段上取K个点，取到的点在哪个词的对应线段就取哪个词</strong></p></li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211153340493.png" alt="image-20220211153340493" style="zoom:67%;" /></p></blockquote><h5 id="1-4-3-层序Softmax（hierarchical-softmax）"><a href="#1-4-3-层序Softmax（hierarchical-softmax）" class="headerlink" title="1.4.3 层序Softmax（hierarchical softmax）"></a>1.4.3 层序Softmax（hierarchical softmax）</h5><ul><li>层序softmax是另⼀种近似训练法。它使⽤了⼆叉树这⼀数据结构，树的每个叶结点代表词典<script type="math/tex">\mathcal{V}</script>中的每个词。</li><li><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220210181128975.png" alt="image-20220210181128975" style="zoom: 80%;" /></li></ul><p>假设<script type="math/tex">L(w)</script>为从⼆叉树的根结点到词<script type="math/tex">w</script>的叶结点的路径（包括根结点和叶结点）上的结点数。 设<script type="math/tex">n(w, j)</script>为该路径上第<script type="math/tex">j</script>个结点，并设该结点的背景词向量为<script type="math/tex">u_{n(w,j)}</script>。以上图为例，<script type="math/tex">L(w_3) = 4</script> 层序softmax将跳字模型中的条件概率近似表示为：</p><script type="math/tex; mode=display">P\left(w_{o} \mid w_{c}\right)=\prod_{j=1}^{L\left(w_{o}\right)-1} \sigma\left([[ n\left(w_{o}, j+1\right)=\operatorname{leftChild}\left(n\left(w_{o}, j\right)\right) ]] \cdot \boldsymbol{u}_{n\left(w_{o}, j\right)}^{\top} \boldsymbol{v}_{c}\right),</script><p>其中如果判断x为真，[[x]] = 1；反之[[x]] = −1。</p><p>例如让我们计算上图中给定词<script type="math/tex">w_c</script>生成词<script type="math/tex">w_3</script>的条件概率：</p><script type="math/tex; mode=display">P\left(w_{3} \mid w_{c}\right)=\sigma\left(\boldsymbol{u}_{n\left(w_{3}, 1\right)}^{\top} \boldsymbol{v}_{c}\right) \cdot \sigma\left(-\boldsymbol{u}_{n\left(w_{3}, 2\right)}^{\top} \boldsymbol{v}_{c}\right) \cdot \sigma\left(\boldsymbol{u}_{n\left(w_{3}, 3\right)}^{\top} \boldsymbol{v}_{c}\right)</script><ul><li><strong>推导过程：</strong></li></ul><blockquote><p>设<script type="math/tex">d_{2}^{w}, d_{3}^{w}, \cdots, d_{l w}^{w} \in\{0,1\}</script>为词<script type="math/tex">w</script>的编码，<script type="math/tex">d_j^w</script>代表<script type="math/tex">w</script>路径上的第<script type="math/tex">j</script>个节点对应的编码（根节点无编码），则：</p><script type="math/tex; mode=display">P(w_0|w_c) = \prod_{j=2}^{L(w)} P\left(d_{j}^{w} \mid v_c, u_{n_(w, j)}\right)</script><p>其中的每一项都是一个Logistic回归：</p><script type="math/tex; mode=display">P\left(d_{j}^{w} \mid v_c, u_{n_(w, j)}\right) = \left\{\begin{array}{ll}\sigma\left(v_c^T u_{n_(w, j)}\right), & d_{j}^{w}=0 \\1-\sigma\left(v_c^T u_{n_(w, j)}\right), & d_{j}^{w}=1\end{array}\right.</script><p>可以将两式合并，并且由于<script type="math/tex">\sigma(x) + \sigma(-x) = 1</script>，可以将上式转化为：</p><script type="math/tex; mode=display">P(w_0|w_c)=\left[\sigma\left(v_c^T u_{n_(w, j)}\right)\right]^{1-d_{j}^{w}} \cdot\left[\sigma\left(-v_c^T u_{n_(w, j)}\right)\right]^{d^{w}}</script><p>我们取目标函数的对数：</p><script type="math/tex; mode=display">\mathcal{L} = \sum_{w \in \mathcal{V}} P(Context(w) | w) = \sum_{w \in \mathcal{V}} \sum_{j=2}^{L(w)}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(v_c^T u_{n_(w, j)}\right)\right]+d_{j}^{w} \cdot \log \left[\sigma\left(-v_c^T u_{n_(w, j)}\right)\right]\right\}</script><p><strong>要最大化上述目标函数是比较困难的，但是我们可以最大化每一子项（即下式），达到局部最大化的效果，层序Softmax的Softmax其实就是体现在此处</strong>：</p><script type="math/tex; mode=display">\mathcal{L}(w, j) = \left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(v_c^T u_{n_(w, j)}\right)\right]+d_{j}^{w} \cdot \log \left[\sigma\left(-v_c^T u_{n_(w, j)}\right)\right]\right\}</script><p><strong>这样就可以将于<script type="math/tex">L(wo) − 1</script>数量级的计算复杂度降为<script type="math/tex">O(log_2 |\mathcal{V}|)</script>，当词典<script type="math/tex">\mathcal{V}</script>很大时，层序softmax在训练中每⼀步的梯度计算开销相较未使用近似训练时大幅降低。</strong></p><p>显然，上式的<script type="math/tex">d_j^w</script>是用来<strong>判断下一个节点是否是本节点的左孩子的</strong>，其作用是为了满足给定任意中心词<script type="math/tex">w_c</script>，生成背景词的条件概率和为1：</p><script type="math/tex; mode=display">\sum_{w \in \mathcal{V}} P\left(w \mid w_{c}\right)=1</script><p>这样才能使似然函数不至于训练到无穷大（和负采样要添加负样本是一样的原因），并且符合概率和为1</p></blockquote><ul><li><strong>层序Softmax中的二叉树是使用的哈夫曼树，可以使搜索次数达到最小</strong></li></ul><h1 id="2-子词嵌入（fastText）"><a href="#2-子词嵌入（fastText）" class="headerlink" title="2 子词嵌入（fastText）"></a>2 子词嵌入（fastText）</h1><ul><li>英语单词通常有其内部结构和形成⽅式。例如，我们可以从“dog” “dogs”和“dogcatcher”的 字面上推测它们的关系。这些词都有同⼀个词根“dog”，但使⽤不同的后缀来改变词的含义。而 且，这个关联可以推⼴⾄其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关 系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。这方面的研究称为构词学</li><li><p>在word2vec中，我们并没有直接利⽤构词学中的信息。⽆论是在跳字模型还是连续词袋模型中， 我们都将形态不同的单词⽤不同的向量来表示。例如，“dog”和“dogs”分别⽤两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌⼊（subword embedding）的⽅法，从而试图将构词信息引⼊word2vec中的跳字模型</p></li><li><p>在fastText中，每个中⼼词被表⽰成子词的集合。举个例子，例如单词”where”，⾸先，我们在单词的<strong>⾸尾分别添加特殊字符“&lt;” 和  “&gt;” 以区分作为前后缀的子词</strong>。然后，<strong>将单词当成⼀个由字符构成的序列来提取n元语法</strong>。例如，当n = 3时，我们得到所有⻓度为3的子词：”<wh" "whe" "her" "ere" "re>“以及特殊子词”where”。</p></li><li>在fastText中，对于⼀个词w，我们将它所有⻓度在3 ∼ 6的子词和特殊子词的并集记为<script type="math/tex">\mathcal{G}_{w}</script>。<strong>那么词典则是所有词的子词集合的并集</strong>。假设词典中子词<script type="math/tex">g</script>的向量为<script type="math/tex">z_g</script>，那么跳字模型中词w的作为中⼼词的向量<script type="math/tex">v_w</script>则表⽰成：</li></ul><script type="math/tex; mode=display">v_{w}=\sum_{g \in \mathcal{G}_{w}} z_{g}</script><ul><li>fastText的其余部分同跳字模型⼀致，不在此重复。可以看到，<strong>与跳字模型相⽐，fastText中词典 规模更大，造成模型参数更多</strong>，同时⼀个词的向量需要对所有子词向量求和，继而导致<strong>计算复杂度更高</strong>。但与此同时，<strong>较生僻的复杂单词，甚⾄是词典中没有的单词，可能会从同它结构类似的 其他词那⾥获取更好的词向量表示。</strong></li></ul><h1 id="3-全局向量的词嵌入（GloVe）"><a href="#3-全局向量的词嵌入（GloVe）" class="headerlink" title="3 全局向量的词嵌入（GloVe）"></a>3 全局向量的词嵌入（GloVe）</h1><ul><li>在词嵌入中，交叉熵损失函数有时并不是一个好的选择。一方面，正如上面所说，<strong>会带来整个词典大小的累加项，带来过大的计算开销</strong>。另一方面，<strong>词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。</strong></li><li>GloVe模型既使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。为了做到这一点GloVe模型引入了<strong>共现矩阵（Cooccurrence Probabilities Matrix）</strong></li></ul><h3 id="3-1-共现矩阵"><a href="#3-1-共现矩阵" class="headerlink" title="3.1 共现矩阵"></a>3.1 共现矩阵</h3><ul><li><p>假设：</p><blockquote><ol><li>共现矩阵为<script type="math/tex">X</script>，<script type="math/tex">X</script>中的元素<script type="math/tex">X_{ij}</script>为语料库中<script type="math/tex">word_i</script>上下文中出现<script type="math/tex">word_j</script>的次数</li><li><script type="math/tex">X_i = \sum_k X_{ik}</script>是出现在<script type="math/tex">word_i</script>上下文中所有词的总次数</li><li><script type="math/tex">P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}</script>为<script type="math/tex">word_j</script>出现在<script type="math/tex">word_i</script>上下文的概率</li></ol></blockquote></li><li><p>下面我们来举个例子：</p></li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235403111.png" alt="image-20220211235403111" style="zoom:80%;" /></p><p>设Ratio = <script type="math/tex">\frac{P_{ik}}{P_{jk}}</script>，，从上面的例子中我们可以总结出：</p><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235553030.png" alt="image-20220211235553030"></p><p><strong>所以Ratio可以反应word之间的相关性，而GloVe就是利用了这个值</strong></p><p>假设<script type="math/tex">i, j, k</script>三者的词向量都已经得到<script type="math/tex">w_i, w_j, w_k</script>，那么我们现在就要找一个函数F，使得：</p><script type="math/tex; mode=display">F\left(w_{i}, w_{j}, w_{k}\right)=\frac{P_{i k}}{P_{j k}}</script><h3 id="3-2-损失函数推导"><a href="#3-2-损失函数推导" class="headerlink" title="3.2 损失函数推导"></a>3.2 损失函数推导</h3><ul><li><strong>上述等式的右边是通过统计得到的已知量，左侧的三个词向量<script type="math/tex">w_i, w_j, w_k</script>是模型要求的量，F是未知的，如果能够将F的形式确定下来， 那么我们就可以通过优化算法求解词向量了，下面是求解过程：</strong></li></ul><ol><li><script type="math/tex">\frac{P_{i k}}{P_{j k}}</script>考察了<script type="math/tex">i,j,k</script>三个词的相关性，不妨先只考虑<script type="math/tex">i,j</script>两个词的词向量<script type="math/tex">w_i, w_j</script>的相关性，向量相关关系的度量可以用两个向量的差，所以上述等式可以先改为：</li></ol><script type="math/tex; mode=display">F\left((w_{i}-w_{j}), w_{k}\right)=\frac{P_{i k}}{P_{j k}}</script><ol><li>由于右边是标量，左边是向量，所以可以联想到使用向量的内积转化为标量：</li></ol><script type="math/tex; mode=display">F\left(\left(w_{i}-w_{j}\right)^{T} w_{k}\right)=F\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{P_{i k}}{P_{j k}}</script><ol><li>因为左边是差，右边是商，所以将F设为exp运算，将差转化为商：</li></ol><script type="math/tex; mode=display">\exp \left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{\exp \left(w_{i}^{T} w_{k}\right)}{\exp \left(w_{j}^{T} w_{k}\right)}=\frac{P_{i k}}{P_{j k}}</script><ol><li>现在只需让分子分母分别相等就能成立：</li></ol><script type="math/tex; mode=display">\exp \left(w_{i}^{T} w_{k}\right)= \alpha P_{i k}, \exp \left(w_{j}^{T} w_{k}\right)= \alpha P_{j k}，\quad  \alpha为常数</script><ol><li>现在只需要在整个词料库中考察<script type="math/tex">\exp \left(w_{i}^{T} w_{k}\right)= \alpha P_{i k}= \alpha \frac{X_{i k}}{X_{i}}</script>，即：</li></ol><script type="math/tex; mode=display">w_{i}^{T} w_{k}=\log \left(\alpha \frac{X_{i k}}{X_{i}}\right)=\log X_{i k}-\log X_{i} + log\alpha</script><ol><li>所以我们需要设置一个偏差项<script type="math/tex">b_i</script>来拟合<script type="math/tex">log X_i - log \alpha</script>，由于如果上式<script type="math/tex">i,k</script>位置交换，左式的值不变，但是右式的值会变，所以我们还要设置一个偏差项<script type="math/tex">b_k</script>，即：</li></ol><script type="math/tex; mode=display">w_{i}^{T} w_{k}  + b_i + b_k=log X_{ik}</script><ol><li>上面公式只是理想状态下，实际中只能要求两者接近    从而就有了代价函数：</li></ol><script type="math/tex; mode=display">J=\sum_{i k}\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}</script><ol><li>如果两个词共同出现的次数越多，那么其在代价函数中的影响就越大，所以要设计一个权重函数：</li></ol><script type="math/tex; mode=display">J=\sum_{i k} f\left(X_{i k}\right)\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}</script><blockquote><p>对于函数f要满足一下条件：</p><ol><li>如果两个词没有共同出现过，那么权重就是0，即f(0) = 0</li><li>两个词共同出现次数越多，那么权重就越大，所以f是一个递增函数</li><li>由一些于高频词（<script type="math/tex">X_{ij}较大</script>，如”the”）的权重很大，但是实际上作用并不是很大，所以f(x)对于较大的x不能取太大的值</li></ol><p>综合上方条件，论文提出了一下函数：</p><script type="math/tex; mode=display">f(x)=\left\{\begin{array}{r}\left(\frac{x}{x_{\max }}\right)^{\alpha}, \text { if } x<x_{\max } \\1, \text { otherwise }\end{array}\right.</script><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212005820238.png" alt="image-20220212005820238" style="zoom:67%;" /></p><p>作者认为<script type="math/tex">x_{max} = 100, \alpha = \frac{3}{4}</script>比较合适</p></blockquote><ul><li>值得强调的是，如果词<script type="math/tex">w_i</script>出现在词<script type="math/tex">w_j</script>的背景窗口里，那么词<script type="math/tex">w_j</script>也会出现在词<script type="math/tex">w_i</script>的背景窗口⾥。也就是说，<script type="math/tex">x_{ij} = x_{ji}</script>。不同于<strong>word2vec中拟合的是非对称的条件概率<script type="math/tex">p_{ij}</script>，GloVe模型拟合的是对称的<script type="math/tex">log x_{ij}</script></strong>。因此，<strong>任意词的中心词向量和背景词向量在GloVe模型中是等价的。但由于初始化值的不同，同⼀个词最终学习到的两组词向量可能不同</strong>。当学习得到所有词向量以后，GloVe模型使⽤<strong>中心词向量与背景词向量之和作为该词的最终词向量，这样做是为了提高鲁棒性（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）</strong></li></ul><h1 id="4-词嵌入应用"><a href="#4-词嵌入应用" class="headerlink" title="4 词嵌入应用"></a>4 词嵌入应用</h1><ul><li>下列应用均为在训练好的预训练模型上进行训练的。预训练的GloVe模型的命名规范⼤致是<strong>“模型.（数据集.）数据集词数.词向量维度.txt”</strong>。如 ‘glove.twitter.27B.100d.txt’。</li></ul><h3 id="4-1-求近义词和类比词"><a href="#4-1-求近义词和类比词" class="headerlink" title="4.1 求近义词和类比词"></a>4.1 求近义词和类比词</h3><ul><li>求近义词可以运用词向量之间的余弦相似度</li><li>求类比词，例如，“man”（男人）: “woman”（女人）:: “son”（儿子）: “daughter”（女儿）是⼀个类⽐例子。<strong>对于类比关系中的4个词 a : b :: c : d，给定前3个词a、b和c，求d。设词w的词向量为vec(w)。求类比词的思路是vec(a) - vec(b) = vec(c) - vec(d)，来求出vec(d)的近似值，然后寻找和其最相似的几个词向量</strong></li></ul><h3 id="4-2-使用循环神经网络进行文本情感分类"><a href="#4-2-使用循环神经网络进行文本情感分类" class="headerlink" title="4.2 使用循环神经网络进行文本情感分类"></a>4.2 使用循环神经网络进行文本情感分类</h3><ul><li>⽂本分类是⾃然语⾔处理的⼀个常⻅任务，它把⼀段不定⻓的⽂本序列变换为⽂本的类别。本节关 注它的⼀个子问题：使⽤⽂本情感分类来分析⽂本作者的情绪。这个问题也叫<strong>情感分析（sentiment analysis）</strong></li><li>本节我们使用电影评论的情感分析（二分类，positive和negtive的情感），在数据处理的时候，一条评论为一个样本，我们要先对评论进行分词（一般都是<strong>基于空格分词</strong>），并且由于每条评论的长度不一样，无法组成小批量，所以我们需要通过<strong>截断或者补充来使长度定长</strong></li><li>对于模型的设计，先需要一个<strong>嵌入层将文本转化为词向量</strong>，再通过<strong>双向循环网络对特征序列进⼀步编码得到序列信息</strong>，然后将<strong>最初时间步和最终时间步的隐藏状态连结</strong>，再传入全连接层输出</li><li>注意由于是预训练的模型，所以<strong>嵌入层的模型参数是不需要更新的</strong>，直接由训练好的模型参数导入即可。但是<strong>导入的模型的词向量维度要和预先设置的嵌入层词向量维度一致</strong></li></ul><h3 id="4-3-使用卷积神经网络（textCNN）进行文本情感分类："><a href="#4-3-使用卷积神经网络（textCNN）进行文本情感分类：" class="headerlink" title="4.3 使用卷积神经网络（textCNN）进行文本情感分类："></a>4.3 使用卷积神经网络（textCNN）进行文本情感分类：</h3><ul><li>我们可以将⽂本数据看作只有⼀个维度的时间序列，并很⾃然地使⽤循环神经⽹络来表征这样的数据。其实，我们也可以将⽂本当作⼀维图像，从而可以⽤⼀维卷积神经⽹络来捕捉临近词之间的关联</li></ul><h5 id="4-3-1-一维卷积层"><a href="#4-3-1-一维卷积层" class="headerlink" title="4.3.1 一维卷积层"></a>4.3.1 一维卷积层</h5><ul><li>和二维卷积层一样，一维卷积层卷积窗口从输⼊数组的最左⽅开始，按从左往右的顺序，依次在输⼊数组上滑动，举个例子：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155418239.png" alt="image-20220212155418239" style="zoom:67%;" /></p><p>输出中的2是由输入的前两个和核逐个相乘得到的：0x1+1x2=2，然后窗口往后滑动一格，以此类推</p><p>对于多通道操作也是和二维卷积层一样的：<br><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155747478.png" alt="image-20220212155747478"></p><p>每个通道分别操作，然后各通道结果相加。如果想要输出多通道，则核还要增加一个维度</p><h5 id="4-3-2-时序最大池化层"><a href="#4-3-2-时序最大池化层" class="headerlink" title="4.3.2 时序最大池化层"></a>4.3.2 时序最大池化层</h5><ul><li>假设输⼊包含多个通道，各通道由不同时间步上的数值组成，<strong>各通道的输出即该通道所有时间步中最⼤的数值</strong>。因此，<strong>时序最⼤池化层的输⼊在各个通道上的时间 步数可以不同</strong></li><li>为提升计算性能，我们常常将不同⻓度的时序样本组成⼀个小批量，<strong>并通过在较短序列后附加特殊字符（如0）令批量中各时序样本⻓度相同</strong>。这些⼈为添加的特殊字符当然是⽆意义的。由于时 序最⼤池化的主要⽬的是抓取时序中最重要的特征，它通常能<strong>使模型不受⼈为添加字符的影响</strong></li></ul><h5 id="4-3-3-textCNN模型"><a href="#4-3-3-textCNN模型" class="headerlink" title="4.3.3 textCNN模型"></a>4.3.3 textCNN模型</h5><ul><li>textCNN模型主要使⽤了⼀维卷积层和时序最⼤池化层。假设输⼊的⽂本序列由n个词组成，每 个词⽤d维的词向量表⽰。那么输⼊样本的宽为n，⾼为1，输⼊通道数为d。textCNN的计算主要分为以下⼏步：</li></ul><blockquote><ol><li>定义多个⼀维卷积核，并使⽤这些卷积核对输⼊分别做卷积计算。宽度不同的卷积核可能 会捕捉到不同个数的相邻词的相关性</li><li>对输出的所有通道分别做时序最⼤池化，再将这些通道的池化输出值连结为向量。</li><li>通过全连接层将连结后的向量变换为有关各类别的输出。这⼀步可以使⽤丢弃层应对过拟合。</li></ol></blockquote><ul><li>举个例子，这⾥的输⼊是⼀个有11个词的句子，每个词⽤6维词向量表⽰。因此输⼊序列的宽为11，输⼊通道数为6。给定2个⼀维卷积核，核宽分别为2和4，输出 通道数分别设为4和5</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212161751535.png" alt="image-20220212161751535" style="zoom: 67%;" /></p><p><strong>尽管每个通道的宽不同，我们依然可以对各个通道做时序最⼤池化， 并将9个通道的池化输出连结成⼀个9维向量</strong>。最终，使⽤全连接将9维向量变换为2维输出，即正⾯情感和负⾯情感的预测</p><h1 id="5-机器翻译"><a href="#5-机器翻译" class="headerlink" title="5 机器翻译"></a>5 机器翻译</h1><ul><li><p>我们使用seq2seq模型进行机器翻译，原理具体可看<a href="https://zlkqz.top/2021/12/10/RNN/#7-Seq2Seq%E6%A8%A1%E5%9E%8B">seq2seq</a></p></li><li><p>这里我们只介绍一下如何评价翻译结果。评价机器翻译结果通常使⽤<strong>BLEU（Bilingual Evaluation Understudy）</strong>。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。</p></li><li>具体来说，设词数为n的⼦序列的精度为<script type="math/tex">p_n</script>。它是<strong>预测序列与标签序列匹配词数为n的⼦序列的数量与预测序列中词数为n的⼦序列的数量之⽐</strong>。举个例子，假设标签序列为A、B、C、D、E、F， 预测序列为A、B、B、C、D，那么<script type="math/tex">p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0</script>。设<script type="math/tex">len_{label}</script>和<script type="math/tex">len_{pred}</script>分 别为标签序列和预测序列的词数，那么，BLEU的定义为：</li></ul><script type="math/tex; mode=display">\exp \left(\min \left(0,1-\frac{\text { len }_{\text {label }}}{\text { len }_{\text {pred }}}\right)\right) \prod_{n=1}^{k} p_{n}^{1 / 2^{n}}</script><p>其中k是我们希望匹配的⼦序列的最⼤词数。可以看到当预测序列和标签序列完全⼀致时， BLEU为1。</p><ul><li>设计思想：</li></ul><blockquote><p>因为匹配较⻓⼦序列⽐匹配较短⼦序列更难，BLEU对匹配较⻓⼦序列的精度赋予了更⼤权重，具体是在上式的<script type="math/tex">p_n^{1/2^n}</script>中，当序列较长，即n较大时，由于<script type="math/tex">p_n \in [0, 1]</script>，所以<script type="math/tex">1/2^n</script>后会变大，并且n越大越接近1，即给较长序列更大的权重</p><p>并且由于较短序列一般会有比较大的<script type="math/tex">p_n</script>，所以连乘项前面的系数是为了惩罚较短的输出而设置的。举个例子，当<script type="math/tex">len_{pred}</script>较小时，连乘项前面的系数就会小于1，以此来减少较短序列的权重。</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>CV基础</title>
    <link href="/2022/01/24/CV%E5%9F%BA%E7%A1%80/"/>
    <url>/2022/01/24/CV%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="1-图像增广"><a href="#1-图像增广" class="headerlink" title="1 图像增广"></a>1 图像增广</h1><ul><li>图像增⼴（image augmentation）技术通过对训练图像做⼀系列随机改变，来产⽣相似但⼜不同的训练样本，从而<strong>扩大训练数据集的规模</strong></li><li><p>随机改变训练样本可以<strong>降低模型对某些属性的依赖，从而提高模型的泛化能力</strong>。例如，我们可以对图像进⾏不同 方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性</p></li><li><p>常用的方法有：<strong>翻转</strong>、<strong>裁剪</strong>、<strong>变换颜色</strong>。对于翻转，大部分情况左右翻转比上下翻转更通用一些。对于颜色，我们可以从亮度、对比度、饱和度和⾊调四方面进行改变</p></li><li>实际应⽤中我们会将多个图像增⼴方法叠加使⽤</li></ul><h1 id="2-微调"><a href="#2-微调" class="headerlink" title="2 微调"></a>2 微调</h1><ul><li><strong>迁移学习（transfer learning）</strong>：<strong>将从源数据集学到的知识迁移到目标数据集上</strong>。例如，虽然ImageNet数据集的图像大多跟椅⼦⽆关，但在该数据集上训练的模型可以抽 取较通⽤的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于 识别椅⼦也可能同样有效。</li><li>迁移学习中的一种常用技术“微调（<strong>fine tuning）</strong>”，由一下四步构成：</li></ul><blockquote><ol><li>在源数据集（如ImageNet数据集）上预训练⼀个神经⽹络模型，即源模型</li><li>创建⼀个新的神经⽹络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设 计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适⽤于目标数据集。我们还假设源模型的输出层与源数据集的标签紧密相关，因此在目标模 型中不予采⽤。</li><li>为目标模型添加⼀个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</li><li>在目标数据集（如椅⼦数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</li></ol><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123170934001.png" alt="image-20220123170934001" style="zoom: 67%;" /></p><ul><li>可以选择保留除输出层以外的所有层，也可以保留除临近输出层的几层以外的所有层</li><li>由于预训练模型是比较接近正确结果的，而新添加的层是随机初始化。<strong>所以两者的学习率是不同的，预训练的学习率更小</strong></li></ul></blockquote><h1 id="3-目标检测和边界框"><a href="#3-目标检测和边界框" class="headerlink" title="3 目标检测和边界框"></a>3 目标检测和边界框</h1><ul><li>很多时候图像⾥有多个我们感兴趣的目标，我们 不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉⾥，我们将这类任务 称为目标检测（object detection）或物体检测</li><li>在目标检测⾥，我们通常使⽤<strong>边界框（bounding box）</strong>来描述目标位置。边界框是⼀个矩形框， 可以由矩形左上⻆的x和y轴坐标与右下⻆的x和y轴坐标确定</li><li>它以每个像素为中⼼⽣ 成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为<strong>锚框（anchor box）</strong></li></ul><h3 id="3-1-锚框的生成"><a href="#3-1-锚框的生成" class="headerlink" title="3.1 锚框的生成"></a>3.1 锚框的生成</h3><ul><li>假设输⼊图像高为h，宽为w。我们分别以图像的每个像素为中⼼⽣成不同形状的锚框。设大小为s <script type="math/tex">\in</script> (0, 1]且宽高比为r &gt; 0，那么锚框的宽和高将分别为<script type="math/tex">ws\sqrt{r}</script>和<script type="math/tex">hs/\sqrt{r}</script>。当中⼼位置给定时，已 知宽和高的锚框是确定的。⾯我们分别设定好⼀组大小<script type="math/tex">s_1, . . . , s_n</script>和⼀组宽高比<script type="math/tex">r_1, . . . , r_m</script>，s和r两两配对能覆盖所有的真实边界框，但是计算复杂度容易更高，所以我们通常只对包含<script type="math/tex">s_1</script>或<script type="math/tex">r_1</script>的大小与宽高比的组合感兴趣，即：</li></ul><script type="math/tex; mode=display">\left(s_{1}, r_{1}\right),\left(s_{1}, r_{2}\right), \ldots,\left(s_{1}, r_{m}\right),\left(s_{2}, r_{1}\right),\left(s_{3}, r_{1}\right), \ldots,\left(s_{n}, r_{1}\right)</script><h3 id="3-2-交并比"><a href="#3-2-交并比" class="headerlink" title="3.2 交并比"></a>3.2 交并比</h3><ul><li>Jaccard系数 （Jaccard index）可以衡量两个集合的相似度。给定集合A和B，它们的Jaccard系数即⼆者交集大小除以⼆者并集大小：</li></ul><script type="math/tex; mode=display">J(\mathcal{A}, \mathcal{B})=\frac{|\mathcal{A} \cap \mathcal{B}|}{|\mathcal{A} \cup \mathcal{B}|}</script><ul><li>我们通 常将Jaccard系数称为交并比（intersection over union，IoU），即两个边界框相交⾯积与相并⾯积之比</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123173756721.png" alt="image-20220123173756721" style="zoom: 80%;" /></p><h3 id="3-3-标注训练集的锚框"><a href="#3-3-标注训练集的锚框" class="headerlink" title="3.3 标注训练集的锚框"></a>3.3 标注训练集的锚框</h3><ul><li>在训练集中，我们<strong>将每个锚框视为⼀个训练样本</strong>。为了训练目标检测模型，我们需要为每个锚框标注两类标签：<strong>⼀是锚框所含目标的类别，简称类别；⼆是真实边界框相对锚框的偏移量，简称偏移量（offset)</strong></li><li>在目标检测时，我们⾸先⽣成多个锚框，然后为每个锚框预测类别以及偏移量， 接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框</li></ul><ul><li><p>假设图像中锚框分别为<script type="math/tex">A_1, A_2, . . . , A_{n_a}</script>，真实边界框分别为<script type="math/tex">B_1, B_2, . . . , B_{n_b}</script>，且<script type="math/tex">n_a</script> ≥ <script type="math/tex">n_b</script>。定义矩 阵<script type="math/tex">X \in R^{n_a\times n_b}</script>，其中第i⾏第j列的元素<script type="math/tex">x_{ij}</script>为锚框<script type="math/tex">A_i</script>与真实边界框<script type="math/tex">B_j</script>的交并比</p><blockquote><ol><li>找到矩阵中最大元素，并将该值对应的真实边界框赋值给锚框，然后从矩阵中去除该行和该列所有元素。然后继续找最大元素，重复上述操作，直到所有真实边界框都被分配完</li><li>这个时候，我们已为<script type="math/tex">n_b</script>个锚框各分配了⼀个真实边界框。接下来，我们 只遍历剩余的<script type="math/tex">n_a − n_b</script>个锚框：给定其中的锚框<script type="math/tex">A_i</script>，根据矩阵<script type="math/tex">X</script>的第i⾏找到与<script type="math/tex">A_i</script>交并比最大的真实边界框<script type="math/tex">B_j</script>，<strong>且只有当该交并比大于预先设定的阈值时，才为锚框<script type="math/tex">A_i</script>分配真实边界框<script type="math/tex">B_j</script></strong></li></ol></blockquote></li><li><p><strong>如果⼀个锚框A被分配了真实边界框B，将锚框A的类别设为B的类别，并根据B和A的中心坐标的相对位置以及两个框的相对大小为锚框A标注偏 移量。</strong></p></li><li><strong>由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要⼀些特殊变换，才能使偏移量的分布更均匀从而更容易拟合</strong>。设锚框A及其被分配的真实边界框B的中 ⼼坐标分别为<script type="math/tex">(x_a, y_a)</script>和<script type="math/tex">(x_b, y_b)</script>，A和B的宽分别为<script type="math/tex">w_a</script>和<script type="math/tex">w_b</script>，高分别为<script type="math/tex">h_a</script>和<script type="math/tex">h_b</script>，⼀个常⽤的技巧是将A的偏移量标注为：</li></ul><script type="math/tex; mode=display">\left(\frac{\frac{x_{b}-x_{a}}{w_{a}}-\mu_{x}}{\sigma_{x}}, \frac{\frac{y_{b}-y_{a}}{h_{a}}-\mu_{y}}{\sigma_{y}}, \frac{\log \frac{w_{b}}{w_{a}}-\mu_{w}}{\sigma_{w}}, \frac{\log \frac{h_{b}}{h_{a}}-\mu_{h}}{\sigma_{h}}\right)</script><p>其中常数的默认值为<script type="math/tex">µ_x = µ_y = µ_w = µ_h = 0, σ_x = σ_y = 0.1, σ_w = σ_h = 0.2</script></p><ul><li><p>如果一个锚框没有被分配真实边界框，我们只需将该锚框的类别设为<strong>背景</strong>。类别为背景的锚框通常被称为<strong>负类锚框</strong>，其余则被称为<strong>正类锚框</strong></p></li><li><p><strong>掩码（mask）：</strong> <strong>形状为(批量大小, 锚框个数的四倍)</strong>。掩码变量中的元素 与每个锚框的4个偏移量⼀⼀对应。由于我们不关⼼对背景的检测，有关负类的偏移量不应影响 目标函数。<strong>通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量</strong></p></li></ul><h3 id="3-4-非极大值抑制"><a href="#3-4-非极大值抑制" class="headerlink" title="3.4 非极大值抑制"></a>3.4 非极大值抑制</h3><ul><li>为了使结果更加简洁，我们可以移除相似的预测边界框。常⽤的方法叫作非极大值抑制（non-maximum suppression，NMS)：</li></ul><blockquote><ol><li>在同⼀图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序， 得到列表L</li><li>从L中选取置信度最高的预测边界框<script type="math/tex">B_1</script>作为基准，将所有与<script type="math/tex">B_1</script>的交并比大于某阈值 的非基准预测边界框从L中移除。这⾥的阈值是预先设定的超参数</li><li>继续重复上述操作，直到L中所有的预测边界框都曾作为基准</li></ol></blockquote><ul><li>实践中，我们可以在执⾏非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。</li></ul><h3 id="3-5-多尺度目标检测"><a href="#3-5-多尺度目标检测" class="headerlink" title="3.5 多尺度目标检测"></a>3.5 多尺度目标检测</h3><ul><li>在不同尺度下，我们可以⽣成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多</li><li>因此，当使⽤较小 锚框来检测较小目标时，我们可以采样较多的区域；而当使⽤较大锚框来检测较大目标时，我们可以采样较少的区域。</li><li>我们可以通过<strong>控制特征图的大小来控制尺度（特征图每个单元在输入图像上对应的感受野可大可小）</strong>。本质上，我们⽤输⼊图像在某个感 受野区域内的信息来预测输⼊图像上与该区域位置相近的锚框的类别和偏移量</li></ul><h3 id="3-6-单发多框检测（SSD）"><a href="#3-6-单发多框检测（SSD）" class="headerlink" title="3.6 单发多框检测（SSD）"></a>3.6 单发多框检测（SSD）</h3><ul><li>它主要由<strong>⼀个基础⽹络块和若干个多尺度特征块串联而成</strong>。其中<strong>基础⽹络块⽤来从原始图像中抽取特征</strong>，因此⼀般会选择常⽤的深度卷积神经⽹络。单 发多框检测论⽂中选⽤了在分类层之前截断的VGG，现在也常⽤ResNet替代</li><li>我们可以设计 基础⽹络，使它输出的高和宽较大。这样⼀来，基于该特征图⽣成的锚框数量较多，可以⽤来检 测尺⼨较小的目标</li><li>接下来的每个多尺度特征块将上⼀层提供的特征图的<strong>高和宽缩小（如减半）</strong>， 并使<strong>特征图中每个单元在输⼊图像上的感受野变得更⼴阔</strong>。如此⼀来，下图中<strong>越靠近顶部的多 尺度特征块输出的特征图越小，故而基于特征图⽣成的锚框也越少，加之特征图中每个单元感受 野越大，因此更适合检测尺⼨较大的目标</strong></li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123182624112.png" alt="image-20220123182624112" style="zoom:80%;" /></p><h5 id="3-6-1-类别预测层"><a href="#3-6-1-类别预测层" class="headerlink" title="3.6.1 类别预测层"></a>3.6.1 类别预测层</h5><ul><li>设目标的类别个数为q。每个锚框的类别个数将是q + 1，其中类别0表⽰锚框只包含背景</li><li>设特征图的高和宽分别为h和w，如果以其中每个单元为中⼼⽣成a个锚框，那么我们需 要对hwa个锚框进⾏分类。如果使⽤全连接层作为输出，<strong>很容易导致模型参数过多。所以我们可以通过通道输出类别</strong></li><li>类别预测层使⽤⼀个<strong>保持输⼊高和宽的卷积层</strong>。这样⼀来，输出和输⼊在特征图宽和高上的空间坐标⼀⼀对应。考虑输出和输⼊同⼀空间坐标(x, y)：<strong>输出特征图上(x, y)坐标的通道里包含了以输⼊特征图(x, y)坐标为中心生成的所有锚框的类别预测</strong>。因此<strong>输出通道数为a(q + 1)， 其中索引为i(q + 1) + j（0 ≤ j ≤ q）的通道代表了索引为i的锚框有关类别索引为j的预测</strong></li></ul><h5 id="3-6-2-边界框预测层"><a href="#3-6-2-边界框预测层" class="headerlink" title="3.6.2 边界框预测层"></a>3.6.2 边界框预测层</h5><ul><li>边界框预测层的设计与类别预测层的设计类似。唯⼀不同的是，这⾥需要为每个锚框预测4个偏移量，而不是q + 1个类别</li></ul><h5 id="3-6-3-连结多尺度的预测"><a href="#3-6-3-连结多尺度的预测" class="headerlink" title="3.6.3 连结多尺度的预测"></a>3.6.3 连结多尺度的预测</h5><ul><li>每个尺度的输出，除了批量大小一样，其他维度的大小均不一样。我们需要将它们变形成统⼀的格式并将多尺度的预测连结，从而让后续计算更简单</li><li>所以我们需要将为(批量大小, 通道数, 高, 宽)格式转换成⼆维的(批量大小, 高×宽×通道数)的格式，以方便之后在维度1上的连结。</li></ul><h5 id="3-6-4-损失函数和评价函数"><a href="#3-6-4-损失函数和评价函数" class="headerlink" title="3.6.4 损失函数和评价函数"></a>3.6.4 损失函数和评价函数</h5><ul><li>目标检测有两个损失：<strong>⼀是有关锚框类别的损失</strong>，我们可以重⽤之前图像分类问题⾥⼀直使⽤ 的交叉熵损失函数；<strong>⼆是有关正类锚框偏移量的损失</strong></li><li>。预测偏移量是⼀个回归问题，但这⾥不 使⽤前⾯介绍过的平方损失，而使⽤L1范数损失，即预测值与真实值之间差的绝对值（其中 使用掩码变量令负类锚框和填充锚框不参与损失的计算）</li><li>最后，我们将有关锚框类别和偏移量的损失相加得到模型的最终损失函数。</li><li>可以将<script type="math/tex">L_1</script>损失换成<strong>平滑的<script type="math/tex">L_1</script>范数损失</strong>，它在零点附近使⽤平方函数从而更加平滑，这是通过⼀个超参数<script type="math/tex">\sigma</script>来控制平滑区域的：</li></ul><script type="math/tex; mode=display">f(x)=\left\{\begin{array}{ll}(\sigma x)^{2} / 2, & \text { if }|x|<1 / \sigma^{2} \\|x|-0.5 / \sigma^{2}, & \text { otherwise }\end{array}\right.</script><p>当<script type="math/tex">\sigma</script>很⼤时该损失类似于<script type="math/tex">L_1</script>范数损失。当它较小时，损失函数较平滑。</p><ul><li>还可以将交叉熵损失换成<strong>焦点损失（focal loss）</strong>：</li></ul><script type="math/tex; mode=display">-\alpha\left(1-p_{j}\right)^{\gamma} \log p_{j}</script><p>焦点损失适用于比较密集的目标检测，即要判定的类别比较多的情况。我们将一个锚框标签的类别作为正类，其余都作为负类（包括背景），那么这就转换成了一个二分类问题，回顾二分类的交叉熵损失函数：</p><script type="math/tex; mode=display">\mathrm{L}=-\mathrm{ylog} y^{\prime}-(1-y) \log \left(1-y^{\prime}\right)=\left\{\begin{array}{ll}-\log y^{\prime} & , \quad y=1 \\-\log \left(1-y^{\prime}\right), & y=0\end{array}\right.</script><p>可以看到当标签为负类（y=0），且正类预测概率<script type="math/tex">y'</script>较大时，会产生较大的损失。而由于正负样本的极其不均衡（比如有1000个类别，正类只有1种，负类则有999种），所以负样本会主导梯度的更新方向，使得整体学习方向跑偏</p><p>上述的负类因为正负样本的不均衡，所以负类是是易分类的样本（<script type="math/tex">p_j > 0.5</script>），而焦点损失中的<script type="math/tex">(1-p_j)^{\gamma}</script>就是为了减轻易分类样本的权重，让对象检测器更关注难分类的样本（即正样本）</p><h1 id="4-区域卷积神经网络（R-CNN）"><a href="#4-区域卷积神经网络（R-CNN）" class="headerlink" title="4 区域卷积神经网络（R-CNN）"></a>4 区域卷积神经网络（R-CNN）</h1><h3 id="4-1-R-CNN"><a href="#4-1-R-CNN" class="headerlink" title="4.1 R-CNN"></a>4.1 R-CNN</h3><ul><li>R-CNN⾸先对图像<strong>选取若干提议区域</strong>（如锚框也是⼀种选取方法）并标注它们的类别和边界框 （如偏移量）。然后，<strong>⽤卷积神经⽹络对每个提议区域做前向计算抽取特征。之后，我们⽤每个提议区域的特征预测类别和边界框</strong></li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123184832784.png" alt="image-20220123184832784" style="zoom:80%;" /></p><ul><li><p>具体来说，R-CNN主要由以下4步构成：</p><blockquote><ol><li>对输⼊图像使⽤选择性搜索（selective search）来<strong>选取多个高质量的提议区域</strong>。这些提议区域通常是在<strong>多个尺度下</strong>选取的，并具有不同的形状和大小。<strong>每个提议区域将被标注类 别和真实边界框。</strong></li><li>选取⼀个预训练的卷积神经⽹络，并将其在输出层之前截断</li><li>将每个提议区域的特征连同其标注的类别作为⼀个样本，训练多个⽀持向量机对目标分类。 其中每个⽀持向量机⽤来判断样本是否属于某⼀个类别</li><li>将每个提议区域的特征连同其标注的边界框作为⼀个样本，训练线性回归模型来预测真实边界框</li></ol></blockquote></li><li><p>一张图片中可能会有很多个提议区域，每个区域都要进行卷积运算。这个<strong>巨大的计算量令R-CNN难以在实际应⽤中被⼴泛采⽤</strong></p></li></ul><h3 id="4-2-Fast-R-CNN"><a href="#4-2-Fast-R-CNN" class="headerlink" title="4.2 Fast R-CNN"></a>4.2 Fast R-CNN</h3><ul><li>R-CNN提议区域通常<strong>有大量重叠， 独⽴的特征抽取会导致大量的重复计算</strong>。Fast R-CNN对R-CNN的⼀个主要改进在于<strong>只对整个图像 做卷积神经⽹络的前向计算。</strong></li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123185433036.png" alt="image-20220123185433036" style="zoom:80%;" /></p><ul><li>主要步骤：</li></ul><blockquote><ol><li>与R-CNN相比，Fast R-CNN⽤来提取特征的卷积神经⽹络的输⼊是整个图像，而不是各个提议区域。而且，<strong>这个⽹络通常会参与训练</strong>，即更新模型参数。设输⼊为⼀张图像，将卷积神经⽹络的输出的形状记为<script type="math/tex">1 × c × h_1 × w_1</script></li><li>假设选择性搜索⽣成n个提议区域。<strong>这些形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域</strong>。这些兴趣区域需要抽取出<strong>形状相同</strong>的特征（假设高和宽均分别指定为<script type="math/tex">h_2</script>和<script type="math/tex">w_2</script>）以便于连结后输出（使用<strong>兴趣区域池化（region of interest pooling，RoI池化）层，将卷积神经⽹络的输出和提议区域作为输⼊，输出连结后的各个提议区域抽取的特征</strong>）</li><li>通过全连接层将输出形状变换为n × d，其中超参数d取决于模型设计</li><li>预测类别时，将全连接层的输出的形状再变换为n × q并使⽤softmax回归（q为类别个数）。 预测边界框时，将全连接层的输出的形状变换为n × 4。也就是说，我们为每个提议区域预 测类别和边界框。</li></ol></blockquote><ul><li>兴趣区域池化层：兴趣区域池化层对每个区域的输 出形状是可以直接指定的如下图：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191333584.png" alt="image-20220123191333584"></p><p>第一张图的蓝色区域是一个提议区域，将其经过2x2兴趣区域池化层后，划分成了4个区域，分别含有元素0、1、4、5（5最 大），2、6（6最大），8、9（9最大），10。输出每个区域的最大元素</p><h3 id="4-3-Faster-R-CNN"><a href="#4-3-Faster-R-CNN" class="headerlink" title="4.3 Faster R-CNN"></a>4.3 Faster R-CNN</h3><ul><li>Fast R-CNN通常需要在选择性搜索中⽣成较多的提议区域，以获得较精确的目标检测结果。Faster R-CNN提出<strong>将选择性搜索替换成区域提议⽹络（region proposal network）</strong>，从而<strong>减少提议区域 的⽣成数量</strong>，并保证目标检测的精度</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191545146.png" alt="image-20220123191545146" style="zoom:67%;" /></p><ul><li>与Fast R-CNN相比，只有⽣成提议区域的方法从选择性搜索变成了区域提议⽹络，而其他部分均保持不变。具体来说，区域提议⽹络的计算步骤如下：</li></ul><blockquote><ol><li>使⽤填充为1的3 × 3卷积层变换卷积神经⽹络的输出，并将输出通道数记为c</li><li>. 以特征图每个单元为中⼼，⽣成多个不同大小和宽高比的锚框并标注它们</li><li>. ⽤锚框中⼼单元⻓度为c的特征分别预测该锚框的<strong>二元类别（含目标还是背景，需要reshape）</strong>和边界框</li><li>使⽤非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测 边界框即兴趣区域池化层所需要的提议区域。</li></ol></blockquote><ul><li>区域提议⽹络作为Faster R-CNN的⼀部分，是和整个模型⼀起训练得到的。也就 是说，<strong>Faster R-CNN的目标函数既包括目标检测中的类别和边界框预测，⼜包括区域提议⽹络中 锚框的⼆元类别和边界框预测</strong></li></ul><h3 id="4-4-Mask-R-CNN"><a href="#4-4-Mask-R-CNN" class="headerlink" title="4.4 Mask R-CNN"></a>4.4 Mask R-CNN</h3><ul><li>如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利⽤这些详尽 的标注信息进⼀步提升目标检测的精度。</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123193610616.png" alt="image-20220123193610616" style="zoom: 80%;" /></p><ul><li>Mask R-CNN将兴趣区域池化层 替换成了兴趣区域对⻬层，即通过<strong>双线性插值（bilinear interpolation）（一种常用的上采样方法，目的是将下一层的特征图的单元于上一层特征图单元对齐）</strong>来保留特征图上的空间信息，从而更适于像素级预测</li></ul><h1 id="5-语义分割"><a href="#5-语义分割" class="headerlink" title="5 语义分割"></a>5 语义分割</h1><ul><li>语义分割（semantic segmentation）关注如何将图像分割成属于不同语义类别的区域。值得⼀提的是，这些语义区域的标注和预测都是像素级的</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123194304346.png" alt="image-20220123194304346" style="zoom: 67%;" /></p><ul><li>计算机视觉领域还有2个与语义分割相似的重要问题，即<strong>图像分割（image segmentation）</strong>和<strong>实例分割（instance segmentation）</strong></li><li><strong>图像分割</strong>将图像分割成若干组成区域。<strong>这类问题的方法通常利⽤图像中像素之间的相关性</strong>。 它在训练时不需要有关图像像素的标签信息，在预测时也⽆法保证分割出的区域具有我们希望得到的语义。如上图图像分割可能将狗分割成两个区域：⼀个覆盖以⿊⾊为主的嘴巴和眼睛，而另⼀个覆盖以⻩⾊为主的其余部分⾝体</li><li><strong>实例分割</strong>研究如何 识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割<strong>不仅需要区分语 义，还要区分不同的目标实例</strong>。如果图像中有两只狗，实例分割需要区分像素属于这两只 狗中的哪⼀只。</li></ul><ul><li>如果我们通过缩放图像使其符合模型的输⼊形状。然而在语义分割⾥，<strong>需要将预测的像素类别重新映射回原始尺⼨的输⼊图像</strong>。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，<strong>我们将图像裁剪成固定尺⼨而不是缩放</strong>（对于实际尺寸小于规定尺寸的图像，需要移除）</li></ul><h1 id="6-全卷积网络（FCN）"><a href="#6-全卷积网络（FCN）" class="headerlink" title="6 全卷积网络（FCN）"></a>6 全卷积网络（FCN）</h1><ul><li>全卷积⽹络（fully convolutional network，FCN）采⽤卷积神经⽹络实现了从图像像素到像素类别的变换。</li><li>全卷积⽹络通过<strong>转置卷积（transposed convolution）层将中间层特征图的高和宽变换回输⼊图像的尺⼨</strong></li></ul><ul><li>因为卷积运算可以用矩阵乘法来实现，假设input进行卷积运算相当于input矩阵乘一个<script type="math/tex">W</script>矩阵得到特征图featrue。那么可以通过featrue乘一个<script type="math/tex">W^T</script>来变回input的形状。<strong>所以可以通过转置卷积层来交换卷积层输入和输出的形状</strong></li></ul><h3 id="6-1-模型构造"><a href="#6-1-模型构造" class="headerlink" title="6.1 模型构造"></a>6.1 模型构造</h3><ul><li>全卷积⽹络先使⽤卷积神经⽹络抽取图像特征，然后通过1 × 1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的⾼和宽变换为输⼊图像的尺⼨</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124154145643.png" alt="image-20220124154145643" style="zoom: 67%;" /></p><h3 id="6-2-初始化转置卷积层"><a href="#6-2-初始化转置卷积层" class="headerlink" title="6.2 初始化转置卷积层"></a>6.2 初始化转置卷积层</h3><ul><li><strong>双线性插值：</strong>首先介绍一下单线性插值（一维）：</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190819135319360.jpg" alt="在这里插入图片描述" style="zoom:67%;" /></p><p>我们知道<script type="math/tex">x_0,x_1,y_0,y_1,x</script>的值，要求<script type="math/tex">y</script>的值：</p><script type="math/tex; mode=display">y = y_0 + \frac{y_1 - y_0}{x_1 - x_0}(x - x_0)</script><p>而<strong>双线性插值其实就是在不同的维度上单线性插值两次</strong>，已知<script type="math/tex">Q_{11}(x_1,y_1),Q_{12}(x_1,y_2),Q_{21}(x_2,y_1),Q_{22}(x_2,y_2)</script>，求其中点P(x,y)的函数值：</p><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-ad3d95548a97aa47ca85867cd0f2e161_720w.jpg" alt="img" style="zoom:67%;" /></p><p>首先在x方向单线性插值两次：</p><script type="math/tex; mode=display">\begin{array}{l}f\left(R_{1}\right)=\frac{x_{2}-x}{x_{2}-x_{1}} f\left(Q_{11}\right)+\frac{x-x_{1}}{x_{2}-x_{1}} f\left(Q_{21}\right) \\f\left(R_{2}\right)=\frac{x_{2}-x}{x_{2}-x_{1}} f\left(Q_{12}\right)+\frac{x-x_{1}}{x_{2}-x_{1}} f\left(Q_{22}\right)\end{array}</script><p>然后再在y方向单线性插值一次：</p><script type="math/tex; mode=display">f(P)=\frac{y_{2}-y}{y_{2}-y_{1}} f\left(R_{1}\right)+\frac{y-y_{1}}{y_{2}-y_{1}} f\left(R_{2}\right)</script><p>即可得到结果。</p><ul><li>在全卷积⽹络中，我们一般<strong>将转置卷积层初始化为双线性插值的上采样</strong>，具体方法是：</li></ul><blockquote><ol><li>为了得到输出图像在坐 标(x, y)上的像素，先将该坐标映射到输⼊图像的坐标(x ′ , y′ )，例如，根据输⼊与输出的尺⼨之⽐来映射。</li><li>映射后的x ′和y ′通常是实数。然后，在输⼊图像上找到与坐标(x ′ , y′ )最近的4像素。最后， 输出图像在坐标(x, y)上的像素依据输⼊图像上这4像素及其与(x ′ , y′ )的相对距离来计算（用双线性插值）</li></ol></blockquote><ul><li><p>如果步幅为s、填充为s/2（假设s/2为整数）、卷积核的⾼和宽为2s，转置卷积核将输⼊的⾼和宽分别放⼤s倍</p></li><li><p>转置卷积层的输出形状可能会不一样，这是因为当<strong>输入图像的高宽无法整除步幅时，输出的高宽会有所偏差</strong>。为了解决这个问题，我们可以在图像中<strong>截取多块⾼和宽为步幅的整数倍的矩形区域</strong>，并分别对这些区域中的像素做前向计算。<strong>这些区域的并集需要完整覆盖输⼊图像</strong>。当⼀个像素被多个区域所覆盖时，它在不同区域 前向计算中转置卷积层输出的<strong>平均值可以作为softmax运算的输⼊</strong>，从而预测类别</p></li></ul><h1 id="7-样式迁移"><a href="#7-样式迁移" class="headerlink" title="7 样式迁移"></a>7 样式迁移</h1><ul><li><p>使⽤卷积神经⽹络⾃动将某图像中的样式应⽤在另⼀图像之上，即样式迁移（style transfer）。需要两张输⼊图像，⼀张是<strong>内容图像</strong>，另⼀张是<strong>样式图像</strong>， 我们将使⽤神经⽹络<strong>修改内容图像使其在样式上接近样式图像</strong></p></li><li><p>主要步骤：</p></li></ul><blockquote><ol><li>⾸先，我们初始化合成图像，例如 将其<strong>初始化成内容图像</strong>。该合成图像是样式迁移过程中<strong>唯⼀需要更新的变量</strong>，</li><li>我们选择⼀个预训练的卷积神经⽹络来<strong>抽取图像的特征</strong>，其中的<strong>模型参数在训练中⽆须更新</strong>。深度卷积神经⽹络<strong>凭借多个层逐级抽取图像的特征。我们可以选择其中某些 层的输出作为内容特征或样式特征</strong>，例如下图，这⾥选取的预训练的神经⽹络含有3个卷积 层，其中第⼆层输出图像的内容特征，而第⼀层和第三层的输出被作为图像的样式特征</li><li>样式迁移常⽤的损失函数由3部分组成：<strong>内容损失（content loss）</strong>使合成图像与内容图像在内容特征上接近，<strong>样式损失（style loss）</strong>令合成图像与样式图像 在样式特征上接近，而<strong>总变差损失（total variation loss）</strong>则有助于减少合成图像中的噪点</li></ol><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124175546385.png" alt="image-20220124175546385" style="zoom:50%;" /></p></blockquote><h3 id="7-1-内容层和样式层的选择"><a href="#7-1-内容层和样式层的选择" class="headerlink" title="7.1 内容层和样式层的选择"></a>7.1 内容层和样式层的选择</h3><ul><li><strong>⼀般来说，越靠近输⼊层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成 图像过多保留内容图像的细节，我们选择较靠近输出的层来输出图像的内容特征</strong></li><li><strong>我们还可以选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层</strong></li><li>例如VGG-19,使⽤了5个卷积块。我们可以选择第四卷积块的最后⼀个卷积层作为内容层，以及每个卷积块的第⼀个卷积层作为样式层</li></ul><h3 id="7-2-损失函数"><a href="#7-2-损失函数" class="headerlink" title="7.2 损失函数"></a>7.2 损失函数</h3><ul><li><p><strong>内容损失：</strong>与线性回归中的损失函数类似，内容损失通过平⽅误差函数衡量合成图像与内容图像在内容特征上的差异。平⽅误差函数的两个输⼊均为内容层的输出。</p></li><li><p><strong>样式损失：</strong>样式损失也⼀样通过平⽅误差函数衡量合成图像与样式图像在样式上的差异。我们将样式层的输出（长为h，宽为w，通道数为c），转化成c行hw列的矩阵X，矩阵X可以看作由c个长度为hw的向量<script type="math/tex">x_1, . . . , x_c</script>组成的。其中向量<script type="math/tex">x_i</script>代表了通道<script type="math/tex">i</script>上的样式特征。</p><p>这些向量的<strong>格拉姆矩阵 （Gram matrix）<script type="math/tex">XX^T\in R^{c\times c}</script>中<script type="math/tex">i</script>⾏<script type="math/tex">j</script>列的元素<script type="math/tex">x_{ij}</script>即向量<script type="math/tex">x_i</script>与<script type="math/tex">x_j</script>的内积</strong>，它表达了通道<script type="math/tex">i</script>和通道<script type="math/tex">j</script>上样式特征的相关性。我们⽤这样的格拉姆矩阵表达样式层输出的样式。</p><p>需要注意的是，当hw的值较⼤时，格拉姆矩阵中的元素容易出现较⼤的值。此外，格拉姆矩阵的⾼和宽皆为通道数c。为了让样式损失不受这些值的⼤小影响，<strong>需要除以矩阵中元素的个数，即chw</strong></p></li><li><p><strong>总变量损失：</strong>有时候，我们学到的合成图像⾥⾯有⼤量⾼频噪点，即有特别亮或者特别暗的颗粒像素。⼀种常⽤的降噪⽅法是总变差降噪（total variation denoising）。</p><p>假设<script type="math/tex">x_{i,j}</script>表⽰坐标为<script type="math/tex">(i, j)</script>的像素值，降低总变差损失：</p><script type="math/tex; mode=display">\sum_{i, j}\left|x_{i, j}-x_{i+1, j}\right|+\left|x_{i, j}-x_{i, j+1}\right|</script><p>能够尽可能使邻近的像素值相似。</p></li></ul><ul><li>样式迁移的损失函数即内容损失、样式损失和总变差损失的<strong>加权和</strong>。通过调节这些<strong>权值超参数</strong>， 我们可以权衡合成图像在保留内容、迁移样式以及降噪三⽅⾯的相对重要性。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>RNN基本概念</title>
    <link href="/2021/12/10/RNN/"/>
    <url>/2021/12/10/RNN/</url>
    
    <content type="html"><![CDATA[<ul><li>与多层感知机和能有效<strong>处理空间信息</strong>的卷积神经⽹络不同，循环神经⽹络是为更好地<strong>处理时序信息</strong>而设计的。它<strong>引⼊状态变量来存储过去的信息</strong>，并⽤其<strong>与当前的输⼊共同决定当前的输出</strong></li></ul><h1 id="1-语言模型"><a href="#1-语言模型" class="headerlink" title="1 语言模型"></a>1 语言模型</h1><h3 id="1-1-语言模型的计算"><a href="#1-1-语言模型的计算" class="headerlink" title="1.1 语言模型的计算"></a>1.1 语言模型的计算</h3><ul><li>我们可以把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。假设⼀段⻓度为 $T$ 的⽂本中的词 依次为 $w_1, w_2, . . . , w_T$，那么在离散的时间序列中，$w_t（1 ≤ t ≤ T）$可看作在时间步（time step）$ t$ 的输出或标签。给定⼀个⻓度为 $T$ 的词的序列 $w_1, w_2, . . . , w_T$，语⾔模型将计算该序列的概率：</li></ul><script type="math/tex; mode=display">P(w_1, w_2, ...,w_T)</script><ul><li><p>语⾔模型可⽤于提升语⾳识别和机器翻译的性能。例如，在语⾳识别中，给定⼀段“厨房⾥⻝油 ⽤完了”的语⾳，有可能会输出“厨房⾥⻝油⽤完了”和“厨房⾥⽯油⽤完了”这两个读⾳完全 ⼀样的⽂本序列。如果语⾔模型判断出前者的概率⼤于后者的概率，我们就可以根据相同读⾳的 语⾳输出“厨房⾥⻝油⽤完了”的⽂本序列。</p></li><li><p><strong>由于 $w_1, w_2, . . . , w_T$ 是依次生成的，所以有：</strong></p></li></ul><script type="math/tex; mode=display">P(w_1, w_2, ...,w_T) = \prod_{t=1}^T P(w_t | w_1, ..., w_{t-1})</script><ul><li>为了计算语⾔模型，我们需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即<strong>语⾔模型参数</strong>。<strong>词的概率可以通过该词在训练数据集中的相对词频来计算</strong></li></ul><h3 id="1-2-n元语法"><a href="#1-2-n元语法" class="headerlink" title="1.2 n元语法"></a>1.2 n元语法</h3><ul><li><strong>当序列⻓度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加</strong></li><li>n元语法通过⻢尔可夫假设（<strong>虽然并不⼀定成立</strong>）简化了语⾔模型的计算。这⾥的<strong>⻢尔可夫假设是指⼀个词的出现只与前⾯n个词相关</strong>，即n阶⻢尔可夫链（Markov chain of order n）</li><li>如果基于$n-1$阶马尔科夫链，我们可以将语言模型改写为：</li></ul><script type="math/tex; mode=display">P(w_1, w_2, ...,w_T) \approx \prod_{t=1}^T P(w_t | w_{t - (n - 1)}, ..., w_{t-1})</script><ul><li><p>以上也叫<strong>n元语法</strong>（n-grams）。它是基于n−1阶⻢尔可夫链的概率语⾔模型</p></li><li><p>当n较小时，n元语法往往并不准确。当n较⼤时，n元语法需要计算并存储⼤量的词频和多词相邻频率。<strong>n权衡了计算复杂度和模型准确性</strong></p></li></ul><h1 id="2-循环神经网络（RNN）"><a href="#2-循环神经网络（RNN）" class="headerlink" title="2 循环神经网络（RNN）"></a>2 循环神经网络（RNN）</h1><ul><li>在n元语法中，时间步t的词 $w_t$ 基于前⾯所有词的条件概率只考虑了最近时间步的n − 1个词。如果要考虑⽐t − (n − 1)更早时间步的词对$w_t$的可能影响，我们需要增⼤n。但这样模型参数的数量将随之呈指数级增⻓</li><li>但是对于循环神经网络，它<strong>并⾮刚性地记忆所有固定⻓度的序列，而是通过隐藏状态来存储之前时间步的信息</strong></li></ul><h3 id="2-1-循环神经⽹络"><a href="#2-1-循环神经⽹络" class="headerlink" title="2.1 循环神经⽹络"></a>2.1 循环神经⽹络</h3><ul><li>假设<script type="math/tex">X_t \in \mathbb{R}^{n \times d}</script>是序列中时间步t的小批量输⼊， <script type="math/tex">H_t \in \mathbb{R}^{n \times h}</script>是该时间步的隐藏变量。与多层感知机不同的是，这⾥我们保存上⼀时间步的隐藏变量<script type="math/tex">H_{t−1}</script>，并引⼊⼀个新的权重参数<script type="math/tex">W_{hh} \in \mathbb{R}^{h×h}</script>，该参数⽤来描述在当前时间步如何使⽤上⼀时间步的隐藏变量。时间步t的隐藏变量的计算由当前时间步的输⼊和上⼀时间步的隐藏变量共同决定：</li></ul><script type="math/tex; mode=display">\boldsymbol{H}_{t}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h h}+\boldsymbol{b}_{h}\right)</script><p>$\phi$是激活函数</p><ul><li><strong>这⾥的隐藏变量能够捕捉截⾄当前时间步的序列的历史信息</strong></li><li>每个时间步还有一个对应的输出：</li></ul><script type="math/tex; mode=display">\boldsymbol{O}_{t}=\boldsymbol{H}_{t} \boldsymbol{W}_{h q}+\boldsymbol{b}_{q}</script><p>在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差</p><ul><li><strong>即便在不同时间步，循环神经⽹络也始终使⽤W， b这些模型参数。因此，循环神经⽹络模型参数的数量不随时间步的增加而增⻓</strong></li></ul><p><img src="https://s2.loli.net/2021/12/10/oAbwO43tDSTWpVg.png" alt="image-20211210101227481"></p><ul><li>隐藏状态中<script type="math/tex">X_{t}W_{x h}+H_{t-1} W_{h h}</script>的计算等价于<script type="math/tex">X_t</script>与<script type="math/tex">H_{t−1}</script>连结后的矩阵乘以<script type="math/tex">W_{xh}</script>与<script type="math/tex">W_{hh}</script>连结后的矩阵</li></ul><ul><li>举一个栗子：基于字符级循环神经⽹络的语⾔模型</li></ul><p><img src="https://s2.loli.net/2021/12/10/OYZwANsxl2oEQaI.png" alt="image-20211210101914126"></p><p><strong>标签序列依次为输入序列的下一个</strong>，并且在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差</p><h3 id="2-2-时序数据的采样"><a href="#2-2-时序数据的采样" class="headerlink" title="2.2 时序数据的采样"></a>2.2 时序数据的采样</h3><ul><li>时序数据的⼀个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想” “要” “有” “直” “升”。<strong>该样本的标签序列为这些字符分别在训练集中的下⼀个字符</strong>，即 “要” “有” “直” “升” “机”。</li><li>我们有两种⽅式对时序数据进⾏采样，分别是随机采样和相邻采样</li></ul><h4 id="2-2-1-随机采样"><a href="#2-2-1-随机采样" class="headerlink" title="2.2.1 随机采样"></a>2.2.1 随机采样</h4><ul><li><p>在随机采样中，每个样本是原始序列上任意截取的⼀段序列。</p></li><li><p>相邻的两个随机小批量在原始序列上的位置不⼀定相毗邻。因此，我们⽆法⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态。<strong>在训练模型时，每次随机采样前都需要重新初始化隐藏状态</strong></p></li></ul><h4 id="2-2-2-相邻采样"><a href="#2-2-2-相邻采样" class="headerlink" title="2.2.2 相邻采样"></a>2.2.2 相邻采样</h4><ul><li>除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就<strong>可以⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态</strong>，从而使下⼀个小批量的输出也取决于当前小批量的输⼊</li></ul><ul><li><strong>两种采样方式的区别：</strong><ol><li>采用相邻采样，在训练模型时，我们只需在每⼀个迭代周期开始时初始化隐藏状态</li><li>当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同⼀迭代周期中，随着迭代次数的增加，梯度的<strong>计算开销会越来越⼤</strong>。为了使模型参数的梯度计算只依赖⼀次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来</li><li><strong>相邻采样不能并行运算</strong>，因为必须等上一个小批量结束了，下一个小批量才能开始</li></ol></li></ul><h3 id="2-3-RNN的训练、预测"><a href="#2-3-RNN的训练、预测" class="headerlink" title="2.3 RNN的训练、预测"></a>2.3 RNN的训练、预测</h3><h4 id="2-3-1-输入输出"><a href="#2-3-1-输入输出" class="headerlink" title="2.3.1 输入输出"></a>2.3.1 输入输出</h4><ul><li>首先是RNN的输入输出，应该包含time_steps、batch_size、vocab_size（时间步、批量大小、词典大小）的维度，之所以有一个维度是词典大小，是因为我们会<strong>把输入输出用独热向量</strong>表示</li></ul><h4 id="2-3-2-激活函数的选择"><a href="#2-3-2-激活函数的选择" class="headerlink" title="2.3.2 激活函数的选择"></a>2.3.2 激活函数的选择</h4><ul><li><p>对于下一个时间步隐藏状态的计算，激活函数一般选用Tanh或者ReLu。对输出的运算，一般做softmax运算。</p></li><li><p>但是很多时候还是选用Tanh而不是ReLu，这一点在Hinton的<a href="https://arxiv.org/abs/1504.00941">IRNN论文</a>里面是很明确的提到的：</p></li></ul><p><img src="https://pic2.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_720w.jpg?source=1940ef5c" alt="img"></p><p><strong>也就是说在RNN中直接把激活函数换成ReLu会导致非常大的输出值</strong></p><ul><li>下面具体来看一下：</li></ul><script type="math/tex; mode=display">\begin{array}{c}\text { net }_{t}=U x_{t}+W h_{t-1} \\h_{t}=f\left(\text { net }_{t}\right)\end{array}</script><p>假设ReLu函数一直处于激活区域（即输入大于0），则有<script type="math/tex">f(x) = x, net_t = Ux_t + W(Ux_{t-1} + Wh_{t-2})</script>，继续将其展开，最终会包含多个W的连乘，如果W不是单位矩阵， 那么最终的结果将会区域0或无穷。</p><p>但是在CNN中就不会发生这种问题，因为CNN中每一层的W是不同的，并且在初始化时它们是独立同分布的，因此可以相互抵消，在多层之后一般不会出现这种严重的数值问题</p><p>我们再来看看反向传播的时候：</p><script type="math/tex; mode=display">\frac{\partial net_t}{\partial net_{t-1}} = W</script><p>所以：</p><script type="math/tex; mode=display">\frac{\partial net_t}{\partial net_{1}} = W^n</script><p>可以看到只要W不是单位矩阵，梯度会出现消失或者爆炸的现象</p><ul><li>综上所述：当采用ReLu作为激活函数时，只有W在单位矩阵附近时才能取得比较好的结果，因此需要将W初始化为单位矩阵。但是ReLu也拥有自己的优点，那就是计算量更小，收敛更快</li></ul><h4 id="2-3-3-预测"><a href="#2-3-3-预测" class="headerlink" title="2.3.3 预测"></a>2.3.3 预测</h4><ul><li><p><strong>在训练时，下一个时间步的输入可以：</strong></p><ol><li>上一个时间步的标签</li><li>上一个时间步的表征（也就是输出）</li></ol><p>采用第一种方法，<strong>更容易收敛，但是泛化能力更差</strong>，而第二种方法<strong>更不易收敛，但是泛化能力更强</strong></p></li><li><p><strong>但是在预测时</strong>，由于我们压根没有标签，所以只能用上述第二种方法。正是由于训练时和预测时干的事情都不一样，所以采用第一种方法泛化能力更差</p></li></ul><h4 id="2-3-4-困惑度"><a href="#2-3-4-困惑度" class="headerlink" title="2.3.4 困惑度"></a>2.3.4 困惑度</h4><ul><li>我们通常使⽤困惑度（perplexity）来评价语⾔模型的好坏</li><li>困惑度是对交叉熵损失函数做指数运算后得到的值</li><li>特别的：</li></ul><ol><li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1</li><li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷</li><li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数</li></ol><p><strong>显然，任何一个有效模型的困惑度必须小于类别个数（要不然模型的预测效果还不如随机点一个类别来的好）</strong></p><h4 id="2-3-5-通过时间反向传播"><a href="#2-3-5-通过时间反向传播" class="headerlink" title="2.3.5 通过时间反向传播"></a>2.3.5 通过时间反向传播</h4><ul><li>我们需要将循环神经⽹络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应⽤反向传播计算并存储梯度</li><li>简单起⻅，我们考虑⼀个⽆偏差项的循环神经⽹络，且激活函数为恒等映射$（\phi(x) = x）$。设时间步t的输⼊为单样本$x_t \in R^d$，标签为$y_t$，那么隐藏状态$h_t \in R^h$的计算表达式为:</li></ul><script type="math/tex; mode=display">h_t = W_{hx}X_t + W_{hh}h_{t-1}</script><p>输出层变量$o_t \in \mathbb{R}^q$为：</p><script type="math/tex; mode=display">o_t = W_{qh}h_t</script><p>设时间步t的损失为$\ell(o_t, y_t)$。则总时间步数为T的损失函数L定义为：</p><script type="math/tex; mode=display">L = \frac{1}{T}\sum_{t = 1}^T \ell(o_t, y_t)</script><ul><li><p>我们假设一共有3个时间步数，那么可以做出计算图：<br><img src="https://s2.loli.net/2021/12/10/Feq2PM3nCslpGO7.png" alt="image-20211210132156070"></p></li><li><p>现在我们开始反向传播：</p></li></ul><p>易得：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \boldsymbol{o}_{t}}=\frac{\partial \ell\left(\boldsymbol{o}_{t}, y_{t}\right)}{T \cdot \partial \boldsymbol{o}_{t}}</script><p>然后可以得到：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \boldsymbol{W}_{q h}}=\sum_{t=1}^{T} \operatorname{prod}\left(\frac{\partial L}{\partial \boldsymbol{o}_{t}}, \frac{\partial \boldsymbol{o}_{t}}{\partial \boldsymbol{W}_{q h}}\right)=\sum_{t=1}^{T} \frac{\partial L}{\partial \boldsymbol{o}_{t}} \boldsymbol{h}_{t}^{\top} .</script><p>接下来就是计算关于<script type="math/tex">W_{hx}</script>和<script type="math/tex">W_{hh}</script>的梯度，那么必然要求关于各时间步隐藏状态的梯度，我们注意到隐藏状态之间也存在依赖关系，，L只通过<script type="math/tex">o_T</script>依赖最终时间步T的隐藏状态<script type="math/tex">h_T</script>。因此，我们先计算⽬标函数有关最终时间步隐藏状态的梯度：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \boldsymbol{h}_{T}}=\operatorname{prod}\left(\frac{\partial L}{\partial \boldsymbol{o}_{T}}, \frac{\partial \boldsymbol{o}_{T}}{\partial \boldsymbol{h}_{T}}\right)=\boldsymbol{W}_{q h}^{\top} \frac{\partial L}{\partial \boldsymbol{o}_{T}}</script><p>接下来对于时间步t &lt; T，在图6.3中，$L$通过$h_{t+1}$和$o_t$依赖$h_t$。依据链式法则，⽬标函数有关时间步t &lt; T的隐藏状态的梯度$\partial L / \partial h_t \in R^h$需要按照时间步从⼤到小依次计算:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \boldsymbol{h}_{t}}=\operatorname{prod}\left(\frac{\partial L}{\partial \boldsymbol{h}_{t+1}}, \frac{\partial \boldsymbol{h}_{t+1}}{\partial \boldsymbol{h}_{t}}\right)+\operatorname{prod}\left(\frac{\partial L}{\partial \boldsymbol{o}_{t}}, \frac{\partial \boldsymbol{o}_{t}}{\partial \boldsymbol{h}_{t}}\right)=\boldsymbol{W}_{h h}^{\top} \frac{\partial L}{\partial \boldsymbol{h}_{t+1}}+\boldsymbol{W}_{q h}^{\top} \frac{\partial L}{\partial \boldsymbol{o}_{t}} .</script><p>对次递归公式展开，我们可以得到对任意时间步1 ≤ t ≤ T，我们可以得到⽬标函数有关隐藏状态梯度的通项公式：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \boldsymbol{h}_{t}}=\sum_{i=t}^{T}\left(\boldsymbol{W}_{h h}^{\top}\right)^{T-i} \boldsymbol{W}_{q h}^{\top} \frac{\partial L}{\partial \boldsymbol{o}_{T+t-i}}</script><p>通过关于每个时间步隐藏状态的梯度，易求得关于<script type="math/tex">W_{hx}</script>和<script type="math/tex">W_{hh}</script>的梯度。</p><ul><li><strong>由上式中的指数项可⻅，当时间步数T较⼤或者时间步t较小时，⽬标函数有关隐藏状态的梯度较容易出现衰减和爆炸</strong></li><li>RNN和一般的网络一样，我们在正向传播和反向传播的时候会进行一些数据的储存，避免重复计算</li></ul><h3 id="2-4-梯度裁剪"><a href="#2-4-梯度裁剪" class="headerlink" title="2.4 梯度裁剪"></a>2.4 梯度裁剪</h3><ul><li>循环神经⽹络中较容易出现梯度衰减或梯度爆炸。<strong>为了应对梯度爆炸</strong>，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成⼀个向量 $g$，并设裁剪的阈值是$\theta$。裁剪后的梯度：</li></ul><script type="math/tex; mode=display">\min \left(\frac{\theta}{\|g\|}, 1\right) g</script><p>的$L_2$范数不超过$\theta$</p><ul><li><strong>但是梯度裁剪无法应对梯度衰减</strong></li></ul><h1 id="3-门控循环单元（GRU）"><a href="#3-门控循环单元（GRU）" class="headerlink" title="3 门控循环单元（GRU）"></a>3 门控循环单元（GRU）</h1><ul><li>我们发现，当时间步数较⼤或者时间步较小时，循环神经⽹络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但⽆法解决梯度衰减的问题。通常由于这个原因，<strong>循环神经⽹络在实际中较难捕捉时间序列中时间步距离较⼤的依赖关系</strong></li><li>门控循环神经⽹络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较⼤的依赖关系。它通过可以学习的⻔来控制信息的流动。其中，门控循环单元（gated recurrent unit，GRU）是⼀种常⽤的⻔控循环神经⽹</li></ul><h3 id="3-1-重置门和更新门"><a href="#3-1-重置门和更新门" class="headerlink" title="3.1 重置门和更新门"></a>3.1 重置门和更新门</h3><ul><li>⻔控循环单元中的重置⻔和更新⻔的输⼊均为当前时间步输⼊<script type="math/tex">X_t</script>与上⼀时间步隐藏状态<script type="math/tex">H_{t−1}</script>，输出由激活函数为sigmoid函数的全连接层计算得到。</li></ul><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20211210135706726.png" alt="image-20211210135706726"></p><p>具体来说，重置门$R_t \in \mathbb{R}^{n \times h}$和更新门$Z_t \in \mathbb{R}^{n \times h}$的计算如下：</p><script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{R}_{t}=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x r}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h r}+\boldsymbol{b}_{r}\right) \\\boldsymbol{Z}_{t}=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x z}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h z}+\boldsymbol{b}_{z}\right)\end{array}</script><h3 id="3-2-候选隐藏状态"><a href="#3-2-候选隐藏状态" class="headerlink" title="3.2 候选隐藏状态"></a>3.2 候选隐藏状态</h3><ul><li><p>我们将当前时间步重置⻔的输出与上⼀时间步隐藏状态做按元素乘法（符号为$\odot$）。如果重置⻔中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上⼀时间步的隐藏状态。如果元素值接近1，那么表⽰保留上⼀时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输⼊连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态</p></li><li><p>具体来说，时间步t的候选隐藏状态$\tilde{H}_t \in \mathbb{R}^{n \times h}$的计算为：</p></li></ul><script type="math/tex; mode=display">\tilde{\boldsymbol{H}}_{t}=\tanh \left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}+\left(\boldsymbol{R}_{t} \odot \boldsymbol{H}_{t-1}\right) \boldsymbol{W}_{h h}+\boldsymbol{b}_{h}\right),</script><ul><li><p><strong>重置门控制了上⼀时间步的隐藏状态如何流⼊当前时间步的候选隐藏状态。而上⼀时间步的隐藏状态可能包含了时间序列截⾄上⼀时间步的全部历史信息。因此，重置门可以⽤来丢弃与预测⽆关的历史信息</strong></p></li><li><p>最后，时间步t的隐藏状态<script type="math/tex">H_t \in \mathbb{R}^{n×h}</script>的计算使⽤当前时间步的更新门<script type="math/tex">Z_t</script>来对上⼀时间步的隐藏状态<script type="math/tex">H_{t−1}</script>和当前时间步的候选隐藏状态<script type="math/tex">\tilde{H}_t</script>做组合：</p></li></ul><script type="math/tex; mode=display">\boldsymbol{H}_{t}=\boldsymbol{Z}_{t} \odot \boldsymbol{H}_{t-1}+\left(1-\boldsymbol{Z}_{t}\right) \odot \tilde{\boldsymbol{H}}_{t}</script><p><img src="https://s2.loli.net/2021/12/10/L7O69pm3Yjzbno5.png" alt="image-20211210141824572"></p><ul><li><strong>更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新</strong>。假设更新⻔在时间步$t’$到$t（t’ &lt; t）$之间⼀直近似1。那么，<strong>在时间步$t’$到$t$之间的输⼊信息⼏乎没有流⼊时间步$t$的隐藏状态$H_t$</strong>。实际上，<strong>这可以看作是较早时刻的隐藏状态$H_{t’−1}$⼀直通过时间保存并传递⾄当前时间步$t$</strong>。这个设计可以应对循环神经⽹络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较⼤的依赖关系</li><li>我们稍作总结：</li></ul><ol><li>重置⻔有助于捕捉时间序列⾥短期的依赖关系</li><li>更新⻔有助于捕捉时间序列⾥⻓期的依赖关系</li><li>重置门和更新门为0、1的情况：</li></ol><div class="table-container"><table><thead><tr><th>对应情况</th><th>重置门</th><th>更新门</th></tr></thead><tbody><tr><td>退化为一般的RNN</td><td>1</td><td>0</td></tr><tr><td>丢弃当前时间步的全部信息，只保留历史信息</td><td>0或1</td><td>1</td></tr><tr><td>完全丢弃历史信息，只保留当前时间步的信息</td><td>0</td><td>0</td></tr></tbody></table></div><h1 id="4-长短期记忆（LSTM）"><a href="#4-长短期记忆（LSTM）" class="headerlink" title="4 长短期记忆（LSTM）"></a>4 长短期记忆（LSTM）</h1><ul><li>还有另外一种十分常用的门控循环神经网络：⻓短期记忆（long short-term memory，LSTM）</li></ul><h3 id="4-1-输⼊门、遗忘门、输出门和候选记忆细胞"><a href="#4-1-输⼊门、遗忘门、输出门和候选记忆细胞" class="headerlink" title="4.1 输⼊门、遗忘门、输出门和候选记忆细胞"></a>4.1 输⼊门、遗忘门、输出门和候选记忆细胞</h3><ul><li>与门控循环单元中的重置门和更新门⼀样，⻓短期记忆的门的输⼊均为当前时间步输⼊<script type="math/tex">X_t</script>与上⼀时间步隐藏状态<script type="math/tex">H_{t−1}</script>，输出由激活函数为sigmoid函数的全连接层计算得到</li><li>⻓短期记忆需要计算候选记忆细胞$\tilde{C}_t$。它的计算与上⾯介绍的3个⻔类似，但使⽤了值域在[−1, 1]的tanh函数作为激活函数:</li></ul><p><img src="https://s2.loli.net/2021/12/10/jzn954wmEqRfyIg.png" alt="image-20211210144009641"></p><ul><li>时间步t的输⼊⻔$I_t \in R^{n×h}$、遗忘⻔$F_t \in R^{n×h}$和输出⻔$O_t \in R^{n×h}$分别计算如下：</li></ul><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{I}_{t} &=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x i}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h i}+\boldsymbol{b}_{i}\right), \\\boldsymbol{F}_{t} &=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x f}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h f}+\boldsymbol{b}_{f}\right), \\\boldsymbol{O}_{t} &=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x o}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h o}+\boldsymbol{b}_{o}\right), \\\tilde{\boldsymbol{C}}_{t}&=\tanh \left(\boldsymbol{X}_{t} \boldsymbol{W}_{x c}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h c}+\boldsymbol{b}_{c}\right),\end{aligned}</script><h3 id="4-2-记忆细胞"><a href="#4-2-记忆细胞" class="headerlink" title="4.2 记忆细胞"></a>4.2 记忆细胞</h3><ul><li>当前时间步记忆细胞$C_t \in R^{n\times h}$的计算组合了上⼀时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘⻔和输⼊⻔来控制信息的流动：</li></ul><script type="math/tex; mode=display">\boldsymbol{C}_{t}=\boldsymbol{F}_{t} \odot \boldsymbol{C}_{t-1}+\boldsymbol{I}_{t} \odot \tilde{\boldsymbol{C}}_{t}</script><ul><li>遗忘门控制上⼀时间步的记忆细胞$C_{t−1}$中的信息是否传递到当前时间步，而输⼊门则控制当前时间步的输⼊$X_t$通过候选记忆细胞$\tilde{C}_t$如何流⼊当前时间步的记忆细胞。如果遗忘门⼀直近似1且输⼊门⼀直近似0，过去的记忆细胞将⼀直通过时间保存并传递⾄当前时间步</li></ul><h3 id="4-3-隐藏状态"><a href="#4-3-隐藏状态" class="headerlink" title="4.3 隐藏状态"></a>4.3 隐藏状态</h3><ul><li>有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$H_t \in R^{n×h}$的信息的流动：</li></ul><script type="math/tex; mode=display">H_t = O_t \odot tanh(C_t)</script><ul><li>当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使⽤；当输出门近似0时，记忆细胞信息只⾃⼰保留</li></ul><p><img src="https://s2.loli.net/2021/12/10/mlYhrVbA47MByvG.png" alt="image-20211210145754660"></p><h3 id="4-4-三种门的作用"><a href="#4-4-三种门的作用" class="headerlink" title="4.4 三种门的作用"></a>4.4 三种门的作用</h3><ul><li><strong>输入门控制当前计算的新状态以多大程度更新到记忆单元中</strong></li><li><strong>遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉</strong></li><li><strong>输出门控制当前的输出有多大程度取决于当前的记忆单元</strong></li><li>在一个训练好的网络中：</li></ul><ol><li>当输入的序列中没有重要信息时，LSTM的遗忘门接近于1，输入门接近于0，此时过去的记忆会被保存，从而实现了长期记忆功能</li><li>当输入的序列中出现了重要的信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近1</li><li>当输入的序列中出现了重要的信息，且该信息意味着之前的记忆不再重要时，输入门的值接近1，而遗忘门的值接近于0，这样旧的记忆被遗忘，新的重要信息被记忆</li></ol><h3 id="4-5-LSTM各模块激活函数"><a href="#4-5-LSTM各模块激活函数" class="headerlink" title="4.5 LSTM各模块激活函数"></a>4.5 LSTM各模块激活函数</h3><ul><li>LSTM中，三种门都用的Sigmoid函数，生成候选记忆时，使用Tanh作为激活函数。值得注意的是，<strong>这两个激活函数都是饱和的</strong>，也就是说在输入达到一定值的情况下，输出就不发生明显变化了。如果使用的是非饱和的激活函数，例如ReLu，将难以实现门控的效果</li><li>Sigmoid的输出在0到1之间，符合门控的物理定义。且当输入较大或较小时，输出会非常接近1或0，从而保证门开或门关</li><li>在生成候选记忆时，使用Tanh函数，是因为其输出在-1到1之间，这与大多数场景下特征分布是zero-centered吻合</li><li>此为Tanh函数在输入为0附近相比于Sigmoid有更大的梯度，通常收敛更快</li></ul><h1 id="5-深度循环神经网络"><a href="#5-深度循环神经网络" class="headerlink" title="5 深度循环神经网络"></a>5 深度循环神经网络</h1><ul><li>到⽬前为⽌介绍的循环神经⽹络只有⼀个单向的隐藏层，在深度学习应⽤⾥，我们通常会⽤到含有多个隐藏层的循环神经⽹络，也称作深度循环神经网络</li><li>下图演⽰了⼀个有L个隐藏层的深度循环神经⽹络，<strong>每个隐藏状态不断传递⾄当前层的下⼀时间步和当前时间步的下⼀层</strong></li></ul><p><img src="https://s2.loli.net/2021/12/10/PUK7CRjyemEbzLM.png" alt="image-20211210150125737"></p><ul><li>第1隐藏层的隐藏状态和之前的计算⼀样：</li></ul><script type="math/tex; mode=display">\boldsymbol{H}_{t}^{(1)}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}^{(1)}+\boldsymbol{H}_{t-1}^{(1)} \boldsymbol{W}_{h h}^{(1)}+\boldsymbol{b}_{h}^{(1)}\right)</script><ul><li>当1 &lt; l ≤ L时，第l隐藏层的隐藏状态的表达式为:</li></ul><script type="math/tex; mode=display">\boldsymbol{H}_{t}^{(l)}=\phi\left(\boldsymbol{H}_{t}^{(l-1)} \boldsymbol{W}_{x h}^{(l)}+\boldsymbol{H}_{t-1}^{(l)} \boldsymbol{W}_{h h}^{(l)}+\boldsymbol{b}_{h}^{(l)}\right)</script><ul><li>最终，输出层的输出只需基于第L隐藏层的隐藏状态：</li></ul><script type="math/tex; mode=display">\boldsymbol{O}_{t}=\boldsymbol{H}_{t}^{(L)} \boldsymbol{W}_{h q}+\boldsymbol{b}_{q}</script><ul><li><p>同多层感知机⼀样，隐藏层个数L是超参数</p></li><li><p><strong>RNN中选择深度网络的原因和一般前馈神经网络一样，都是为了能够用来表征更复杂的情况</strong></p></li></ul><h1 id="6-双向循环神经⽹络"><a href="#6-双向循环神经⽹络" class="headerlink" title="6 双向循环神经⽹络"></a>6 双向循环神经⽹络</h1><ul><li>之前介绍的循环神经⽹络模型都是假设当前时间步是由前⾯的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后⾯时间步决定。例如，当我们写下⼀个句⼦时，可能会根据句⼦后⾯的词来修改句⼦前⾯的⽤词</li><li><strong>双向循环神经⽹络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息</strong>，下图演⽰了⼀个含单隐藏层的双向循环神经⽹络的架构</li></ul><p><img src="https://s2.loli.net/2021/12/10/VtFmigZ58qw2aeb.png" alt="image-20211210151934211"></p><ul><li>具体来看，设时间步t正向隐藏状态为$\overrightarrow{H}_t \in R^{n×h}$（正向隐藏单元个数为h），反向隐藏状态为$\overleftarrow{H}_t \in R^{n×h}$（反向隐藏单元个数为h）。我们可以分别计算正向隐藏状态和反向隐藏状态：</li></ul><script type="math/tex; mode=display">\begin{array}{l}\overrightarrow{\boldsymbol{H}}_{t}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}^{(f)}+\overrightarrow{\boldsymbol{H}}_{t-1} \boldsymbol{W}_{h h}^{(f)}+\boldsymbol{b}_{h}^{(f)}\right) \\\overleftarrow{\boldsymbol{H}}_{t}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}^{(b)}+\overleftarrow{\boldsymbol{H}}_{t+1} \boldsymbol{W}_{h h}^{(b)}+\boldsymbol{b}_{h}^{(b)}\right)\end{array}</script><p><strong>然后我们连结两个⽅向的隐藏状态$\overrightarrow{H}_t$和 $\overleftarrow{H}_t$来得到隐藏状态$H_t \in R^{n×2h}$</strong>，并将其输⼊到输出层。 输出层计算输出$O_t \in R^{n×q}$（输出个数为q）：</p><script type="math/tex; mode=display">O_t = H_tW_{hq} + b_q</script><ul><li><strong>不同方向的隐藏状态的隐藏单元个数也可以不同</strong></li><li>双向循环神经⽹络在每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊）</li></ul><h1 id="7-Seq2Seq模型"><a href="#7-Seq2Seq模型" class="headerlink" title="7 Seq2Seq模型"></a>7 Seq2Seq模型</h1><ul><li>在许多时候，输入和输出都是不定长序列，这采用一般的RNN肯定是行不通的。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，例如：英语输⼊：“They”、“are”、“watching”、“.” ； 法语输出：“Ils”、“regardent”、“.”</li><li>当输⼊和输出都是不定⻓序列时，我们可以使⽤seq2seq模型，本质上都⽤到了两个循环神经网络，分别叫做编码器和解码器。<strong>编码器用来分析输⼊序列，解码器⽤来生成输出序列</strong></li></ul><p><img src="https://s2.loli.net/2021/12/10/FQfVxdq2wuC7aSs.png" alt="image-20211210170200879"></p><ul><li>&lt; bos &gt;（beginning of sequence）和 &lt; eos &gt;（end of sequence）分别表示序列的开始和结束</li><li>编码器每个时间步的输⼊依次为英语句⼦中的单词、标点和特殊符号。上图中使⽤了编码器在最终时间步的隐藏状态作为输⼊句⼦的表征或编码信息。解码器在各个时间步中使⽤输⼊句⼦的编码信息和上个时间步的输出以及隐藏状态作为输⼊</li></ul><h3 id="7-1-编码器"><a href="#7-1-编码器" class="headerlink" title="7.1 编码器"></a>7.1 编码器</h3><ul><li><strong>编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量$c$，并在该背景变量中编码输⼊序列信息</strong>。编码器可以使⽤循环神经网络</li><li>编码器通过⾃定义函数q将各个时间步的隐藏状态变换为背景变量</li></ul><script type="math/tex; mode=display">c = q(h_1, ...,h_T)</script><ul><li><strong>也可以使⽤双向循环神经⽹络构造编码器</strong>，在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊），并编码了整个序列的信息。</li></ul><h3 id="7-2-解码器"><a href="#7-2-解码器" class="headerlink" title="7.2 解码器"></a>7.2 解码器</h3><ul><li>解码器就是通过编码器输出的背景向量$c$，和每一个时间步的输入，输出所需要的结果</li><li><strong>解码器在预测和训练时是不一样的</strong>，我们先介绍<strong>预测</strong>时的解码器：</li></ul><p>编码器输出的背景变量$c$编码了整个输⼊序列<script type="math/tex">x_1, . . . , x_T</script>的信息。给定训练样本中的输出序列<script type="math/tex">y_1, y_2, . . . , y_{T'}</script>，对每个时间步<script type="math/tex">t'</script>（符号与输⼊序列或编码器的时间步<script type="math/tex">t</script>有区别），解码器输出<script type="math/tex">y_{t'}</script>的条件概率将基于之前的输出序列<script type="math/tex">y_1, . . . , y_{t'−1}</script>和背景变量<script type="math/tex">c</script>，即<script type="math/tex">P(y_{t'} | y_1, . . . , y_{t'−1}, c)</script></p><p>为此，我们可以使⽤另⼀个循环神经网络作为解码器。在输出序列的时间步<script type="math/tex">t'</script>，解码器将上⼀时间步的输出<script type="math/tex">y_{t'−1}</script>以及背景变量<script type="math/tex">c</script>作为输⼊，并将它们与上⼀时间步的隐藏状态<script type="math/tex">s_{t'−1}</script>变换为当前时间步的隐藏状态<script type="math/tex">s_{t'}</script>：</p><script type="math/tex; mode=display">s_{t'} = g(y_{t' -1}, c, s_{t' - 1})</script><p>有了解码器的隐藏状态后，我们可以使⽤⾃定义的输出层和softmax运算来计算<script type="math/tex">P(y_{t'} | y_1, . . . , y_{t'−1}, c)</script></p><ul><li><strong>训练</strong>时的解码器，每一时间步的输入序列可以是上一时间步的输出，也可以是上一时间步的真实标签序列。后者叫做<strong>强制教学</strong></li></ul><p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率：</p><script type="math/tex; mode=display">\begin{aligned}P\left(y_{1}, \ldots, y_{T^{\prime}} \mid x_{1}, \ldots, x_{T}\right) &=\prod_{t^{\prime}=1}^{T^{\prime}} P\left(y_{t^{\prime}} \mid y_{1}, \ldots, y_{t^{\prime}-1}, x_{1}, \ldots, x_{T}\right) \\&=\prod_{t^{\prime}=1}^{T^{\prime}} P\left(y_{t^{\prime}} \mid y_{1}, \ldots, y_{t^{\prime}-1}, c\right)\end{aligned}</script><p>并得到该输出序列的损失：</p><script type="math/tex; mode=display">-\log P\left(y_{1}, \ldots, y_{T^{\prime}} \mid x_{1}, \ldots, x_{T}\right)=-\sum_{t^{\prime}=1}^{T^{\prime}} \log P\left(y_{t^{\prime}} \mid y_{1}, \ldots, y_{t^{\prime}-1}, c\right)</script><p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数</p><h3 id="7-3-预测时的搜索方式"><a href="#7-3-预测时的搜索方式" class="headerlink" title="7.3 预测时的搜索方式"></a>7.3 预测时的搜索方式</h3><h4 id="7-3-1-贪婪搜索"><a href="#7-3-1-贪婪搜索" class="headerlink" title="7.3.1 贪婪搜索"></a>7.3.1 贪婪搜索</h4><ul><li>贪婪搜索（greedy search）。对于输出序列任⼀时间步$t’$，我 们从$|Y|$（Y为词典）个词中搜索出条件概率最⼤的词：</li></ul><script type="math/tex; mode=display">y_{t'} = argmax_{y \in Y}P(y|y_1, ..., y_{t' - 1}, c)</script><p>作为输出。⼀旦搜索出“&lt; eos &gt;”符号，或者输出序列⻓度已经达到了最⼤⻓度$T’$，便完成输出。</p><ul><li>我们将该条件概率最⼤的输出序列称为<strong>最优输出序列</strong>，<strong>而贪婪搜索无法保证得到最优输出序列</strong>，下面举个栗子：</li></ul><blockquote><p>假设输出词典⾥⾯有“A” “B” “C” 和“&lt; eos &gt;”这4个词。下图中每个时间步下的4个数字分别代表了该时间步⽣成“A” “B” “C” 和“&lt; eos &gt;”这4个词的条件概率。在每个 间步，贪婪搜索选取条件概率最⼤的词。因此，将⽣成输出序列“A” “B” “C” “&lt; eos &gt;”。 该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 = 0.048</p></blockquote><p><img src="https://s2.loli.net/2021/12/10/JUgILXFCjWviaQw.png" alt="image-20211210180148791"></p><blockquote><p>但是现在我们考虑下面一种情况，在时间步2中选取了条件概率第⼆⼤的词“C”，<strong>由于之后时间步的概率值是基于前面的时间步的，所以时间步2结果选取的改变，会导致后续的概率发生改变</strong>，如下图，所以我们现在的输出序列“A” “C” “B” “&lt; eos &gt;”的条件概率是0.5 × 0.3 × 0.6 × 0.6 = 0.054，大于贪婪搜索的概率，所以贪婪搜索得到的不是最优的</p></blockquote><p><img src="https://s2.loli.net/2021/12/10/zSKDLamFWqseyuN.png" alt="image-20211210180404095"></p><h4 id="7-3-2-穷举搜索"><a href="#7-3-2-穷举搜索" class="headerlink" title="7.3.2 穷举搜索"></a>7.3.2 穷举搜索</h4><ul><li>我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最⼤的序列</li><li><strong>虽然穷举搜索可以得到最优输出序列，但它的计算开销$O(|Y|^{T’})$很容易过⼤</strong>，而贪婪搜索的开销为$O(|Y|T’)$，明显小于穷举搜索</li></ul><h4 id="7-3-3-束搜索"><a href="#7-3-3-束搜索" class="headerlink" title="7.3.3 束搜索"></a>7.3.3 束搜索</h4><ul><li><p>束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个<strong>束宽</strong>（beam size）超参数。我们将它设为k。在时间步1时，选取当前时间步条件概率最⼤的k个词，分别组成k个候选输出序列的⾸词。在之后的每个时间步，基于上个时间步的k个候选输出序列，从k$ |Y|$个可能的输出序列中选取条件概率最⼤的k个，作为该时间步的候选输出序列。最终，<strong>我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt; eos &gt;”的序列，并将它们中所有特殊符号“&lt; eos &gt;”后⾯的⼦序列舍弃</strong>，得到最终候选输出序列的集合</p></li><li><p>下面举个栗子：</p></li></ul><p><img src="https://s2.loli.net/2021/12/10/QEBb3gUVxmuDS7O.png" alt="image-20211210182131362"></p><p>第一个时间步找出概率最大的”A”和”C”，然后再根据”A”和”C”，在时间步2中寻找概率最大的2个，分别为”AB”和”CE”，再根据这两个，在时间步3，输出”ABD”和”CED”，最后减去&lt; eos &gt;符号以后的内容，得出输出序列</p><ul><li>在最终候选输出序列的集合中，我们取以下分数最⾼的序列作为输出序列：</li></ul><script type="math/tex; mode=display">\frac{1}{L^{\alpha}} \log P\left(y_{1}, \ldots, y_{L}\right)=\frac{1}{L^{\alpha}} \sum_{t^{\prime}=1}^{L} \log P\left(y_{t^{\prime}} \mid y_{1}, \ldots, y_{t^{\prime}-1}, c\right)</script><p>其中$L$为最终候选序列⻓度，$\alpha$⼀般可选为0.75。分⺟上的$L^{\alpha}$是为了惩罚较⻓序列在以上分数中较多的对数相加项</p><ul><li>束搜索的计算开销为$O(k|Y|T’)$，介于贪婪搜索和穷举搜索之间</li><li><strong>束搜索通过灵活的束宽来权衡计算开销和搜索质量</strong></li></ul><h3 id="7-4-注意力机制"><a href="#7-4-注意力机制" class="headerlink" title="7.4 注意力机制"></a>7.4 注意力机制</h3><h4 id="7-4-1-seq2seq中的注意力机制"><a href="#7-4-1-seq2seq中的注意力机制" class="headerlink" title="7.4.1 seq2seq中的注意力机制"></a>7.4.1 seq2seq中的注意力机制</h4><ul><li><strong>解码器在⽣成输出序列中的每⼀个词时可能只需利⽤输⼊序列某⼀部分的信息， 而不需要由整个输入序列生成的背景向量</strong>，例如，在输出序列的时间步1，解码器可以主要依赖“They” “are” 的信息来⽣成“Ils”，在时间步2则主要使⽤来⾃“watching”的编码信息⽣成“regardent”，而不需要”They are watching”整个句子</li><li><p>若没引入注意力机制，在实际使用中，那么会发现随着输入序列的增长，模型的性能发生了显著下降。<strong>这是因为编码时输入序列的全部信息压缩到了一个向量中，随着序列的增长，越前面的词丢失越严重</strong>。<strong>同时，seq2seq模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词对应的源语言词的上下文信息和位置信息在编解码过程中丢失了</strong></p></li><li><p>注意⼒机制通过对编码器所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量。解码器在每⼀时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量</p></li></ul><p><img src="https://s2.loli.net/2021/12/10/5kGV8uqeaYn3wlF.png" alt="image-20211210184622950"></p><p>⾸先，函数a根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输⼊。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量</p><ul><li>具体来说，令编码器在时间步t的隐藏状态为$h_t$，且总时间步数为T。那么解码器在时间步$t’$（$t’$代表的是解码器的时间步数）的背景变量为所有编码器隐藏状态的加权平均：</li></ul><script type="math/tex; mode=display">\boldsymbol{c}_{t^{\prime}}=\sum_{t=1}^{T} \alpha_{t^{\prime} t} \boldsymbol{h}_{t}</script><p>其中给定$t’$时，权重$\alpha_{t’t}$在$t = 1, . . . , T$的值是⼀个概率分布。为了得到概率分布，我们可以使⽤softmax运算:</p><script type="math/tex; mode=display">\alpha_{t^{\prime} t}=\frac{\exp \left(e_{t^{\prime} t}\right)}{\sum_{k=1}^{T} \exp \left(e_{t^{\prime} k}\right)}, \quad t=1, \ldots, T</script><p>由于<script type="math/tex">e_{t't}</script>同时取决于解码器的时间步<script type="math/tex">t'</script>和编码器的时间步t，我们不妨以解码器在时间步<script type="math/tex">t'− 1</script>的隐藏状态<script type="math/tex">s_{t'−1}</script>与编码器在时间步t的隐藏状态<script type="math/tex">h_t</script>为输⼊，并通过函数a计算<script type="math/tex">e_{t't}</script>：</p><script type="math/tex; mode=display">e_{t't} = a(s_{t'-1}, h_t)</script><p>其中a是一个自定义函数，有多种选择。如果两个输⼊向量⻓度相同，⼀个简单的选择是计算它们的内积$a(s,h) = s^⊤h$</p><ul><li>我们对注意力机制可以有一个直观的理解：<strong>在生成一个输出词时，会考虑每一个输入词和当前输出词的对应关系，对齐越好的词，会有越大的权重</strong></li><li>还可以结合双向RNN和注意力机制，每个隐状态包含了$\overleftarrow{h_t}$和$\overrightarrow{h_t}$</li></ul><h4 id="7-4-2-矢量化计算"><a href="#7-4-2-矢量化计算" class="headerlink" title="7.4.2 矢量化计算"></a>7.4.2 矢量化计算</h4><ul><li><p>我们还可以对注意⼒机制采⽤更⾼效的⽮量化计算。⼴义上，注意⼒机制的输⼊包括<strong>查询项</strong>以及<strong>⼀⼀对应的键项和值项</strong>，其中值项是需要加权平均的⼀组项。在加权平均中，<strong>值项的权重来⾃查询项以及与该值项对应的键项的计算</strong></p></li><li><p>在上面的例⼦中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。假设编码器和解码器的隐藏单元个数均为h，且函数<script type="math/tex">a(s, h) = s^ ⊤h</script>。若我们希望根据解码器单个隐藏状态<script type="math/tex">s_{t'−1} \in \mathbb{R}^h</script>和编码器所有隐藏状态<script type="math/tex">h_t \in \mathbb{R}^h , t = 1, . . . , T</script>来计算背景向量<script type="math/tex">c_{t'} \in \mathbb{R}^h</script>。我们可以将查询项矩阵<script type="math/tex">Q \in R^{1\times h}</script>设为<script type="math/tex">s^⊤_{t'−1}</script>，并令键项矩阵<script type="math/tex">K \in \mathbb{R}^{T\times h}</script>和值项矩阵<script type="math/tex">V \in \mathbb{R}^{T\times h}</script>相同且第t⾏均为<script type="math/tex">h^⊤_t</script> 。此时，我们只需要通过⽮量化计算：</p></li></ul><script type="math/tex; mode=display">softmax(QK^T)V</script><p>即可算出转置后的背景向量<script type="math/tex">c^⊤_{t'}</script>。当查询项矩阵Q的⾏数为n时，上式将得到n行的输出矩阵。输出矩阵与查询项矩阵在相同行上⼀⼀对应。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>CNN基本概念</title>
    <link href="/2021/11/26/CNN/"/>
    <url>/2021/11/26/CNN/</url>
    
    <content type="html"><![CDATA[<h1 id="1-卷积层"><a href="#1-卷积层" class="headerlink" title="1 卷积层"></a>1 卷积层</h1><h3 id="1-1-互相关运算"><a href="#1-1-互相关运算" class="headerlink" title="1.1 互相关运算"></a>1.1 互相关运算</h3><ul><li><strong>在⼆维卷积层中，⼀个⼆维输⼊数组和⼀个⼆维核（kernel）数组通过互相 关运算输出⼀个⼆维数组</strong>，如下图：</li></ul><p><img src="https://i.loli.net/2021/11/24/8amf1EYbuU7JPKd.png" alt="image-20211124131936578"></p><p>卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依 次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘 并求和，得到输出数组中相应位置的元素</p><ul><li><strong>⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出</strong></li><li><strong>使用互相关运算做边缘检测：</strong></li></ul><p>比如我们把kernel设为[1, -1]，要识别的图像为单通道，只有黑白：<img src="https://i.loli.net/2021/11/24/Uro3neFEKigbCd2.png" alt="image-20211124132335469"></p><p>则进行互相关运算之后可以变为：<img src="https://i.loli.net/2021/11/24/kuH17Ef3p8Rbjdi.png" alt="image-20211124132405678"></p><p>由此我们可以看出边缘是在第2和第6列</p><ul><li><strong>卷积运算：</strong>其实就是把核数组左右翻转并 上下翻转，再与输⼊数组做互相关运算。但因为我们的参数都是学习出来的，而不是一开始设定好的，<strong>所以卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出</strong>，而我们一般使用的也是互相关运算</li></ul><h3 id="1-2-特征图和感受野"><a href="#1-2-特征图和感受野" class="headerlink" title="1.2 特征图和感受野"></a>1.2 特征图和感受野</h3><ul><li><strong>⼆维卷积层输出的⼆维数组可以看作输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）</strong></li><li><strong>影响元素x的前向计算的所有可能输⼊区域叫做x的感受野（receptive field）</strong>，用上图距离，阴影中19的感受野就是输入的四个阴影，如果输入的元素又是另一个卷积运算的输出，则我们还可以将19的感受野扩大</li><li>可⻅，我们可以通过更深的卷积神经⽹络使特征 图中单个元素的感受野变得更加⼴阔，从而捕捉输⼊上更⼤尺⼨的特征</li></ul><h3 id="1-3-填充和步幅"><a href="#1-3-填充和步幅" class="headerlink" title="1.3 填充和步幅"></a>1.3 填充和步幅</h3><ul><li><strong>填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）</strong>，如下图就是对3$\times$3数组进行填充，填充为1：</li></ul><p><img src="https://i.loli.net/2021/11/24/YE4p8vg31IaWbuK.png" alt="image-20211124133849360"></p><ul><li><p><strong>我们将每次滑动的⾏数和列数称为步幅（stride）</strong></p></li><li><p><strong>一般来说输出的高宽为：</strong></p><script type="math/tex; mode=display">[(n_h - k_h + p_h + s_h) / s_h] \times [(n_w - k_w + p_w + s_w) / s_w]</script><p>其中：$n为输入，k为核，p为填充，s为步幅$</p></li><li><p>步幅为1时，很多情况下，我们会设置$p_h = k_h−1$和$p_w = k_w −1$来使输⼊和输出具有相同的⾼和宽</p></li></ul><h3 id="1-4-多输入通道和多输出通道"><a href="#1-4-多输入通道和多输出通道" class="headerlink" title="1.4 多输入通道和多输出通道"></a>1.4 多输入通道和多输出通道</h3><ul><li>当输入通道 $c_i &gt; 1$ 时，我们为每个输入通道都分配一个核数组，则得到一个形状为$c_i \times k_h \times k_w$的卷积核，最后再将每个通道上的卷积结果相加，就得到输出，如下图：</li></ul><p><img src="https://i.loli.net/2021/11/24/iNGt96JXUlSAEKZ.png" alt="image-20211124140843742"></p><ul><li>可是按上面的方法，无论输入通道是多少，输出通道总为1，我们设输入、输出通道分别为$c_i, c_o$，如果我们想要多通道的输出，则可以给每个输出通道都创建一个形状为$c_i \times k_h \times k_w$的核，则可以得到形状为$c_o \times c_i \times k_h \times k_w$的卷积核</li></ul><h3 id="1-5-1-times-1卷积层"><a href="#1-5-1-times-1卷积层" class="headerlink" title="1.5 1$\times$1卷积层"></a>1.5 1$\times$1卷积层</h3><ul><li>卷积窗口形状为1 × 1的多通道卷积层。我们通常称之为1 × 1卷 积层，并将其中的卷积运算称为1 × 1卷积</li><li>因为使⽤了最小窗口，1 × 1卷积失去了卷积层可以 识别⾼和宽维度上相邻元素构成的模式的功能。<strong>实际上，1 × 1卷积的主要计算发⽣在通道维上， 1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度</strong></li><li><strong>假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那 么1 × 1卷积层的作⽤与全连接层等价</strong></li></ul><h3 id="1-6-卷积层相对于全连接层的优点"><a href="#1-6-卷积层相对于全连接层的优点" class="headerlink" title="1.6 卷积层相对于全连接层的优点"></a>1.6 卷积层相对于全连接层的优点</h3><ul><li><strong>卷积层使图像的像素在⾼和宽两个⽅向上的相关性均可能被有效识别</strong>，用全连接层举例，全连接层就相当于一个很大的卷积核，但是过大的卷积核带来的问题是我们可能无法提取到有效特征，并且像素在高宽方向上的相关性可能提取不到，如果选择合适的卷积核大小才更可能提取到</li><li>卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重复计算，同一输入通道使用同一卷积核，<strong>实现参数共享</strong>，从而<strong>避免参数过多</strong>。同时参数共享也具有物理意义，他使卷积层<strong>具有平移等特性</strong>，比如图像中有一只猫，那么无论他在图像的任何位置，我们都能提取到他的主要特征，将他识别为猫</li><li>对于全连接层，<strong>任意一对输入和输出之间都会产生交互</strong>，形成稠密的连接结构</li></ul><p><img src="https://i.loli.net/2021/11/25/BuGFLnhli2NJPCv.png" alt="image-20211125234024022"></p><p>而在卷积神经网络中，卷积核尺度远小于输入的维度，<strong>这样每个输出神经元仅与前一层部分神经元产生交互</strong></p><p><img src="https://i.loli.net/2021/11/26/HBjoOyvEcqxSna6.png" alt="image-20211125234143051"></p><p>我们将这种特性称为<strong>稀疏交互</strong>，这样我们可以将<strong>优化过程的时间复杂度减少好几个数量级，并且缓解过拟合</strong></p><p>稀疏交互的物理意义是<strong>许多现实中的数据都具有局部的特征结构，我们可以先学习局部的特征，然后再将局部的特征组合起来形成更复杂和抽象的特征</strong></p><h1 id="2-池化层"><a href="#2-池化层" class="headerlink" title="2 池化层"></a>2 池化层</h1><ul><li><strong>池化层的提出是为了缓解卷积层对位置的过度敏感性</strong>。不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均值。该运算也分别叫做<strong>最⼤池化</strong>或<strong>平均池化</strong></li><li><strong>池化层没有参数</strong></li><li>Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，这个妥协会越来越小，下面介绍几种常用的池化层</li></ul><h3 id="2-1-平均池化层（mean-pooling）"><a href="#2-1-平均池化层（mean-pooling）" class="headerlink" title="2.1 平均池化层（mean-pooling）"></a>2.1 平均池化层（mean-pooling）</h3><ul><li>即对邻域内特征点只求平均</li><li>优缺点：<strong>抑制邻域大小受限造成的估计值方差增大</strong>，能很好的保留背景，但容易使得图片变模糊</li></ul><h3 id="2-2-最大池化层（max-pooling）"><a href="#2-2-最大池化层（max-pooling）" class="headerlink" title="2.2 最大池化层（max-pooling）"></a>2.2 最大池化层（max-pooling）</h3><ul><li>即对邻域内特征点取最大</li><li>优缺点：<strong>抑制积层参数误差造成估计均值的偏移</strong>，能很好的保留纹理特征，一般现在都用max-pooling而很少用mean-pooling</li></ul><h3 id="2-3-全局平均池化（global-average-pooling）"><a href="#2-3-全局平均池化（global-average-pooling）" class="headerlink" title="2.3 全局平均池化（global average pooling）"></a>2.3 全局平均池化（global average pooling）</h3><ul><li>对每个通道中所有元素求平均并直接⽤于分类</li><li>优点：大幅度减少网络参数，理所当然的减少了过拟合现象</li></ul><h3 id="2-4-池化层的作用"><a href="#2-4-池化层的作用" class="headerlink" title="2.4 池化层的作用"></a>2.4 池化层的作用</h3><ol><li><strong>对特征进行压缩，保留主要特征的同时减少参数和计算量，减少模型复杂度，防止过拟合。</strong>用池化层做互相关运算，本身就能减少特征的个数。并且对于全局平均优化，如果我们要进行图像分类，我们需要使用参数很多的全连接层，最后再导入Softmax，而如果我们使用全局平均优化，则可以规避全连接庞大的参数量，减少过拟合</li><li><strong>实现不变性</strong>，包括平移不变性、旋转不变性和尺度不变性。</li></ol><h3 id="2-5-池化层的多通道"><a href="#2-5-池化层的多通道" class="headerlink" title="2.5 池化层的多通道"></a>2.5 池化层的多通道</h3><ul><li>和卷积层有区别， 在处理多通道输⼊数据时，<strong>池化层对每个输⼊通道分别池化，而不是像卷积层那样将各通道的输 ⼊按通道相加。这意味着池化层的输出通道数与输⼊通道数相等</strong></li></ul><h1 id="3-LeNet"><a href="#3-LeNet" class="headerlink" title="3 LeNet"></a>3 LeNet</h1><ul><li>分为卷积层块和全连接层块两个部分</li></ul><h3 id="3-1-卷积层块"><a href="#3-1-卷积层块" class="headerlink" title="3.1 卷积层块"></a>3.1 卷积层块</h3><ul><li>卷积层块⾥的基本单位是卷积层后接最⼤池化层，卷积层⽤来识别图像⾥的空间模式，如线条和 物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性</li><li>在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似</li><li>卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2</li></ul><h3 id="3-2-全连接层块"><a href="#3-2-全连接层块" class="headerlink" title="3.2 全连接层块"></a>3.2 全连接层块</h3><ul><li>当卷积层块的输出传⼊全连接层块时，全连接 层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输⼊形状将变成⼆维，其中第⼀维是小批量中的样本，第⼆维是每个样本变平后的向量表⽰，且向量⻓度为通道、⾼和宽的乘积</li><li>全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数</li></ul><p><img src="https://i.loli.net/2021/11/28/NH5zDmge7RiwaGA.png" alt="20150903212346407"></p><h1 id="4-AlexNet（相对于LeNet较深）"><a href="#4-AlexNet（相对于LeNet较深）" class="headerlink" title="4 AlexNet（相对于LeNet较深）"></a>4 AlexNet（相对于LeNet较深）</h1><ul><li>相对于较小的LeNet，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层</li><li>AlexNet第⼀层中的卷积窗口形状是11 × 11，步幅为4。因为ImageNet中绝⼤多数图像的⾼和宽均 ⽐MNIST图像的⾼和宽⼤10倍以上，ImageNet图像的物体占⽤更多的像素，所以需要更⼤的卷积窗口来捕获物体。第⼆层中的卷积窗口形状减小到5 × 5，之后全采⽤3 × 3</li><li>第⼀、第⼆和第五个卷积层之后都使⽤了窗口形状为3 × 3、步幅为2的最⼤池化层</li><li>AlexNet使⽤的卷积通道数也⼤于LeNet中的卷积通道数数⼗倍，5层的输出通道数分别为96，256，384，384，256</li><li>紧接着最后⼀个卷积层的是两个输出个数为4096的全连接层</li><li>AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数</li><li>AlexNet使用了Dropout和图像增广</li><li>AlexNet相比于LeNet有更小的学习率</li><li><strong>AlexNet的一个很大的意义就是在图像识别上AI超越人工，并且首次使用了GPU加速</strong></li></ul><p><img src="https://pic2.zhimg.com/v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b.jpg" alt="v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b"></p><h1 id="5-VGG（使用重复元素）"><a href="#5-VGG（使用重复元素）" class="headerlink" title="5 VGG（使用重复元素）"></a>5 VGG（使用重复元素）</h1><h3 id="5-1-VGG块"><a href="#5-1-VGG块" class="headerlink" title="5.1 VGG块"></a>5.1 VGG块</h3><ul><li>连续使⽤<strong>数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层</strong>。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。可以指定卷积层的数量num_convs和输出通道数num_channels</li></ul><h3 id="5-2-VGG网络"><a href="#5-2-VGG网络" class="headerlink" title="5.2 VGG网络"></a>5.2 VGG网络</h3><ul><li>我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积层和3个全连接层，所以经常被称为VGG-11</li><li>之后的3个全连接层神经元个数分别为：4096，4096，10（最后一层为类别个数）</li><li>使用VGG后，每次输⼊的⾼和宽将减半，直到最终⾼和宽变成7后传⼊全连接层。与此同时，输出通道数每次翻倍，直到变成512。VGG这种⾼和宽减半以及通道翻倍的设计使多数卷积层都有相同的模型参数尺⼨和计算复杂度</li></ul><ul><li><strong>VGG的一个很大的意义就是VGG用数个小卷积核代替大卷积核，使参数量减少</strong></li></ul><p><img src="https://i.loli.net/2021/11/28/tTvmMekgd81PJX6.png" alt="20180205192403250"></p><h1 id="6-NiN（网络中的网络）"><a href="#6-NiN（网络中的网络）" class="headerlink" title="6 NiN（网络中的网络）"></a>6 NiN（网络中的网络）</h1><ul><li>的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深</li><li>但是NiN有所不同，由于1$\times$1卷积层可以替代全连接层，所以NiN中就进行了替换：</li><li><strong>NiN之所以被称为网络中的网络是因为：NiN串联多个由卷积层和“准全连接层”（1X1的网络）的小网络来构成一个深层网络</strong></li></ul><p><img src="https://i.loli.net/2021/11/25/NjfabGRcVe8ysZk.png" alt="image-20211125152432130"></p><h3 id="6-1-NiN块"><a href="#6-1-NiN块" class="headerlink" title="6.1 NiN块"></a>6.1 NiN块</h3><ul><li>NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 × 1卷积层串联而成。其中<strong>第⼀个卷积层的超参数可以⾃⾏设置，而第⼆和第三个卷积层的超参数⼀般是固定的</strong></li><li>三个卷积层的通道数是相同的</li></ul><h3 id="6-2-NiN模型"><a href="#6-2-NiN模型" class="headerlink" title="6.2 NiN模型"></a>6.2 NiN模型</h3><ul><li>NiN使⽤卷积窗口形状分别为11 × 11、5 × 5和3 × 3的卷积层，相应的输出通道数也与AlexNet中的⼀致。每个NiN块后接⼀ 个步幅为2、窗口形状为3 × 3的最⼤池化层</li><li>除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤<strong>全局平均池化层</strong>对每个通道中所有元素求平均并直接⽤于分类。<strong>这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合，然而，该设计有时会造成获得有效模型的训练时间的增加</strong></li><li>NiN的学习率一般比AlexNet和VGG大</li></ul><p><img src="https://i.loli.net/2021/11/28/2eiOxQsw4XoBFN9.png" alt="image-20211128200750143"></p><h1 id="7-GoogLeNet（含并行连接的网络）"><a href="#7-GoogLeNet（含并行连接的网络）" class="headerlink" title="7 GoogLeNet（含并行连接的网络）"></a>7 GoogLeNet（含并行连接的网络）</h1><h3 id="7-1-Inception块"><a href="#7-1-Inception块" class="headerlink" title="7.1 Inception块"></a>7.1 Inception块</h3><p><img src="https://i.loli.net/2021/11/25/Q4sfSlhjntDaXFM.png" alt="image-20211125155525157"></p><ul><li><p>Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 × 3和5 × 5的卷积层来<strong>抽取不同空间尺⼨下的信息</strong></p></li><li><p>其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，<strong>以降低模型复杂度</strong>。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数</p></li><li><p>4条线路都使⽤了合适的填充来使<strong>输⼊与输出的⾼和宽⼀致</strong>。最后我们将每条线路的输出<strong>在通道维上连结</strong>，并输⼊接下来的层中去</p></li></ul><h3 id="7-2-GoogLeNet模型"><a href="#7-2-GoogLeNet模型" class="headerlink" title="7.2 GoogLeNet模型"></a>7.2 GoogLeNet模型</h3><ul><li>在主体卷积部分中使⽤5个模块（block），<strong>每个模块之间使⽤步幅为2的3× 3最⼤池化层来减小输出⾼宽</strong></li><li>第⼀模块使⽤⼀个64通道的7 × 7卷积层，步幅为2</li></ul><ul><li>第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层，然后是将通道增⼤3倍（变为192）的3 × 3卷积层</li></ul><ul><li><p>第三模块串联2个完整的Inception块。第⼀个Inception块的输出通道数为64+128+32+32 = 256（4个加数对应4条线路的通道数），其中第⼆、第三条线路由于有两个卷积层，所以两条线路的第一个<script type="math/tex">1 \times 1</script>卷积层先将输出通道减少为96和16，再接上第⼆层卷积层。</p><p>第⼆个Inception块 输出通道数增⾄128 + 192 + 96 + 64 = 480。其中第⼆、第三条线路<script type="math/tex">1 \times 1</script>卷积层的输出通道分别为128和32</p></li></ul><ul><li><p>第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + (96,208) + (16,48) + 64 = 512、 160+(112,224)+(24,64)+64 = 512、128+(128,256)+(24,64)+64 = 512、112+(144,288)+(32,64)+64 = 528和 256+(160,320)+(32,128)+128 = 832</p><p>其中括号里的第一个数字为二三条通道的第一个卷积核输出通道数</p></li></ul><ul><li>第五模块有输出通道数为256 + (160,320) + (32,128) + 128 = 832和384 + (192,384) + (48,128) + 128 = 1024的两个Inception块，<strong>然后再接上一个全局平均池化层</strong></li></ul><ul><li><strong>五个模块相连之后最后再接上一个全连接层，神经元个数为类别个数</strong></li></ul><p><img src="https://i.loli.net/2021/11/28/rdpGSFRzuKZeTs2.jpg" alt="142051802f8de8513fe61601277f03c8"></p><h1 id="8-批量归一化"><a href="#8-批量归一化" class="headerlink" title="8 批量归一化"></a>8 批量归一化</h1><ul><li>我们一般在前向传播开始之前会对数据进行归一化，<strong>使不同特征之间具有可比性，并且更快收敛</strong></li><li>通常来说，数据标准化预处理对于浅层模型就⾜够有效了，<strong>但对深层神经网络来说，即使输⼊数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。</strong>这种计算数值的不稳定性通常令我们难以训练出有效的深度模型</li><li>而批量归一化则是对每一层的输出都做一次归一化，<strong>使均值永远为0，方差永远为1</strong></li></ul><h3 id="8-1-批量归一化层"><a href="#8-1-批量归一化层" class="headerlink" title="8.1 批量归一化层"></a>8.1 批量归一化层</h3><ul><li><strong>通常，我们将批量归⼀化层置于全连接层中的仿射变换和激活函数之间</strong></li><li>首先要对小批量求均值和方差：</li></ul><script type="math/tex; mode=display">\begin{array}{c}\boldsymbol{\mu}_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} \boldsymbol{x}^{(i)} \\\boldsymbol{\sigma}_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}\right)^{2}\end{array}</script><p>得到的均值和方差是两个向量，维度为特征个数</p><ul><li>然后：</li></ul><script type="math/tex; mode=display">\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}}{\sqrt{\boldsymbol{\sigma}_{\mathcal{B}}^{2}+\epsilon}}</script><ul><li>$\epsilon$ 为一个很小的数，一般取$10^{-8}$ ，是为了保证分母大于0</li><li><strong>但是我们不确定是否一定是所有层都进行批量归一化才是最好的情况，所以我们要给予一个能变回归一化前的值的可能性，所以引入拉伸参数（scale）$\gamma$ 和  偏移参数（shift）$\beta$</strong></li></ul><script type="math/tex; mode=display">\boldsymbol{y}^{(i)} \leftarrow \gamma \odot \hat{\boldsymbol{x}}^{(i)}+\boldsymbol{\beta}</script><p>将$\gamma$ 和 $\beta$ 作为两个可变化参数，然后通过学习来决定拉伸多少和偏移多少</p><h3 id="8-2-对卷积层做批量归一化"><a href="#8-2-对卷积层做批量归一化" class="headerlink" title="8.2 对卷积层做批量归一化"></a>8.2 对卷积层做批量归一化</h3><ul><li>批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前</li><li><p>如果卷积计算输出多个通道，我们需要<strong>对这些通道的输出分别做批量归⼀化，且每个通道都拥有独立的拉伸和偏移参数</strong></p></li><li><p>设小批量中有m个样本。在单个通道上，假设卷积计算输出的⾼和宽分别为p和q。我们需要对该通道中m × p × q个元素同时做批量归⼀化</p></li></ul><h3 id="8-3-预测时的批量归一化"><a href="#8-3-预测时的批量归一化" class="headerlink" title="8.3 预测时的批量归一化"></a>8.3 预测时的批量归一化</h3><ul><li><strong>批量归一化在训练模式和预测模式的计算结果是不⼀样的</strong>。确定均值和方差时，单个样本的输出不应取决于批量归⼀化所需要的随机小批量中的均值和⽅差。⼀种常⽤的⽅法是<strong>通过移动平均（指数加权平均）估算整个训练数据集的样本均值和方差</strong>，并在预测时使⽤它们得到确定的输出</li></ul><h1 id="9-残差网络（ResNet）"><a href="#9-残差网络（ResNet）" class="headerlink" title="9 残差网络（ResNet）"></a>9 残差网络（ResNet）</h1><h3 id="9-1-残差块"><a href="#9-1-残差块" class="headerlink" title="9.1 残差块"></a>9.1 残差块</h3><p><img src="https://i.loli.net/2021/11/25/zADLUxHm7wS8Z9Q.png" alt="image-20211125221657826"></p><ul><li>左图是一般形式的映射，右图为残差映射，是将输入x加权-&gt;激活-&gt;再加权后，再和原输入x相加，再送入激活函数</li><li>这样的结构中，输⼊可通过跨层的数据线路更快地向前传播</li><li>残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接⼀个批量归⼀化层和ReLU激活函数。然后我们<strong>将输⼊跳过这2个卷积运算后直接加在最后的ReLU激活函数前</strong></li><li>这样的设计要求2个卷积层的输出与输⼊形状⼀样，从而可以相加。如果想改变通道数，就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算</li></ul><p><img src="https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg" alt=""></p><h3 id="9-2-ResNet模型"><a href="#9-2-ResNet模型" class="headerlink" title="9.2 ResNet模型"></a>9.2 ResNet模型</h3><ul><li>ResNet的前两层跟之前介绍的GoogLeNet中的⼀样：在输出通道数为64、步幅为2的7 × 7卷积层后接步幅为2的3 × 3的最⼤池化层。不同之处在于<strong>ResNet每个卷积层（对应上图每一次加权运算）后增加的批量归⼀化层</strong></li></ul><ul><li><strong>后接4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块</strong></li><li>第⼀个模块的通道数同输⼊通道数⼀致。由于之前已经使⽤了步幅为2的最⼤池化层，所以⽆须减小⾼和宽。<strong>之后的每个模块在第⼀个残差块⾥将上⼀个模块的通道数翻倍，并将高和宽减半</strong></li></ul><ul><li>接着我们为ResNet加⼊所有残差块。这⾥每个模块的残差块个数可以自行定义</li></ul><ul><li>最后，与GoogLeNet⼀样，加⼊全局平均池化层后接上全连接层输出</li></ul><p><img src="https://i.loli.net/2021/11/25/9aypGjvX1ESYtrM.png" alt="image-20211125223902681"></p><p>其中10为类别个数</p><p><img src="https://pic3.zhimg.com/80/v2-862e1c2dcb24f10d264544190ad38142_720w.jpg" alt=""></p><h3 id="9-3-ResNet的作用"><a href="#9-3-ResNet的作用" class="headerlink" title="9.3 ResNet的作用"></a>9.3 ResNet的作用</h3><ul><li>在深度神经网络中，如果我们进行反向传播，那么由链式求导法则可知，我们会<strong>涉及到非常多参数和导数的连乘</strong>，这时误差很容易产生消失或膨胀，通过实验也可以发现，层数更深的神经网络结果反而没有浅层的好</li></ul><p><img src="https://i.loli.net/2021/11/26/YecToJs5xNf6H24.png" alt="image-20211125235804507"></p><p>这种结果很大程度归结于深度神经网络的<strong>梯度消失问题</strong></p><ul><li>而ResNet的提出就是为了解决梯度消失的问题，<strong>既然离输入近的神经网络层较难训练，那么我们可以将他短接到更接近输出的层</strong>，并且这种方法并不会对结果产生影响，假设输入为X，我们在一般网络中想要拟合f(X)，放到残差网络中就只需要拟合f(X) - X 即可</li><li><strong>残差网络的设计思想不仅仅局限于卷积网络，实际上很多其他的网络也用到了残差，也取得了很不错的效果，都是为了解决深度神经网络的梯度消失问题</strong></li></ul><h1 id="10-稠密连接网络（DenseNet）"><a href="#10-稠密连接网络（DenseNet）" class="headerlink" title="10 稠密连接网络（DenseNet）"></a>10 稠密连接网络（DenseNet）</h1><p><img src="https://i.loli.net/2021/11/25/NTajCiqAydPvoVt.png" alt="image-20211125224307704"></p><ul><li>DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是<strong>在通道维上连结</strong></li><li>DenseNet的主要构建模块是<strong>稠密块（dense block）</strong>和<strong>过渡层（transition layer）</strong>。前者定义了输⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤</li></ul><h3 id="10-1-稠密块"><a href="#10-1-稠密块" class="headerlink" title="10.1 稠密块"></a>10.1 稠密块</h3><ul><li>我们将批量归⼀化、激活和卷积组合到一起形成一种块：</li></ul><p><img src="https://i.loli.net/2021/11/25/hfHF7BLySM1swZG.png" alt="image-20211125225431518"></p><ul><li><strong>稠密块由多个conv_block组成，每块使⽤相同的输出通道数。但在前向计算时，我们将每块的输⼊和输出在通道维上连结</strong></li></ul><h3 id="10-2-过渡层"><a href="#10-2-过渡层" class="headerlink" title="10.2 过渡层"></a>10.2 过渡层</h3><ul><li>由于每个稠密块都会带来通道数的增加，使⽤过多则会带来过于复杂的模型。过渡层⽤来控制模型复杂度。它<strong>通过1 × 1卷积层来减小通道数，并使⽤步幅为2的平均池化层减半高和宽</strong>，并且也要进行激活和BN运算</li></ul><h3 id="10-3-DenseNet模型"><a href="#10-3-DenseNet模型" class="headerlink" title="10.3 DenseNet模型"></a>10.3 DenseNet模型</h3><ul><li>DenseNet⾸先使⽤同ResNet⼀样的单卷积层和最⼤池化层</li></ul><p><img src="https://i.loli.net/2021/11/25/DqcGv6FtMioNez1.png" alt="image-20211125230039527"></p><ul><li>接着使用4个稠密块，同ResNet⼀样，我们可以设置每个稠密块使⽤多少个卷积层</li></ul><ul><li><p>在稠密块之间我们使用过渡层来减半高宽，并减半通道数</p><p>注意最后一层是不使用过渡层进行减半的</p></li><li><p>最后再和ResNet一样，接上全局平均池化层和全连接层来输出</p></li></ul><p><img src="https://pic2.zhimg.com/80/v2-c81da515c8fa9796601fde82e4d36f61_720w.jpg" alt=""></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>欠拟合和过拟合以及正则化</title>
    <link href="/2021/11/12/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2021/11/12/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="1-什么是过拟合和欠拟合"><a href="#1-什么是过拟合和欠拟合" class="headerlink" title="1 什么是过拟合和欠拟合"></a>1 什么是过拟合和欠拟合</h1><p>我们知道进行反向传播的目的是让学习器去拟合数据，而当拟合程度不够时就称为<strong>欠拟合</strong>，拟合程度过于高时则称为<strong>过拟合</strong></p><p>我们对欠拟合和过拟合的判断可以根据<strong>训练误差</strong>和<strong>泛化误差</strong>，具体可看<a href="https://zlkqz.top/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/#8-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">我的前一篇帖子</a></p><h1 id="2-过拟合和欠拟合是怎么发生的"><a href="#2-过拟合和欠拟合是怎么发生的" class="headerlink" title="2 过拟合和欠拟合是怎么发生的"></a>2 过拟合和欠拟合是怎么发生的</h1><p>用下图可以很好的解释：</p><p><img src="https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png" alt="image-20211111215618245" style="zoom:150%;" /></p><h3 id="2-1-欠拟合"><a href="#2-1-欠拟合" class="headerlink" title="2.1 欠拟合"></a>2.1 欠拟合</h3><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法</p><h3 id="2-2-过拟合"><a href="#2-2-过拟合" class="headerlink" title="2.2 过拟合"></a>2.2 过拟合</h3><h5 id="2-2-1-基本概念"><a href="#2-2-1-基本概念" class="headerlink" title="2.2.1 基本概念"></a>2.2.1 基本概念</h5><p>过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<strong>模型在训练集上表现很好，但在测试集上却表现很差</strong>。模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，<strong>泛化能力差</strong></p><h5 id="2-2-2-为什么会出现过拟合现象"><a href="#2-2-2-为什么会出现过拟合现象" class="headerlink" title="2.2.2 为什么会出现过拟合现象"></a>2.2.2 为什么会出现过拟合现象</h5><ul><li><strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型</li><li><strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系</li><li><strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素</li></ul><h1 id="3-如何防止过拟合"><a href="#3-如何防止过拟合" class="headerlink" title="3 如何防止过拟合"></a>3 如何防止过拟合</h1><h3 id="3-1-获取和使用更多的数据（数据集增强）"><a href="#3-1-获取和使用更多的数据（数据集增强）" class="headerlink" title="3.1 获取和使用更多的数据（数据集增强）"></a>3.1 <strong>获取和使用更多的数据（数据集增强）</strong></h3><ul><li><strong>更多解决过拟合的根本性方法</strong></li><li>但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是<strong>创建“假数据”并添加到训练集中——数据集增强</strong>。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力</li></ul><p>我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。</p><h3 id="3-2-采用合适的模型（控制模型的复杂度）"><a href="#3-2-采用合适的模型（控制模型的复杂度）" class="headerlink" title="3.2 采用合适的模型（控制模型的复杂度）"></a>3.2 采用合适的模型（控制模型的复杂度）</h3><ul><li><p>过于复杂的模型会带来过拟合问题。对于模型的设计，目前公认的一个深度学习规律”deeper is better”。国内外各种大牛通过实验和竞赛发现，对于CNN来说，层数越多效果越好，但是也更容易产生过拟合，并且计算所耗费的时间也越长。</p></li><li><p>根据<strong>奥卡姆剃刀法则</strong>：在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。对于模型的设计而言，我们应该<strong>选择简单、合适的模型解决复杂的问题</strong></p></li></ul><h3 id="3-3-降低特征的数量"><a href="#3-3-降低特征的数量" class="headerlink" title="3.3 降低特征的数量"></a>3.3 降低特征的数量</h3><ul><li>对于一些特征工程而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题</li></ul><h3 id="3-4-正则化"><a href="#3-4-正则化" class="headerlink" title="3.4 正则化"></a>3.4 正则化</h3><p>常用正则化方法可看：<a href="https://zlkqz.top/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/#8-5-%E6%AD%A3%E5%88%99%E5%8C%96">正则化</a></p><h3 id="3-5-Early-stopping（提前终止）"><a href="#3-5-Early-stopping（提前终止）" class="headerlink" title="3.5 Early stopping（提前终止）"></a>3.5 Early stopping（提前终止）</h3><ul><li><strong>Early stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合</strong></li><li>为了获得性能良好的神经网络，训练过程中可能会经过很多次epoch（遍历整个数据集的次数，一次为一个epoch）。如果epoch数量太少，网络有可能发生欠拟合；如果epoch数量太多，则有可能发生过拟合</li><li>Early stopping旨在解决epoch数量需要手动设置的问题。具体做法：每个epoch（或每N个epoch）结束后，在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练，将停止之后的权重作为网络的最终参数</li><li><strong>缺点：</strong></li></ul><p><strong>没有采取不同的方式来解决优化损失函数和过拟合这两个问题</strong>，而是用一种方法同时解决两个问题 ，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化损失函数，你可能会发现损失函数的值不够小，同时你又不希望过拟合</p><h1 id="4-L1正则化防止过拟合的原理"><a href="#4-L1正则化防止过拟合的原理" class="headerlink" title="4 L1正则化防止过拟合的原理"></a>4 L1正则化防止过拟合的原理</h1><h3 id="4-1-产生原因"><a href="#4-1-产生原因" class="headerlink" title="4.1 产生原因"></a>4.1 产生原因</h3><ul><li>我们知道过拟合是由于<strong>记住了不适用于测试集的训练集性质或特点</strong>，没有理解数据背后的规律，导致泛化能力差，所以我们所要做的就是<strong>减少某些特征的重要性</strong>，并且<strong>降低模型复杂度</strong>，<strong>减少不同神经元间的相关性</strong></li><li>过拟合发生在模型完美拟合训练数据，对新的数据效果不好</li></ul><p><img src="https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png" alt="image-20211111215618245" style="zoom:150%;" /></p><h3 id="4-2-奥坎姆剃刀理论"><a href="#4-2-奥坎姆剃刀理论" class="headerlink" title="4.2 奥坎姆剃刀理论"></a>4.2 奥坎姆剃刀理论</h3><ul><li><strong>优先选择拟合数据的最简单的假设。 简单的模型才是最好的</strong>。通俗来讲就是当我们有了足够低的训练误差后，尽量选择简单的模型，如上图的情况</li></ul><h3 id="4-3-减少特征的重要性"><a href="#4-3-减少特征的重要性" class="headerlink" title="4.3 减少特征的重要性"></a>4.3 减少特征的重要性</h3><ul><li>用$L_2$正则化举例，其中一个参数$w_1$的更新：</li></ul><script type="math/tex; mode=display">w1 = (1 - \eta\lambda)w1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial\ell^{(i)}(...)}{\partial{w1}}</script><p>我们可以看到$L_2$正则化主要是通过设置一个<strong>权重的惩罚项</strong>，使权重不会过大，<strong>而减少权重就起到了减少某些特征的重要性的作用</strong>，<strong>使用降低权重值对结果的影响可以减小网络加深对训练准确度降低而产生的影响</strong>，惩罚项让$w_1$往0靠近，其实就是减少了模型的复杂度</p><p>至于减少哪些权重才能得到好的结果，我们可以看到，对于惩罚项，是<strong>参数越大，惩罚越大</strong>，而我们是发生了过拟合才使用的正则化，过拟合的根本原因就是模型<strong>对于不重要的特征过度看重，换而言之就是权重过大</strong>，所以正则化能很大削减这一类权重。然而，<strong>对于重要的特征，可能权重也很大</strong>，如果加入惩罚项，会使重要特征权重削减。首先，<strong>如果重要的特征权重很大，甚至可能都不会发生严重的过拟合，就算发生了过拟合，我们也可以通过增加训练，使更靠近最优点</strong>，这也是为什么使用正则化后需要增加训练次数。当然，正则化也不是万能的，有些时候确实使用了效果也不大。</p><h3 id="4-4-降低模型复杂度"><a href="#4-4-降低模型复杂度" class="headerlink" title="4.4 降低模型复杂度"></a>4.4 降低模型复杂度</h3><ul><li><strong>当模型复杂度过高的时候，拟合函数的系数往往非常大，需要顾忌每一个点，最终形成的拟合函数波动很大</strong>，如上图过拟合情况。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，所以才需要惩罚项来降低</li></ul><h3 id="4-5-减少神经元间的相关性"><a href="#4-5-减少神经元间的相关性" class="headerlink" title="4.5 减少神经元间的相关性"></a>4.5 减少神经元间的相关性</h3><ul><li><strong>我们可以通过减少神经元之间的相关性而降低模型的复杂度</strong>，用Dropout来举例：</li></ul><p><strong>Dropout指把一些神经元进行暂时的消去（具体方法就是把该神经元相关的权值设为0）</strong>，然后再进行正向传播和反向传播，当我们过拟合的时候，往往是因为要顾及每一个点，最终造成拟合函数的波动很大，<strong>而在把一些神经元进行消去后，这样我们就减少了神经元之间的相关性，就不需要因为顾及所有而产生过大的波动</strong></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>三种梯度下降的方法</title>
    <link href="/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<ul><li>梯度下降法作为机器学习中较常使用的优化算法，其有着3种不同的形式：<strong>批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）、小批量梯度下降（Mini-Batch Gradient Descent）</strong></li></ul><h1 id="1-批量梯度下降（BGD）"><a href="#1-批量梯度下降（BGD）" class="headerlink" title="1 批量梯度下降（BGD）"></a>1 批量梯度下降（BGD）</h1><p>使用整个训练集的优化算法被称为<strong>批量</strong>(batch)或<strong>确定性</strong>(deterministic)梯度算法，因为它们会<strong>在一个大批量中同时处理所有样本</strong></p><p><strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新</p><ul><li><strong>优点：</strong></li></ul><ol><li>在训练过程中，使用固定的学习率</li><li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向</li><li>一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行</li></ol><ul><li><strong>缺点：</strong></li></ul><ol><li><strong>尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了</strong></li><li>不能投入新数据实时更新模型</li><li>对非凸函数可能只能收敛到局部最小点，而非全局最小点</li></ol><p><img src="https://i.loli.net/2021/11/11/mCkfeDIPyh3a5iY.png" alt="BGD"></p><h1 id="2-随机梯度下降（SGD）"><a href="#2-随机梯度下降（SGD）" class="headerlink" title="2 随机梯度下降（SGD）"></a>2 随机梯度下降（SGD）</h1><p><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是在<strong>每次迭代时</strong>使用<strong>一个样本</strong>来对参数进行更新（mini-batch size =1）</p><ul><li><strong>优点：</strong></li></ul><ol><li>在学习过程中加入了噪声，提高了泛化误差</li><li><strong>噪声造成的扰动可能可以使其脱离局部最小点</strong></li><li>SGD一次只遍历一个样本就可以进行更新 ，所以收敛很快，并且可以新增样本</li></ol><ul><li><strong>缺点：</strong></li></ul><ol><li><strong>不收敛，在最小值附近波动</strong></li><li><strong>不能在一个样本中使用并行化计算，学习过程变得很慢</strong></li><li>单个样本并不能代表全体样本的趋势，所以学习过程可能变得特别的曲折，<strong>虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的</strong>，如下图</li></ol><p><img src="https://i.loli.net/2021/11/11/GLBkox67lvnmsOA.png" alt="image-20211111201918007" style="zoom: 67%;" /></p><ul><li><strong>对上面期望的证明，SGD的梯度梯度是BGD梯度的无偏估计：</strong></li></ul><script type="math/tex; mode=display">E(\nabla f_i(x)) = \frac{1}{n}\sum_{i = 1}^n\nabla f_i(x) = \nabla f(x)</script><h1 id="3-Mini-batch梯度下降"><a href="#3-Mini-batch梯度下降" class="headerlink" title="3 Mini-batch梯度下降"></a>3 Mini-batch梯度下降</h1><p>大多数用于深度学习的梯度下降算法介于以上两者之间，<strong>使用一个以上而又不是全部的训练样本</strong></p><ul><li><p>在一次取样本的时候我们需要在所有样本中<strong>随机</strong>取batch-size个样本</p></li><li><p><strong>优点：</strong></p></li></ul><ol><li>收敛速度比BGD快，因为只遍历部分样例就可执行更新</li><li>随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例</li><li>每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果</li><li>因为有噪音（但是噪音比SGD小），所以<strong>可能脱离局部最小点</strong></li></ol><ul><li><strong>缺点：</strong></li></ul><ol><li>在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛</li><li>学习过程会有更多的振荡，为更接近最小值，需要增加<strong>学习率衰减项</strong>，以降低学习率，避免过度振荡</li></ol><ul><li><strong>小批量大小</strong>（mini-batch size）的选择：</li></ul><ol><li>更大的批量会计算更精确的梯度，但是回报却是小于线性的</li><li>极小的批量通常难以充分利用多核结构。当批量低于某个数值时，计算时间不会减少</li><li>批量处理中的所有样本可以并行处理，<strong>内存消耗和批量大小会成正比</strong>。对于很多硬件设备，这是批量大小的限制因素</li><li>在使用<strong>GPU</strong>时，通常使用<strong>2的幂数作为批量大小</strong>可以获得更少的运行时间。一般，2的幂数取值范围是<strong>32~256</strong>。16有时在尝试大模型时使用</li></ol><p>使用三种梯度下降的收敛过程：</p><p><img src="https://i.loli.net/2021/11/11/paxvbFh9fRiDKNg.png" alt="image-20211111204100788" style="zoom: 80%;" /></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>激活函数的作用和比较</title>
    <link href="/2021/11/11/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E6%AF%94%E8%BE%83/"/>
    <url>/2021/11/11/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E6%AF%94%E8%BE%83/</url>
    
    <content type="html"><![CDATA[<h1 id="1-线性结构"><a href="#1-线性结构" class="headerlink" title="1 线性结构"></a>1 线性结构</h1><p>如果满足：</p><script type="math/tex; mode=display">y = wx + b</script><p>则可称y、x间具有线性关系</p><p>而对于神经网络，相邻两层之间的输出之间满足：</p><script type="math/tex; mode=display">X^{[l]} = w^{[l]}X^{[l - 1]} + b^{[l]}</script><p>则可称其满足线性结构</p><h1 id="2-激活函数的作用"><a href="#2-激活函数的作用" class="headerlink" title="2 激活函数的作用"></a>2 激活函数的作用</h1><ul><li>我们先假设不用激活函数，假设神经网络有3层，每层的权重为$w^{[l]}$，偏差都为$b^{[l]}$，输入为$X$，输出为$Y$，则从头到尾的计算为：</li></ul><script type="math/tex; mode=display">Y = w^{[3]}(w^{[2]}(w^{[1]}X + b^{[1]}) + b^{[2]}) + b^{[3]}</script><p>就相当于做一次线性运算：</p><script type="math/tex; mode=display">Y = w'X + b'</script><p>则从头到尾都是线性结构，这在某些情况是适用的，但是大多数时候输入和输出的关系是非线性的，<strong>这时候我们就需要引入非线性因素，即激活函数，来使得神经网络有足够的capacity来抓取复杂的pattern</strong></p><p>所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。</p><ul><li><strong>能用线性拟合的情况：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/NdbYFTOkwyBj7xh.png" alt="image-20211111115632781" style="zoom:50%;" /></p><ul><li><strong>无法用线性拟合的情况：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/Zvo5O17LesqBbVF.png" alt="image-20211111115745462" style="zoom:50%;" /></p><h1 id="3-常用激活函数的优缺点"><a href="#3-常用激活函数的优缺点" class="headerlink" title="3 常用激活函数的优缺点"></a>3 常用激活函数的优缺点</h1><h3 id="3-1-Sigmoid函数"><a href="#3-1-Sigmoid函数" class="headerlink" title="3.1 Sigmoid函数"></a>3.1 Sigmoid函数</h3><ol><li><p>Sigmoid具有一个很好的特性就是能将输入值映射到$(0, 1)$，便于我们将值转化为概率，并且sigmoid平滑，便于求导</p></li><li><p>但sigmoid还有三大缺点：</p></li></ol><ul><li><strong>Gradient Vanishing：</strong></li></ul><p><img src="https://i.loli.net/2021/11/11/TWxNvl6CrD4kuAU.png" alt="image-20211111155149051"></p><p><strong>由图可以看出，在输入值较大或者较小时，其导数是趋于0，而在梯度下降时，其值就基本不会被更新</strong></p><ul><li><strong>函数输出并不是zero-centered</strong></li></ul><p>我们以一个二维的情况举例：</p><script type="math/tex; mode=display">f(\vec{x} ; \vec{w}, b)=f\left(w_{0} x_{0}+w_{1} x_{1}+b\right)</script><p>现在假设，参数 $w_0, w_1$ 的最优解 $w_0^∗, w_1^∗$ 满足条件:</p><script type="math/tex; mode=display">\left\{\begin{array}{l}w_{0}<w_{0}^{*} \\w_{1} \geqslant w_{1}^{*}\end{array}\right.</script><p>所以我们现在就是要让$w_0$变大，$w_1$变小，而：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_0} = x_0 \frac{\partial L}{\partial f} \\\frac{\partial L}{\partial w_1} = x_1 \frac{\partial L}{\partial f}</script><p>由于上一层sigmoid输出的$x_0, x_1$为正值，所以$w_0, w_1$的梯度不可能符号相同，而我们要逼近最小值点，只能走下图红色线路：</p><p><img src="https://i.loli.net/2021/11/11/IXqb8Brk9KOH7WG.png" alt="image-20211111160220315" style="zoom:50%;" /></p><p>而显然绿色线路才是最快的，所以这会<strong>影响梯度下降的速度</strong></p><ul><li><strong>幂运算相对来说比较耗时</strong></li></ul><h3 id="3-2-Tanh函数"><a href="#3-2-Tanh函数" class="headerlink" title="3.2 Tanh函数"></a>3.2 Tanh函数</h3><ul><li>tanh一般优于sigmoid，就是因为tanh是zero-centered，但是tanh仍然没有解决sigmoid的其他两个问题</li></ul><h3 id="3-3-ReLu函数"><a href="#3-3-ReLu函数" class="headerlink" title="3.3 ReLu函数"></a>3.3 ReLu函数</h3><ol><li>优点：</li></ol><ul><li>解决了gradient vanishing问题 (在正区间)</li><li>计算速度非常快，只需要判断输入是否大于0</li><li>收敛速度快</li></ul><ol><li>缺点：</li></ol><ul><li>ReLU的输出不是zero-centered</li><li><strong>Dead ReLU Problem：</strong></li></ul><p>指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。</p><p>在x&lt;0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应</p><p><strong>产生原因：</strong></p><ol><li><p>参数初始化问题（比较少见）</p></li><li><p>learning rate太高导致在训练过程中参数更新太大</p></li></ol><p>而为了防止这种情况，我们可以使用改进的ReLu函数，如Leaky ReLu：</p><script type="math/tex; mode=display">f(x) = max(0.01x, x)</script><p>但是大部分时候我们还是使用ReLu，很多时候LReLu的表现和ReLu其实相差不多，只有神经元大量坏死，ReLu表现不好的时候，我们才会考虑使用LReLu，甚至更复杂的PReLu和ELU</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>常用损失函数和评估模型的指标</title>
    <link href="/2021/11/11/%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8C%87%E6%A0%87/"/>
    <url>/2021/11/11/%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h1 id="1-常用损失函数"><a href="#1-常用损失函数" class="headerlink" title="1 常用损失函数"></a>1 常用损失函数</h1><h3 id="1-1-0-1损失函数"><a href="#1-1-0-1损失函数" class="headerlink" title="1.1 0-1损失函数"></a>1.1 0-1损失函数</h3><script type="math/tex; mode=display">L(y, \hat{y}) = \begin{cases}1 & y\neq \hat{y}\\0 & y = \hat{y}\end{cases}</script><ul><li>0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用</li><li>感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足$|y - \hat{y}| &lt; T$时认为相等，即：</li></ul><script type="math/tex; mode=display">L(y, \hat{y}) = \begin{cases}1 & |y - \hat{y}| \geq T\\0 & |y - \hat{y}| < T\end{cases}</script><h3 id="1-2-均方差损失函数（MSE）"><a href="#1-2-均方差损失函数（MSE）" class="headerlink" title="1.2 均方差损失函数（MSE）"></a>1.2 均方差损失函数（MSE）</h3><script type="math/tex; mode=display">J_{MSE} = \frac{1}{N} \sum_{i = 1}^N(y_i - \hat{y_i})^2</script><ul><li>也称L2 Loss</li></ul><h5 id="1-2-1-证明"><a href="#1-2-1-证明" class="headerlink" title="1.2.1 证明"></a>1.2.1 证明</h5><p>假设预测值和真实值的误差$\epsilon$服从标准正态分布，则给定一个$x_i$输出真实值$y_i$的概率为：</p><script type="math/tex; mode=display">p(\hat y_i = y_i|x_i) = p(\hat y_i = f(x_i) + \epsilon) | x_i) = p(\epsilon) = \frac{1}{\sqrt{2\pi}}exp(-\frac{(y_i - \hat{y_i})^2}{2})</script><p>其实就是极大似然估计，我们要寻找一组参数，使$p(y_i|x_i)$最大</p><p>进一步对所有样本，由于他们相互独立，所以所有样本都正好取到真实值$y$的概率为：</p><script type="math/tex; mode=display">L(x, y) = \prod_{i = 1}^N\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_i - \hat{y_i})^2}{2})</script><p>现在我们就要使$L(x, y)$最大，为了方便计算，我们取对数：</p><script type="math/tex; mode=display">LL(x, y) = log(L(x, y)) = -\frac{N}{2}log2\pi - \frac{1}{2}\sum_{i = 1}^N(y_i - \hat{y_i})^2</script><p>把第一项无关项去掉，再取负：</p><script type="math/tex; mode=display">NLL(x, y) = \frac{1}{2}\sum_{i = 1}^N(y_i - \hat{y_i})^2</script><p>即得到均方差形式</p><h5 id="1-2-2-为什么可以用极大似然"><a href="#1-2-2-为什么可以用极大似然" class="headerlink" title="1.2.2 为什么可以用极大似然"></a>1.2.2 为什么可以用极大似然</h5><p><strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，拟合数据最好的情况就是所有的$y_i = \hat{y_i}$，即每个样本的$p(y_i|x_i)$取最大，即$L(x, y)$取最大，由于对数运算不改变单调性，并且最后取了个负值，所以即$NLL(x, y)$取最小</strong></p><h3 id="1-3-平均绝对误差损失（MAE）"><a href="#1-3-平均绝对误差损失（MAE）" class="headerlink" title="1.3 平均绝对误差损失（MAE）"></a>1.3 平均绝对误差损失（MAE）</h3><script type="math/tex; mode=display">J_{MAE} = \frac{1}{N}\sum_{i = 1}^N|y_i - \hat{y_i}|</script><ul><li>也称L1 Loss</li></ul><h5 id="1-3-1-拉普拉斯分布"><a href="#1-3-1-拉普拉斯分布" class="headerlink" title="1.3.1 拉普拉斯分布"></a>1.3.1 拉普拉斯分布</h5><script type="math/tex; mode=display">f(x|\mu, b) = \frac{1}{2b}exp(-\frac{|x - \mu|}{b})</script><p>期望值：$\mu$             方差：$2b^2$</p><p><img src="https://i.loli.net/2021/11/08/yWY3hI4ioKpADRF.png" alt="Laplace_distribution_pdf" style="zoom: 25%;" /></p><h5 id="1-3-2-证明"><a href="#1-3-2-证明" class="headerlink" title="1.3.2 证明"></a>1.3.2 证明</h5><p>假设预测值和真实值的误差服从拉普拉斯分布（$\mu = 0, b = 1$）</p><script type="math/tex; mode=display">p(y_i | x_i) = \frac{1}{2}exp(-{|y_i - \hat{y_i}|})</script><p>剩余证明和上述MSE证明过程一样</p><h5 id="1-3-3-MSE和MAE的区别："><a href="#1-3-3-MSE和MAE的区别：" class="headerlink" title="1.3.3 MSE和MAE的区别："></a>1.3.3 MSE和MAE的区别：</h5><ul><li><strong>MSE 损失相比 MAE 通常可以更快地收敛</strong></li></ul><p>关于$\hat{y_i}$求导时，MSE为$-(y_i - \hat{y_i})$，MAE为$\pm1$，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的（当然也可以通过调整学习率缓解这个问题，但总体来说还是MAE更快）。</p><ul><li><strong>MAE对于离群值更加健壮，即更加不易受到离群值影响</strong></li></ul><ol><li>由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失</li><li>MSE 假设了误差服从正态分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于离群值更加健壮</li></ol><h5 id="1-3-4-MSE和MAE的收敛"><a href="#1-3-4-MSE和MAE的收敛" class="headerlink" title="1.3.4 MSE和MAE的收敛"></a>1.3.4 MSE和MAE的收敛</h5><ul><li>MSE收敛于均值</li></ul><p>将$\hat{y_i}$设为变量$t$：</p><script type="math/tex; mode=display">J_{MSE} = \frac{1}{N}\sum_{i = 1}^N(t - y_i)^2</script><p>关于$t$求导：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial t} = \frac{2}{N}\sum_{i = 1}^N(t - y_i) = 0</script><p>求得：</p><script type="math/tex; mode=display">t = \frac{1}{N}\sum_{i = 1}^Ny_i = E(y)</script><ul><li>MAE收敛于中值</li></ul><p>将$\hat{y_i}$设为变量$t$：</p><script type="math/tex; mode=display">J_{MAE} = \frac{1}{N}\sum_{i = 1}^N|t - y_i|</script><p>关于$t$求导：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial t} = \frac{1}{N}\sum_{i = 1}^Nsgn(t - y_i) = 0</script><p>显然在该种情况下应该取$t$为中值</p><h3 id="1-4-Huber-Loss"><a href="#1-4-Huber-Loss" class="headerlink" title="1.4 Huber Loss"></a>1.4 Huber Loss</h3><ul><li>上面介绍了MSE和MAE，他们各有各的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，而Huber Loss则是将两者结合起来，原理很简单，就是误差接近 0 时使用 MSE，误差较大时使用 MAE：</li></ul><script type="math/tex; mode=display">J_{huber} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right| \leq \delta} \frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}+\mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right|>\delta}\left(\delta\left|y_{i}-\hat{y}_{i}\right|-\frac{1}{2} \delta^{2}\right)</script><ul><li>前半部分是MSE部分，后半部分是MAE部分，超参数$\delta$为两个部分的连接处</li><li>MAE部分为$\delta |y_i - \hat{y_i}| - \frac{1}{2}\delta ^2$是为了在$|y_i - \hat{y_i}| = \delta$ 端点处连续可导</li></ul><p><img src="https://i.loli.net/2021/11/09/e6FQMBY42PDGxtI.png" alt="超参数为1的Huber Loss"></p><ul><li>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个$\delta$超参数。</li></ul><h3 id="1-5-分位数损失（Quantile-Loss）"><a href="#1-5-分位数损失（Quantile-Loss）" class="headerlink" title="1.5 分位数损失（Quantile Loss）"></a>1.5 分位数损失（Quantile Loss）</h3><script type="math/tex; mode=display">J_{\text {quant }}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\hat{y}_{i} \geq y_{i}}(1-r)\left|y_{i}-\hat{y}_{i}\right|+\mathbb{I}_{\hat{y}_{i}<y_{i}} r\left|y_{i}-\hat{y}_{i}\right|</script><ul><li>这是一个分段函数，这个损失函数是一个分段的函数 ，将$\hat{y_i} \geq y_i$（高估） 和$\hat{y_i} &lt; y_i$（低估) 两种情况分开来，并分别给予不同的系数</li><li>分位数损失实现了分别用不同的系数（r和1-r）控制高估和低估的损失</li><li>特别地，当$r = 0.5$时分位数损失退化为 MAE 损失</li></ul><p><img src="https://i.loli.net/2021/11/09/BvpTeXqWFYRKoMU.png" alt="Quantile Loss"></p><h3 id="1-6-交叉熵损失（Cross-Entropy-Loss）"><a href="#1-6-交叉熵损失（Cross-Entropy-Loss）" class="headerlink" title="1.6 交叉熵损失（Cross Entropy Loss）"></a>1.6 交叉熵损失（Cross Entropy Loss）</h3><ul><li>上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 </li></ul><h5 id="1-6-1-二分类"><a href="#1-6-1-二分类" class="headerlink" title="1.6.1 二分类"></a>1.6.1 二分类</h5><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N}\left(y_{i} \log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right)\right)</script><p><img src="https://i.loli.net/2021/11/09/hPt1nITJ624Llfp.png" alt="二分类的交叉熵"></p><ul><li>证明：</li></ul><p>在二分类中我们通常将输出结果用sigmoid映射到区间$(0, 1)$，并将其作为该类的概率，由于只有两类，所以给定$x_i$求出类别为1或0的概率分别为：</p><script type="math/tex; mode=display">p(y_i = 1|x_i) = \hat{y_i} \\p(y_i = 0|x_i) = 1 - \hat{y_i}</script><p>合并成一个式子：</p><script type="math/tex; mode=display">p(y_i|x_i) = (\hat{y_i})^{y_i}(1 - \hat{y_i})^{1 - y_i}</script><p>由于各数据点独立同分布，则似然可以表示为：</p><script type="math/tex; mode=display">L(x, y) = \prod_{i=1}^{N}\left(\hat{y}_{i}\right)^{y_{i}}\left(1-\hat{y}_{i}\right)^{1-y_{i}}</script><p>取负对数：</p><script type="math/tex; mode=display">N L L(x, y)=J_{C E}=-\sum_{i=1}^{N}\left(y_{i} \log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right)\right)</script><h5 id="1-6-2-多分类"><a href="#1-6-2-多分类" class="headerlink" title="1.6.2 多分类"></a>1.6.2 多分类</h5><p>在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值$y_i$现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 Softmax 函数</p><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log \left(\hat{y}_{i}^{k}\right)</script><p>因为$y_i$是一个One-hot向量，所以还可以写为：</p><script type="math/tex; mode=display">J_{C E}=-\sum_{i=1}^{N} y_{i}^{c_{i}} \log \left(\hat{y}_{i}^{c_{i}}\right)</script><p>其中$c_i$为样本$x_i$的目标类</p><ul><li>证明：</li></ul><p>对于一个样本，分类正确的概率为：</p><script type="math/tex; mode=display">p(y_i|x_i) = \prod_{k=1}^{K}\left(\hat{y}_{i}^{k}\right)^{y_{i}^{k}}</script><p>（其中$y_i^k和\hat{y_i}^k$为该向量的第k维）</p><p>因为所有样本相互，所有相乘再取负对数即可得到：</p><script type="math/tex; mode=display">N L L(x, y)=J_{C E}=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log \left(\hat{y}_{i}^{k}\right)</script><h3 id="1-7-合页损失（Hinge-Loss）"><a href="#1-7-合页损失（Hinge-Loss）" class="headerlink" title="1.7 合页损失（Hinge Loss）"></a>1.7 合页损失（Hinge Loss）</h3><ul><li>Hinge Loss也是一种二分类损失函数</li></ul><script type="math/tex; mode=display">J_{\text {hinge }}=\sum_{i=1}^{N} \max \left(0,1-\operatorname{sgn}\left(y_{i}\right) \hat{y}_{i}\right)</script><p>下图是$y$为正类， 即$sgn(y) = 1$时，不同输出的合页损失示意图：</p><p><img src="https://i.loli.net/2021/11/09/KngeQOwp54JZRMf.png" alt="image-20211109225642368"></p><ul><li>可以看到当$y$为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在$(0, 1)$区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。<strong>使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类</strong></li><li><strong>Hinge Loss常用在支持向量机（SVM）中</strong></li></ul><h1 id="2-评估模型的指标"><a href="#2-评估模型的指标" class="headerlink" title="2 评估模型的指标"></a>2 评估模型的指标</h1><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><img src="https://i.loli.net/2021/11/09/OCNnaMs2ceBQIJz.png" alt="image-20211109233859657"></p><h3 id="2-2-查准率和查全率"><a href="#2-2-查准率和查全率" class="headerlink" title="2.2 查准率和查全率"></a>2.2 查准率和查全率</h3><script type="math/tex; mode=display">查准率 P（Precision） = \frac{TP}{TP + FP}</script><script type="math/tex; mode=display">查全率 R（Recall） = \frac{TP}{TP + FN}</script><ul><li>查准率可以直观理解为所有预测为正项的样本中有多少是真正的正项</li><li>查全率可以直观理解为所有label是正项的样本中有多少被成功预测出来了</li><li>理想情况下，查准率和查全率两者都越高越好。<strong>然而事实上这两者在某些情况下是矛盾的，一般来说，查准率高时，查全率低；查确率低时，查全率高</strong></li></ul><h3 id="2-3-准确率和错误率"><a href="#2-3-准确率和错误率" class="headerlink" title="2.3 准确率和错误率"></a>2.3 准确率和错误率</h3><p>准确率：</p><script type="math/tex; mode=display">accuracy = \frac{TP + TF}{TP + TN + FP + FN}</script><ul><li>即有多少样本被分类正确</li></ul><p>而错误率：</p><script type="math/tex; mode=display">errorrate = 1 - accuracy</script><h3 id="2-4-P-R曲线"><a href="#2-4-P-R曲线" class="headerlink" title="2.4 P-R曲线"></a>2.4 P-R曲线</h3><p><img src="https://i.loli.net/2021/11/10/zV1Sj4HyYfD3G5e.png" alt="image-20211110000609300" style="zoom: 80%;" /></p><ul><li>P-R曲线直观地显示出学习器在样本总体上地查全率、查准率，在进行比较时，<strong>若一个学习器的P-R曲线被另一个学习器曲线“完全包住”，则可以断言后者的性能一定优于前者，如上图的B性能优于C，而A、B不一定</strong></li><li>平衡点（BEP）时$P = R$时的取值，如上图A的BEP为0.8，而如果基于BEP比较，可知A优于B</li></ul><h3 id="2-5-F函数"><a href="#2-5-F函数" class="headerlink" title="2.5 F函数"></a>2.5 F函数</h3><p>BEP还是过于简化了些，更常用得是F1度量，我们可以取P和R的调和平均：</p><script type="math/tex; mode=display">\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}</script><p>求得：</p><script type="math/tex; mode=display">F1 = \frac{2PR}{P + R}</script><p>但是在许多应用中我们对查准率和查全率得重视程度不同，所以可以给予P和R不同的权重，取P和R的加权调和平均：</p><script type="math/tex; mode=display">\frac{1}{F_{\beta}} = \frac{1}{1 + \beta ^2}(\frac{1}{P} + \frac{\beta ^2}{R})</script><p>求得：</p><script type="math/tex; mode=display">F_{\beta} = \frac{(1 + \beta ^2)PR}{\beta ^2P + R}</script><ul><li>$\beta &gt; 0$度量了查全率和查准率的相对重要性，$\beta = 1$退化为标准的F1，$\beta &gt; 1$查全率有更大影响，$\beta &lt; 1$查准率有更大影响</li></ul><h3 id="2-6-ROC与AUC"><a href="#2-6-ROC与AUC" class="headerlink" title="2.6 ROC与AUC"></a>2.6 ROC与AUC</h3><h5 id="2-6-1-基本概念"><a href="#2-6-1-基本概念" class="headerlink" title="2.6.1 基本概念"></a>2.6.1 基本概念</h5><ul><li><p>大多二分类问题是将输出的预测值与一个<strong>分类阈值（threshold）</strong>进行比较，若预测值大于阈值则为正类，反之则为负类</p></li><li><p>根据预测值，我们可将测试样本进行排序，根据预测值由大到小，“最可能”是正例的排在前面，“最不可能”是正例的排到后面。<strong>这样，分类过程就相当于以中间某个截断点（也就是分类阈值）将样本分为两部分，前一部分判定为正例，后一部分判定为反例</strong></p></li><li><p>在不同的任务中，我们我们可以根据任务需求采取不同的截断点。例如如更重视查准率，可以将截断点设置靠前，更注重查全率，可以将截断点靠后</p></li><li>而我们根据预测值进行排序后，按顺序将样本逐个作为正例进行预测，每次计算出<strong>真正例率（TPR）</strong>和<strong>假正例率（FPR）</strong>，以他们为横纵坐标就得到了<strong>ROC曲线</strong></li></ul><h5 id="2-6-2-ROC曲线"><a href="#2-6-2-ROC曲线" class="headerlink" title="2.6.2 ROC曲线"></a>2.6.2 ROC曲线</h5><ul><li>首先介绍真正例率和假正例率：</li></ul><script type="math/tex; mode=display">TPR = \frac{TP}{TP + FN} \\FPR = \frac{FP}{TN + FP}</script><ul><li>ROC曲线：</li></ul><p>首先将分类阈值设最大，则所有类都被分类为负类，TPR和FPR都为0，然后每次将分类阈值设为下一个样本点的预测值（按预测值由大到小进行排序），记录每次的TPR和FPR，组成ROC曲线</p><p><img src="https://i.loli.net/2021/11/10/TUMHuC3N9rX18iz.png" alt="image-20211110140633061"></p><p>但是现实中我们是基于有限个样本画的图，所以不会产生这么平滑的曲线，更多情况应该像下图：</p><p><img src="https://i.loli.net/2021/11/10/xG1TrPNfyimzq7Q.png" alt="image-20211110140853013"></p><ul><li>基于ROC的比较方法</li></ul><blockquote><p>如果一个学习器的ROC曲线完全被另一个学习器的”包住“，则后者性能优于前者</p><p>若两者的曲线交叉，则可以通过ROC曲线所包裹的面积进行判断，即<strong>AUC</strong></p></blockquote><h5 id="2-6-3-AUC"><a href="#2-6-3-AUC" class="headerlink" title="2.6.3 AUC"></a>2.6.3 AUC</h5><ul><li><strong>AUC就是ROC曲线下的面积</strong>，假定ROC曲线是由坐标为$\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right),\left(x<em>{3}, y</em>{3}\right), \cdots,\left(x<em>{m}, y</em>{m}\right)$的点按序连接而形成，则AUC为：</li></ul><script type="math/tex; mode=display">A U C=\frac{1}{2} \sum_{i=1}^{m-1}\left(x_{i+1}-x_{i}\right)\left(y_{i}+y_{i+1}\right)</script><ul><li><p>从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1&gt;p0的概率就等于AUC， 即<strong>AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性</strong></p></li><li><p><strong>所以AUC反应的是分类器对样本的排序能力</strong></p></li></ul><p>证明：</p><p>设所有正类的集合$X = { \hat{X_1}, \hat{X_2}, …, \hat{X_m}}$和负类的集合$Y = { \hat{Y_1}, \hat{Y_2}, …, \hat{Y_n}}$，其中是每个样本对应的预测值，设分类阈值为c，$F_X(x)$和$F_Y(y)$分别为X和Y的分布函数，则$TPR(c) = 1 - F_X(c)$，$FPR(c) = 1 - F_Y(c)$</p><p>设$t = FPR(c)$， 则$c = F_Y^{-1}(1 - t)$，则$ROC(t) = 1 - F_X(F_Y^{-1}(1 - t))$</p><p>则：</p><script type="math/tex; mode=display">AUC = \int_0^1ROC(t)dt \\=\int_0^1 [1 - F_X(F_Y^{-1}(1 - t))] dt \\=\int_0^1 [1 - F_X(F_Y^{-1}(t))] dt \\=\int_{-\infty}^{+\infty} [1 - F_X(y)] dF_Y(y) \\=\int_{-\infty}^{+\infty}P(X > y)f_Y(y)dy \\=\int_{-\infty}^{+\infty}P(X > y, Y = y)dy \\=P(X > Y)</script><h5 id="2-6-4-使用ROC和AUC的优点"><a href="#2-6-4-使用ROC和AUC的优点" class="headerlink" title="2.6.4 使用ROC和AUC的优点"></a>2.6.4 使用ROC和AUC的优点</h5><ul><li><strong>AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡（正负类样本不相同）的情况下，依然能够对分类器做出合理的评价</strong></li></ul><script type="math/tex; mode=display">TPR = P(\hat{Y} = 1 | Y = 1) \\FPR = P(\hat{Y} = 1 | Y = 0)</script><p><strong>由上式可得：无论Y的真实概率是多少， 都不会影响TPR和FPR</strong></p><p>而PR曲线更关注正例</p><ul><li>ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响，有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的查全率就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少</li></ul><ul><li>上面几种评估方法都是<strong>用于分类</strong>的评估方法，而在<strong>回归问题</strong>当中，这些一般是不适用的，回归问题中我们比较常用的评估方法有一下两种</li></ul><h3 id="2-7-平方根误差（RMSE）"><a href="#2-7-平方根误差（RMSE）" class="headerlink" title="2.7 平方根误差（RMSE）"></a>2.7 平方根误差（RMSE）</h3><script type="math/tex; mode=display">RMSE = \sqrt{\frac{\sum_{i = 1}^n(y_i - \hat y_i)^2}{n}}</script><ul><li>其实RMSE就是MSE开了个根，但是我们做这样的处理能让<strong>误差和结果值在同一个数量级上，这样能更直观有效的反应拟合程度</strong></li><li>但是RMSE有着和MSE一样的缺点，那就是<strong>对离群值十分敏感，健壮性很差</strong></li><li>比如在实际应用中，有可能在对于预测某些剧集的流量时，以便进行广告投放，在95%的区间内的预测误差都十分低，比如小于1%，这是相当不错的预测结果。但是在总体上，无论运用何种模型，RMSE可能都一直居高不下。<strong>原因是可能在剩余的5%区间里有非常严重的离群点，比如某些冷门剧、新上映的剧</strong></li><li>对此我们可以选择对数据进行处理，或者换一种模型指标</li></ul><h3 id="2-8-平均绝对百分比误差（MAPE）"><a href="#2-8-平均绝对百分比误差（MAPE）" class="headerlink" title="2.8 平均绝对百分比误差（MAPE）"></a>2.8 平均绝对百分比误差（MAPE）</h3><script type="math/tex; mode=display">MAPE = \sum_{i = 1}^n |\frac{y_i - \hat{y}_i}{y_i}| \times \frac{100}{n}</script><ul><li>相比RMSE， MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的影响</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习基础总结</title>
    <link href="/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/"/>
    <url>/2021/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="1-行业知识体系"><a href="#1-行业知识体系" class="headerlink" title="1 行业知识体系"></a>1 行业知识体系</h1><h3 id="1-1-机器学习算法"><a href="#1-1-机器学习算法" class="headerlink" title="1.1 机器学习算法"></a>1.1 机器学习算法</h3><p><img src="https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg" alt="qq_pic_merged_1635482805011"></p><h3 id="1-2-机器学习分类"><a href="#1-2-机器学习分类" class="headerlink" title="1.2 机器学习分类"></a>1.2 机器学习分类</h3><p><img src="https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg" alt="qq_pic_merged_1635482964705"></p><h3 id="1-3-问题领域"><a href="#1-3-问题领域" class="headerlink" title="1.3 问题领域"></a>1.3 问题领域</h3><script type="math/tex; mode=display">\begin{cases}\pmb{语言识别} \\\pmb{字符识别} \\\pmb{计算机视觉}（CV） \\\pmb{自然语言处理}（NLP）\\\pmb{知识推理}\\\pmb{自动控制}\\\pmb{游戏理论和人机对弈}\\\pmb{数据挖掘}\end{cases}</script><h1 id="2-线性回归"><a href="#2-线性回归" class="headerlink" title="2 线性回归"></a>2 线性回归</h1><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><img src="https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png" alt="image-20211029131504886"></p><ul><li><p>线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</p></li><li><p>线性回归输出是⼀个连续值，因此适⽤于回归问题</p></li></ul><h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><ul><li>均方差（mse）：</li></ul><script type="math/tex; mode=display">\ell^{(i)}(W, b) = \frac{(\hat{y}^{(i)} - y^{(i)})^2}{2}</script><p>其中$\hat{y}$为计算出的预测值，$y$为真实值（label）</p><h1 id="3-全连接层（稠密层）"><a href="#3-全连接层（稠密层）" class="headerlink" title="3  全连接层（稠密层）"></a>3  全连接层（稠密层）</h1><ul><li>输出层中的神经元和输⼊ 层中各个输⼊完全连接</li><li>计算完全依赖于输入层。</li></ul><h1 id="4-小批量随机梯度下降"><a href="#4-小批量随机梯度下降" class="headerlink" title="4  小批量随机梯度下降"></a>4  小批量随机梯度下降</h1><ul><li><p>在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\eta$）作为模型参数在本次迭代的减小量。</p></li><li><p>每次mini-batch梯度下降计算过程：</p></li></ul><script type="math/tex; mode=display">w_1 = w_1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial{\ell(...)}}{\partial{w_1}}</script><ul><li>注意学习率要取正数</li></ul><h1 id="5-Softmax分类"><a href="#5-Softmax分类" class="headerlink" title="5  Softmax分类"></a>5  Softmax分类</h1><h3 id="5-1-基本概念"><a href="#5-1-基本概念" class="headerlink" title="5.1 基本概念"></a>5.1 基本概念</h3><ul><li>softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数</li><li>由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为该类的概率，具体计算如下：</li></ul><p>假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:</p><script type="math/tex; mode=display">\hat{y}_1, \hat{y}_2, \hat{y}_3 = softmax(o_1, o_2, o_3)</script><p>其中</p><script type="math/tex; mode=display">\hat{y}_i = \frac{e^{o_i}}{\sum_{j = 1}^{3}e^{o_j}}</script><ul><li>容易看出$\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）</li><li>通常，我 们把预测概率最⼤的类别作为输出类别</li></ul><h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 损失函数</h3><ul><li>mse过于严格，因为并不需要所有的输出都和真实值相近</li><li>这里我们运用交叉熵损失函数（cross entropy）：</li></ul><script type="math/tex; mode=display">H(y^{(i)}, \hat{y}^{(i)}) = -\sum_{j = 1}^qy_j^{(i)}\log{\hat{y}^{(i)}}</script><p>q为输出元个数$\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值</p><h1 id="6-多层感知机（MLP）"><a href="#6-多层感知机（MLP）" class="headerlink" title="6 多层感知机（MLP）"></a>6 多层感知机（MLP）</h1><ul><li>多层感知机有一到多个隐藏层</li><li>多层感知机中的隐藏层和输出层都是全连接层</li><li>多层感知机具有激活函数，下面介绍常用激活函数</li></ul><h1 id="7-激活函数"><a href="#7-激活函数" class="headerlink" title="7 激活函数"></a>7 激活函数</h1><h3 id="7-1-Sigmoid函数"><a href="#7-1-Sigmoid函数" class="headerlink" title="7.1 Sigmoid函数"></a>7.1 Sigmoid函数</h3><script type="math/tex; mode=display">sigmoid(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">sigmoid'(x) = sigmoid(x)(1 - sigmoid(x))</script><p><img src="https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png" alt="image-20211030004640247"></p><ul><li>一般用在二分类的输出层中</li></ul><h3 id="7-2-tanh函数"><a href="#7-2-tanh函数" class="headerlink" title="7.2 tanh函数"></a>7.2 tanh函数</h3><script type="math/tex; mode=display">tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}</script><script type="math/tex; mode=display">tanh'(x) = 1 - tanh^2(x)</script><p><img src="https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png" alt="image-20211030005138300"></p><ul><li>tanh和sigmoid比较相似，但是性能一般要优于sigmoid</li></ul><h3 id="7-3-ReLU函数"><a href="#7-3-ReLU函数" class="headerlink" title="7.3 ReLU函数"></a>7.3 ReLU函数</h3><script type="math/tex; mode=display">ReLU(x) = max(0, x)</script><p><img src="https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png" alt="image-20211030005121490"></p><ul><li>十分常用</li></ul><h3 id="7-4-Leaky-ReLU函数"><a href="#7-4-Leaky-ReLU函数" class="headerlink" title="7.4 Leaky ReLU函数"></a>7.4 Leaky ReLU函数</h3><script type="math/tex; mode=display">Leaky\_Relu(x) = max(0.1 * x, x)</script><p><img src="https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png" alt="image-20211030005818507"></p><h1 id="8-训练误差和泛化误差"><a href="#8-训练误差和泛化误差" class="headerlink" title="8 训练误差和泛化误差"></a>8 训练误差和泛化误差</h1><h3 id="8-1-基本概念"><a href="#8-1-基本概念" class="headerlink" title="8.1 基本概念"></a>8.1 基本概念</h3><ul><li>训练误差：指模型在训练数据集上表现出的误差</li><li><p>泛化误差：指模型在任意⼀个测试数据样本上表 现出的误差的期望</p></li><li><p>我们通常假设训练数据集（训练题）和测试数据集（测试题）⾥的每⼀个样本都 是从同⼀个概率分布中相互独⽴地⽣成的</p></li></ul><h3 id="8-2-验证数据集"><a href="#8-2-验证数据集" class="headerlink" title="8.2 验证数据集"></a>8.2 验证数据集</h3><ul><li>在机器学习中，通常需要评估若⼲候选模型的表现并从中选择模型。这⼀过程称为模型选择 （model selection）。可供选择的候选模型可以是有着不同超参数的同类模型。</li><li>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使⽤⼀次。不可以使⽤测试数据选择模型，如调参。由于⽆法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留⼀部分在训练数据集和测试数据集以外的数据来进⾏模型选择。</li><li>我们可以从给定的训练集中随机选取⼀ 小部分作为验证集，而将剩余部分作为真正的训练集</li><li>对于总样本数量比较小的情况，一般训练集：验证集：测试集 = 6：2：1；如果数据量比较大，训练集：验证集：测试集 = 8：1：1</li></ul><h3 id="8-3-k折交叉验证"><a href="#8-3-k折交叉验证" class="headerlink" title="8.3 k折交叉验证"></a>8.3 k折交叉验证</h3><ul><li>当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法</li><li>即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个⼦数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。</li></ul><h3 id="8-4-欠拟合和过拟合"><a href="#8-4-欠拟合和过拟合" class="headerlink" title="8.4 欠拟合和过拟合"></a>8.4 欠拟合和过拟合</h3><ul><li>欠拟合：模型⽆法得到较低的训练误差</li><li><p>过拟合：是模型的训练误差远小于它在测试数据集上 的误差</p></li><li><p>如果模型的复杂度过低，很容易出现⽋拟合；如果模型复杂度过⾼，很容易出现过拟合</p></li><li><p>⼀般来说，如果训练数据集中样本 数过少，特别是⽐模型参数数量（按元素计）更少时，过拟合更容易发⽣。</p></li><li>泛化误差不会 随训练数据集⾥样本数量增加而增⼤</li></ul><h3 id="8-5-正则化"><a href="#8-5-正则化" class="headerlink" title="8.5 正则化"></a>8.5 正则化</h3><ul><li>正则化通过为模型损失函数添加惩罚项使学出 的模型参数值较小</li></ul><h4 id="8-5-1-L2正则化（权重衰减）"><a href="#8-5-1-L2正则化（权重衰减）" class="headerlink" title="8.5.1 L2正则化（权重衰减）"></a>8.5.1 L2正则化（权重衰减）</h4><ul><li>L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积</li><li>损失函数添加一个L2惩罚项：</li></ul><script type="math/tex; mode=display">\ell(...) + \frac{\lambda}{2}\begin{Vmatrix} W \end{Vmatrix}^2</script><p>求导后：（每个mini-batch）</p><script type="math/tex; mode=display">w1 = (1 - \eta\lambda)w1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial\ell^{(i)}(...)}{\partial{w1}}</script><p>其中超参数$\lambda$ &gt; 0。当权重参数均为0时，惩罚项最小。当$\lambda$较⼤时，惩罚项在损失函数中的⽐重较⼤，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作⽤。</p><ul><li>一般训练集中的损失函数要加惩罚项，测试集中不用</li></ul><h4 id="8-5-2-丢弃法（Dropout）"><a href="#8-5-2-丢弃法（Dropout）" class="headerlink" title="8.5.2 丢弃法（Dropout）"></a>8.5.2 丢弃法（Dropout）</h4><ul><li>对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：</li></ul><script type="math/tex; mode=display">h_i' = \frac{\xi_i}{1 - p}h_i</script><p>$\xi_i$为0和1的概率分别为p和1 - p</p><p>分母中的1- p是为了不改变其输⼊的期望值：</p><script type="math/tex; mode=display">由于E(\xi_i) = 1 - p \\所以E(h_i') = \frac{E(\xi_i)}{1 - p}h_i = h_i</script><ul><li><p>测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法</p></li><li><p>进行了Dropout的多层感知机：</p></li></ul><p><img src="https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png" alt="image-20211030141251677"></p><p>可以看到隐藏层中的$h_2$和$h_5$消失了</p><ul><li>每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点</li></ul><h1 id="9-正向传播和反向传播"><a href="#9-正向传播和反向传播" class="headerlink" title="9 正向传播和反向传播"></a>9 正向传播和反向传播</h1><h3 id="9-1-正向传播"><a href="#9-1-正向传播" class="headerlink" title="9.1 正向传播"></a>9.1 正向传播</h3><ul><li>正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）</li><li>中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等</li><li>正向传播的计算图：</li></ul><p><img src="https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png" alt="image-20211030142019713"></p><p>其中：</p><ol><li>左下⻆是输⼊，右上⻆是输出</li><li>⽅框代表变量，圆圈代表运算符</li><li>$J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项</li></ol><h3 id="9-2-反向传播"><a href="#9-2-反向传播" class="headerlink" title="9.2 反向传播"></a>9.2 反向传播</h3><ul><li>反向传播最重要的是通过链式法则求导，从后往前进行计算</li><li>反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传 播计算得到的</li></ul><h3 id="9-3-衰减和爆炸"><a href="#9-3-衰减和爆炸" class="headerlink" title="9.3 衰减和爆炸"></a>9.3 衰减和爆炸</h3><ul><li>如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：</li></ul><p>假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积</p><h3 id="9-4-随机初始化参数"><a href="#9-4-随机初始化参数" class="headerlink" title="9.4 随机初始化参数"></a>9.4 随机初始化参数</h3><ul><li>例如：</li></ul><p>假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。</p><ul><li><strong>在这种情况下，⽆论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作⽤</strong></li></ul><h3 id="9-5-Xavier随机初始化"><a href="#9-5-Xavier随机初始化" class="headerlink" title="9.5 Xavier随机初始化"></a>9.5 Xavier随机初始化</h3><ul><li>假设某全连接层的输⼊个数为a， 输出个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：</li></ul><script type="math/tex; mode=display">U(-\sqrt{\frac{6}{a + b}}, \sqrt{\frac{6}{a + b}})</script><ul><li>该设计主要为了每层输出的⽅差不该受该层输⼊个数影响，且每层梯度的⽅差也不该受该层输出个数影响</li></ul><h1 id="10-Block类"><a href="#10-Block类" class="headerlink" title="10 Block类"></a>10 Block类</h1><ul><li>Block类是⼀个模型构造类，我们可以继承它来定义我们想要的模型。</li><li>它的⼦类既可以是⼀个层（如Dense类），⼜可以是深度学习计算的⼀个模型，或者是模型的⼀个部分</li></ul><h1 id="11-模型参数的延后初始化"><a href="#11-模型参数的延后初始化" class="headerlink" title="11 模型参数的延后初始化"></a>11 模型参数的延后初始化</h1><h3 id="11-1-基本概念"><a href="#11-1-基本概念" class="headerlink" title="11.1 基本概念"></a>11.1 基本概念</h3><ul><li>如果我们在创建层时没有指定输入输出的个数，那么和该层相邻的参数的维度一开始我们是不知道的，只有我们将最开始的输入值输进去，这时系统才能推断出参数的维度，才能开始参数初始化。系统将这种真正的参数初始化延后到获得⾜够信息时才执⾏的⾏为叫作延后初始化（deferred initialization）</li><li>这个初始化只会在第⼀次前向计算时被调⽤</li></ul><h3 id="11-2-避免延后初始化"><a href="#11-2-避免延后初始化" class="headerlink" title="11.2 避免延后初始化"></a>11.2 避免延后初始化</h3><ul><li>可以额外做⼀次前向计算来迫使参数被真正地初始化</li><li>也可以在创建层的时候指定了它的输⼊个数</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Markdown &amp; Letax常用语法</title>
    <link href="/2021/10/05/%E8%AF%AD%E6%B3%95/"/>
    <url>/2021/10/05/%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="1、标题"><a href="#1、标题" class="headerlink" title="1、标题"></a>1、标题</h1><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>~<span class="hljs-number">6</span>级标题<br><span class="hljs-attribute">1</span>~<span class="hljs-number">6</span>个#加个空格<br></code></pre></td></tr></table></figure><h1 id="2、代码块"><a href="#2、代码块" class="headerlink" title="2、代码块"></a>2、代码块</h1><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">​``` + 代码语言<br></code></pre></td></tr></table></figure><h1 id="3、字体"><a href="#3、字体" class="headerlink" title="3、字体"></a>3、字体</h1><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">加粗：<span class="hljs-strong">**文本**</span><br>删除线:~~文本~~<br>斜体：<span class="hljs-strong">*文本*</span><br></code></pre></td></tr></table></figure><p>加粗：<strong>文本</strong><br>删除线：<del>文本</del><br>斜体：<em>文本</em></p><h1 id="4、引用"><a href="#4、引用" class="headerlink" title="4、引用"></a>4、引用</h1><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css">&gt;<span class="hljs-selector-tag">A</span><br>&gt;&gt;<span class="hljs-selector-tag">A</span><br>&gt;&gt;&gt;<span class="hljs-selector-tag">A</span><br></code></pre></td></tr></table></figure><blockquote><p>A</p><blockquote><p>A</p><blockquote><p>A</p></blockquote></blockquote></blockquote><h1 id="5、分割线"><a href="#5、分割线" class="headerlink" title="5、分割线"></a>5、分割线</h1><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><hr><h1 id="6、图片插入"><a href="#6、图片插入" class="headerlink" title="6、图片插入"></a>6、图片插入</h1><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs less">!<span class="hljs-selector-attr">[名字]</span>(本地或网页链接)<br></code></pre></td></tr></table></figure><h1 id="7、超链接"><a href="#7、超链接" class="headerlink" title="7、超链接"></a>7、超链接</h1><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clojure">[名字](链接)<br></code></pre></td></tr></table></figure><p><a href="www.baidu.com">百度</a></p><h1 id="8、列表"><a href="#8、列表" class="headerlink" title="8、列表"></a>8、列表</h1><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gcode">无序列表<br>- <span class="hljs-comment">(空格)</span> 内容<br>有序列表<br>数字.<span class="hljs-comment">(空格)</span>内容<br></code></pre></td></tr></table></figure><ul><li>无序</li></ul><ol><li>有序</li></ol><h1 id="9、表格"><a href="#9、表格" class="headerlink" title="9、表格"></a>9、表格</h1><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">干脆直接快捷键<br></code></pre></td></tr></table></figure><h1 id="10、Latex"><a href="#10、Latex" class="headerlink" title="10、Latex"></a>10、Latex</h1><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><div class="table-container"><table><thead><tr><th>Latex</th><th>对应形式</th><th>Latex</th><th>对应形式</th></tr></thead><tbody><tr><td>\alpha</td><td>$\alpha$</td><td>\Alpha</td><td>$\Alpha$</td></tr><tr><td>\beta</td><td>$\beta$</td><td>\Beta</td><td>$\Beta$</td></tr><tr><td>\gamma</td><td>$\gamma$</td><td>\Gamma</td><td>$\Gamma$</td></tr><tr><td>\delta</td><td>$\delta$</td><td>\Delta</td><td>$\Delta$</td></tr><tr><td>\epsilon</td><td>$\epsilon$</td><td>\Epsilon</td><td>$\Epsilon$</td></tr><tr><td>\zeta</td><td>$\zeta$</td><td>\Zeta</td><td>$\Zeta$</td></tr><tr><td>\eta</td><td>$\eta$</td><td>\Eta</td><td>$\Eta$</td></tr><tr><td>\theta</td><td>$\theta$</td><td>\Theta</td><td>$\Theta$</td></tr><tr><td>\lambda</td><td>$\lambda$</td><td>\Lambda</td><td>$\Lambda$</td></tr><tr><td>\mu</td><td>$\mu$</td><td>\Mu</td><td>$\Mu$</td></tr><tr><td>\nu</td><td>$\nu$</td><td>\Nu</td><td>$\Mu$</td></tr><tr><td>\xi</td><td>$\xi$</td><td>\Xi</td><td>$\Xi$</td></tr><tr><td>\pi</td><td>$\pi$</td><td>\Pi</td><td>$\Pi$</td></tr><tr><td>\rho</td><td>$\rho$</td><td>\Rho</td><td>$\Rho$</td></tr><tr><td>\sigma</td><td>$\sigma$</td><td>\Sigma</td><td>$\Sigma$</td></tr><tr><td>\varphi  //  \phi</td><td>$\varphi$ // $\phi$</td><td>\Phi</td><td>$\Phi$</td></tr><tr><td>\chi</td><td>$\chi$</td><td>\Chi</td><td>$\Chi$</td></tr><tr><td>\psi</td><td>$\psi$</td><td>\Psi</td><td>$\Psi$</td></tr><tr><td>\omega</td><td>$\omega$</td><td>\Omega</td><td>$\Omega$</td></tr><tr><td>\ell</td><td>$\ell$</td><td>\varepsilon</td><td>$\varepsilon$</td></tr></tbody></table></div><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><div class="table-container"><table><thead><tr><th>解释</th><th>代码</th></tr></thead><tbody><tr><td>粗体</td><td>\pmb{…..}</td></tr><tr><td>单空格</td><td>\quad</td></tr><tr><td>双空格</td><td>\qquad</td></tr><tr><td>$\times$</td><td>\times</td></tr><tr><td>$\div$</td><td>\div</td></tr><tr><td>下标</td><td>_</td></tr><tr><td>上标</td><td>^</td></tr><tr><td>$\hat{a}$</td><td>\hat{a}</td></tr><tr><td>$\vec{a}$</td><td>\vec{a}</td></tr><tr><td>$\overrightarrow{a}$</td><td>\overrightarrow{a}</td></tr><tr><td>$\overleftarrow{a}$</td><td>\overleftarrow{a}</td></tr><tr><td>$\log_{32}{xy}$</td><td>\log_{32}{xy}</td></tr><tr><td>{ }</td><td>需要转义</td></tr><tr><td>$\sum_1^n$</td><td>\sum_1^n</td></tr><tr><td>$\prod_{k=1}^nk^2$</td><td>\prod_{k=1}^nk^2</td></tr><tr><td>$\tilde{H}$</td><td>\tilde{H}</td></tr><tr><td></td><td></td></tr><tr><td>$\int_a^b$</td><td>\int_a^b</td></tr><tr><td>$\iint$</td><td>\iint</td></tr><tr><td>$\iiint$</td><td>\iiint</td></tr><tr><td>$\infty$</td><td>\infty</td></tr><tr><td>$\lim_{x\to0}$</td><td>极限       \lim_{x\to0}</td></tr><tr><td>$f’(x)$</td><td>导数        f’(x)</td></tr><tr><td>$\int<em>{\theta=0}^{2\pi}    \int</em>{r=0}^R$</td><td>\int<em>{\theta=0}^{2\pi}    \int</em>{r=0}^R$</td></tr><tr><td>$\nabla$</td><td>\nabla</td></tr><tr><td>$\partial$</td><td>\partial</td></tr><tr><td></td><td></td></tr><tr><td>$\subset$</td><td>\subset</td></tr><tr><td>$\subseteq$</td><td>\subseteq</td></tr><tr><td>$\in$</td><td>\in</td></tr><tr><td>$\notin$</td><td>\notin</td></tr><tr><td>$\emptyset$</td><td>\emptyset</td></tr><tr><td>$\varnothing$</td><td>\varnothing</td></tr><tr><td>$\bigcup$</td><td>\bigcup</td></tr><tr><td>$\cup$</td><td>\cup</td></tr><tr><td>$\bigcap$</td><td>\bigcap</td></tr><tr><td>$\cap$</td><td>\cap</td></tr><tr><td></td><td></td></tr><tr><td>$\frac{a+1}{b+1}$</td><td>分数    \frac{a+1}{b+1}</td></tr><tr><td>$\sqrt{x^5}$</td><td>开方      \sqrt{x^5}</td></tr><tr><td>$\sqrt[3]{xy}$</td><td>开方       \sqrt[3]{xy}</td></tr><tr><td></td><td></td></tr><tr><td>$\le$</td><td>\le</td></tr><tr><td>$\geq$</td><td>\geq</td></tr><tr><td>$\neq$</td><td>\neq</td></tr><tr><td>$\approx$</td><td>\approx</td></tr><tr><td>$\equiv$</td><td>\equiv</td></tr><tr><td>$\pm$</td><td>\pm</td></tr><tr><td>$\mp$</td><td>\mp</td></tr><tr><td>$\mathbb{R}$</td><td>将字母改为黑板字体\mathbb{R}</td></tr><tr><td>$\mathcal{B}$</td><td>\mathcal{B}</td></tr><tr><td>$\leftarrow$</td><td>\leftarrow</td></tr><tr><td>$\rightarrow$</td><td>\rightarrow</td></tr><tr><td>$\odot$</td><td>\odot</td></tr></tbody></table></div><h3 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h3><script type="math/tex; mode=display">f(x) = \begin{cases}1 & x = 2\\2 & x > 2\\3 & x \leqslant 2\\\end{cases}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs livescript">$$<br>f(x) = <span class="hljs-string">\begin&#123;cases&#125;</span><br><span class="hljs-number">1</span> &amp; x = <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">2</span> &amp; x &gt; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; x <span class="hljs-string">\leqslant</span> <span class="hljs-number">2</span><span class="hljs-string">\\</span><br><span class="hljs-string">\end&#123;cases&#125;</span><br>$$<br></code></pre></td></tr></table></figure><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><script type="math/tex; mode=display">\begin{matrix}1 & 2 \\3 & 4 \end{matrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;matrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;matrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{pmatrix}1 & 2 \\3 & 4 \end{pmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;pmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;pmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;bmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;bmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Bmatrix}1 & 2 \\3 & 4 \end{Bmatrix}</script><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$$</span><br>\<span class="hljs-keyword">begin</span>&#123;Bmatrix&#125;<br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> \\<br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br>\<span class="hljs-keyword">end</span>&#123;Bmatrix&#125;<br><span class="hljs-variable">$$</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{vmatrix}1 & 2 \\3 & 4 \end{vmatrix}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-string">\begin&#123;vmatrix&#125;</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br><span class="hljs-string">\end&#123;vmatrix&#125;</span><br></code></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Vmatrix}1 & 2 \\3 & 4 \end{Vmatrix}</script><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-string">\begin&#123;Vmatrix&#125;</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">2</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; <span class="hljs-number">4</span> <br><span class="hljs-string">\end&#123;Vmatrix&#125;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
