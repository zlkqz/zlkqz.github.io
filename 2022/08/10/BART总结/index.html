

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="bia">
  <meta name="author" content="zlk">
  <meta name="keywords" content="">
  <meta name="description" content="作者提出了一种seq2seq的模型，是一种去噪自编码器，大体的设计思路是：使用随机的噪音破坏文本，然后使用该模型将模型恢复回来，模型的结构是BERT的Encoder+GPT的Decoder，取名叫做BART（Bidirectional and Auto-Regressive Transformers）  由于BART是seq2seq的模型，所以相比于BERT，可以拿来做翻译任务。并且通过实验发现">
<meta property="og:type" content="article">
<meta property="og:title" content="BART总结">
<meta property="og:url" content="https://zlkqz.github.io/2022/08/10/BART%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="ZLK">
<meta property="og:description" content="作者提出了一种seq2seq的模型，是一种去噪自编码器，大体的设计思路是：使用随机的噪音破坏文本，然后使用该模型将模型恢复回来，模型的结构是BERT的Encoder+GPT的Decoder，取名叫做BART（Bidirectional and Auto-Regressive Transformers）  由于BART是seq2seq的模型，所以相比于BERT，可以拿来做翻译任务。并且通过实验发现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809212642716.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809214445327.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809230327711.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809232721147.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809235547649.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810000923604.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810001908759.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002027643.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002302848.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002422747.png">
<meta property="article:published_time" content="2022-08-09T16:25:36.618Z">
<meta property="article:modified_time" content="2022-08-09T16:25:26.400Z">
<meta property="article:author" content="zlk">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809212642716.png">
  
  <title>BART总结 - ZLK</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"zlkqz.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ZLK</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="BART总结">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-08-10 00:25" pubdate>
        2022年8月10日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3.5k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      11 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">BART总结</h1>
            
            <div class="markdown-body">
              <ul>
<li><p>作者提出了一种seq2seq的模型，是一种去噪自编码器，大体的设计思路是：<strong>使用随机的噪音破坏文本，然后使用该模型将模型恢复回来</strong>，模型的<strong>结构是BERT的Encoder+GPT的Decoder</strong>，取名叫做BART（Bidirectional and Auto-Regressive Transformers）</p>
</li>
<li><p>由于BART是seq2seq的模型，所以相比于BERT，可以拿来做翻译任务。并且通过实验发现，<strong>BART在文本生成和理解任务等方面是优于BERT的</strong></p>
</li>
</ul>
<h1 id="1-BART的结构"><a href="#1-BART的结构" class="headerlink" title="1 BART的结构"></a>1 BART的结构</h1><ul>
<li>BART的结构就是BERT的Encoder+GPT的Decoder，<strong>对于Decoder，将原本的ReLu改为了GeLu。并且参数初始化改为服从<script type="math/tex">N(0, 0.02)</script></strong></li>
<li>base model分别有6个Encoder和Decoder，large model分别有12个</li>
<li>同等的规模，BART比BERT的参数量多10%</li>
<li><strong>BERT和GPT和BART的对比：</strong></li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809212642716.png" srcset="/img/loading.gif" lazyload alt="image-20220809212642716" style="zoom:90%;" /></p>
<p>BERT适合具有<strong>双向表征和可并行化的优点</strong>，但是由于其不是自回归的， 并且每个词是各自独立进行预测的，所以并<strong>不适合文本生成领域</strong>。而GPT由于其自回归性，可以用于文本生成。所以BART就将两者结合，结合两者有点，生成一个seq2seq模型，<strong>使输入和输出不需要对齐</strong>，可以用于文本生成、翻译等任务。</p>
<h1 id="2-BART的Pre-training"><a href="#2-BART的Pre-training" class="headerlink" title="2 BART的Pre-training"></a>2 BART的Pre-training</h1><ul>
<li>BART的预训练通过引入噪音破坏文本再恢复文本的方式进行学习，损失采用Decoder的输出和原文本的交叉熵</li>
<li><strong>BART相较于其他去噪自编码器最大的优点就是：它可以应用任何文本破坏方式，而不是特定的方法</strong></li>
</ul>
<blockquote>
<p>Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption</p>
</blockquote>
<h3 id="2-1-BART中使用的破坏文本方式"><a href="#2-1-BART中使用的破坏文本方式" class="headerlink" title="2.1 BART中使用的破坏文本方式"></a>2.1 BART中使用的破坏文本方式</h3><ul>
<li><p><strong>Token Masking：</strong>和BERT中的一样，随机词改为[MASK]</p>
</li>
<li><p><strong>Token Deletion：</strong>随机删除词</p>
</li>
<li><p><strong>Text Infilling：</strong>采样多个文本片段，每个文本片段长度服从<script type="math/tex">\lambda = 3</script>的泊松分布<strong>（长度也可为0）</strong>，每个文本片段用<strong>单个</strong>[MASK] token替换，替换成单个[MASK]能够迫使模型学习到一个片段中所缺失的token数量</p>
</li>
<li><p><strong>Sentence Permutation：</strong>按句号将文档分割成多个句子，然后随机打乱这些句子。</p>
</li>
<li><p><strong>Document Rotation：</strong>随机均匀地选择一个token，再旋转文档使文档以该token作为起始。该任务的目的是训练模型识别文档开头</p>
</li>
<li><p>举个栗子：</p>
</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809214445327.png" srcset="/img/loading.gif" lazyload alt="image-20220809214445327" style="zoom:67%;" /></p>
<ul>
<li>BART的一个关键优势是噪声的随意性，可以动用任何方式(包括改变长度)对原始文本进行破坏。<strong>这种方式让模型学习过程中更多地考虑句子的整体长度，并对输入进行更大范围的转换，从而将BERT中MLM和NSP目标统一起来。</strong></li>
</ul>
<blockquote>
<p>This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input  </p>
</blockquote>
<h1 id="3-BART的Fine-tuning"><a href="#3-BART的Fine-tuning" class="headerlink" title="3 BART的Fine-tuning"></a>3 BART的Fine-tuning</h1><h3 id="3-1-句子分类任务"><a href="#3-1-句子分类任务" class="headerlink" title="3.1 句子分类任务"></a>3.1 句子分类任务</h3><ul>
<li>方法类似于使用BERT中的[CLS]。<strong>将相同的句子同时输入Encoder和Decoder，取Decoder最后一个时间步的输出</strong></li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809230327711.png" srcset="/img/loading.gif" lazyload alt="image-20220809230327711" style="zoom:75%;" /></p>
<ul>
<li>这种方法很像seq2seq模型翻译任务中的做法，以上图为例，区别在于翻译任务只在Decoder中输入A、B、C、D，而不输入E，然后期望输出A、B、C、D、E。而在此句子分类任务中，输入A、B、C、D、E，期望输出A、B、C、D、E、Label，只取最后一个时间步的Label，用作分类。</li>
</ul>
<h3 id="3-2-Token分类和序列生成"><a href="#3-2-Token分类和序列生成" class="headerlink" title="3.2 Token分类和序列生成"></a>3.2 Token分类和序列生成</h3><ul>
<li><p><strong>Token分类：</strong>将整个文档输入encoder和decoder，每个token用其对应的最上方的decoder输出值用以分类</p>
</li>
<li><p><strong>序列生成：</strong>由于Decoder的自回归性，所以很适合序列生成，直接把数据输入进Encoder和Decoder（Decoder中输入的是label数据）即可</p>
</li>
</ul>
<h3 id="3-3-翻译任务"><a href="#3-3-翻译任务" class="headerlink" title="3.3 翻译任务"></a>3.3 翻译任务</h3><ul>
<li>翻译任务有所不同，<strong>在原本的Encoder前面又额外增加了一个随机初始化的Encoder</strong>，结构如下：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809232721147.png" srcset="/img/loading.gif" lazyload alt="image-20220809232721147" style="zoom: 85%;" /></p>
<ul>
<li><strong>新加的Encoder作用是：先将外语输入进去，然后通过该Encoder将其编码成带噪音的目标端语言，然后再通过BART降噪，作用和pre-training类似</strong></li>
</ul>
<blockquote>
<p>These layers are trained to essentially translate the foreign language to noised English, by propagation through BART, thereby using BART as a pre-trained target-side language model</p>
</blockquote>
<ul>
<li>步骤：</li>
</ul>
<blockquote>
<ol>
<li>冻结BART的大部分参数，仅更新新增加的encoder、BART位置嵌入和BART每个encoder第一层的自注意力输入投影矩阵</li>
<li>将所有模型参数进行少量迭代训练</li>
</ol>
</blockquote>
<h1 id="4-对比试验"><a href="#4-对比试验" class="headerlink" title="4 对比试验"></a>4 对比试验</h1><ul>
<li>文章对比了不同预训练目标之间的影响，包括：</li>
</ul>
<blockquote>
<ol>
<li><strong>Language Model：</strong>与GPT类似，训练一个从左到右的Transformer语言模型。该模型相当于BART的decoder，只是没有交叉注意(cross-attention)</li>
<li><strong>Permuted Language Model：</strong>该模型基于XLNet，采样1/6的token，并以自回归的随机顺序生成。为了与其他模型保持一致，这里没有引入相对位置编码和XLNet中的片段级的循环注意力机制</li>
<li><p><strong>Masked Language Model：</strong>与BERT相同，15%的token用 [MASK] token替换，训练模型重建出这些被遮蔽掉的token</p>
</li>
<li><p><strong>Multitask Masked Language Model：</strong>与 UniLM 一样，使用额外self-attention mask训练带遮蔽的语言模型。自注意力遮蔽按如下比例随机选择:1/6从左到右；1/6从右到左；1/3未遮蔽；剩余的1/3中前50%的未遮蔽，其余的从左到右遮蔽</p>
</li>
<li><strong>Masked Seq-to-Seq：</strong>与MASS模型类似，遮蔽一个片段中50%的token，并训练一个序列到序列模型预测被遮蔽的tokens</li>
</ol>
</blockquote>
<ul>
<li>实验结果如下：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809235547649.png" srcset="/img/loading.gif" lazyload alt="image-20220809235547649" style="zoom: 67%;" /></p>
<ul>
<li>通过实验对比，总结出如下结果：</li>
</ul>
<blockquote>
<ol>
<li>在不同的任务中，预训练方法的表现有显著差异。换句话说，预训练方法的有效性高度依赖于任务本身。比如，一个简单的语言模型在ELI5数据集上可以夺冠，但是在SQUAD上的结果却是最差的</li>
<li>遮蔽Token至关重要。只使用旋转文档或句子组合的预训练目标则效果较差，效果较好的都是使用了token的删除或遮蔽作为预训练目标。此外，在生成任务上，删除token似乎比遮蔽token更胜一筹</li>
<li>从左到右的预训练目标有助于文本生成任务。Masked Language Model和Permuted Language Model在文本生成任务上不如其他模型。而这两种模型在预训练阶段都没有用到从左到右的自回归语言模型</li>
<li>对于SQuAD而言双向的encoder至关重要。因为上下文在分类决策中至关重要</li>
<li>预训练目标并不是唯一重要的因素。这里的Permuted Language Model略逊于XLNet，其中一些差异可能是由于没有使用XLNet架构中的其他的改进，如相对位置编码和片段级的循环机制</li>
<li>Language Model在ELI5数据集上技压群雄，其困惑度远优于其他模型。这表明当输出仅受到输入的松散约束时，BART较为低效</li>
</ol>
</blockquote>
<h1 id="5-在各种下游任务上的表现"><a href="#5-在各种下游任务上的表现" class="headerlink" title="5 在各种下游任务上的表现"></a>5 在各种下游任务上的表现</h1><ul>
<li>在此实验中，使用large规模的模型，预训练使用RoBerta的batch size=8000和steps=500000，以及使用BPE。预处理使用了text infilling和sentence permutation，并且mask掉了30%的token，重排所有句子  </li>
</ul>
<h3 id="5-1-自然语言理解任务"><a href="#5-1-自然语言理解任务" class="headerlink" title="5.1 自然语言理解任务"></a>5.1 自然语言理解任务</h3><ul>
<li>结果如下：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810000923604.png" srcset="/img/loading.gif" lazyload alt="image-20220810000923604" style="zoom:80%;" /></p>
<ul>
<li>BART在自然语言理解任务上与其他先进模型不相上下。这表明<strong>BART在生成任务上的进一步突破并不是以牺牲自然语言理解性能为代价</strong></li>
</ul>
<h3 id="5-2-自然语言生成任务"><a href="#5-2-自然语言生成任务" class="headerlink" title="5.2 自然语言生成任务"></a>5.2 自然语言生成任务</h3><ul>
<li><p>在微调时，使用了label smooth的交叉熵损失，平滑参数为0.1。并在生成时使用大小为5的束搜索</p>
</li>
<li><p>文本摘要任务结果：</p>
</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810001908759.png" srcset="/img/loading.gif" lazyload alt="image-20220810001908759" style="zoom:80%;" /></p>
<ul>
<li><p>在这两个摘要任务上，BART 在所有度量指标上均优于之前的模型，但与人类的摘要结果相比仍然有差距</p>
</li>
<li><p>对话生成任务结果：</p>
</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002027643.png" srcset="/img/loading.gif" lazyload alt="image-20220810002027643" style="zoom:80%;" /></p>
<ul>
<li>BART 在对话生成任务上的性能同样优于之前的模型</li>
<li>抽象QA任务结果：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002302848.png" srcset="/img/loading.gif" lazyload alt="image-20220810002302848" style="zoom:80%;" /></p>
<h3 id="5-3-翻译任务"><a href="#5-3-翻译任务" class="headerlink" title="5.3 翻译任务"></a>5.3 翻译任务</h3><p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002422747.png" srcset="/img/loading.gif" lazyload alt="image-20220810002422747" style="zoom:80%;" /></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/08/10/SimCSE%E6%80%BB%E7%BB%93/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">SimCSE总结</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/08/09/RoBERTa%E6%80%BB%E7%BB%93/">
                        <span class="hidden-mobile">RoBERTa总结</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://zlkqz.github.io/2022/08/10/BART%E6%80%BB%E7%BB%93/';
          this.page.identifier = '/2022/08/10/BART%E6%80%BB%E7%BB%93/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'fluid' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
