

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="bia">
  <meta name="author" content="zlk">
  <meta name="keywords" content="">
  <meta name="description" content="1 行业知识体系1.1 机器学习算法 1.2 机器学习分类 2 线性回归2.1 基本概念  线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析  线性回归输出是⼀个连续值，因此适⽤于回归问题   2.2 损失函数 均方差（mse）：   \ell^{(i)}(W, b) &#x3D; \frac{(\hat{y}^{(i)} - y^{(i)})^2}{2}其中$\hat{y">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习基础总结">
<meta property="og:url" content="https://zlkqz.github.io/2022/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="ZLK">
<meta property="og:description" content="1 行业知识体系1.1 机器学习算法 1.2 机器学习分类 2 线性回归2.1 基本概念  线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析  线性回归输出是⼀个连续值，因此适⽤于回归问题   2.2 损失函数 均方差（mse）：   \ell^{(i)}(W, b) &#x3D; \frac{(\hat{y}^{(i)} - y^{(i)})^2}{2}其中$\hat{y">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg">
<meta property="og:image" content="https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg">
<meta property="og:image" content="https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png">
<meta property="og:image" content="https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png">
<meta property="og:image" content="https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png">
<meta property="og:image" content="https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png">
<meta property="og:image" content="https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png">
<meta property="og:image" content="https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png">
<meta property="og:image" content="https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png">
<meta property="og:image" content="https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png">
<meta property="article:published_time" content="2022-12-08T08:40:31.431Z">
<meta property="article:modified_time" content="2022-11-11T08:27:16.000Z">
<meta property="article:author" content="zlk">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg">
  
  <title>深度学习基础总结 - ZLK</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"zlkqz.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ZLK</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="深度学习基础总结">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-12-08 16:40" pubdate>
        2022年12月8日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      6.2k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      19 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">深度学习基础总结</h1>
            
            <div class="markdown-body">
              <h1 id="1-行业知识体系"><a href="#1-行业知识体系" class="headerlink" title="1 行业知识体系"></a>1 行业知识体系</h1><h3 id="1-1-机器学习算法"><a href="#1-1-机器学习算法" class="headerlink" title="1.1 机器学习算法"></a>1.1 机器学习算法</h3><p><img src="https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg" srcset="/img/loading.gif" lazyload alt="qq_pic_merged_1635482805011"></p>
<h3 id="1-2-机器学习分类"><a href="#1-2-机器学习分类" class="headerlink" title="1.2 机器学习分类"></a>1.2 机器学习分类</h3><p><img src="https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg" srcset="/img/loading.gif" lazyload alt="qq_pic_merged_1635482964705"></p>
<h1 id="2-线性回归"><a href="#2-线性回归" class="headerlink" title="2 线性回归"></a>2 线性回归</h1><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><img src="https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png" srcset="/img/loading.gif" lazyload alt="image-20211029131504886"></p>
<ul>
<li><p>线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</p>
</li>
<li><p>线性回归输出是⼀个连续值，因此适⽤于回归问题</p>
</li>
</ul>
<h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><ul>
<li><strong>均方差（mse）：</strong></li>
</ul>
<script type="math/tex; mode=display">
\ell^{(i)}(W, b) = \frac{(\hat{y}^{(i)} - y^{(i)})^2}{2}</script><p>其中$\hat{y}$为计算出的预测值，$y$为真实值（label）</p>
<h1 id="3-全连接层（稠密层）"><a href="#3-全连接层（稠密层）" class="headerlink" title="3  全连接层（稠密层）"></a>3  全连接层（稠密层）</h1><ul>
<li>输出层中的神经元和输⼊ 层中各个输⼊完全连接</li>
<li>计算完全依赖于输入层。</li>
</ul>
<h1 id="4-小批量随机梯度下降"><a href="#4-小批量随机梯度下降" class="headerlink" title="4  小批量随机梯度下降"></a>4  小批量随机梯度下降</h1><ul>
<li><p>在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\eta$）作为模型参数在本次迭代的减小量。</p>
</li>
<li><p>每次mini-batch梯度下降计算过程：</p>
</li>
</ul>
<script type="math/tex; mode=display">
w_1 = w_1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial{\ell(...)}}{\partial{w_1}}</script><ul>
<li>注意学习率要取正数</li>
</ul>
<h1 id="5-Softmax分类"><a href="#5-Softmax分类" class="headerlink" title="5  Softmax分类"></a>5  Softmax分类</h1><h3 id="5-1-基本概念"><a href="#5-1-基本概念" class="headerlink" title="5.1 基本概念"></a>5.1 基本概念</h3><ul>
<li>softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数</li>
<li>由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为该类的概率，具体计算如下：</li>
</ul>
<p>假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:</p>
<script type="math/tex; mode=display">
\hat{y}_1, \hat{y}_2, \hat{y}_3 = softmax(o_1, o_2, o_3)</script><p>其中</p>
<script type="math/tex; mode=display">
\hat{y}_i = \frac{e^{o_i}}{\sum_{j = 1}^{3}e^{o_j}}</script><ul>
<li>容易看出$\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）</li>
<li>通常，我们把预测概率最⼤的类别作为输出类别</li>
</ul>
<h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 损失函数</h3><ul>
<li>这里我们运用交叉熵损失函数（cross entropy）：</li>
</ul>
<script type="math/tex; mode=display">
H(y^{(i)}, \hat{y}^{(i)}) = -\sum_{j = 1}^qy_j^{(i)}\log{\hat{y}^{(i)}}</script><p>q为输出元个数$\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值</p>
<h1 id="6-多层感知机（MLP）"><a href="#6-多层感知机（MLP）" class="headerlink" title="6 多层感知机（MLP）"></a>6 多层感知机（MLP）</h1><ul>
<li>多层感知机有一到多个隐藏层</li>
<li>多层感知机中的隐藏层和输出层都是全连接层</li>
<li>多层感知机具有激活函数，下面介绍常用激活函数</li>
</ul>
<h1 id="7-激活函数"><a href="#7-激活函数" class="headerlink" title="7 激活函数"></a>7 激活函数</h1><h3 id="7-1-Sigmoid函数"><a href="#7-1-Sigmoid函数" class="headerlink" title="7.1 Sigmoid函数"></a>7.1 Sigmoid函数</h3><script type="math/tex; mode=display">
sigmoid(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">
sigmoid'(x) = sigmoid(x)(1 - sigmoid(x))</script><p><img src="https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png" srcset="/img/loading.gif" lazyload alt="image-20211030004640247"></p>
<ul>
<li>一般用在二分类的输出层中</li>
</ul>
<h3 id="7-2-tanh函数"><a href="#7-2-tanh函数" class="headerlink" title="7.2 tanh函数"></a>7.2 tanh函数</h3><script type="math/tex; mode=display">
tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}</script><script type="math/tex; mode=display">
tanh'(x) = 1 - tanh^2(x)</script><p><img src="https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png" srcset="/img/loading.gif" lazyload alt="image-20211030005138300"></p>
<ul>
<li>tanh和sigmoid比较相似，但是性能一般要优于sigmoid</li>
</ul>
<h3 id="7-3-ReLU函数"><a href="#7-3-ReLU函数" class="headerlink" title="7.3 ReLU函数"></a>7.3 ReLU函数</h3><script type="math/tex; mode=display">
ReLU(x) = max(0, x)</script><p><img src="https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png" srcset="/img/loading.gif" lazyload alt="image-20211030005121490"></p>
<ul>
<li>十分常用</li>
</ul>
<h3 id="7-4-Leaky-ReLU函数"><a href="#7-4-Leaky-ReLU函数" class="headerlink" title="7.4 Leaky ReLU函数"></a>7.4 Leaky ReLU函数</h3><script type="math/tex; mode=display">
Leaky\_Relu(x) = max(0.1 * x, x)</script><p><img src="https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png" srcset="/img/loading.gif" lazyload alt="image-20211030005818507"></p>
<h1 id="8-训练误差和泛化误差"><a href="#8-训练误差和泛化误差" class="headerlink" title="8 训练误差和泛化误差"></a>8 训练误差和泛化误差</h1><h3 id="8-1-基本概念"><a href="#8-1-基本概念" class="headerlink" title="8.1 基本概念"></a>8.1 基本概念</h3><ul>
<li><strong>训练误差：</strong>指模型在训练数据集上表现出的误差</li>
<li><strong>泛化误差：</strong>指模型在任意⼀个测试数据样本上表现出的误差的期望</li>
</ul>
<h3 id="8-2-k折交叉验证"><a href="#8-2-k折交叉验证" class="headerlink" title="8.2 k折交叉验证"></a>8.2 k折交叉验证</h3><ul>
<li>当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法</li>
<li>即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个子数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。</li>
</ul>
<h3 id="8-3-欠拟合和过拟合"><a href="#8-3-欠拟合和过拟合" class="headerlink" title="8.3 欠拟合和过拟合"></a>8.3 欠拟合和过拟合</h3><ul>
<li><p><strong>欠拟合：</strong>训练误差过高</p>
</li>
<li><p><strong>过拟合：</strong>泛化误差过高</p>
</li>
<li><p>我们知道进行反向传播的目的是让学习器去拟合数据，而当拟合程度不够时就称为<strong>欠拟合</strong>，拟合程度过于高时则称为<strong>过拟合</strong>，用下图可以很好的解释：</p>
<p><img src="https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png" srcset="/img/loading.gif" lazyload alt="image-20211111215618245"  /></p>
</li>
<li><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了，<strong>可以使用early stop来防止训练时间不足导致的欠拟合</strong>。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法。而一般我们更加关注的是如何如何防止过拟合</p>
</li>
</ul>
<h3 id="8-4-造成过拟合的原因"><a href="#8-4-造成过拟合的原因" class="headerlink" title="8.4 造成过拟合的原因"></a>8.4 造成过拟合的原因</h3><ul>
<li><strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型</li>
<li><strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系</li>
<li><strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。<blockquote>
<ul>
<li>模型太复杂是过拟合的重要因素，下面提到的正则化方法和Dropout方法都是基于减少模型复杂度来进行的。</li>
<li>另外，在模型设计方面有一个根本设计原则<strong>奥卡姆剃刀法则：优先选择拟合数据的最简单的假设。 简单的模型才是最好的</strong></li>
<li><strong>当模型复杂度过高的时候，拟合函数的系数往往非常大，需要顾忌每一个点，最终形成的拟合函数波动很大</strong>，如上图过拟合情况。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的梯度非常大，所以才需要惩罚项来降低</li>
<li><strong>模型复杂度也可以解释为：模型对某些非必要的特征过于看重（即给予了较大的权值）</strong></li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="8-5-对应过拟合手段"><a href="#8-5-对应过拟合手段" class="headerlink" title="8.5 对应过拟合手段"></a>8.5 对应过拟合手段</h3><h4 id="8-5-1-增加数据"><a href="#8-5-1-增加数据" class="headerlink" title="8.5.1 增加数据"></a>8.5.1 增加数据</h4><ul>
<li>由于<strong>数据量不足是造成过拟合的根本原因</strong>，所以对应过拟合最有效的手段肯定是增大数据集，但是这种方法成本过高</li>
<li>在数据层面，还可以进行<strong>数据增强</strong>，创造一些假数据，还可以提升模型的泛化能力</li>
<li>在CV方面常用的数据增强有旋转图像、缩放图像、随机裁剪等，而在NLP方面还有同义词替换、随机删除词、随机颠倒句子顺序等方法</li>
</ul>
<h4 id="8-5-2-正则化"><a href="#8-5-2-正则化" class="headerlink" title="8.5.2 正则化"></a>8.5.2 正则化</h4><ul>
<li>L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积</li>
<li>损失函数添加一个L2惩罚项：</li>
</ul>
<script type="math/tex; mode=display">
\ell(...) + \frac{\lambda}{2}\begin{Vmatrix} W \end{Vmatrix}^2</script><p>求导后：（每个mini-batch）</p>
<script type="math/tex; mode=display">
w1 = (1 - \eta\lambda)w1 - \frac{\eta}{|\mathbb{B}|}\sum_{i \in \mathbb{B}}\frac{\partial\ell^{(i)}(...)}{\partial{w1}}</script><p>其中超参数$\lambda$ &gt; 0。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作⽤。</p>
<ul>
<li>除此之外还有L1正则化，使用L1范数作为惩罚项：</li>
</ul>
<script type="math/tex; mode=display">
\ell(...) + \frac{\lambda}{2}\begin{Vmatrix} W \end{Vmatrix}</script><ul>
<li>一般训练集中的损失函数要加惩罚项，测试集中不用</li>
<li>正则化的原理可以从<strong>拉格朗日乘子法和最大后验概率估计</strong>两方面解释</li>
<li>首先是拉格朗日乘子法：</li>
</ul>
<blockquote>
<ul>
<li>设模型本来的优化目标为：</li>
</ul>
<script type="math/tex; mode=display">
\min _{w} J(w ; X, y)</script><ul>
<li>而过拟合的原因在于模型复杂度过高，而一般模型复杂度和参数<script type="math/tex">w</script>的稀疏度相关，即<strong>参数越少或参数中0的个数越多，模型复杂度越小</strong></li>
<li>那么0的个数可以让我们联想到<strong>w的L0范数（代表w中非零元素的个数）</strong>，那么可以<strong>添加一个约束，使得优化目标变为：</strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{c}
\min _{w} J(w ; X, y) \\
s \cdot t .\|w\|_{0} \leq C
\end{array}</script><ul>
<li>但是<strong>使用L0范数太过困难，主要是L0范数无法进行求导来反向传播，所以我们退而求其次，要求权重w向量中某些维度的非零参数尽可能接近于0，尽可能的小，所以就可以使用L1、L2范数：</strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{c}
\min _{w} J(w ; X, y) \\
s \cdot t .\|w\|_{1} \leq C 或 s \cdot t .\|w\|_{2} \leq C 
\end{array}</script><ul>
<li>以L2正则化为例，采用拉格朗日乘子法，其拉格朗日函数为：</li>
</ul>
<script type="math/tex; mode=display">
\min _{w} J(w ; X, y)+\alpha^{*}\|w\|_{2}^{2}</script><ul>
<li>这就是现在的优化目标，而后面的一项恰好就是惩罚项</li>
</ul>
</blockquote>
<ul>
<li>还可以用最大后验概率估计来解释：</li>
</ul>
<blockquote>
<ul>
<li>最大后验估计的优化目标是：</li>
</ul>
<script type="math/tex; mode=display">
M A P=\log P(y \mid X, w) P(w)=\log P(y \mid X, w)+\log P(w)</script><ul>
<li>上式左边的一项即最大化似然的优化目标，也就是本来的不加惩罚项的优化目标；<strong>而右边的一项仅关于参数w的先验分布</strong></li>
<li>我们对w的先验分布进行不同的假设，即可得到不同的惩罚项</li>
<li><strong>在L2正则化中，假设<script type="math/tex">w \sim N(0, \sigma^2)</script>，那么有：</strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{l}
\log P(w)=\log \prod_{j} P\left(w_{j}\right)= \\
\log \prod_{j}\left[\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(w_{j}\right)^{2}}{2 \sigma^{2}}}\right] \\
=-\frac{1}{2 \sigma^{2}} \sum_{j} w_{j}^{2}+C
\end{array}</script><p>那么就得到了L2范数作为惩罚项</p>
<ul>
<li><strong>L1正则化也一样，假设<script type="math/tex">w</script>服从均值为0参数为a的拉普拉斯分布，那么有：</strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{l}
\log P(w)=\log \prod_{j} P\left(w_{j}\right)= \\
\log \prod_{j}\left[\frac{1}{\sqrt{2 a} \sigma} e^{-\frac{w_{j}}{a}}\right] \\
=-\frac{1}{2 a} \sum_{j}\left|w_{j}\right|+C
\end{array}</script></blockquote>
<h4 id="8-5-2-丢弃法（Dropout）"><a href="#8-5-2-丢弃法（Dropout）" class="headerlink" title="8.5.2 丢弃法（Dropout）"></a>8.5.2 丢弃法（Dropout）</h4><ul>
<li>对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：</li>
</ul>
<script type="math/tex; mode=display">
h_i' = \frac{\xi_i}{1 - p}h_i</script><p>$\xi_i$为0和1的概率分别为p和1 - p</p>
<p>分母中的1- p是为了不改变其输⼊的期望值：</p>
<script type="math/tex; mode=display">
由于E(\xi_i) = 1 - p \\
所以E(h_i') = \frac{E(\xi_i)}{1 - p}h_i = h_i</script><ul>
<li><p>测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法</p>
</li>
<li><p>进行了Dropout的多层感知机：</p>
</li>
</ul>
<p><img src="https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png" srcset="/img/loading.gif" lazyload alt="image-20211030141251677"></p>
<p>可以看到隐藏层中的$h_2$和$h_5$消失了</p>
<ul>
<li><p>每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点</p>
</li>
<li><p><strong>Dropout的原理：</strong></p>
</li>
</ul>
<blockquote>
<ol>
<li><strong>集成学习：</strong>每次随机置0都可以得到一个新模型，而Dropout可以当作对这些所有新得到的模型的集成</li>
<li><strong>正则化：</strong>因为Dropout导致两个神经元不一定每次都在一个dropout网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 ，迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。正因为这样，网络由于不知道浅层的哪些神经元会失活，导致网络不敢赋予浅层神经元太大的权重，这样就减轻了网络对某些局部特征的依赖，换句话说网络不会对一些特定的线索片段太过敏感，即使丢失特定的线索，它也可以从众多其它线索中学习一些共同的特征。从这个角度看Dropout就类似于L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</li>
</ol>
</blockquote>
<h1 id="9-正向传播和反向传播"><a href="#9-正向传播和反向传播" class="headerlink" title="9 正向传播和反向传播"></a>9 正向传播和反向传播</h1><h3 id="9-1-正向传播"><a href="#9-1-正向传播" class="headerlink" title="9.1 正向传播"></a>9.1 正向传播</h3><ul>
<li>正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）</li>
<li>中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等</li>
<li>正向传播的计算图：</li>
</ul>
<p><img src="https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png" srcset="/img/loading.gif" lazyload alt="image-20211030142019713"></p>
<p>其中：</p>
<ol>
<li>左下⻆是输⼊，右上⻆是输出</li>
<li>⽅框代表变量，圆圈代表运算符</li>
<li>$J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项</li>
</ol>
<h3 id="9-2-反向传播"><a href="#9-2-反向传播" class="headerlink" title="9.2 反向传播"></a>9.2 反向传播</h3><ul>
<li>反向传播最重要的是通过链式法则求导，从后往前进行计算</li>
<li>反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的</li>
</ul>
<h3 id="9-3-衰减和爆炸"><a href="#9-3-衰减和爆炸" class="headerlink" title="9.3 衰减和爆炸"></a>9.3 衰减和爆炸</h3><ul>
<li>如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：</li>
</ul>
<p>假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积</p>
<h3 id="9-4-随机初始化参数"><a href="#9-4-随机初始化参数" class="headerlink" title="9.4 随机初始化参数"></a>9.4 随机初始化参数</h3><ul>
<li>考虑三种参数的极端情况：</li>
</ul>
<blockquote>
<ol>
<li><strong>参数很稀疏，那么输入的特征会在经过前向计算后逐渐消失（计算出来都接近0）</strong></li>
<li><strong>参数很大，输出值容易进入激活函数的饱和区，比如Sigmoid函数</strong></li>
<li>假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。<strong>在这种情况下，无论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作用</strong></li>
</ol>
</blockquote>
<ul>
<li>可以看出，我们<strong>希望每一层的输出方差能够固定</strong>，因为这样能够防止我们的信号变得很大/直接消失。换句话来说，<strong>我们需要一种权重初始化的手段，使得输入、输出的方差保持不变，</strong>这就是Xavier初始化做的事。</li>
</ul>
<h3 id="9-5-Xavier随机初始化"><a href="#9-5-Xavier随机初始化" class="headerlink" title="9.5 Xavier随机初始化"></a>9.5 Xavier随机初始化</h3><ul>
<li>假设某层的参数，前一层神经元个数为a，后一层神经元个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：</li>
</ul>
<script type="math/tex; mode=display">
U(-\sqrt{\frac{6}{a + b}}, \sqrt{\frac{6}{a + b}})</script><ul>
<li><p>该设计主要为了<strong>在前向计算的时候，经过此层的参数后得到的输出方差不变（即输入输出方差相等）；并且在反向计算的时候，后一层的梯度和前一层的梯度方差也相等</strong></p>
</li>
<li><p><strong>Xavier初始化在sigmoid和tanh激活函数上有很好的表现，但是在Relu激活函数上表现很差。</strong></p>
</li>
<li><p>现在开始推导，这里只展示前向计算时，首先是有3个假设：</p>
</li>
</ul>
<blockquote>
<ul>
<li>激活函数为<script type="math/tex">f(x) = x</script></li>
<li>偏置项b初始化为0</li>
<li>输入和参数的期望都为0</li>
</ul>
</blockquote>
<ul>
<li>设某一层值为<script type="math/tex">x</script>，参数为<script type="math/tex">w</script>，对应输入为<script type="math/tex">y = x^Tw</script>（注意<script type="math/tex">x,w</script>为向量，<script type="math/tex">y</script>为标量），那么我们的目的是：</li>
</ul>
<script type="math/tex; mode=display">
Var[y] = Var[x] \\
Var[x^Tw] = Var[x]</script><ul>
<li>引入公式：</li>
</ul>
<script type="math/tex; mode=display">
Var[XY] = E[X^2]E[Y^2] - E^2[X]E^2[Y] \\
在E[X] = E[Y]时，可以得出： Var[XY] = Var[X]Var[Y]</script><ul>
<li>而由于<script type="math/tex">E[w] = E[x] = 0</script>，所以：</li>
</ul>
<script type="math/tex; mode=display">
Var[x] = Var[x^Tw] = Var[\sum^{N}_ix_iw_i] = N* Var[x]Var[w]</script><p>其中<script type="math/tex">N = dim(x) = dim(w)</script>，即当前层的神经元个数</p>
<ul>
<li>由上式可以得出：</li>
</ul>
<script type="math/tex; mode=display">
N * Var[w] = 1 \\
Var[w] = \frac{1}{N}</script><ul>
<li>而在反向传播的时候，我们同样可以推出<script type="math/tex">Var[w] = \frac{1}{N'}</script>，其中<script type="math/tex">N'</script>为下一层神经元个数</li>
<li>而由于方差取值无法同时满足前后向，所以采用两者的平均数：</li>
</ul>
<script type="math/tex; mode=display">
Var[w] = \frac{2}{N + N'}</script><ul>
<li>我们可以通过<script type="math/tex">N(0, \frac{2}{N+N'})</script>对w取样，也可以使用均匀分布采样，采用均匀函数的时候需要进行一定的放缩（Var[w]的分子不一定为2）</li>
</ul>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/12/08/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E6%AF%94%E8%BE%83/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">激活函数的作用和比较</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/12/08/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/">
                        <span class="hidden-mobile">最大熵模型</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://zlkqz.github.io/2022/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/';
          this.page.identifier = '/2022/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'fluid' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
