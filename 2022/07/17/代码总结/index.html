

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="bia">
  <meta name="author" content="zlk">
  <meta name="keywords" content="">
  <meta name="description" content="NLTK用法 nltk用于英文分词分句等应用  基本的预处理 分句：nltk.sent_tokenize(text, language&#x3D;”english”):   输入：一个str段落 输出：一个list，每个元素是一个str句子   分词：nltk.word_tokenize(text, language&#x3D;”english”, preserve_line&#x3D;False):   输入：一个str句子">
<meta property="og:type" content="article">
<meta property="og:title" content="代码总结（持续更新）">
<meta property="og:url" content="https://zlkqz.github.io/2022/07/17/%E4%BB%A3%E7%A0%81%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="ZLK">
<meta property="og:description" content="NLTK用法 nltk用于英文分词分句等应用  基本的预处理 分句：nltk.sent_tokenize(text, language&#x3D;”english”):   输入：一个str段落 输出：一个list，每个元素是一个str句子   分词：nltk.word_tokenize(text, language&#x3D;”english”, preserve_line&#x3D;False):   输入：一个str句子">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20191219223810610.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724143414199.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211345194.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724153110907.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211302209.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211544134.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706160148320.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706210417422.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220711120934316.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123006189.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123836380.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725142611139.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143928089.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143939723.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143953768.png">
<meta property="og:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725144005108.png">
<meta property="article:published_time" content="2022-07-16T16:00:00.000Z">
<meta property="article:modified_time" content="2022-12-20T06:23:15.580Z">
<meta property="article:author" content="zlk">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20191219223810610.png">
  
  <title>代码总结（持续更新） - ZLK</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"zlkqz.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ZLK</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="代码总结（持续更新）">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-07-17 00:00" pubdate>
        2022年7月17日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      23k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      73 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">代码总结（持续更新）</h1>
            
            <div class="markdown-body">
              <h1 id="NLTK用法"><a href="#NLTK用法" class="headerlink" title="NLTK用法"></a>NLTK用法</h1><ul>
<li>nltk用于<strong>英文</strong>分词分句等应用</li>
</ul>
<h3 id="基本的预处理"><a href="#基本的预处理" class="headerlink" title="基本的预处理"></a>基本的预处理</h3><ul>
<li><strong>分句：nltk.sent_tokenize(text, language=”english”):</strong></li>
</ul>
<blockquote>
<p>输入：一个str段落</p>
<p>输出：一个list，每个元素是一个str句子</p>
</blockquote>
<ul>
<li><strong>分词：nltk.word_tokenize(text, language=”english”, preserve_line=False):</strong></li>
</ul>
<blockquote>
<p>输入：一个str句子</p>
<p>输出：一个list，每个元素是一个str词</p>
</blockquote>
<ul>
<li><strong>词性标注（POS_tag）：nltk.postag(tokens)</strong></li>
</ul>
<blockquote>
<p>输入：一个list，每个元素是一个str，一个句子分好词的结果</p>
<p>输入：一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[(‘football’, ‘NN’), (‘is’, ‘VBZ’), (‘a’, ‘DT’), (‘family’, ‘NN’)]</p>
<p>词性所对应的意义，大致来说<strong>N开头对应名词NOUN，V开头对应动词VERB，J开头对应形容词ADJ，R开头对应副词ADV</strong></p>
<p>词性具体所对应意义如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标记</th>
<th>含义</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>CC</td>
<td>连词</td>
<td>and, or,but, if, while,although</td>
</tr>
<tr>
<td>CD</td>
<td>数词</td>
<td>twenty-four, fourth, 1991,14:24</td>
</tr>
<tr>
<td>DT</td>
<td>限定词</td>
<td>the, a, some, most,every, no</td>
</tr>
<tr>
<td>EX</td>
<td>存在量词</td>
<td>there, there’s</td>
</tr>
<tr>
<td>FW</td>
<td>外来词</td>
<td>dolce, ersatz, esprit, quo,maitre</td>
</tr>
<tr>
<td>IN</td>
<td>介词连词</td>
<td>on, of,at, with,by,into, under</td>
</tr>
<tr>
<td>JJ</td>
<td>形容词</td>
<td>new,good, high, special, big, local</td>
</tr>
<tr>
<td>JJR</td>
<td>比较级词语</td>
<td>bleaker braver breezier briefer brighter brisker</td>
</tr>
<tr>
<td>JJS</td>
<td>最高级词语</td>
<td>calmest cheapest choicest classiest cleanest clearest</td>
</tr>
<tr>
<td>LS</td>
<td>标记</td>
<td>A A. B B. C C. D E F First G H I J K</td>
</tr>
<tr>
<td>MD</td>
<td>情态动词</td>
<td>can cannot could couldn’t</td>
</tr>
<tr>
<td>NN</td>
<td>名词</td>
<td>year,home, costs, time, education</td>
</tr>
<tr>
<td>NNS</td>
<td>名词复数</td>
<td>undergraduates scotches</td>
</tr>
<tr>
<td>NNP</td>
<td>专有名词</td>
<td>Alison,Africa,April,Washington</td>
</tr>
<tr>
<td>NNPS</td>
<td>专有名词复数</td>
<td>Americans Americas Amharas Amityvilles</td>
</tr>
<tr>
<td>PDT</td>
<td>前限定词</td>
<td>all both half many</td>
</tr>
<tr>
<td>POS</td>
<td>所有格标记 ’</td>
<td>‘s</td>
</tr>
<tr>
<td>PRP</td>
<td>人称代词</td>
<td>hers herself him himself hisself</td>
</tr>
<tr>
<td>PRP$</td>
<td>所有格</td>
<td>her his mine my our ours</td>
</tr>
<tr>
<td>RB</td>
<td>副词</td>
<td>occasionally unabatingly maddeningly</td>
</tr>
<tr>
<td>RBR</td>
<td>副词比较级</td>
<td>further gloomier grander</td>
</tr>
<tr>
<td>RBS</td>
<td>副词最高级</td>
<td>best biggest bluntest earliest</td>
</tr>
<tr>
<td>RP</td>
<td>虚词</td>
<td>aboard about across along apart</td>
</tr>
<tr>
<td>SYM</td>
<td>符号</td>
<td>% &amp; ’ ‘’ ‘’. ) )</td>
</tr>
<tr>
<td>TO</td>
<td>词to</td>
<td>to</td>
</tr>
<tr>
<td>UH</td>
<td>感叹词</td>
<td>Goodbye Goody Gosh Wow</td>
</tr>
<tr>
<td>VB</td>
<td>动词</td>
<td>ask assemble assess</td>
</tr>
<tr>
<td>VBD</td>
<td>动词过去式</td>
<td>dipped pleaded swiped</td>
</tr>
<tr>
<td>VBG</td>
<td>动词现在分词</td>
<td>telegraphing stirring focusing</td>
</tr>
<tr>
<td>VBN</td>
<td>动词过去分词</td>
<td>multihulled dilapidated aerosolized</td>
</tr>
<tr>
<td>VBP</td>
<td>动词现在式非第三人称时态</td>
<td>predominate wrap resort sue</td>
</tr>
<tr>
<td>VBZ</td>
<td>动词现在式第三人称时态</td>
<td>bases reconstructs marks</td>
</tr>
<tr>
<td>WDT</td>
<td>Wh限定词</td>
<td>who,which,when,what,where,how</td>
</tr>
<tr>
<td>WP</td>
<td>WH代词</td>
<td>that what whatever</td>
</tr>
<tr>
<td>WP$</td>
<td>WH代词所有格</td>
<td>whose</td>
</tr>
<tr>
<td>WRB</td>
<td>WH副词</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<ul>
<li><strong>词形还原Lemmatization：nltk.stem.WordNetLemmatizer().lemmatize(word, pos=”n”)：</strong></li>
</ul>
<blockquote>
<p>这是一个类的内置方法，首先需要创建WordNetLemmatizer对象（假设为wnl），则通过<code>wnl.lemmatize()</code>调用</p>
<p>输入：word是单个词，pos是其对应的单词词性（n对应noun，v对应verb，a对应adj，r对应adv，s对应satellite adjectives(不咋用)）</p>
<p>输出：word经过还原后的词，如cars还原为car</p>
</blockquote>
<h3 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h3><ul>
<li>先介绍一下命名实体识别（Named Entity Recognition，NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。</li>
<li>nltk中对NER类别的分类如下：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20191219223810610.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>其中LOCATION和GPE有重合，GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。</p>
<ul>
<li><strong>进行NER：ne_chunk(tagged_tokens, binary=False)</strong></li>
</ul>
<blockquote>
<p>输入：tagged_tokens为<code>pos_tag()</code>函数的输出，一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[(‘football’, ‘NN’), (‘is’, ‘VBZ’), (‘a’, ‘DT’), (‘family’, ‘NN’)]</p>
<p>输出：同样的是一个list，每个元素变为了一个封装对象（Tree类），和输入一一对应</p>
<ul>
<li><strong>对于该封装对象：</strong></li>
</ul>
<p>输出的list中，有些元素是没有NER的结果的，其对应的封装对象没有label属性，<strong>可使用<code>hasattr(ne_word, &#39;label&#39;)</code>函数判断是否有NER结果</strong></p>
<p>假设一个该对象命名为ne_word，调用<code>ne_word.leaves()</code>可返回一个list，每个元素为一个tuple，tuple[0]为token，tuple[1]为该token的pos_tag。对于NER，list中只有一个tuple，返回结果示例：[(‘FIFA’, ‘NNP’)]</p>
<p>调用<code>ne_word.label()</code>函数可返回该token对应的NER结果，一定要先使用<code>hasattr()</code>函数才能使用<code>label()</code>函数</p>
</blockquote>
<h3 id="计算BLEU"><a href="#计算BLEU" class="headerlink" title="计算BLEU"></a>计算BLEU</h3><ul>
<li><p><a target="_blank" rel="noopener" href="https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#5-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91">BLEU的定义</a></p>
</li>
<li><p><strong>nltk.translate.bleu_score.sentence_bleu(references, hypothesis, …, smoothing_function=None, …)</strong></p>
</li>
</ul>
<blockquote>
<p>references：参照序列，label。<strong>type=list(list(str))，其中的每个str为词或字</strong></p>
<p>hypothesis：候选序列，也就是预测出的序列。<strong>type=list(str)，其中的每个str为词或字</strong></p>
<p>smoothing_function，是论文中使用到的平滑技巧，一般输入<code>nltk.translate.bleu_score.SmoothingFunction().methodi()</code>，最后的i为0~7</p>
</blockquote>
<ul>
<li>另外还可以使用<code>corpus_bleu()</code>计算多个句子的BLEU；用<code>modified_precision()</code>计算修正的n-gram精确度</li>
</ul>
<h1 id="Pyltp用法"><a href="#Pyltp用法" class="headerlink" title="Pyltp用法"></a>Pyltp用法</h1><ul>
<li>Pyltp主要用于中文的预处理等，包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注</li>
<li>使用Pyltp时注意其编码使用的都是utf-8，而Windows终端使用GBK编码，如果出现乱码可以将输出重定向到文件，然后用utf-8编码查看</li>
<li><strong>分句：pyltp.SentenceSplitter()</strong></li>
</ul>
<blockquote>
<p>使用时先创建实例：<code>sp = SentenceSplitter()</code>，再进行分句：<code>sents = sp.split(doc)</code></p>
</blockquote>
<ul>
<li><strong>分词：pyltp.Segmentor()</strong></li>
</ul>
<blockquote>
<p>Segmentor加载模型可以用<code>load()</code>也可以用<code>load_with_lexicon()</code>，后者还要加一个用户词典的参数</p>
</blockquote>
<ul>
<li><strong>词性标注：pyltp.Postagger()</strong></li>
</ul>
<blockquote>
<p>直接举个栗子吧：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">sent = <span class="hljs-string">&quot;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&quot;</span><br><span class="hljs-comment"># 加载模型</span><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br>postagger = Postagger()<br>postagger.load(pos_model_path)<br><span class="hljs-comment"># 分词和词性标注</span><br>words = segmentor.segment(sent)<br>postags = postagger.postag(words)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(postags))<br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br></code></pre></td></tr></table></figure>
<p>注意：进行这些处理后返回的都是VectorOfString类，是一个可迭代类，也可用list()转换为列表</p>
<p>词性标注的词性如下：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724143414199.png" srcset="/img/loading.gif" lazyload alt="image-20220724143414199" style="zoom: 50%;" /></p>
<p>词性标注也可以添加用户词典</p>
</blockquote>
<ul>
<li><strong>NER识别：pyltp.NamedEntityRecognizer()</strong></li>
</ul>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> NamedEntityRecognizer<br>recognizer = NamedEntityRecognizer()<br>recognizer.load(ner_model_path)<br>ner_results = recognizer.recognize(words, postags)<br></code></pre></td></tr></table></figure>
<p>LTP 采用 BIESO 标注体系。<strong>B 表示实体开始词，I表示实体中间词，E表示实体结束词，S表示单独成实体，O表示不构成命名实体。</strong></p>
<p>LTP 提供的命名实体类型为：<strong>人名（Nh）、地名（Ns）、机构名（Ni）</strong>。</p>
<p>B、I、E、S位置标签和实体类型标签之间用一个横线 <code>-</code> 相连；O标签后没有类型标签。</p>
</blockquote>
<ul>
<li><strong>依存句法分析：pyltp.Parser()</strong></li>
</ul>
<blockquote>
<p>依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系</p>
<p>首先也是创建实例和加载模型，然后<code>parse_results = parser.parse(words, postags)</code>，得到的结果是一个可迭代对象，其中的每一个元素也是个自定义类（假设parse_results[i]为result），调用<code>result.head</code>和<code>result.relation</code>可分别获得words[i]对应的关系词和关系，其中head是返回关系词所对应的索引+1，如果为0则为”Root”，表示无对应关系词</p>
<p>举个栗子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">relations = [result.relation <span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> parse_results]<br>heads = [words[result.head - <span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> result.head <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;Root&quot;</span> <span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> parse_results]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br> <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;relations[i]&#125;</span> : (<span class="hljs-subst">&#123;words[i]&#125;</span>, <span class="hljs-subst">&#123;heads[i]&#125;</span>)&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>输出结果：</p>
<p><div align=center><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211345194.png" srcset="/img/loading.gif" lazyload alt="image-20220724211345194" style="zoom: 80%;" /></p>
<p>依存句法关系如下：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724153110907.png" srcset="/img/loading.gif" lazyload alt="image-20220724153110907" style="zoom:50%;" /></p>
</blockquote>
<ul>
<li><strong>语义角色标注：pyltp.SementicRoleLabeller()</strong></li>
</ul>
<blockquote>
<p>语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等<br>进行语义角色标注时首先同样是创建实例，再加载模型，然后调用方法：<code>roles = labeller.label(words, postags, parse_results)</code>。结果得到一个可迭代对象，<strong>其中的每一个元素也是一个自定义类（假设每个元素为role），则<code>role.index</code>为谓语对应的索引，<code>role.arguments</code>又是一个可迭代对象，每个元素对应一个和该谓语对应的语义角色，也是一个自定义类（假设为argument），</strong>那么可以通过<code>argument.name</code>获取角色和谓语的关系，<code>argument.range.start</code>和<code>argument.range.end</code>对应该角色的开始和结束索引（<strong>end要算上的</strong>）</p>
<p>给个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">roles = labeller.label(words, postags, parse_results)<br><span class="hljs-keyword">for</span> role <span class="hljs-keyword">in</span> roles:<br> arguments = role.arguments<br> index = role.index<br> <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;谓语: <span class="hljs-subst">&#123;words[index]&#125;</span> (索引: <span class="hljs-subst">&#123;index&#125;</span>)&quot;</span>)<br> <span class="hljs-keyword">for</span> argument <span class="hljs-keyword">in</span> arguments:<br>     start, end = argument.<span class="hljs-built_in">range</span>.start, argument.<span class="hljs-built_in">range</span>.end<br>     obj = <span class="hljs-string">&quot;&quot;</span><br>     <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words[start : end+<span class="hljs-number">1</span>]:<br>         obj += word<br>     <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;argument.name&#125;</span>: <span class="hljs-subst">&#123;obj&#125;</span> (索引: <span class="hljs-subst">&#123;start&#125;</span>:<span class="hljs-subst">&#123;end&#125;</span>)&quot;</span>)<br> <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>输出结果：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211302209.png" srcset="/img/loading.gif" lazyload alt="image-20220724211302209" style="zoom:67%;" /></p>
<p>语义角色如下：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211544134.png" srcset="/img/loading.gif" lazyload alt="image-20220724211544134" style="zoom:60%;" /></p>
</blockquote>
<h1 id="rouge用法"><a href="#rouge用法" class="headerlink" title="rouge用法"></a>rouge用法</h1><ul>
<li>rouge和bleu一样也是一种对序列质量的评估标准，但是相比于bleu更关注召回率而非精准率（通过计算F1_score时改变<script type="math/tex">\beta</script>参数），rouge-n计算公式如下：</li>
</ul>
<script type="math/tex; mode=display">
Rough-N=\frac{\sum_{S \in\{\text { ReferemceSummaries }\}} \sum_{\text {gram }_{n} \in S} \text { Count }_{\text {match }}\left(\text { gram }_{n}\right)}{\sum_{S \in\{\text { ReferenceSummaries }\}} \text { gram }_{n} \in S}</script><ul>
<li>rouge-1和rouge-2都可通过上述公式计算出来，和bleu中的P1和P2计算公式很像。可以在下面看到，每个rough-N都会输出p和r，其实就是精准率和召回率，两者也就是分母不一样（一个分母是refs的n-gram个数，一个分母是hyps的n-gram个数）</li>
<li>而rouge-l是rouge-N的改进：</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{l}
R_{l c s}=\frac{L C S(X, Y)}{m} \\
P_{l c s}=\frac{L C S(X, Y)}{n} \\
F_{l c s}=\frac{\left(1+\beta^{2}\right) R_{l c s} P_{l c s}}{R_{l c s}+\beta^{2} P_{l c s}}
\end{array}</script><p>其中X、Y为hyps和refs，m、n为他们的长度，LCS(X, Y)为X、Y的最长共同序列</p>
<ul>
<li><strong>rouge.Rouge.get_scores(hyps, refs, avg=False, ignore_empty=False)</strong></li>
</ul>
<blockquote>
<p><strong>注意：输入的hyps和refs两个字符串，不管是中文英文，都需要每个字或词使用空格间隔</strong></p>
<p>输出是list(dict(dict))</p>
</blockquote>
<ul>
<li>直接给栗子吧：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> rouge <span class="hljs-keyword">import</span> Rouge <br><br>hypothesis = <span class="hljs-string">&quot;the #### transcript is a written version of each day &#x27;s cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news&quot;</span><br><br>reference = <span class="hljs-string">&quot;this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students &#x27; knowledge of even ts in the news&quot;</span><br><br>rouge = Rouge()<br>scores = rouge.get_scores(hypothesis, reference)<br></code></pre></td></tr></table></figure>
<p><strong>output：</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs json">[<br>  &#123;<br>    <span class="hljs-attr">&quot;rouge-1&quot;</span>: &#123;<br>      <span class="hljs-attr">&quot;f&quot;</span>: <span class="hljs-number">0.4786324739396596</span>,<br>      <span class="hljs-attr">&quot;p&quot;</span>: <span class="hljs-number">0.6363636363636364</span>,<br>      <span class="hljs-attr">&quot;r&quot;</span>: <span class="hljs-number">0.3835616438356164</span><br>    &#125;,<br>    <span class="hljs-attr">&quot;rouge-2&quot;</span>: &#123;<br>      <span class="hljs-attr">&quot;f&quot;</span>: <span class="hljs-number">0.2608695605353498</span>,<br>      <span class="hljs-attr">&quot;p&quot;</span>: <span class="hljs-number">0.3488372093023256</span>,<br>      <span class="hljs-attr">&quot;r&quot;</span>: <span class="hljs-number">0.20833333333333334</span><br>    &#125;,<br>    <span class="hljs-attr">&quot;rouge-l&quot;</span>: &#123;<br>      <span class="hljs-attr">&quot;f&quot;</span>: <span class="hljs-number">0.44705881864636676</span>,<br>      <span class="hljs-attr">&quot;p&quot;</span>: <span class="hljs-number">0.5277777777777778</span>,<br>      <span class="hljs-attr">&quot;r&quot;</span>: <span class="hljs-number">0.3877551020408163</span><br>    &#125;<br>  &#125;<br>]<br></code></pre></td></tr></table></figure>
<p>其中p、r、f分别为精准率、召回率、F1_score</p>
<h1 id="zhconv用法"><a href="#zhconv用法" class="headerlink" title="zhconv用法"></a>zhconv用法</h1><ul>
<li><p>该库用于中文简繁体转换，但是也可以用<strong>OpenCC库，精准度更高、覆盖率更高、速度更快</strong></p>
</li>
<li><p><strong>逐字转换：zhconv.convert(s, locale, update=None)</strong></p>
</li>
<li><p><strong>基于MediaWiki的转换：zhconv.ocnvert_for_mw(s, locale, update=None)</strong></p>
</li>
</ul>
<blockquote>
<p>两个函数用法相同</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sent2 = <span class="hljs-string">&quot;計算機軟體&quot;</span><br><span class="hljs-built_in">print</span>(convert(sent2, <span class="hljs-string">&quot;zh-hans&quot;</span>))<br></code></pre></td></tr></table></figure>
<p>locale可为以下值：</p>
<p><code>zh-cn</code> 大陆简体、<code>zh-tw</code> 台灣正體、<code>zh-hk</code> 香港繁體、<code>zh-sg</code> 马新简体、<code>zh-hans</code> 简体、<code>zh-hant</code> 繁體</p>
</blockquote>
<h1 id="gensim用法"><a href="#gensim用法" class="headerlink" title="gensim用法"></a>gensim用法</h1><ul>
<li>gensim主要用于创建语料库，和计算各种特征（如相似度、tf-idf）等任务</li>
</ul>
<h3 id="创建语料库和计算相似度"><a href="#创建语料库和计算相似度" class="headerlink" title="创建语料库和计算相似度"></a>创建语料库和计算相似度</h3><ul>
<li><strong>创建语料库类：gensim.corpora.Dictionary(texts)</strong></li>
</ul>
<blockquote>
<p>输入：整个语料库，分好句和分好词的结果，一个list，每个元素又是一个list，里面的每个子元素是一个str词</p>
<p>输出：一个Dictionary对象</p>
</blockquote>
<ul>
<li><strong>将句子转化为词袋形式：Dictionary.doc2bow(text)</strong></li>
</ul>
<blockquote>
<p>输入：分好词的一个句子，一个list，每个元素是一个str词</p>
<p>输入：一个list，每个元素是一个二元tuple，tuple[0]是句子中存在的词的索引，tuple[1]是其在句子中出现的次数（出现0次的不计入）</p>
<p>该类的用法和python自带的字典对象基本相同，values、key、item之类的</p>
</blockquote>
<ul>
<li><strong>获取索引字典：Dictionary.token2id</strong></li>
</ul>
<blockquote>
<p>返回一个{token : id}的字典</p>
</blockquote>
<ul>
<li><strong>创建相似度类：gensim.similarities.Similarity(output_prefix, corpus, num_features)</strong></li>
</ul>
<blockquote>
<p>out_prefix是存储这个对象文件的名称</p>
<p>corpus是整个语料库，但是必须先转化为词袋模型</p>
<p>num_fuatures是整个词典的数量，一般使用len(dictionary)表示</p>
<p>对于得到的对象（取名为similarity），具体的用法为：similarity[test]，其中test为要比较的对象，可以是单个句子，也可以是整个语料库</p>
</blockquote>
<p><strong>举个栗子：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br><span class="hljs-keyword">from</span> gensim.corpora <span class="hljs-keyword">import</span> Dictionary<br><span class="hljs-keyword">from</span> gensim.similarities <span class="hljs-keyword">import</span> Similarity<br><br>sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br>sents = [sent1, sent2]<br><span class="hljs-comment"># 分词</span><br>texts = [word_tokenize(sent) <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br><br><span class="hljs-comment"># 创建字典对象</span><br>dictionary = Dictionary(texts)<br><span class="hljs-comment"># 获得词袋模型表示的词料库</span><br>corpus = [dictionary.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The corpus is : <span class="hljs-subst">&#123;corpus&#125;</span>&quot;</span>)<br><span class="hljs-comment"># 创建相似度对象</span><br>similarity = Similarity(<span class="hljs-string">&quot;Similarity-excise1&quot;</span>, corpus, num_features=<span class="hljs-built_in">len</span>(dictionary))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Created class : <span class="hljs-subst">&#123;similarity&#125;</span>&quot;</span>)<br><span class="hljs-comment"># 计算余弦相似度</span><br>test_corpus = dictionary.doc2bow(word_tokenize(sent1))<br><span class="hljs-built_in">print</span>(similarity[test_corpus])<br></code></pre></td></tr></table></figure>
<p>输出：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706160148320.png" srcset="/img/loading.gif" lazyload alt="image-20220706160148320"></p>
<h3 id="计算TF-IDF"><a href="#计算TF-IDF" class="headerlink" title="计算TF-IDF"></a>计算TF-IDF</h3><ul>
<li><strong>首先介绍一下tf-idf：</strong></li>
</ul>
<p>词频（term frequency，tf）指的是某一个给定的词语在该文件中对应的频率</p>
<p>逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量，可由文件总数除以包含该词的文件的总数求得</p>
<p>具体的计算公式如下：</p>
<script type="math/tex; mode=display">
\mathrm{tf}_{\mathrm{i}, \mathrm{j}}=\frac{n_{i, j}}{\sum_{k} n_{k, j}} \\
\operatorname{idf}_{\mathrm{i}}=\lg \frac{|D|}{\left|\left\{j: t_{i} \in d_{j}\right\}\right|}\\
\operatorname{tfidf}_{i, j}=\mathrm{tf}_{\mathrm{i}, \mathrm{j}} \times \mathrm{idf}_{\mathrm{i}}</script><p>其中<script type="math/tex">tf_{i, j}</script>为<script type="math/tex">word_i</script>在<script type="math/tex">doc_j</script>中出现的词频，<script type="math/tex">n_{i, j}</script>为<script type="math/tex">word_i</script>在<script type="math/tex">doc_j</script>中出现的个数</p>
<p><script type="math/tex">idf_i</script>为<script type="math/tex">word_i</script>的逆向文件频率，<script type="math/tex">|D|</script>为文档总数，<script type="math/tex">\left|\left\{j: t_{i} \in d_{j}\right\}\right|</script>为包含<script type="math/tex">word_i</script>的文档个数</p>
<p>对于idf，底数可用2、e、10，并且分母可以+1，避免除以0</p>
<ul>
<li><strong>创建tfidf模型类：gensim.models.TfidfModel(corpus)</strong></li>
</ul>
<blockquote>
<p>输入为整个语料库，必须先转化为词袋模型</p>
<p>在具体使用时，和Similarity类似，tfidf_model[test]，其中test可以是单个句子也可以是整个语料库，返回一个列表，列表中每个元素为一个tuple，tuple[0]为词索引，tuple[1]为对应的tfidf值</p>
<p><strong>gensim算出来的tf-idf是经过了规范化的，每个句子的tfidf值，是一个单位向量</strong></p>
</blockquote>
<p><strong>举个栗子：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># texts的构建是每个段落分词，有三个段落，则len(texts)=3</span><br>texts = [text1, text2, text3]<br>texts = [get_tokens(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br>dictionary = Dictionary(texts)<br>id2token_dict = &#123;v : k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus = [dictionary.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br>tfidf_model = TfidfModel(corpus)<br>result = tfidf_model[corpus]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> result:<br>    <span class="hljs-built_in">print</span>(i)<br></code></pre></td></tr></table></figure>
<p>输出：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706210417422.png" srcset="/img/loading.gif" lazyload alt="image-20220706210417422"></p>
<h1 id="Pandas用法"><a href="#Pandas用法" class="headerlink" title="Pandas用法"></a>Pandas用法</h1><ul>
<li><strong>创建DataFrame对象：pd.DataFrame(data, columns, index)</strong></li>
</ul>
<blockquote>
<p>输入：data是一个list, 每个对象是一个tuple或list，tuple的长度和columns的个数对应。data也可以是numpy形式</p>
<p>index是一个list，是行索引值，可以用于自行设置行索引<strong>（索引没有规定一定是int）</strong></p>
<p>示例：pd.DataFrame([(1, 2), (3, 4), (5, 6)], columns=[“name”, “NER result”])，输出为：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220711120934316.png" srcset="/img/loading.gif" lazyload alt="image-20220711120934316" style="zoom: 80%;" /></p>
</blockquote>
<ul>
<li><strong>获取DataFrame特征：DataFrame.describe()</strong></li>
<li><strong>获取DataFrame的某列：DataFrame[“column_name”]</strong></li>
</ul>
<blockquote>
<p> 这样可获取到column列，结果是一个Series类（<strong>只有一个列则是Series，多个列则是DataFrame</strong>）</p>
<p> 如果不想出现重复结果，可调用<code>unique()</code>函数，示例：data[“ner_result”].unique()，返回的结果是一个ndarray，也可使用<code>list()</code>将其转换为列表</p>
<p> 如果想把这个Series类转换为ndarray，则可使用Series.values，这是Series类其中的一个属性，不是方法，类型是一个ndarrary，也可使用<code>list()</code>转换为列表</p>
</blockquote>
<ul>
<li><p><strong>获取DataFrame列名：DataFrame.columns</strong></p>
</li>
<li><p><strong>DataFrame根据某列，进行分组：DataFrame.groupby(“column_name”)</strong></p>
</li>
</ul>
<blockquote>
<p>将DataFrame通过column_name列进行分组</p>
<p>结果是一个可迭代的DataFrameGroupBy类，该类的用法和DataFrame几乎一样，比如可使用<code>DataFrameGroupBy[&quot;column_name&quot;]</code>将其转换为SeriesGroupby类</p>
<p>也可转化为list，转换后，其中每一个元素为一个tuple，tuple[0]为column_name列对应的值，tuple[1]为原DataFrame的一部分，根据column_name的值进行截断。如果是将SeriesGroupby类转换为list，则tuple[1]变为一个Series类</p>
<p>举个例子：<br>input_data为一个DataFrame，值如下：</p>
<p><div align=center><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123006189.png" srcset="/img/loading.gif" lazyload alt="image-20220716123006189" style="zoom:67%;" /></p>
<p>现在将input_data根据sent_order列进行分组：input_data.groupby(“sent_order”)，结果如下：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123836380.png" srcset="/img/loading.gif" lazyload alt="image-20220716123836380" style="zoom: 67%;" /></p>
<p>只展示了前三个元组，每个元组，tuple[0]为sent_order的值，tuple[1]为对应的DataFrame</p>
</blockquote>
<ul>
<li><strong>应用自定义方法：DaraFrame.apply(func, axis=0, …)</strong></li>
</ul>
<blockquote>
<p>Series、DataFrame和GroupbyDataFrame等均可使用</p>
<p>func是一个自定义函数</p>
<p>axis决定func的输入是什么，axis=0则输入为行，axis=1则输入为列</p>
<p>Series无需指定axis，DataFrame可调节axis更换操作的维度，<strong>DaraFrameGroupby也无需指定，func的输入为分组后的每个DataFrame，最后再将每个分组返回的结果总和起来</strong></p>
</blockquote>
<ul>
<li><strong>DataFrame的count()方法：</strong></li>
</ul>
<blockquote>
<p><code>count()</code>有很多种用法，结果是可以对Series、DataFrame和GroupbyDataFrame等使用，结果是返回一个Series或DataFrame</p>
<p>下面举几个示例，其中使用的df如下：</p>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725142611139.png" srcset="/img/loading.gif" lazyload alt="image-20220725142611139" style="zoom: 60%;" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">df.count()    <span class="hljs-comment"># DataFrame执行count()，对每一列分别执行count，结果返回一个Series</span><br>df.data[<span class="hljs-string">&quot;length&quot;</span>].count()    <span class="hljs-comment"># Series执行count()，直接返回length列的长度，类型为int</span><br>df.groupy(<span class="hljs-string">&quot;length&quot;</span>).count()   <span class="hljs-comment"># 对DataFrameGroupby类执行，返回一个DataFrame，index为length的各指，colums为出length的其他列执行count的结果</span><br>df.groupy(<span class="hljs-string">&quot;length&quot;</span>)[<span class="hljs-string">&quot;evaluation&quot;</span>].count()  <span class="hljs-comment"># 对SeriesGroupby类执行，则只返回evaluation列的count结果，类型为Series</span><br></code></pre></td></tr></table></figure>
<p>上述四句语句的执行结果：</p>
<ul>
<li>第一句：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143928089.png" srcset="/img/loading.gif" lazyload alt="image-20220725143928089" style="zoom:67%;" /></p>
<ul>
<li>第二句：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143939723.png" srcset="/img/loading.gif" lazyload alt="image-20220725143939723" style="zoom:67%;" /></p>
<ul>
<li>第三句：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143953768.png" srcset="/img/loading.gif" lazyload alt="image-20220725143953768" style="zoom:50%;" /></p>
<ul>
<li>第四句：</li>
</ul>
<p><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725144005108.png" srcset="/img/loading.gif" lazyload alt="image-20220725144005108" style="zoom:50%;" /></p>
</blockquote>
<h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><ul>
<li><strong>判断空值：DataFrame.isnull()或DataFrame.notnull()</strong></li>
</ul>
<blockquote>
<p>DataFrame和Series都可用，结果也是返回一个DataFrame或Series，和输入一一对应，其中每个值为布尔值</p>
</blockquote>
<ul>
<li><strong>删除缺失值：DataFrame.dropna(axis=0, inplace=False, how=None)</strong></li>
</ul>
<blockquote>
<p>axis：删除行还是列，{0 or ‘index’, 1 or ‘columns’}</p>
<p>how : 如果等于any则任何值为空都删除，如果等于all则所有值都为空才删除</p>
</blockquote>
<ul>
<li><strong>填充缺失值：DataFrame.fillna(value, method, axis, …)</strong></li>
</ul>
<blockquote>
<p>value：用于填充的值，可以是单个值，或者字典（用于不同列填充不同值的情况，key是列名，value是值）<br>method : 等于”ffill”使用前一个；不为空的值填充forword fill；等于”bfill”使用后一个不为空的值填充backword fill<br>axis : 按行还是列填充，{0 or ‘index’, 1 or ‘columns’}</p>
</blockquote>
<h3 id="Padas中DataFrame的增删查改"><a href="#Padas中DataFrame的增删查改" class="headerlink" title="Padas中DataFrame的增删查改"></a>Padas中DataFrame的增删查改</h3><ul>
<li><strong>增加列：</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">df = DataFrame(...)    <span class="hljs-comment"># 有三行数据</span><br>cities = [<span class="hljs-string">&quot;成都&quot;</span>, <span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&quot;北京&quot;</span>]<br><span class="hljs-comment"># 三种插入方式</span><br>df.insert(<span class="hljs-number">0</span>, <span class="hljs-string">&quot;city&quot;</span>, citys)    <span class="hljs-comment"># 参数分别为：插入列的位置、列名、插入内容</span><br>df[<span class="hljs-string">&quot;city&quot;</span>] = cities<br>df.loc[:, <span class="hljs-string">&quot;city&quot;</span>] = cities<br></code></pre></td></tr></table></figure>
<ul>
<li><strong>增加行：</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">df = DataFrame(...)  <span class="hljs-comment"># 有两列 </span><br>df.loc[<span class="hljs-number">3</span>] = [<span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>]       <span class="hljs-comment"># 如果已经存在index=3的行，则修改值；反之直接添加该行</span><br>df = df.append(df_insert)    <span class="hljs-comment"># 合成两个DataFrame，列要相同才行</span><br></code></pre></td></tr></table></figure>
<ul>
<li><strong>loc[]和iloc[]的使用：</strong></li>
</ul>
<blockquote>
<p>两者功能是相同的，区别在于loc中对于列，只能使用列名（str类型）进行搜索，而iloc对于列只能使用整数索引进行搜索，DataFrame的查和改基本都是基于两者的</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&quot;column_name&quot;</span>]或df.loc[:, <span class="hljs-string">&quot;column_name&quot;</span>]   <span class="hljs-comment"># 查找column_name列</span><br>df.loc[<span class="hljs-string">&quot;index_name&quot;</span>]    <span class="hljs-comment"># 查找index_name行</span><br>df.loc[<span class="hljs-string">&quot;index_name&quot;</span>, <span class="hljs-string">&quot;column_name&quot;</span>]     <span class="hljs-comment"># 查找(index_name, column_name)处的值</span><br><span class="hljs-comment"># 还可以使用list或者切片来代替，一次操作多行或多列</span><br>df.loc[[<span class="hljs-string">&quot;index_name1&quot;</span>, <span class="hljs-string">&quot;index_name2&quot;</span>], [<span class="hljs-string">&quot;column_name1&quot;</span>, <span class="hljs-string">&quot;column_name2&quot;</span>]]  <span class="hljs-comment"># 同时操作index_name1&amp;2行和column_name1&amp;2列</span><br>df.loc[<span class="hljs-number">0</span>:<span class="hljs-number">3</span>, <span class="hljs-string">&quot;column_name&quot;</span>]    <span class="hljs-comment"># 进行切片，对0、1、2、3行操作</span><br></code></pre></td></tr></table></figure>
<blockquote>
<p><code>iloc[]</code>是使用整数列索引，比如：<code>df.iloc[:3, 2:6]</code></p>
<p>注意<code>loc[]</code>中的切片，是包含了end所指的索引（和list的索引稍有不同），<code>iloc[]</code>是不包含end的索引的</p>
</blockquote>
<ul>
<li><strong>删除：</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">df.drop(<span class="hljs-number">0</span>, axis=<span class="hljs-number">0</span>)   <span class="hljs-comment"># 删除第0行</span><br><span class="hljs-comment"># 删除列的三种方法</span><br>df.drop(<span class="hljs-string">&quot;column_name&quot;</span>, axis=<span class="hljs-number">1</span>)<br><span class="hljs-keyword">del</span> df[<span class="hljs-string">&quot;column_name&quot;</span>]<br>ndf = df.pop(<span class="hljs-string">&quot;column_name&quot;</span>)<br><span class="hljs-comment"># 使用drop的时候，第一个参数也可以是list，一次操作多行或多列</span><br>df.drop([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], axis=<span class="hljs-number">0</span>)<br>df.drop([<span class="hljs-string">&quot;column_name1&quot;</span>, <span class="hljs-string">&quot;column_name2&quot;</span>], axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<h1 id="Pickle用法"><a href="#Pickle用法" class="headerlink" title="Pickle用法"></a>Pickle用法</h1><ul>
<li><p><strong>储存pk文件：pickle.dump(obj, file)</strong></p>
</li>
<li><p><strong>加载pk文件：pickle.load(file)</strong></p>
</li>
</ul>
<blockquote>
<p>可以存储和加载对象，其中<code>dump()</code>中的obj是要存的对象，file是<code>open()</code>函数返回的文件描述符</p>
<p><code>load()</code>返回值为所存储的对象</p>
<p>储存的文件后缀为.pk</p>
<p>举个栗子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">a = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br>file_name = <span class="hljs-string">&quot;test.pk&quot;</span><br><span class="hljs-comment"># 存</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    pickle.dump(a, f)<br><span class="hljs-comment"># 取</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    b = pickle.load(f)<br>    <br><span class="hljs-comment"># a和b是一样的</span><br></code></pre></td></tr></table></figure>
</blockquote>
<h1 id="Json用法"><a href="#Json用法" class="headerlink" title="Json用法"></a>Json用法</h1><ul>
<li><p><strong>json可进行Python对象和json格式之间的互换</strong></p>
</li>
<li><p><strong>Python转json：json.dumps(obj, ensure_ascii=True, …, indent=None, …)</strong></p>
</li>
<li><strong>json转Python：json.loads(s, …)</strong></li>
</ul>
<blockquote>
<p>obj为Python对象，s为json对象</p>
<p>ensure_ascii：如果为False，则返回值可以包含非 ASCII</p>
<p>indent：为json中每个元素的间隔数，如果为0或None，则每个元素之间紧凑排版</p>
<p><strong>注意：</strong></p>
<p><strong>1. 以上两个函数只是进行对象的转换，而不会进行保存，所以是一般配合<code>open()</code>、<code>f.write()</code>、<code>f.read()</code>使用，给两个示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 存储json对象</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br> f.write(json.dumps(data, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">2</span>))<br><span class="hljs-comment"># 读取json对象</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br> data = json.loads(f.read())<br></code></pre></td></tr></table></figure>
<p><strong>2. 转换为json格式时，如果键值的类型不是字符串，而是int、float之类的，<code>dumps()</code>后会自动转换为str，加载的时候需要自己转回去</strong></p>
</blockquote>
<h1 id="Matplotlib用法"><a href="#Matplotlib用法" class="headerlink" title="Matplotlib用法"></a>Matplotlib用法</h1><ul>
<li><strong>折线图：plt.plot(x, y)</strong></li>
<li><strong>直方图：plt.bar(x, y)</strong></li>
<li><strong>散点图：plt.scatter(x, y)</strong></li>
</ul>
<blockquote>
<p>上述函数还可以指定颜色、大小等参数：</p>
<ul>
<li>color：颜色</li>
</ul>
<blockquote>
<p>可为：”b”：蓝色、”g”：绿色、”r”：红色、”c”：青色、”m”：品红、”y”：黄色、”k”：黑色、”w”：白色</p>
<p>也只直接用全名，如”blue”</p>
</blockquote>
<ul>
<li>s：点的大小</li>
<li>linewidth：线宽</li>
<li><p>linestyle：线样式，如设为”dashed”将线设为虚线</p>
</li>
<li><p>label：名字， 若要打印label，则使用<code>plt.legend()</code></p>
</li>
</ul>
</blockquote>
<ul>
<li><strong>设置标题：plt.title()</strong></li>
</ul>
<blockquote>
<p>同样可以使用fontsize指定字体大小</p>
</blockquote>
<ul>
<li><strong>设置x、y轴标题：plt.xlabel()和plt.ylabel()</strong></li>
<li><strong>保存图片：plt.savefig()</strong></li>
<li><strong>画完图之后记得关闭：plt.close()</strong></li>
<li><strong>画一条水平的线：plt.hlines(y, xmin, xmax)</strong></li>
<li><strong>画一条垂直的线：plt.vlines(x, ymin, ymax)</strong></li>
<li><strong>在指定座标处加注释：plt.text(x, y, s)</strong></li>
</ul>
<h1 id="Numpy用法"><a href="#Numpy用法" class="headerlink" title="Numpy用法"></a>Numpy用法</h1><ul>
<li><strong>获取最大值的位置（索引）np.argmax(a, axis=None, …)：</strong></li>
</ul>
<blockquote>
<p>axis：如果为None, 则代表把整个数组当作一维的，然后返回最大的索引值。如果指定axis，则只在该维度搜索最大值，示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">array([[<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>],<br>       [<span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.argmax(a)<br><span class="hljs-number">5</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>np.argmax(a, axis=<span class="hljs-number">0</span>)<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.argmax(a, axis=<span class="hljs-number">1</span>)<br>array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure>
<p>axis指定为哪个维度，则stack哪个维度</p>
</blockquote>
<ul>
<li><strong>减少维度：np.squeeze(a, axis=None)</strong></li>
</ul>
<blockquote>
<p>axis如果为None，则丢弃所有大小为1的维度</p>
</blockquote>
<h1 id="Tensorflow用法"><a href="#Tensorflow用法" class="headerlink" title="Tensorflow用法"></a>Tensorflow用法</h1><h3 id="禁用GPU"><a href="#禁用GPU" class="headerlink" title="禁用GPU"></a>禁用GPU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;-1&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ul>
<li><strong>填充序列：tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.)</strong></li>
</ul>
<blockquote>
<p>输入：<br>sequences是要填充的序列</p>
<p>maxlen是要填充的最大长度，如果为None，则设为所有序列中最长的那个的长度</p>
<p>padding和truncating可以设为”pre”或者”post”，选择是在序列前面还是后面填充/截断</p>
<p>value为填充的值</p>
<p>返回值是一个ndarray</p>
</blockquote>
<ul>
<li><strong>划分训练、测试集：sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</strong></li>
</ul>
<blockquote>
<p>arrays是要划分的数据，可以使用list、numpy array、matric、dataframe<strong>（不能用Tensor）</strong>，示例：</p>
<p><code>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)</code></p>
<p>train_size和test_size指定一个就行</p>
<p>random_state为随机种子</p>
<p>shuffle决定是否打乱数据</p>
<p>stratify决定是否进行数据分层</p>
<p><strong>输入的数据是什么类型，输出的数据就是什么类型，返回值依次为：x_train, x_test, y_train, y_test</strong></p>
</blockquote>
<h3 id="各种层"><a href="#各种层" class="headerlink" title="各种层"></a>各种层</h3><ul>
<li><strong>tf.keras.layers.Dense(units, activation=None, use_bias=True, …)</strong></li>
<li><strong>tf.keras.layers.Embedding(input_dim, output_dim,…., mask_zero=False, input_length=None)</strong></li>
</ul>
<blockquote>
<p>input_dim：词汇表大小</p>
<p>output_dim：词嵌入向量的维度</p>
<p>mask_zero：决定输入值0是否是应被屏蔽的mask“填充”值。如果为True，则使用mask，相应的input_dim也应该为词汇表大小+1（因为增加了一个mask token，索引为0）</p>
<p>intput_length：输入序列的长度，如果为None，则自动为最长序列的长度</p>
</blockquote>
<ul>
<li><strong>tf.keras.layers.LSTM(units, activation=”tanh”, recurrent_activation=”sigmoid”, …, return_sequences=False)</strong></li>
</ul>
<blockquote>
<p>其中return_sequences决定是否在最后返回所有时间步的输出，若为False只会返回最后一个时间步的输出，如果下一层需要使用到所有时间步的输出则需要设为True</p>
</blockquote>
<ul>
<li><strong>tf.keras.layers.Bidirectional(layer, merge_mode=”concat”, …)</strong></li>
</ul>
<blockquote>
<p>是RNN的双向包装器，可将单向RNN变为双向RNN</p>
<p>layer是要包装的层，merge_mode决定前后信息汇总的 方式，可为：”sum”、”mul”、”concat”、”ave”、None，如果为None，则不会合并，而是将他们作为列表输出</p>
</blockquote>
<ul>
<li><strong>tf.keras.layers.Softmax(axis=-1)</strong></li>
</ul>
<blockquote>
<p>axis：决定对那个维度执行softmax，默认为最后一个维度</p>
</blockquote>
<h3 id="模型的常用方法"><a href="#模型的常用方法" class="headerlink" title="模型的常用方法"></a>模型的常用方法</h3><ul>
<li><strong>序列容器：tf.keras.models.Sequential()</strong></li>
</ul>
<blockquote>
<p>可以和<code>add()</code>配合使用，也可以里面直接加个列表Sequential([…])</p>
</blockquote>
<ul>
<li><strong>配置模型：model.compile(optimizer=’rmsprop’, loss=None, metrics=None,…)</strong></li>
</ul>
<blockquote>
<p>optimizer：字符串或者Opitimizer实例</p>
<p>loss：字符串或Loss示例或一个函数</p>
<p>metrics：字符串或Metric示例或一个函数</p>
</blockquote>
<ul>
<li><strong>训练模型：model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0., validation_data=None, …)</strong></li>
</ul>
<blockquote>
<p>x/y：可以是numpy数组或者Tensor或list或迭代器等，这样x/y应一一对应。x也可为DataSet、Generator、Sequence，但是这种情况下就不应该指定y，因为y已经在x中了</p>
<p>batch_size如果未指定则默认32</p>
<p>callbacks：一个列表，每个元素是一个callbacks中的实例，如EarlyStopping类</p>
<p>validation_split和validaton_data只需要指定一个，前者是验证集所占比例，后者是直接指定验证集<strong>（如果指定了该参数，则x/y只能Tensor或ndarray）</strong></p>
</blockquote>
<ul>
<li><strong>画模型：tf.keras.utils.plot_model(model, to_file=”model.png”, show_shapes=False, show_dtype=False, show_layer_names=True, …)</strong></li>
</ul>
<blockquote>
<p>model是要打印的模型，一个keras model实例</p>
</blockquote>
<ul>
<li><strong>测试模型：model.evaluate(x=None, y=None, batch_size=None, verbose=1, …)</strong></li>
</ul>
<blockquote>
<p>用法和fit基本一样</p>
</blockquote>
<ul>
<li><strong>采用另一种方法训练模型：</strong></li>
</ul>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br>    predictions = model(inputs, training = <span class="hljs-literal">True</span>)<br>    loss = loss_func(labels, predictions)<br>gradients = tape.gradient(loss, model.trainable_variables)<br>optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, model.trainable_variables))<br></code></pre></td></tr></table></figure>
</blockquote>
<h3 id="保存和加载模型"><a href="#保存和加载模型" class="headerlink" title="保存和加载模型"></a>保存和加载模型</h3><ul>
<li>既有keras保存方式，也有tf原生方式保存方式，<strong>但是前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署</strong>。既可以直接保存整个模型，也可以分别保存模型的权重和结构</li>
</ul>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 直接保存</span><br>model.save(<span class="hljs-string">&quot;model.h5&quot;</span>, save_format = <span class="hljs-string">&quot;h5&quot;</span>)<br>model = tf.keras.models.load_model(<span class="hljs-string">&quot;model.h5&quot;</span>)<br><br><span class="hljs-comment"># 分别保存模型的权重和结构</span><br>json_str = model.to_json()<br>model.save_weights(<span class="hljs-string">&quot;model_weights.h5&quot;</span>, save_format = <span class="hljs-string">&quot;h5&quot;</span>)<br>model = tf.keras.models.model_from_json(json_str)<br>model.<span class="hljs-built_in">compile</span>(...)    <span class="hljs-comment"># 这种方法重新加载后需要重新配置优化器</span><br>model.load_weights（<span class="hljs-string">&quot;model_weights.h5&quot;</span>）<br><br><span class="hljs-comment"># 上面演示的是keras保存方式</span><br><span class="hljs-comment"># 要使用tf原生方式，需要在model.save()和model.save_weights()时添加参数save_format=&quot;tf&quot;，并且更改文件后缀</span><br><span class="hljs-comment"># 演示一下：</span><br>model.save_weights(<span class="hljs-string">&#x27;tf_model_weights.ckpt&#x27;</span>,save_format=<span class="hljs-string">&quot;tf&quot;</span>)<br>model.save(<span class="hljs-string">&#x27;tf_model_savedmodel&#x27;</span>, save_format=<span class="hljs-string">&quot;tf&quot;</span>)<br></code></pre></td></tr></table></figure>
</blockquote>
<h3 id="模型的反馈"><a href="#模型的反馈" class="headerlink" title="模型的反馈"></a>模型的反馈</h3><ul>
<li><strong>设置Early Stop：tf.keras.callbacks.EarlyStopping(monitor=’val_loss’, min_delta=0, patience=0, …, restore_best_weights=False)</strong></li>
</ul>
<blockquote>
<p>monitor：要监视的量，如：”val_loss”、”val_accuracy”、”train_loss”等</p>
<p>min_delta：若小于min_delta的绝对变化，将被视为没有改进</p>
<p>patience：容忍可以没有提升的epochs数</p>
<p>restore_best_weights：是否保存整个训练过程中最好的一个epoch</p>
</blockquote>
<ul>
<li><strong>设置学习率衰减：tf.keras.callbacks.ReduceLROnPlateau(monitor=”val_loss”, factor=0.1, patience=10, verbose=0, …, min_delta=0.0001, …, min_lr=0)</strong></li>
</ul>
<blockquote>
<p>当经过patience个epochs仍没有改进，则降低学习率，学习率*factor来降低</p>
<p>factor：每次降低的倍数</p>
<p>min_lr：学习率可降低到的最小值</p>
</blockquote>
<h3 id="Dataset用法"><a href="#Dataset用法" class="headerlink" title="Dataset用法"></a>Dataset用法</h3><ul>
<li>在<code>model.fit</code>中需要可以选择分别用list等类型指定x和y，也可以直接使用Dataset等类指定x而忽略y。对于前者，需要将所有数据一次性读入内存，内存负担较大。而后者可以使用TFRecordDataset类进行外存到内存的流式读取，只需要牺牲小部分IO性能，可大大减轻内存的负担</li>
<li>简单来说，一个.tfrecords文件包含多个样本<strong>（官方建议每个文件大小100-200M）</strong>，每个样本都对应着一个<code>tf.train.Example</code>类，每个样本可能有多个特征，每个特征用<code>tf.train.Feature()</code>封装</li>
<li>tf.train.Featrue可接收以下三种类型，大多数其他通用类型也可以强制转换成下面的其中一种：</li>
</ul>
<blockquote>
<ol>
<li><strong>tf.train.BytesList：</strong>string、byte<strong>（非标量数据也使用这个，但是需要先将Tensor序列化）</strong></li>
<li><strong>tf.train.FloatList：</strong>float、double</li>
<li><strong>tf.train.Int64List：</strong>bool、enum、int32、unit32、int64、uint64g</li>
</ol>
</blockquote>
<ul>
<li><strong>首先进行.tfrecords文件的存入：</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_tfrecords</span>(<span class="hljs-params">data, label, desfile</span>):</span><br>    <span class="hljs-keyword">with</span> tf.io.TFRecordWriter(desfile) <span class="hljs-keyword">as</span> writer:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data)):<br>            features = tf.train.Features(<br>                feature = &#123;<br>                    <span class="hljs-string">&quot;data&quot;</span>:tf.train.Feature(bytes_list = tf.train.BytesList(value =[tf.io.serialize_tensor(data[i]).numpy()])),<br>                    <span class="hljs-string">&quot;label&quot;</span>:tf.train.Feature(float_list = tf.train.FloatList(value = label[i])),<br>                &#125;<br>            )<br>            example = tf.train.Example(features = features)<br>            serialized = example.SerializeToString()<br>            writer.write(serialized)<br>   <br><br>save_tfrecords(x_in_sample1, y_in_sample1, <span class="hljs-string">&quot;path1.tfrecords&quot;</span>)<br>save_tfrecords(x_in_sample2, y_in_sample2, <span class="hljs-string">&quot;path2.tfrecords&quot;</span>)<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>首先要用<code>tf.io.TFRecordWriter()</code>打开写入器，data和label是两个列表，每个元素对应一个Example的特征，desfile是文件名</li>
<li>对于每个Example的特征，使用一个字典表示，字典还需要被<code>tf.train.Features()</code>封装起来，字典的每个值可以是上述三种类型之一，需要使用<code>tf.train.Feature()</code>封装，其中value的值必须为一个list<strong>（注意前者是Features，后者是Feature）</strong>。并且<strong>由于data是非标量的高维数据，需要先使用<code>tf.io.serialize_tensor()</code>进行序列化，然后再使用其numpy值</strong></li>
<li>最后通过<code>tf.train.Example</code>封装feature，然后调用<code>SerializeToString()</code>函数，将编码后的字符串写入</li>
</ul>
</blockquote>
<ul>
<li><strong>接下来进行数据读取和使用：</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># TFR数据反编译</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">map_func</span>(<span class="hljs-params">example</span>):</span><br>    feature_description = &#123;<br>        <span class="hljs-string">&#x27;data&#x27;</span>: tf.io.FixedLenFeature([], tf.string),<br>        <span class="hljs-string">&#x27;label&#x27;</span>: tf.io.FixedLenFeature([], tf.float32),<br>    &#125;<br>    parsed_example = tf.io.parse_single_example(example, features=feature_description)<br>    <br>    x_sample = tf.io.parse_tensor(parsed_example[<span class="hljs-string">&#x27;data&#x27;</span>], tf.float32)<br>    y_sample = parsed_example[<span class="hljs-string">&#x27;label&#x27;</span>]<br>    <br>    <span class="hljs-keyword">return</span> x_sample, y_sample<br><br><span class="hljs-comment"># 加载数据集</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_dataset</span>(<span class="hljs-params">filepaths</span>):</span><br>    shuffle_buffer_size = <span class="hljs-number">3000</span><br>    batch_size = <span class="hljs-number">256</span><br><br>    dataset = tf.data.TFRecordDataset(filepaths)<br>    dataset = dataset.shuffle(shuffle_buffer_size)<br>    dataset = dataset.<span class="hljs-built_in">map</span>(map_func=map_func, num_parallel_calls=tf.data.AOTOTUNE)<br>    dataset = dataset.batch(batch_size).prefetch(<span class="hljs-number">64</span>)<br>    <br>    <span class="hljs-keyword">return</span> dataset<br><br><br>train_set = load_dataset([<span class="hljs-string">&quot;path1.tfrecords&quot;</span>,<span class="hljs-string">&quot;path2.tfrecords&quot;</span>])<br>valid_set = load_dataset([<span class="hljs-string">&quot;path3.tfrecords&quot;</span>,<span class="hljs-string">&quot;path4.tfrecords&quot;</span>])<br>model.fit(train_set,epochs=model_epochs, validation_data=valid_set, callbacks=[early_stopping])<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>首先要生成<code>tf.data.TFRecordDataset</code>类，传入参数是一个list，其中元素是.tfrecords文件名</li>
<li>每次读取后要先打乱</li>
<li>之后调用<code>map()</code>，其中指定的函数是<strong>对每个样本操作的</strong>，由于储存时序列化为了二进制字符串，所以数据是什么类型是的信息是丢失了的，解码时需要自己制定数据类型，所以在<code>map()</code>中首先需要指定feature_description，说明每个数据的类型</li>
<li>然后使用<code>tf.io.parse_single_example()</code>解码单个样本，结果返回一个字典，字典的每个元素对应一个特征，其中<strong>由于data是string类型，由于是编码后的非标量数据，所以还需要使用<code>tf.io.parse_tensor</code>解码一次（非标量数据的shape的编码解码的过程中是会丢失的，所以需要把shape一起储存）</strong></li>
<li>如果储存的是列表，需要在<code>tf.io.FixedLenFeature()</code>指定shape，也就是(len(list), )</li>
</ul>
</blockquote>
<h1 id="keras-bert用法"><a href="#keras-bert用法" class="headerlink" title="keras_bert用法"></a>keras_bert用法</h1><ul>
<li><strong>加载预训练模型：keras_bert.load_trained_model_from_checkpoint(config_file, checkpoint_file, training=False, trainable=None, output_layer_num=1, seq_len=int(1e9), kwargs)</strong></li>
</ul>
<blockquote>
<p>config_file和checkpoint_file分别为bert_config.json文件和bert_model.ckpt文件</p>
<p>training：如果为True，则将返回带有 MLM 和 NSP输出的模型；否则，将返回输入层和最后一个特征提取层。</p>
<p>trainable：模型是否可训练（也可通过for循环分别制定每层是否可训练）</p>
<p>seq_len为最大序列长度</p>
</blockquote>
<ul>
<li><strong>编码输入：keras_bert.Tokenizer(token_dict, …)</strong></li>
</ul>
<blockquote>
<p>bert模型的输入有两个，一个是语义token，一个是序列token，需要创建Tokenizer实例，再对句子进行编码，产生两个token</p>
<p>token_dict：是输入的映射字典，在vocab.txt中可以获取所有token，但是需要自行建立token -&gt; id的字典</p>
<p>创建Tokenizer实例后，调用<code>Tokenizer.encode(first, second=None, max_len=None)</code>进行编码，结果返回一个list，元素分别为两个list，分别对应两个token</p>
</blockquote>
<ul>
<li><strong>将数据输入模型：</strong></li>
</ul>
<blockquote>
<p>在<code>model.fit()</code>中，由于是多输入，采用<code>x=[train_token_embeddings, train_seq_embeddings]</code>和<code>validation_data=([test_token_embeddings, test_seq_embeddings], test_labels)</code>这种形式进行输入</p>
</blockquote>
<ul>
<li><strong>加载模型时所需要注意的：</strong></li>
</ul>
<blockquote>
<p>加载保存好的模型时同样是使用<code>tf.keras.models.load_model()</code>，但是会出现如下报错：</p>
<p><strong>ValueError: Unknown layer: TokenEmbedding</strong></p>
<p>这种情况需要：先<code>from keras_bert import get_custom_objects</code>，再在<code>load_model()</code>时添加参数<code>custom_objects=get_custom_objects()</code></p>
</blockquote>
<h1 id="bert4keras用法"><a href="#bert4keras用法" class="headerlink" title="bert4keras用法"></a>bert4keras用法</h1><ul>
<li><strong>由于keras和tf.keras的兼容问题，所以最开始需要添加环境变量TF_KERAS=1：</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&quot;TF_KERAS&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span><br></code></pre></td></tr></table></figure>
<ul>
<li>直接看<a target="_blank" rel="noopener" href="https://github.com/Sniper970119/bert4keras_document">民间的API文档</a>，写的还可以</li>
</ul>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/08/09/RoBERTa%E6%80%BB%E7%BB%93/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">RoBERTa总结</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/06/14/HMM%E5%92%8CCRF/">
                        <span class="hidden-mobile">HMM和CRF</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://zlkqz.github.io/2022/07/17/%E4%BB%A3%E7%A0%81%E6%80%BB%E7%BB%93/';
          this.page.identifier = '/2022/07/17/%E4%BB%A3%E7%A0%81%E6%80%BB%E7%BB%93/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'fluid' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
