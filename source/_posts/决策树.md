---
title: 决策树总结
math: true
date: 2022-5-7
---



# 1 基本概念

- 一颗决策树包括一个根结点、若干内部结点和若干叶结点，叶结点对应于决策结果，易知：

> * 每个非叶节点表示一个特征属性测试。
> * 每个分支代表这个特征属性在某个值域上的输出。
> * 每个叶子节点存放一个类别。
> * 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。

- **决策树的构造：**决策树的构造是一个递归的过程，有三种情形会导致递归返回：
  1. 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别
  2.  当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别
  3. 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。

  算法的基本流程如下图所示：

  <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc728ecc27fe.png" alt="2.png" style="zoom:80%;" />





# 2 划分算法

- 各种算法的不同之处在于用什么指标来选择属性，来对每个结点划分

### 2.1 ID3算法

- **信息熵**是度量样本集合程度的最常用的一种指标，设当前样本集合$$D$$中第$$k$$类样本所占比例为$$p_k$$，则$$D$$的信息熵定义为：

$$
\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}
$$

 Ent(D)的值越小，D的纯度越高

- 假设属性a有V个可能的取值$$\{a^1, ..., a^V\}$$，若用a进行划分，则会产生V个分支结点，其中第v个分支包含的数据集为在D中取$$a = a^v$$的样本集，记为$$D^v$$，则选择使用a进行划分的**信息增益**为： 

$$
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)
$$

其中$$|D^v|/|D|$$表示给不同的分支赋予权重，即样本数越多的分支结点影响越大。一般而言信息增益越大，代表使用a进行划分获得的“纯度提升”越大。而ID3算法就是选择$$a_{*}=\underset{a \in A}{\arg \max } \operatorname{Gain}(D, a)$$



### 2.2 C4.5算法

- **信息增益准则对可取值数目较多的属性有所偏好**，为减少这种偏好可能带来的不好影响，可以改用**增益率**来进行划分。增益率定义如下：

$$
\begin{array}{c}
\text { Gain_ratio }(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)} \\
\operatorname{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}
\end{array}
$$

其中IV(a)成为属性a的**固有值**，属性a可能取值越多，则IV(a)越大。

- **但是增益率准则对可取值数目较少的属性有所偏好**。所以C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：**先从候选属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的**



### 2.3 CART算法

- CART算法采用**基尼指数**来选择划分属性，数据集D的纯度可用**基尼值**来度量：

$$
\begin{aligned}
\operatorname{Gini}(D) &=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\
&=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2}
\end{aligned}
$$

直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其标记不一致的概率。因此，Gini(D)越小，则数据集纯度越高

- 属性a的基尼指数定义为：

$$
\text { Gini_index }(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)
$$

每次选择基尼指数最小的属性$$a_{*}=\underset{a \in A}{\arg \min } \text { Gini_index }(D, a)$$





# 3 剪枝

- 剪枝(pruning)是决策树学习算法对付 “过拟合” 的主要手段.。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得 “太好”了，以致于把训练集自身 的一些特点当作所有数据都具有的一般性质而导致过拟合。因此, 可通过主动去掉一些分支来降低过拟合的风险

- 剪枝的基本策略有：
> - **预剪枝：**指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点
>
> - **后剪枝：**先从训练集生成一棵完成的决策树，然后自底向上地对非叶结点（每个属性）进行考察，若将该结点对应的子树替换成叶结点能带来决策树泛化性能提升，则将该子树替换成叶结点

- 下面将根据下图（未剪枝的决策树）来讲解预剪枝和后剪枝：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921212202836.png" alt="未剪枝决策树" style="zoom:67%;" />

### 3.1 预剪枝

- **预剪枝步骤：**
> 1. 在生成决策树的时候，最开始是在根节点，基于信息增益准则，选择属性脐部进行划分。
> 2. 首先将该结点当成叶结点，然后根据其包含的数据集，决定该结点类别（选择数据集中最多的类别，好瓜or坏瓜），然后用该决策树在验证集上跑，得出准确率$$acc_i$$。
> 3. 然后选择用脐部进行属性划分，得到三个分支，再分别将这三个分支当作叶结点，并确定每个叶结点的类别，再用该决策树跑一遍验证集，得到准确率$$acc_j$$
> 4. 若$$acc_i < acc_j$$则划分，反之不划分（两者相等时，由于“奥卡姆剃刀准则”，是不进行划分的）
> 5. 重复上述步骤，直到属性选完

预剪枝得到的决策树如下：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921214432617.png" alt="预剪枝得到的决策树" style="zoom:73%;" />

- 预剪枝不仅降低了过拟合的风险，还减少了训练和测试时间开销。但是由于预剪枝基于“贪心“本质禁止展开这些分支，**所以可能会带来欠拟合的风险**。并且一些分支可能刚展开时泛化性能会下降，但是后续划分会导致泛化性能提升



### 3.2 后剪枝

- **后剪枝步骤：**

> 1. 剪枝顺序是自底向上，用例子中的图，则是依次考察6、5、2、3、1结点
> 2. 每次考察都将该节点替换为叶结点，然后通过对应数据集确定类别，再在验证集上跑，若准确率得到提升或不变，则执行剪枝

后剪枝得到的决策树如下：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921215744390.png" alt="后剪枝得到的决策树" style="zoom:73%;" />

- 一般来说，后剪枝欠拟合的风险很小，泛化性能往往优于预剪枝。但是要自底向上对所有非叶结点进行考察，所以训练开销要大得多





# 4 连续值处理

- 在对连续值处理时，可采用**连续属性离散化技术**，最简单的策略是采用**二分法**，C4.5决策树算法正使用了这种机制
- 给定数据集D和连续属性a，将a在D上的取值从小到大排列$${a^1, ..., a^n}$$。基于划分点t可将数据集D分为$$D^+_t$$和$$D^-_t$$，分别代表在属性a上大于t和不大于t的样本。显然对相邻的取值$$a^i$$和$$a^{i+1}$$，选择$$t \in [a^i, a^{i+1})$$的划分结果都是相同的，所以可以考察n-1个候选划分点：

$$
T_{a}=\left\{\frac{a^{i}+a^{i+1}}{2} \mid 1 \leqslant i \leqslant n-1\right\}
$$

即把$$[a^i, a^{i+1})$$的中位点作为候选划分点，然后就可以像离散值一样处理，比如计算信息增益：
$$
\begin{aligned}
\operatorname{Gain}(D, a) &=\max _{t \in T_{a}} \operatorname{Gain}(D, a, t) \\
&=\max _{t \in T_{a}} \operatorname{Ent}(D)-\sum_{\lambda \in\{-,+\}} \frac{\left|D_{t}^{\lambda}\right|}{|D|} \operatorname{Ent}\left(D_{t}^{\lambda}\right)
\end{aligned}
$$

- 在选择划分点的时候，可以不使用中位点，而是将中位点换成在训练集中出现过的不大于中位点的最大值，从而使得最终决策树使用的划分点在训练集中出现过





# 5 缺失值处理

- 当存在属性值缺失的时候，有两个问题需要解决：(1).如何在属性值缺失的情况下进行划分属性的选择；(2).给定划分属性，若样本在该属性值上缺失，则如何划分该样本
- 给定数据集D和属性a，$$\tilde{D}$$表示属性a的值不缺失的样本子集，$$\tilde{D}^v$$表示$$\tilde{D}$$的a属性取值为$$a^v$$的样本子集，$$\tilde{D}_k$$表示$$\tilde{D}$$的类别为k的子集。我们可以为每个样本赋予一个权重$$w_x$$**（训练开始时将根节点所有样本权重初始化为1）**，并定义：

$$
\rho=\frac{\sum_{\boldsymbol{x} \in \tilde{D}}{ }^{w_{\boldsymbol{x}}}}{\sum_{\boldsymbol{x} \in D} w_{\boldsymbol{x}}}
$$

$$
\tilde{p}_{k}=\frac{\sum_{\boldsymbol{x} \in \tilde{D}_{k}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}} \quad(1 \leqslant k \leqslant|\mathcal{Y}|)
$$

$$
\tilde{r}_{v}=\frac{\sum_{\boldsymbol{x} \in \tilde{D}^{v}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}} \quad(1 \leqslant v \leqslant V)
$$

直观地看，对于属性a，$$\rho$$表示五确实样本所占比例，$$\tilde{p}_k$$表示无缺失样本中第k类样本所占比例，$$\tilde{r}_v$$表示无缺失样本中在属性a上取值$$a^v$$的样本所占比例

- 基于上述定义，可将信息增益的公式推广为：

$$
\begin{aligned}
\operatorname{Gain}(D, a) &=\rho \times \operatorname{Gain}(\tilde{D}, a) \\
&=\rho \times\left(\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right)\right) \\
& \operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{|\mathcal{Y}|} \tilde{p}_{k} \log _{2} \tilde{p}_{k}
\end{aligned}
$$

- **若样本$$x$$在a上的取值已知，则正常划入子结点，样本权值仍然保持为$$w_x$$；若在a上的取值未知，则将该样本划入所有子结点，样本权值在每个分支上分别调整为$$\tilde{r}_{v} \cdot w_{\boldsymbol{x}}$$**