---
title: 优化算法
math: true
date: 2021-12-7
---



# 1 沿梯度方向函数值一定下降

- 证明上述的这个结论（这里只证明了一维输入的时候），首先通过泰勒公式：

$$
f(x)=\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n !}(x-a)^{n}
$$

- 将上式的x和a替换为$$x + \epsilon$$和x，得到：

$$
f(x+\epsilon) \approx f(x)+f^{\prime}(x) \epsilon+\mathcal{O}\left(\epsilon^{2}\right)
$$

- 当$$\epsilon$$足够小时，$$\mathcal{O}(\epsilon^2)$$可以忽略不计：

$$
f(x+\epsilon) \approx f(x)+f^{\prime}(x) \epsilon
$$

- 如果存在$$\eta > 0$$，使得$$|\eta f'(x)|$$足够小，那么：

$$
f\left(x-\eta f^{\prime}(x)\right) \approx f(x)-\eta f^{\prime}(x)^{2} \lesssim f(x)
$$

- 对于多维输入时的证明，可以看[牛顿法的原理](https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#4-1-%E7%89%9B%E9%A1%BF%E6%B3%95)





# 2 梯度下降法

- 梯度下降法即使用所有样本或部分样本关于loss的梯度和，作为整体的梯度：

$$
\begin{array}{c}
f(\boldsymbol{x})=\frac{1}{n} \sum_{i=1}^{n} f_{i}(\boldsymbol{x}) . \\
\nabla f(\boldsymbol{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\boldsymbol{x}) .
\end{array}
$$

- 不同batch-size的梯度下降有一定的区别，具体可看[不同batch size梯度下降的影响](https://zlkqz.site/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/)





# 3 动量法（Momentum）

### 3.1 梯度下降法的问题

- 举个栗子，考虑函数$$f(x) = 0.1x_1^2 + 2x_2^2$$，**输入中不同维度变量的梯度相差较大**，假设使用较小的学习率，其迭代路线如下：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221104163804950.png" alt="image-20221104163804950" style="zoom:80%;" />

- **可以看到，由于使用了较小的学习率，所以梯度较小的维度收敛得很慢，但是学习率又不能的过大，因为有较大的梯度的维度会无法收敛，**



### 3.2 指数加权移动平均

- 定义如下：

$$
y_{t}=\gamma y_{t-1}+(1-\gamma) x_{t} \\
y_0 = 0
$$

- 是可以对$$y_t$$进行展开的：

$$
\begin{aligned}
y_{t} &=(1-\gamma) x_{t}+\gamma y_{t-1} \\
&=(1-\gamma) x_{t}+(1-\gamma) \cdot \gamma x_{t-1}+\gamma^{2} y_{t-2} \\
&=(1-\gamma) x_{t}+(1-\gamma) \cdot \gamma x_{t-1}+(1-\gamma) \cdot \gamma^{2} x_{t-2}+\gamma^{3} y_{t-3} \\
&= \quad ......
\end{aligned}
$$

- 从上面可以看出，**可以将$$y_t$$当作前面时间步的$$x_i$$的加权平均**。但是我们需要一些近似，由于越往前的$$x_i$$涉及到的连乘越多，越接近0，**所以对于权值为$$\mathcal{O}(\gamma^{1 / (1-\gamma)})$$的项进行忽略，即把$$y_t$$看作是对最近$$1/(1 − \gamma)$$个时间步的$$x_i$$值的加权平均，且越接近当前时间步，权重越大**



### 3.3 算法流程

- 算法流程如下：

$$
\begin{array}{l}
\boldsymbol{v}_{t} \leftarrow \gamma \boldsymbol{v}_{t-1}+\eta \boldsymbol{g}_{t} \\
\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\boldsymbol{v}_{t}
\end{array}
$$

其中$$g_t$$为现在step的小批量梯度，最开始$$v_0 = 0$$，动量超参数$$\gamma$$满⾜$$0 \le \gamma < 1$$。当$$\gamma = 0$$时，动量法等价于小批量随机梯度下降



### 3.4 算法原理

- 动量法其实就是对指数加权移动平均中$$x_t$$做了一个变形：

$$
\boldsymbol{v}_{t} \leftarrow \gamma \boldsymbol{v}_{t-1}+(1-\gamma)\left(\frac{\eta}{1-\gamma} g_{t}\right)
$$

- 由上式可知，动量法其实就是对最近$$1/(1 − \gamma)$$个时间步的梯度（只是梯度进行了一个放缩）进行加权平均

- **所以，在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决于当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致**。如果梯度不一致，则代表梯度过大，那么动量法就会在加权平均时减去相反的梯度，以达到减少梯度的目的





# 4 AdaGrad算法

- 在前面梯度下降的问题中，造成这种现象的原因是**不同维度使用了相同的学习率**。而AdaGrad则自变量在每个维度的梯度值的大小来**调整各个维度上的学习率**，从而避免统⼀的学习率难以适应所有维度的问题



### 4.1 算法流程

- 算法流程如下：

$$
\boldsymbol{s}_{t} \leftarrow \boldsymbol{s}_{t-1}+g_{t} \odot g_{t} \\
\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\frac{\eta}{\sqrt{\boldsymbol{s}_{t}+\epsilon}} \odot \boldsymbol{g}_{t},
$$

其中$$\epsilon$$是为了防止除以0，一般为$$10^{-6}$$，$$\odot$$为按元素相乘，在最开始时$$s_0 = 0$$



### 4.2 算法原理

- 小批量随机梯度按元素平方的累加变量$$s_t$$出现在学习率的分母项中。**因此，如果 目标函数有关目变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；反之， 如果目标函数有关目变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢**

- 但是AdaGrad算法也有缺点，由于$$s_t$$⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的**学习率在迭代过程中⼀直在降低**。所以，**当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有用的解**





# 5 RMSProp算法

- 针对上述AdaGrad的问题，学习率会一直下降，而RMSProp对AdaGrad做了一点微小的改动，**使用了指数加权移动平均：**

$$
\begin{array}{c}
\boldsymbol{s}_{t} \leftarrow \gamma \boldsymbol{s}_{t-1}+(1-\gamma) \boldsymbol{g}_{t} \odot \boldsymbol{g}_{t} . \\
\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\frac{\eta}{\sqrt{s_{t}+\epsilon}} \odot \boldsymbol{g}_{t}
\end{array}
$$

- 可以看到，**AdaGrad中的$$s_t$$是所有steps中梯度的平方和，而RMSProp中的$$s_t$$是最近$$1 / (1-\gamma)$$个steps的加权平均**，这样学习率就不会一直降低





# 6 AdaDelta算法

- AdaDelta同样是针对AdaGrad的问题进行了改进，值得注意的是**AdaDelta没有学习率这一超参**



### 6.1 算法流程

- AdaDelta和RMSProp同样使用了指数加权移动平均，以减少梯度惩罚致使学习率过小的影响，给定超参数$$\rho$$（即前面中的$$\gamma$$），首先和RMSProp一样，计算：

$$
\boldsymbol{s}_{t} \leftarrow \rho \boldsymbol{s}_{t-1}+(1-\rho) \boldsymbol{g}_{t} \odot \boldsymbol{g}_{t}
$$

- 另外，他还维护另一个变量$$\Delta x_t$$：

$$
\boldsymbol{g}_{t}^{\prime} \leftarrow \sqrt{\frac{\Delta \boldsymbol{x}_{t-1}+\epsilon}{\boldsymbol{s}_{t}+\epsilon}} \odot \boldsymbol{g}_{t} \\

\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\boldsymbol{g}_{t}^{\prime}
$$

- $$\Delta x_t$$同样是使用指数加权移动平均：

$$
\Delta \boldsymbol{x}_{t} \leftarrow \rho \Delta \boldsymbol{x}_{t-1}+(1-\rho) \boldsymbol{g}_{t}^{\prime} \odot \boldsymbol{g}_{t}^{\prime}
$$



### 6.2 算法原理

- 首先是牛顿法中的更新公式：

$$
x_t = x_{t-1} - H^{-1}g
$$

其中$$H^{-1}$$为黑塞矩阵的逆

- 以及梯度下降中的更新公式：

$$
x_t = x_{t-1} - \eta g
$$

其中$$\eta$$为学习率

- 从上面的两个式子可以得出：**我们并需要显式的设置学习率，而可以直接用$$H^{-1}$$来代替学习率**

- 而由于黑塞矩阵的计算复杂度过高，所以我们需要进行一些迭代逼近，由于：

$$
x_t - x_{t-1} = \Delta x = -H^{-1}g
$$

所以：
$$
H^{-1} = -\frac{\Delta x}{g}
$$
我们就使用这个式子来进行迭代逼近，其中$$\Delta x$$和$$g$$使用前面时间步的指数加权平均来取近似

- 所以就得到了：

$$
\Delta x_{t}=-\frac{\operatorname{RMS}[\Delta x]_{t-1}}{\operatorname{RMS}[g]_{t}} g_{t}
$$

其中$$\operatorname{RMS}[g]_{t}=\sqrt{E\left[g^{2}\right]_{t}+\epsilon}$$，$$E[g^2]_t$$为最近t个时间步的$$g^2$$的指数加权移动平均，$$\operatorname{RMS}[\Delta x]_{t-1}$$同理



### 6.3 二阶导和一阶导

- AdaDelta中使用了黑塞矩阵，涉及到二阶导，**而作者认为使用二阶导是优于一阶导的**

- 一阶导可以得到一组近似：（RMSProp举例）

$$
\Delta x \propto g \propto \frac{\partial f}{\partial x} \propto \frac{1}{x}
$$

- 而二阶导又可以得到另一组近似：（牛顿法举例）

$$
\Delta x \propto H^{-1}g \propto \frac{\frac{\partial f}{\partial x}}{\frac{\partial^2 f}{\partial x^2}} \propto x
$$

- 由上面两个式子可以得出：**一阶方法最终正比于1/x，即与参数逆相关：参数逐渐变大的时候，更新值反而变小；而二阶方法最终正比于x，即与参数正相关：参数逐渐变大的时候，更新值也会变大。因此，作者称Hessian方法得到了Correct Units(正确的更新单元)。**





# 7 Adam算法

- Adam其实就是**动量法和RMSProp算法的结合**
- 其实就是在RMSProp有惩罚项的基础上，对梯度也做了指数加权平均：

$$
\boldsymbol{v}_{t} \leftarrow \beta_{1} \boldsymbol{v}_{t-1}+\left(1-\beta_{1}\right) \boldsymbol{g}_{t} \\
\boldsymbol{s}_{t} \leftarrow \beta_{2} \boldsymbol{s}_{t-1}+\left(1-\beta_{2}\right) \boldsymbol{g}_{t} \odot \boldsymbol{g}_{t}
$$

其中超参建议值：$$\beta_1 = 0.9, \beta_2 = 0.999$$

- 将$$v_t$$展开：

$$
\boldsymbol{v}_{t}=\left(1-\beta_{1}\right) \sum_{i=1}^{t} \beta_{1}^{t-i} \boldsymbol{g}_{i \circ}
$$

将所有权值相加得到和为$$1 - \beta_1^t$$，所以在t较小时，其权值和是不等于1的，所以进行了一个**偏差修正**：
$$
\begin{array}{l}
\hat{\boldsymbol{v}}_{t} \leftarrow \frac{\boldsymbol{v}_{t}}{1-\beta_{1}^{t}} \\
\hat{\boldsymbol{s}}_{t} \leftarrow \frac{\boldsymbol{s}_{t}}{1-\beta_{2}^{t}}
\end{array}
$$
这样权重值相加就等于1了

- 然后使用偏差修正后的变量进行更新：

$$
\begin{array}{l}
\boldsymbol{g}_{t}^{\prime} \leftarrow \frac{\eta \hat{\boldsymbol{v}}_{t}}{\sqrt{\hat{\boldsymbol{s}}_{t}}+\epsilon} \\
\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\boldsymbol{g}_{t}^{\prime}
\end{array}
$$

