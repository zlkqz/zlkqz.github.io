---
title: 最大熵模型
math: true
date: 2022-5-26
---



# 1 最大熵模型

### 1.1 最大熵原理

- **最大熵原理：**学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型

- 假设离散变量$$X$$的概率分布为$$P(X)$$，则其熵为：

$$
H(P) = -\sum_xP(x)\log P(x)
$$

熵的取值范围：$$0 \le H(P) \le \log|X|$$，在$$X$$为均匀分布时熵取得最大

- 所以在没有任何约束时，$$X$$直接取均匀分布，熵即可达到最大
- **条件熵：**在已知X的情况下，求Y的条件熵：

$$
H(Y|X) = \sum_{x}p(x)H(Y|X=x) = -\sum_xp(x)\sum_yp(y|x)log(p(y|x))
$$



### 1.2 模型约束

- 前面说到最大熵原理是在约束的模型集合中寻找熵最大的，那么介绍一下最大熵模型的约束为什么
- 假设是一个分类模型$$P(Y|X)$$，首先给定训练集$$T = \{(x_1, y_1), ...,(x_N, y_N)\}$$，那么可以由统计得到经验分布：

$$
\begin{array}{l}
\tilde{P}(X=x, Y=y)=\frac{v(X=x, Y=y)}{N} \\
\tilde{P}(X=x)=\frac{v(X=x)}{N}
\end{array}
$$

- **特征函数：**$$f(x, y)$$用于描述输入x和输出y之间的某个事实：

$$
f(x, y)=\left\{\begin{array}{ll}
1, & x \text { 与 } y \text { 满足某一事实 } \\
0, & \text { 否则 }
\end{array}\right.
$$

特征函数的形式很多样，可以是对数据中字、词的统计信息作为输入，甚至可以直接用某些模型（如BERT）的输出作为输入

- 那么特征函数关于经验分布$$\tilde{P}(X,Y)$$的期望值为：

$$
E_{\tilde{P}}(f) = \sum_{x,y}\tilde{P}(x,y)f(x,y)
$$

- 特征函数关于模型$$P(X|Y)$$和经验分布$$\tilde{P}(X)$$的期望值为：

$$
E_P(f) = \sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)
$$

- **而最大熵模型的假设为这两个期望值相等，即：**

$$
E_P(f) = E_{\tilde{P}}(f)
$$

这就是最大熵模型的约束，有几个特征函数就有几个约束



### 1.3 模型定义

- 假设满足所有约束条件的模型集合为$$\mathcal{C} \equiv\left\{P \in \mathcal{P} \mid E_{P}\left(f_{i}\right)=E_{\tilde{P}}\left(f_{i}\right), \quad i=1,2, \cdots, n\right\}$$，定义在概率分布$$P(Y|X)$$上的条件熵为：

$$
H(P)=-\sum_{x, y} \tilde{P}(x) P(y \mid x) \log P(y \mid x)
$$

则模型集合$$\mathcal{C}$$中$$H(P)$$最大的模型就称为最大熵模型





# 2 最大熵模型的学习

- 根据上面所述，现在问题就变为了一个带约束的最优化问题：

$$
\begin{array}{ll}
\max _{P \in \mathrm{C}} & H(P)=-\sum_{x, y} \tilde{P}(x) P(y \mid x) \log P(y \mid x) \\
\text { s.t. } & E_{P}\left(f_{i}\right)=E_{\tilde{P}}\left(f_{i}\right), \quad i=1,2, \cdots, n \\
& \sum_{y} P(y \mid x)=1
\end{array}
$$

- 上述问题可使用拉格朗日优化，引入拉格朗如乘子并得到对偶问题进行求解，具体过程可参考[SVM的学习](https://zlkqz.site/2022/09/26/SVM/#2-2-%E5%8E%9F%E9%97%AE%E9%A2%98%E8%BD%AC%E5%8C%96%E5%88%B0%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98)，**首先引入拉格朗日乘子$$w_0, w_1, ...,w_n$$得到拉格朗日函数：**

$$
L(P, w) \equiv-H(P)+w_{0}\left(1-\sum_{y} P(y \mid x)\right)+\sum_{i=1}^{n} w_{i}\left(E_{\tilde{P}}\left(f_{i}\right)-E_{P}\left(f_{i}\right)\right)
$$

- **然后将原始问题$$\min _{P \in \mathbf{C}} \max _{w} L(P, w)$$转化为对偶问题$$\max _{w} \min _{P \in \mathbf{C}} L(P, w)$$，先只考虑里面的最小化问题$$\min _{P \in \mathbf{C}} L(P, w)$$，求偏导$$\frac{\partial L(P,w)}{\partial P(y|x)} = 0$$，最后可以解得：**

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

其中$$w_i$$为每个特征函数对应的权重，也是拉格朗日优化时的拉格朗日乘子。$$Z_w(x)$$为规范化因子：
$$
Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

- 那么接下来的任务变成了外部的极大化问题，即将上述$$P_w(y|x)$$带入$$L(P,w)$$得到$$\Psi(w)$$，然后寻找：

$$
w^{*}=\underset{w}{\arg \max } \Psi(w)
$$

- **而上述对偶函数的极大化等价于最大熵模型的极大似然估计**

> **证明：**
>
> 当概率分布$$P(y|x)$$是最大熵模型时，对数似然函数为：
> $$
> \begin{aligned}
> L_{\tilde{P}}\left(P_{w}\right) &=\log\prod_{x,y}P(y|x)^{\tilde{P}(x,y)} = \sum_{x, y} \tilde{P}(x, y) \log P(y \mid x) \\
> &=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x, y} \tilde{P}(x, y) \log Z_{w}(x) \\
> &=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
> \end{aligned}
> $$
> 而上述对偶函数$$\Psi(w)$$中，有一项$$w_{0}\left(1-\sum_{y} P(y \mid x)\right)$$，由于$$w$$中并没有包含$$w_0$$，所以这一项可以直接去掉，不影响优化，所以：
> $$
> \begin{aligned}
> \Psi(w)=& \sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) \log P_{w}(y \mid x) +\sum_{i=1}^{n} w_{i}\left(\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y)\right) \\
> =& \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)+\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x)\left(\log P_{w}(y \mid x)-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
> =& \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) \log Z_{w}(x) \\
> =& \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
> \end{aligned}
> $$
> 由上述两个式子可以得到：
> $$
> \Psi(w)=L_{\tilde{p}}\left(P_{w}\right)
> $$





# 3 优化算法

- 现在我们是要最大化似然函数或者对偶函数，但是**该函数并没有显式的解析解**（有解析解，但是找不到），所以需要运用一些优化算法
- 值得庆幸的是，对偶函数的目标函数具有很好的性质，**他是光滑的凸函数，因此多种最优化方法都适用，并且保证能找到全局最优解**



### 3.1 改进的迭代尺度法

- 算法流程：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027204416341.png" alt="image-20221027204416341" style="zoom:80%;" />

- 这一算法的关键是(a)，即求解$$\delta_i$$，如果$$f\#(x,y)$$是常数，即对于任意x,y，都有$$f\#(x,y) = M$$，那么：

$$
\delta_{i}=\frac{1}{M} \log \frac{E_{\tilde{p}}\left(f_{i}\right)}{E_{p}\left(f_{i}\right)}
$$

如果$$f\#(x,y)$$不为常数，最简单有效的是**牛顿法**，将(a)中需要求解的方程表示为$$g(\delta_i) = 0$$，然后通过牛顿法最小化$$g(\delta_i)$$，每次迭代的公式为：
$$
\delta_{i}^{(k+1)}=\delta_{i}^{(k)}-\frac{g\left(\delta_{i}^{(k)}\right)}{g^{\prime}\left(\delta_{i}^{(k)}\right)}
$$


### 3.2 拟牛顿法

- 算法流程：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027205121849.png" alt="image-20221027205121849" style="zoom:80%;" />





# 4 牛顿法和拟牛顿法

- 上面提到了牛顿法和拟牛顿法，这节就简要介绍以下其原理
- 他们都是**求解无约束最优化问题**的常用方法，有收敛速度快的优点



### 4.1 牛顿法

- 考虑最优化：$$\min_{x \in R^n}f(x)$$，假设$$f(x)$$具有二阶连续偏导数，若第k次迭代之为$$x^{(k)}$$，则可将$$f(x)$$在$$x^{(k)}$$处进行二阶泰勒展开：

$$
f(x)=f\left(x^{(k)}\right)+g_{k}^{\mathrm{T}}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{\mathrm{T}} H\left(x^{(k)}\right)\left(x-x^{(k)}\right)
$$

其中$$g_k = g(x^{(k)}) = \nabla f(x^{(k)})$$是$$f(x)$$的梯度在$$x^{(k)}$$的值，$$H(x^{k})$$是黑塞矩阵$$H(x)=\left[\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}\right]_{n \times n}$$在$$x^{(k)}$$的值

- 函数$$f(x)$$有极值的必要条件是一阶导数为0，即$$\nabla f(x) = 0$$。特别的当$$H(x)$$为正定矩阵时，该点为极小值点
- 牛顿法的每次迭代，从$$x^{(k)}$$开始，将目标函数的极小值点作为下次迭代的$$x^{(k+1)}$$，其满足$$\nabla f(x^{(k+1)}) = 0$$

- 而将上面的泰勒展开求梯度，得到：

$$
\nabla f(x) = g_k + H_k(x - x^{(k)}) = 0
$$

其中$$H_k = H(x^{(k)})$$

- 所以得到：

$$
x^{(k+1)} = x^{(k)} - H_k^{-1}g_k = x^{(k)} + p_k
$$

- 将上面的流程总结：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027212238000.png" alt="image-20221027212238000" style="zoom:80%;" />



### 4.2 拟牛顿法

- 由于计算黑塞矩阵的逆$$H^{-1}$$比较复杂，所以考虑使用一个n阶矩阵$$G_k$$来代替$$H_k^{-1}$$，这就是拟牛顿法的思想
- 由上面牛顿法中的$$\nabla f(x^{(k+1)}) = 0$$的方程可得：

$$
g_{k+1} - g_k = H_k(x^{(k)} - x^{(k)})
$$

设$$y_k = g_{k+1} - g_k, \delta_k = x^{(k)} - x^{(k)}$$，上式可写为：
$$
y_k = H_k\delta_k  \\
H_k^{-1}y_k = \delta_k
$$
上式称为**拟牛顿条件**

- 而上面说过，要保证$$H_k$$是正定矩阵，才能保证驻点一定是极小值点。所以$$H_k$$同时满足拟牛顿条件和正定矩阵两个条件

- **而近似矩阵$$G_k$$同样满足这两个条件，即可将其作为近似的矩阵**



#### 4.2.1 DFP算法

- 现在重点就变成了如何对$$G_k$$进行更新，才能保证$$G_{k+1}$$同样满足条件
- 假设G的更新是加上两个附加项构成的，即：

$$
G_{k+1} = G_k + P_k + Q_k
$$

- 这时：

$$
G_{k+1}y_k = G_ky_k + P_ky_k + Q_ky_k
$$

为使$$G_{k+1}$$满足拟牛顿条件，$$P_k, Q_k$$要满足：
$$
\begin{array}{c}
P_{k} y_{k}=\delta_{k} \\
Q_{k} y_{k}=-G_{k} y_{k}
\end{array}
$$

- 这样的矩阵不难找出，例如：

$$
\begin{array}{c}
P_{k}=\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}} \\
Q_{k}=-\frac{G_{k} y_{k} y_{k}^{\mathrm{T}} G_{k}}{y_{k}^{\mathrm{T}} G_{k} y_{k}}
\end{array}
$$

- 所以更新公式为：

$$
G_{k+1}=G_{k}+\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}-\frac{G_{k} y_{k} y_{k}^{\mathrm{T}} G_{k}}{y_{k}^{\mathrm{T}} G_{k} y_{k}}
$$

- 总结一下DFP算法的算法流程：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214008428.png" alt="image-20221027214008428" style="zoom:80%;" />



#### 4.2.2 BFGS算法

- BFGS算法比DFS算法更加常用，后者是用$$G_k$$逼近$$H^{-1}$$，而BFGS是用$$B_k$$逼近$$H$$

- 算法流程：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214231809.png" alt="image-20221027214231809" style="zoom:80%;" />