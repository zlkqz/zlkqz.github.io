---
title: NLP基础
math: true
date: 2022-1-8
---



# 1 词嵌入（word embedding）

- 词向量是⽤来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）



### 1.1 词嵌入相比于one-hot向量的优点

- 虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，**one-hot词 向量⽆法准确表达不同词之间的相似度**，如我们常常使⽤的余弦相似度。对于向量$$x, y \in R^d$$，它 们的余弦相似度是它们之间夹⻆的余弦值：

$$
\frac{\boldsymbol{x}^{\top} \boldsymbol{y}}{\|\boldsymbol{x}\|\|\boldsymbol{y}\|} \in[-1,1]
$$

由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。

word2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表示成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。word2vec⼯具包含了两个模型，即**跳字模型（skip-gram）** 和**连续词袋模型（continuous bag of words，CBOW）**



### 1.2 跳字模型（skip-gram）

- 跳字模型假设基于某个词来生成它在⽂本序列周围的词
- 举个例子，假设⽂本序列是 “the” “man” “loves” “his” “son”。以“loves”作为中⼼词，设背景窗口大小为2。如下图所示，跳字模型所关⼼的是，给定中⼼词“loves”，生成与它距离不超过2个词的背景词 “the” “man” “his” “son”的条件概率，即：

$$
P(the, man, his, son | loves)
$$

假设给定中⼼词的情况下，背景词的生成是相互独⽴的，那么上式可以改写成：
$$
P(the| loves) \times P(man | loves) \times P(his | loves) \times P(son | loves)
$$
<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125151903218.png" alt="image-20220125151903218" style="zoom: 80%;" />

- 在跳字模型中，**每个词被表示成两个$$d$$维向量**，⽤来计算条件概率。假设这个词在词典中索引为$$i$$， 当它为中心词时向量表示为$$v_i \in R^d$$，而为背景词时向量表示为$$u_i \in R^d$$。设中⼼词$$w_c$$在词典中索引为$$c$$，背景词$$w_o$$在词典中索引为$$o$$，**给定中⼼词$$w_c$$生成背景词$$w_o$$的条件概率可以通过对向量内积做softmax运算而得到：**

$$
P\left(w_{o} \mid w_{c}\right)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)},
$$

其中词典索引集$$V = \{0, 1, . . . , |V|−1\}$$

- 假设给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$。 假设给定中⼼词的情况下背景词的生成相互独⽴，当背景窗口大小为m时，跳字模型的**似然函数**即给定任⼀中⼼词生成所有背景词的概率：

$$
\prod_{t=1}^{T} \prod_{-m \leq j \leq m, j \neq 0} P\left(w^{(t+j)} \mid w^{(t)}\right),
$$

这⾥小于1或大于T的时间步可以被忽略



- **跳字模型的训练：**

>- **跳字模型的参数是每个词所对应的中心词向量和背景词向量**
>- 先把把最大似然函数取负对数：
>
>$$
>-\sum_{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} \log P\left(w^{(t+j)} \mid w^{(t)}\right)
>$$
>
>如果使⽤随机梯度下降，那么在每⼀次迭代⾥我们随机采样⼀个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数
>
>- 由Softmax的运算结果可以得到：
>
>$$
>\log P\left(w_{o} \mid w_{c}\right)=\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}-\log \left(\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)\right)
>$$
>
>那么我们可以求得上式关于各中心词向量和背景词向量的梯度，比如关于$$v_c$$的梯度：
>$$
>\frac{\partial \log P\left(w_{o} \mid w_{c}\right)}{\partial v_{c}}=\boldsymbol{u}_{o}-\sum_{j \in \mathcal{V}} P\left(w_{j} \mid w_{c}\right) \boldsymbol{u}_{j} .
>$$
>训练结束后，对于词典中的任⼀索引为$$i$$的词，我们均得到该词作为中⼼词和背景词的两组词向量$$v_i$$和$$u_i$$。在⾃然语⾔处理应⽤中，**⼀般使⽤跳字模型的中心词向量作为词的表征向量**



- **跳字模型的具体实现：**

> - 可以用一个input-hidden-output的三层神经网络来建模上面的skip-model：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526195633208.jpg" alt="在这里插入图片描述" style="zoom: 50%;" />
>
> - 输入的表示：输入层中每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1,其余维度的值均为0
>
> - 网络中传播的前向过程：输出层向量的值可以通过**隐含层向量（K维，即每一个词向量的维度）**，以及连接隐藏层和输出层之间的**KxN维权重矩阵**计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算每一个单词的生成概率。
> - **input层和hidden层之间的N$$\times$$K权重矩阵即N个词的中心词向量，从input层到hidden层的运算即对中心词向量的提取，具体见下图。而hidden层和output层之间的K$$\times$$N权重矩阵即N个词的背景词向量，hidden层到output层的运算即实现$$u^T_ov_c$$，然后再在最后做一个softmax**
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526202027163.jpg" alt="在这里插入图片描述" style="zoom: 67%;" />
>
> 左边矩阵是input，右边矩阵是input到hidden的权重矩阵，即N个词的中心词向量，由于输入是ont-hot向量，所以运算结果其实就是对中心词向量的一个提取
>
> 
>
> - 在代码实现中是直接使用一个**嵌入层（Embedding层）**，此层的权值形状为**词典大小$$\times$$每个词的维度**，可以直接将中心词和背景词对应的one-hot向量转化为词向量。以skip-gram举例，中心词的one-hot向量形状为(批量大小，1)，背景词为(批量大小，词典大小)，两者皆通过嵌入层转化成词向量后形状分别为(批量大小，1，每个词维度)、(批量大小，词典大小，每个词维度)。然后通过**小批量乘法**实现中心词向量和背景词向量的相乘$$u^T_ov_c$$，得到形状为(批量大小，词典大小)的结果，然后进行Softmax
> - 上面的背景词one-hot向量第二维度不一定要为词典大小，如果我们采用**负采样**，则应该改为**正负样本和的大小**，具体可看下方负采样原理
> - 现在我们来解释一下**小批量乘法**，假设第⼀个小批量包含n个形状为$$a \times b$$的矩阵$$X_1, . . . , X_n$$，第⼆个小批量包含n个形状为$$b \times c$$的矩阵$$Y_1, . . . , Y_n$$。 这两个小批量的矩阵乘法输出为n个形状为$$a \times c$$的矩阵$$X_1Y_1, . . . , X_nY_n$$



### 1.3 连续词袋模型（CBOW）

- 连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，**连续词袋模型假设基于某中⼼词在 ⽂本序列前后的背景词来生成该中心词**

- 如下图，在同样的⽂本序列“the” “man” “loves” “his” “son” ⾥，以“loves”作为中⼼词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词 “the” “man” “his” “son”生成中心词“loves”的条件概率，也就是：

$$
P(loves|the,man,his,son)
$$

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125172417014.png" alt="image-20220125172417014" style="zoom:80%;" />

- 因为连续词袋模型的背景词有多个，我们**将这些背景词向量取平均**，然后使⽤和跳字模型⼀样的 ⽅法来计算条件概率

  设$$v_i \in R^d$$和$$u_i \in R^d$$分别表示词典中索引为$$i$$的词作为背景词和中⼼词的向 量（**注意符号的含义与跳字模型中的相反**）。设中⼼词$$w_c$$在词典中索引为$$c$$，背景词$$w_{o1} , . . . , w_{o2m}$$在 词典中索引为$$o_1, . . . , o_{2m}$$，那么给定背景词生成中⼼词的条件概率：
  $$
  .P\left(w_{c} \mid w_{o_{1}}, \ldots, w_{o_{2 m}}\right)=\frac{\exp \left(\frac{1}{2 m} \boldsymbol{u}_{c}^{\top}\left(\boldsymbol{v}_{o_{1}}+\ldots+\boldsymbol{v}_{o_{2 m}}\right)\right)}{\sum_{i \in \mathcal{V}} \exp \left(\frac{1}{2 m} \boldsymbol{u}_{i}^{\top}\left(\boldsymbol{v}_{o_{1}}+\ldots+\boldsymbol{v}_{o_{2 m}}\right)\right)}
  $$
  设$$W_o = \{w_{o1} , . . . , w_{o2m}\}$$，$$\overline{\boldsymbol{v}}_{o}=\left(\boldsymbol{v}_{o_{1}}+\ldots+\boldsymbol{v}_{o_{2 m}}\right) /(2 m)$$，可将上式简化为：
  $$
  P\left(w_{c} \mid \mathcal{W}_{o}\right)=\frac{\exp \left(\boldsymbol{u}_{c}^{\top} \overline{\boldsymbol{v}}_{o}\right)}{\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \overline{\boldsymbol{v}}_{o}\right)}
  $$

- 给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$，背景窗口大小为m。连续词袋模型的似然函数是由背景词生成任⼀中⼼词的概率：

$$
\prod_{t=1}^{T} P\left(w^{(t)} \mid w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)}\right)
$$

- 接着再像跳字模型一样求似然函数的负对数，然后再求关于背景向量和中心向量的梯度。**同跳字模型不⼀样的⼀点在于，我们⼀般使⽤连续词袋模型的背景词向量作为词的表征向量**



- **跳字模型更适合大型语料库，CBOW更适合小型的语料库**

### 1.4 近似训练

- 无论是跳字模型还是连续词袋模型，在求梯度的时候，由于条件概率使⽤了softmax运 算，每⼀步的梯度计算都包含词典大小数⽬的项的累加。对于含⼏⼗万或上百万词的较大词典， 每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练⽅法，即**负采样（negative sampling）**或**层序softmax（hierarchical softmax）**

##### 1.4.1 高频词抽样

- 首先介绍一下对高频词进行抽样（也称⼆次采样(subsampling)）。对于文本中的高频词，比如"the"，不进行抽样会带来两个问题：

> 1. 当我们得到成对的单词训练样本时，**("fox", "the") 这样的训练样本并不会给我们提供关于“fox”更多的语义信息**，因为“the”在每个单词的上下文中几乎都会出现
> 2. 由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，...）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数

- Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：**对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关，词频越高被删除的概率就越大**



##### 1.4.2 负采样（negative sampling）

- **负采样（negative sampling）**解决了因为词典大小过大的计算开销问题。不同于原本每个训练样本更新所有的权重，**负采样每次让一个训练样本仅仅更新一小部分的权重**，这样就会降低梯度下降过程中的计算量。
- 当我们用训练样本 ( input word: "fox"，output word: "quick") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们**期望输出为0的神经元结点**所对应的单词我们称为**“negative” word**。
- 当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。**（在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。）**

- **每个单词被选为“negative words”的概率计算公式与其出现的频次有关，词频越高，被选上的概率就越高**，论文中给的公式是：

$$
P\left(w_{i}\right)=\frac{f\left(w_{i}\right)^{3 / 4}}{\sum_{j=0}^{n}\left(f\left(w_{j}\right)^{3 / 4}\right)}
$$

- **负采样概述：**

> - 负采样修改了原来的⽬标函数。给定中⼼词$$w_c$$的⼀个背景窗口，我们把背景词$$w_o$$出现在该背景窗口看作⼀个事件，并将该事件的概率计算为：
>
> $$
> P\left(D=1 \mid w_{c}, w_{o}\right)=\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)
> $$
>
> - 我们先考虑最大化⽂本序列中所有该事件的联合概率来训练词向量。具体来说，给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$且背景窗口大小为m，考虑最大化联合概率：
>
> $$
> \prod_{t=1}^{T} \prod_{-m \leq j \leq m, j \neq 0} P\left(D=1 \mid w^{(t)}, w^{(t+j)}\right)
> $$
>
> 然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为⽆穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫⽆意义。**负采样通过采样并添加负类样本使⽬标函数更有意义。**
>
> - 在采集了K个负样本后，我们可以将所求概率近似为：
>
> $$
> P\left(w^{(t+j)} \mid w^{(t)}\right)=P\left(D=1 \mid w^{(t)}, w^{(t+j)}\right) \prod_{k=1, w_{k} \sim P(w)}^{K} P\left(D=0 \mid w^{(t)}, w_{k}\right)
> $$
>
> - 取负对数后变成了：
>
> $$
> -\log P\left(w^{(t+j)} \mid w^{(t)}\right)=-\log \sigma\left(\boldsymbol{u}_{i_{t+j}}^{\top} v_{i_{t}}\right)-\sum_{k=1, w_{k} \sim P(w)}^{K} \log \sigma\left(-\boldsymbol{u}_{h_{k}}^{\top} v_{i_{t}}\right)
> $$
>
> 现在，**训练中每⼀步的梯度计算开销不再与词典大小相关，而与K线性相关**。当K取较小的常数时，负采样在每⼀步的梯度计算开销较小。

- **负采样的具体实现：**

> - 在实际应用中，我们并没有严格的将不在窗口中的词定义为负样本，而是根据词频直接随机取负样本，这样的实验结果是更优的。
>
> - **设词典大小为$$\mathcal{V}$$，将一根长为1的线段分为$$\mathcal{V}$$段，每段分别对应一个词，每个词对应的长度就是上面所写的每个词对应的频率$$P(w_i)$$，然后取一个M（M>>$$\mathcal{V}$$），论文中M取的是$$10^8$$，将长度为1的线段均分为M份（如下图），然后取K个负样本即在该线段上取K个点，取到的点在哪个词的对应线段就取哪个词**
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211153340493.png" alt="image-20220211153340493" style="zoom:67%;" />



##### 1.4.3 层序Softmax（hierarchical softmax）

- 层序softmax是另⼀种近似训练法。它使⽤了⼆叉树这⼀数据结构，树的每个叶结点代表词典$$\mathcal{V}$$中的每个词。
- <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220210181128975.png" alt="image-20220210181128975" style="zoom: 80%;" />

假设$$L(w)$$为从⼆叉树的根结点到词$$w$$的叶结点的路径（包括根结点和叶结点）上的结点数。 设$$n(w, j)$$为该路径上第$$j$$个结点，并设该结点的背景词向量为$$u_{n(w,j)}$$。以上图为例，$$L(w_3) = 4$$ 层序softmax将跳字模型中的条件概率近似表示为：
$$
P\left(w_{o} \mid w_{c}\right)=\prod_{j=1}^{L\left(w_{o}\right)-1} \sigma\left([[ n\left(w_{o}, j+1\right)=\operatorname{leftChild}\left(n\left(w_{o}, j\right)\right) ]] \cdot \boldsymbol{u}_{n\left(w_{o}, j\right)}^{\top} \boldsymbol{v}_{c}\right),
$$
其中如果判断x为真，[[x]] = 1；反之[[x]] = −1。

例如让我们计算上图中给定词$$w_c$$生成词$$w_3$$的条件概率：
$$
P\left(w_{3} \mid w_{c}\right)=\sigma\left(\boldsymbol{u}_{n\left(w_{3}, 1\right)}^{\top} \boldsymbol{v}_{c}\right) \cdot \sigma\left(-\boldsymbol{u}_{n\left(w_{3}, 2\right)}^{\top} \boldsymbol{v}_{c}\right) \cdot \sigma\left(\boldsymbol{u}_{n\left(w_{3}, 3\right)}^{\top} \boldsymbol{v}_{c}\right)
$$

- **推导过程：**

> 设$$d_{2}^{w}, d_{3}^{w}, \cdots, d_{l w}^{w} \in\{0,1\}$$为词$$w$$的编码，$$d_j^w$$代表$$w$$路径上的第$$j$$个节点对应的编码（根节点无编码），则：
> $$
> P(w_0|w_c) = \prod_{j=2}^{L(w)} P\left(d_{j}^{w} \mid v_c, u_{n_(w, j)}\right)
> $$
> 其中的每一项都是一个Logistic回归：
> $$
> P\left(d_{j}^{w} \mid v_c, u_{n_(w, j)}\right) = \left\{\begin{array}{ll}
> \sigma\left(v_c^T u_{n_(w, j)}\right), & d_{j}^{w}=0 \\
> 1-\sigma\left(v_c^T u_{n_(w, j)}\right), & d_{j}^{w}=1
> \end{array}\right.
> $$
> 可以将两式合并，并且由于$$\sigma(x) + \sigma(-x) = 1$$，可以将上式转化为：
> $$
> P\left(d_{j}^{w} \mid v_c, u_{n_(w, j)}\right)=\left[\sigma\left(v_c^T u_{n_(w, j)}\right)\right]^{1-d_{j}^{w}} \cdot\left[\sigma\left(-v_c^T u_{n_(w, j)}\right)\right]^{d^{w}}
> $$
> 我们取目标函数的对数：
> $$
> \mathcal{L} = \sum_{w \in \mathcal{V}}\log P(Context(w) | w) = \sum_{w \in \mathcal{V}} \sum_{j=2}^{L(w)}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(v_c^T u_{n_(w, j)}\right)\right]+d_{j}^{w} \cdot \log \left[\sigma\left(-v_c^T u_{n_(w, j)}\right)\right]\right\}
> $$
> **要最大化上述目标函数是比较困难的，但是我们可以最大化每一子项（即下式），达到局部最大化的效果**：
> $$
> \mathcal{L}(w, j) = \left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(v_c^T u_{n_(w, j)}\right)\right]+d_{j}^{w} \cdot \log \left[\sigma\left(-v_c^T u_{n_(w, j)}\right)\right]\right\}
> $$
> **这样就可以将计算复杂度降为$$O(log_2 |\mathcal{V}|)$$，当词典$$\mathcal{V}$$很大时，层序softmax在训练中每⼀步的梯度计算开销相较未使用近似训练时大幅降低。**
>
> 显然，上式的$$d_j^w$$是用来**判断下一个节点是否是本节点的左孩子的**，其作用是为了满足给定任意中心词$$w_c$$，生成背景词的条件概率和为1：
> $$
> \sum_{w \in \mathcal{V}} P\left(w \mid w_{c}\right)=1
> $$
> 这样才能使似然函数不至于训练到无穷大（和负采样要添加负样本是一样的原因），并且符合概率和为1

- **层序Softmax中的二叉树是使用的哈夫曼树，可以使搜索次数达到最小**



# 2 子词嵌入（fastText）

- 英语单词通常有其内部结构和形成⽅式。例如，我们可以从“dog” “dogs”和“dogcatcher”的 字面上推测它们的关系。这些词都有同⼀个词根“dog”，但使⽤不同的后缀来改变词的含义。而 且，这个关联可以推⼴⾄其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关 系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。这方面的研究称为构词学
- 在word2vec中，我们并没有直接利⽤构词学中的信息。⽆论是在跳字模型还是连续词袋模型中， 我们都将形态不同的单词⽤不同的向量来表示。例如，“dog”和“dogs”分别⽤两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌⼊（subword embedding）的⽅法，从而试图将构词信息引⼊word2vec中的跳字模型

- 在fastText中，每个中⼼词被表⽰成子词的集合。举个例子，例如单词"where"，⾸先，我们在单词的**⾸尾分别添加特殊字符“<” 和  “>” 以区分作为前后缀的子词**。然后，**将单词当成⼀个由字符构成的序列来提取n元语法**。例如，当n = 3时，我们得到所有⻓度为3的子词："<wh"、"whe"、"her"、"ere"、"re>"以及特殊子词"where"。
- 在fastText中，对于⼀个词w，我们将它所有⻓度在3 ∼ 6的子词和特殊子词的并集记为$$\mathcal{G}_{w}$$。**那么词典则是所有词的子词集合的并集**。假设词典中子词$$g$$的向量为$$z_g$$，那么跳字模型中词w的作为中⼼词的向量$$v_w$$则表⽰成：

$$
v_{w}=\sum_{g \in \mathcal{G}_{w}} z_{g}
$$

- fastText的其余部分同跳字模型⼀致，不在此重复。可以看到，**与跳字模型相⽐，fastText中词典 规模更大，造成模型参数更多**，同时⼀个词的向量需要对所有子词向量求和，继而导致**计算复杂度更高**。但与此同时，**较生僻的复杂单词，甚⾄是词典中没有的单词，可能会从同它结构类似的 其他词那⾥获取更好的词向量表示。**



# 3 全局向量的词嵌入（GloVe）

- 在词嵌入中，交叉熵损失函数有时并不是一个好的选择。一方面，正如上面所说，**会带来整个词典大小的累加项，带来过大的计算开销**。另一方面，**词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。**
- GloVe模型既使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。为了做到这一点GloVe模型引入了**共现矩阵（Cooccurrence Probabilities Matrix）**



### 3.1 共现矩阵

- 假设：
> 1. 共现矩阵为$$X$$，$$X$$中的元素$$X_{ij}$$为语料库中$$word_i$$上下文中出现$$word_j$$的次数
> 2. $$X_i = \sum_k X_{ik}$$是出现在$$word_i$$上下文中所有词的总次数
> 3. $$P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}$$为$$word_j$$出现在$$word_i$$上下文的概率

- 下面我们来举个例子：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235403111.png" alt="image-20220211235403111" style="zoom:80%;" />

设Ratio = $$\frac{P_{ik}}{P_{jk}}$$，，从上面的例子中我们可以总结出：

![image-20220211235553030](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235553030.png)

**所以Ratio可以反应word之间的相关性，而GloVe就是利用了这个值**

假设$$i, j, k$$三者的词向量都已经得到$$w_i, w_j, w_k$$，那么我们现在就要找一个函数F，使得：
$$
F\left(w_{i}, w_{j}, w_{k}\right)=\frac{P_{i k}}{P_{j k}}
$$


### 3.2 损失函数推导

- **上述等式的右边是通过统计得到的已知量，左侧的三个词向量$$w_i, w_j, w_k$$是模型要求的量，F是未知的，如果能够将F的形式确定下来， 那么我们就可以通过优化算法求解词向量了，下面是求解过程**

- $$\frac{P_{i k}}{P_{j k}}$$考察了$$i,j,k$$三个词的相关性，不妨先只考虑$$i,j$$两个词的词向量$$w_i, w_j$$的相关性，向量相关关系的度量可以用两个向量的差，所以上述等式可以先改为：

$$
F\left((w_{i}-w_{j}), w_{k}\right)=\frac{P_{i k}}{P_{j k}}
$$

- 由于右边是标量，左边是向量，所以可以联想到使用向量的内积转化为标量：

$$
F\left(\left(w_{i}-w_{j}\right)^{T} w_{k}\right)=F\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{P_{i k}}{P_{j k}}
$$

- 因为左边是差，右边是商，所以将F设为exp运算，将差转化为商：

$$
\exp \left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{\exp \left(w_{i}^{T} w_{k}\right)}{\exp \left(w_{j}^{T} w_{k}\right)}=\frac{P_{i k}}{P_{j k}}
$$

- 现在只需让分子分母分别相等就能成立：

$$
\exp \left(w_{i}^{T} w_{k}\right)= \alpha P_{i k}, \exp \left(w_{j}^{T} w_{k}\right)= \alpha P_{j k}，\quad  \alpha为常数
$$

- 现在只需要在整个词料库中考察$$\exp \left(w_{i}^{T} w_{k}\right)= \alpha P_{i k}= \alpha \frac{X_{i k}}{X_{i}}$$，即：

$$
w_{i}^{T} w_{k}=\log \left(\alpha \frac{X_{i k}}{X_{i}}\right)=\log X_{i k}-\log X_{i} + log\alpha
$$

- 所以我们需要设置一个偏差项$$b_i$$来拟合$$log X_i - log \alpha$$，由于如果上式$$i,k$$位置交换，左式的值不变，但是右式的值会变，所以我们还要设置一个偏差项$$b_k$$，即：

$$
w_{i}^{T} w_{k}  + b_i + b_k=log X_{ik}
$$

- 上面公式只是理想状态下，实际中只能要求两者接近	从而就有了代价函数：

$$
J=\sum_{i, k}\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}
$$

- 如果两个词共同出现的次数越多，那么其在代价函数中的影响就越大，所以要设计一个权重函数：

$$
J=\sum_{i, k} f\left(X_{i k}\right)\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}
$$

> 对于函数f要满足一下条件：
>
> 1. 如果两个词没有共同出现过，那么权重就是0，即f(0) = 0
> 2. 两个词共同出现次数越多，那么权重就越大，所以f是一个递增函数
> 3. 由一些于高频词（$$X_{ij}较大$$，如"the"）的权重很大，但是实际上作用并不是很大，所以f(x)对于较大的x不能取太大的值
>
> 综合上方条件，论文提出了一下函数：
> $$
> f(x)=\left\{\begin{array}{r}
> \left(\frac{x}{x_{\max }}\right)^{\alpha}, \text { if } x<x_{\max } \\
> 1, \text { otherwise }
> \end{array}\right.
> $$
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212005820238.png" alt="image-20220212005820238" style="zoom:67%;" />
>
> 作者认为$$x_{max} = 100, \alpha = \frac{3}{4}$$比较合适

- 值得强调的是，如果词$$w_i$$出现在词$$w_j$$的背景窗口里，那么词$$w_j$$也会出现在词$$w_i$$的背景窗口⾥。也就是说，$$x_{ij} = x_{ji}$$。不同于**word2vec中拟合的是非对称的条件概率$$p_{ij}$$，GloVe模型拟合的是对称的$$\log X _{ij}$$**。因此，**任意词的中心词向量和背景词向量在GloVe模型中是等价的。但由于初始化值的不同，同⼀个词最终学习到的两组词向量可能不同**。当学习得到所有词向量以后，GloVe模型使⽤**中心词向量与背景词向量之和作为该词的最终词向量，这样做是为了提高鲁棒性（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）**





# 4 词嵌入应用

- 下列应用均为在训练好的预训练模型上进行训练的




### 4.1 求近义词和类比词

- 求近义词可以运用词向量之间的余弦相似度
- 求类比词，例如，“man”（男人）: “woman”（女人）:: “son”（儿子）: “daughter”（女儿）是⼀个类⽐例子。**对于类比关系中的4个词 a : b :: c : d，给定前3个词a、b和c，求d。设词w的词向量为vec(w)。求类比词的思路是vec(a) - vec(b) = vec(c) - vec(d)，来求出vec(d)的近似值，然后寻找和其最相似的几个词向量**



### 4.2 使用循环神经网络进行文本情感分类

- ⽂本分类是⾃然语⾔处理的⼀个常⻅任务，它把⼀段不定⻓的⽂本序列变换为⽂本的类别。本节关 注它的⼀个子问题：使⽤⽂本情感分类来分析⽂本作者的情绪。这个问题也叫**情感分析（sentiment analysis）**
- 本节我们使用电影评论的情感分析（二分类，positive和negtive的情感），在数据处理的时候，一条评论为一个样本，我们要先对评论进行分词（一般都是**基于空格分词**），并且由于每条评论的长度不一样，无法组成小批量，所以我们需要通过**截断或者补充来使长度定长**
- 对于模型的设计，先需要一个**嵌入层将文本转化为词向量**，再通过**双向循环网络对特征序列进⼀步编码得到序列信息**，然后将**最初时间步和最终时间步的隐藏状态连结**，再传入全连接层输出
- 注意由于是预训练的模型，所以**嵌入层的模型参数是不需要更新的**，直接由训练好的模型参数导入即可。但是**导入的模型的词向量维度要和预先设置的嵌入层词向量维度一致**



### 4.3 使用卷积神经网络（textCNN）进行文本情感分类：

- 我们可以将⽂本数据看作只有⼀个维度的时间序列，并很⾃然地使⽤循环神经⽹络来表征这样的数据。其实，我们也可以将⽂本当作⼀维图像，从而可以⽤⼀维卷积神经⽹络来捕捉临近词之间的关联

- **textCNN基于的假设是：一个词和其相邻的词（即卷积核中的词）是相关的**



##### 4.3.1 一维卷积层

- 和二维卷积层一样，一维卷积层卷积窗口从输⼊数组的最左⽅开始，按从左往右的顺序，依次在输⼊数组上滑动，举个例子：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155418239.png" alt="image-20220212155418239" style="zoom:67%;" />

输出中的2是由输入的前两个和核逐个相乘得到的：0x1+1x2=2，然后窗口往后滑动一格，以此类推

对于多通道操作也是和二维卷积层一样的：
![image-20220212155747478](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155747478.png)

每个通道分别操作，然后各通道结果相加。如果想要输出多通道，则核还要增加一个维度



##### 4.3.2 时序最大池化层

- 假设输⼊包含多个通道，各通道由不同时间步上的数值组成，**各通道的输出即该通道所有时间步中最⼤的数值**。因此，**时序最⼤池化层的输⼊在各个通道上的时间步数可以不同**
- 为提升计算性能，我们常常将不同⻓度的时序样本组成⼀个小批量，**并通过在较短序列后附加特殊字符（如0）令批量中各时序样本⻓度相同**。这些⼈为添加的特殊字符当然是⽆意义的。由于时序最⼤池化的主要⽬的是抓取时序中最重要的特征，它通常能**使模型不受⼈为添加字符的影响**



##### 4.3.3 textCNN模型

- textCNN模型主要使⽤了⼀维卷积层和时序最⼤池化层。假设输⼊的⽂本序列由n个词组成，每 个词⽤d维的词向量表⽰。那么输⼊样本的宽为n，⾼为1，输⼊通道数为d。textCNN的计算主要分为以下⼏步：

> 1. 定义多个⼀维卷积核，并使⽤这些卷积核对输⼊分别做卷积计算。宽度不同的卷积核可能 会捕捉到不同个数的相邻词的相关性
> 2. 对输出的所有通道分别做时序最⼤池化，再将这些通道的池化输出值连结为向量。
> 3. 通过全连接层将连结后的向量变换为有关各类别的输出。这⼀步可以使⽤丢弃层应对过拟合。

- 举个例子，这⾥的输⼊是⼀个有11个词的句子，每个词⽤6维词向量表⽰。因此输⼊序列的宽为11，输⼊通道数为6。给定2个⼀维卷积核，核宽分别为2和4，输出 通道数分别设为4和5

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212161751535.png" alt="image-20220212161751535" style="zoom: 67%;" />

**尽管每个通道的宽不同，我们依然可以对各个通道做时序最⼤池化， 并将9个通道的池化输出连结成⼀个9维向量**。最终，使⽤全连接将9维向量变换为2维输出，即正⾯情感和负⾯情感的预测





# 5 机器翻译

- 我们使用seq2seq模型进行机器翻译，原理具体可看[seq2seq](https://zlkqz.top/2021/12/10/RNN/#7-Seq2Seq%E6%A8%A1%E5%9E%8B)

- 这里我们只介绍一下如何评价翻译结果。评价机器翻译结果通常使⽤**BLEU（Bilingual Evaluation Understudy）**。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。
- 具体来说，设词数为n的⼦序列的精度为$$p_n$$。它是**预测序列与标签序列匹配词数为n的⼦序列的数量与预测序列中词数为n的⼦序列的数量之⽐**。举个例子，假设标签序列为A、B、C、D、E、F， 预测序列为A、B、B、C、D，那么$$p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$$。设$$len_{label}$$和$$len_{pred}$$分 别为标签序列和预测序列的词数，那么，BLEU的定义为：

$$
\exp \left(\min \left(0,1-\frac{\text { len }_{\text {label }}}{\text { len }_{\text {pred }}}\right)\right) \prod_{n=1}^{k} p_{n}^{1 / 2^{n}}
$$

其中k是我们希望匹配的⼦序列的最⼤词数。可以看到当预测序列和标签序列完全⼀致时， BLEU为1。

- 设计思想：

> 因为匹配较⻓⼦序列⽐匹配较短⼦序列更难，BLEU对匹配较⻓⼦序列的精度赋予了更⼤权重，具体是在上式的$$p_n^{1/2^n}$$中，当序列较长，即n较大时，由于$$p_n \in [0, 1]$$，所以$$1/2^n$$后会变大，并且n越大越接近1，即给较长序列更大的权重
>
> 并且由于较短序列一般会有比较大的$$p_n$$，所以连乘项前面的系数是为了惩罚较短的输出而设置的。举个例子，当$$len_{pred}$$较小时，连乘项前面的系数就会小于1，以此来减少较短序列的权重。
