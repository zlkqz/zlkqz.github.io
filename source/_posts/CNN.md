---
title: CNN基本概念
math: true
date: 2021-11-26
---



# 1 卷积层

### 1.1 互相关运算

- **在⼆维卷积层中，⼀个⼆维输⼊数组和⼀个⼆维核（kernel）数组通过互相 关运算输出⼀个⼆维数组**，如下图：

![image-20211124131936578](https://i.loli.net/2021/11/24/8amf1EYbuU7JPKd.png)

卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依 次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘 并求和，得到输出数组中相应位置的元素

- **⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出**
- **使用互相关运算做边缘检测：**

比如我们把kernel设为[1, -1]，要识别的图像为单通道，只有黑白：![image-20211124132335469](https://i.loli.net/2021/11/24/Uro3neFEKigbCd2.png)

则进行互相关运算之后可以变为：![image-20211124132405678](https://i.loli.net/2021/11/24/kuH17Ef3p8Rbjdi.png)

由此我们可以看出边缘是在第2和第6列

- **卷积运算：**其实就是把核数组左右翻转并 上下翻转，再与输⼊数组做互相关运算。但因为我们的参数都是学习出来的，而不是一开始设定好的，**所以卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出**，而我们一般使用的也是互相关运算



### 1.2 特征图和感受野

- **⼆维卷积层输出的⼆维数组可以看作输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）**
- **影响元素x的前向计算的所有可能输⼊区域叫做x的感受野（receptive field）**，用上图距离，阴影中19的感受野就是输入的四个阴影，如果输入的元素又是另一个卷积运算的输出，则我们还可以将19的感受野扩大
- 可⻅，我们可以通过更深的卷积神经⽹络使特征 图中单个元素的感受野变得更加⼴阔，从而捕捉输⼊上更⼤尺⼨的特征



### 1.3 填充和步幅

- **填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）**，如下图就是对3$\times$3数组进行填充，填充为1：

![image-20211124133849360](https://i.loli.net/2021/11/24/YE4p8vg31IaWbuK.png)

- **我们将每次滑动的⾏数和列数称为步幅（stride）**

- **一般来说输出的高宽为：**
  $$
  [(n_h - k_h + p_h + s_h) / s_h] \times [(n_w - k_w + p_w + s_w) / s_w]
  $$
  其中：$n为输入，k为核，p为填充，s为步幅$

- 步幅为1时，很多情况下，我们会设置$p_h = k_h−1$和$p_w = k_w −1$来使输⼊和输出具有相同的⾼和宽



### 1.4 多输入通道和多输出通道

- 当输入通道 $c_i > 1$ 时，我们为每个输入通道都分配一个核数组，则得到一个形状为$c_i \times k_h \times k_w$的卷积核，最后再将每个通道上的卷积结果相加，就得到输出，如下图：

![image-20211124140843742](https://i.loli.net/2021/11/24/iNGt96JXUlSAEKZ.png)

- 可是按上面的方法，无论输入通道是多少，输出通道总为1，我们设输入、输出通道分别为$c_i, c_o$，如果我们想要多通道的输出，则可以给每个输出通道都创建一个形状为$c_i \times k_h \times k_w$的核，则可以得到形状为$c_o \times c_i \times k_h \times k_w$的卷积核



### 1.5 1$\times$1卷积层

- 卷积窗口形状为1 × 1的多通道卷积层。我们通常称之为1 × 1卷 积层，并将其中的卷积运算称为1 × 1卷积
- 因为使⽤了最小窗口，1 × 1卷积失去了卷积层可以 识别⾼和宽维度上相邻元素构成的模式的功能。**实际上，1 × 1卷积的主要计算发⽣在通道维上， 1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度**
- **假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那 么1 × 1卷积层的作⽤与全连接层等价**



### 1.6 卷积层相对于全连接层的优点

- **卷积层使图像的像素在⾼和宽两个⽅向上的相关性均可能被有效识别**，用全连接层举例，全连接层就相当于一个很大的卷积核，但是过大的卷积核带来的问题是我们可能无法提取到有效特征，并且像素在高宽方向上的相关性可能提取不到，如果选择合适的卷积核大小才更可能提取到
- 卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重复计算，同一输入通道使用同一卷积核，**实现参数共享**，从而**避免参数过多**。同时参数共享也具有物理意义，他使卷积层**具有平移等特性**，比如图像中有一只猫，那么无论他在图像的任何位置，我们都能提取到他的主要特征，将他识别为猫
- 对于全连接层，**任意一对输入和输出之间都会产生交互**，形成稠密的连接结构

![image-20211125234024022](https://i.loli.net/2021/11/25/BuGFLnhli2NJPCv.png)

而在卷积神经网络中，卷积核尺度远小于输入的维度，**这样每个输出神经元仅与前一层部分神经元产生交互**

![image-20211125234143051](https://i.loli.net/2021/11/26/HBjoOyvEcqxSna6.png)

我们将这种特性称为**稀疏交互**，这样我们可以将**优化过程的时间复杂度减少好几个数量级，并且缓解过拟合**

稀疏交互的物理意义是**许多现实中的数据都具有局部的特征结构，我们可以先学习局部的特征，然后再将局部的特征组合起来形成更复杂和抽象的特征**





# 2 池化层

- **池化层的提出是为了缓解卷积层对位置的过度敏感性**。不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均值。该运算也分别叫做**最⼤池化**或**平均池化**
- **池化层没有参数**
- Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，这个妥协会越来越小，下面介绍几种常用的池化层



### 2.1 平均池化层（mean-pooling）

- 即对邻域内特征点只求平均
- 优缺点：**抑制邻域大小受限造成的估计值方差增大**，能很好的保留背景，但容易使得图片变模糊



### 2.2 最大池化层（max-pooling）

- 即对邻域内特征点取最大
- 优缺点：**抑制积层参数误差造成估计均值的偏移**，能很好的保留纹理特征，一般现在都用max-pooling而很少用mean-pooling



### 2.3 全局平均池化（global average pooling）

- 对每个通道中所有元素求平均并直接⽤于分类
- 优点：大幅度减少网络参数，理所当然的减少了过拟合现象



### 2.4 池化层的作用

1. **对特征进行压缩，保留主要特征的同时减少参数和计算量，减少模型复杂度，防止过拟合。**用池化层做互相关运算，本身就能减少特征的个数。并且对于全局平均优化，如果我们要进行图像分类，我们需要使用参数很多的全连接层，最后再导入Softmax，而如果我们使用全局平均优化，则可以规避全连接庞大的参数量，减少过拟合
2. **实现不变性**，包括平移不变性、旋转不变性和尺度不变性。



### 2.5 池化层的多通道

- 和卷积层有区别， 在处理多通道输⼊数据时，**池化层对每个输⼊通道分别池化，而不是像卷积层那样将各通道的输 ⼊按通道相加。这意味着池化层的输出通道数与输⼊通道数相等**





# 3 LeNet

- 分为卷积层块和全连接层块两个部分

### 3.1 卷积层块

- 卷积层块⾥的基本单位是卷积层后接最⼤池化层，卷积层⽤来识别图像⾥的空间模式，如线条和 物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性
- 在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似
- 卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2



### 3.2 全连接层块

- 当卷积层块的输出传⼊全连接层块时，全连接 层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输⼊形状将变成⼆维，其中第⼀维是小批量中的样本，第⼆维是每个样本变平后的向量表⽰，且向量⻓度为通道、⾼和宽的乘积
- 全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数



![20150903212346407](https://i.loli.net/2021/11/28/NH5zDmge7RiwaGA.png)





# 4 AlexNet（相对于LeNet较深）

- 相对于较小的LeNet，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层
- AlexNet第⼀层中的卷积窗口形状是11 × 11，步幅为4。因为ImageNet中绝⼤多数图像的⾼和宽均 ⽐MNIST图像的⾼和宽⼤10倍以上，ImageNet图像的物体占⽤更多的像素，所以需要更⼤的卷积窗口来捕获物体。第⼆层中的卷积窗口形状减小到5 × 5，之后全采⽤3 × 3
- 第⼀、第⼆和第五个卷积层之后都使⽤了窗口形状为3 × 3、步幅为2的最⼤池化层
- AlexNet使⽤的卷积通道数也⼤于LeNet中的卷积通道数数⼗倍，5层的输出通道数分别为96，256，384，384，256
- 紧接着最后⼀个卷积层的是两个输出个数为4096的全连接层
- AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数
- AlexNet使用了Dropout和图像增广
- AlexNet相比于LeNet有更小的学习率
- **AlexNet的一个很大的意义就是在图像识别上AI超越人工，并且首次使用了GPU加速**

![v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b](https://pic2.zhimg.com/v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b.jpg)





# 5 VGG（使用重复元素）

### 5.1 VGG块

- 连续使⽤**数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层**。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。可以指定卷积层的数量num_convs和输出通道数num_channels



### 5.2 VGG网络

- 我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积层和3个全连接层，所以经常被称为VGG-11
- 之后的3个全连接层神经元个数分别为：4096，4096，10（最后一层为类别个数）
- 使用VGG后，每次输⼊的⾼和宽将减半，直到最终⾼和宽变成7后传⼊全连接层。与此同时，输出通道数每次翻倍，直到变成512。VGG这种⾼和宽减半以及通道翻倍的设计使多数卷积层都有相同的模型参数尺⼨和计算复杂度



- **VGG的一个很大的意义就是VGG用数个小卷积核代替大卷积核，使参数量减少**

![20180205192403250](https://i.loli.net/2021/11/28/tTvmMekgd81PJX6.png)





# 6 NiN（网络中的网络）

- 的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深
- 但是NiN有所不同，由于1$\times$1卷积层可以替代全连接层，所以NiN中就进行了替换：
- **NiN之所以被称为网络中的网络是因为：NiN串联多个由卷积层和“准全连接层”（1X1的网络）的小网络来构成一个深层网络**

![image-20211125152432130](https://i.loli.net/2021/11/25/NjfabGRcVe8ysZk.png)

### 6.1 NiN块

- NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 × 1卷积层串联而成。其中**第⼀个卷积层的超参数可以⾃⾏设置，而第⼆和第三个卷积层的超参数⼀般是固定的**
- 三个卷积层的通道数是相同的



### 6.2 NiN模型

- NiN使⽤卷积窗口形状分别为11 × 11、5 × 5和3 × 3的卷积层，相应的输出通道数也与AlexNet中的⼀致。每个NiN块后接⼀ 个步幅为2、窗口形状为3 × 3的最⼤池化层
- 除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤**全局平均池化层**对每个通道中所有元素求平均并直接⽤于分类。**这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合，然而，该设计有时会造成获得有效模型的训练时间的增加**
- NiN的学习率一般比AlexNet和VGG大

![image-20211128200750143](https://i.loli.net/2021/11/28/2eiOxQsw4XoBFN9.png)





# 7 GoogLeNet（含并行连接的网络）

### 7.1 Inception块

![image-20211125155525157](https://i.loli.net/2021/11/25/Q4sfSlhjntDaXFM.png)

- Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 × 3和5 × 5的卷积层来**抽取不同空间尺⼨下的信息**

- 其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，**以降低模型复杂度**。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数

- 4条线路都使⽤了合适的填充来使**输⼊与输出的⾼和宽⼀致**。最后我们将每条线路的输出**在通道维上连结**，并输⼊接下来的层中去



### 7.2 GoogLeNet模型

- 在主体卷积部分中使⽤5个模块（block），**每个模块之间使⽤步幅为2的3× 3最⼤池化层来减小输出⾼宽**
- 第⼀模块使⽤⼀个64通道的7 × 7卷积层，步幅为2



- 第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层，然后是将通道增⼤3倍（变为192）的3 × 3卷积层



- 第三模块串联2个完整的Inception块。第⼀个Inception块的输出通道数为64+128+32+32 = 256（4个加数对应4条线路的通道数），其中第⼆、第三条线路由于有两个卷积层，所以两条线路的第一个$$1 \times 1$$卷积层先将输出通道减少为96和16，再接上第⼆层卷积层。

  第⼆个Inception块 输出通道数增⾄128 + 192 + 96 + 64 = 480。其中第⼆、第三条线路$$1 \times 1$$卷积层的输出通道分别为128和32



- 第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + (96,208) + (16,48) + 64 = 512、 160+(112,224)+(24,64)+64 = 512、128+(128,256)+(24,64)+64 = 512、112+(144,288)+(32,64)+64 = 528和 256+(160,320)+(32,128)+128 = 832

  其中括号里的第一个数字为二三条通道的第一个卷积核输出通道数



- 第五模块有输出通道数为256 + (160,320) + (32,128) + 128 = 832和384 + (192,384) + (48,128) + 128 = 1024的两个Inception块，**然后再接上一个全局平均池化层**



- **五个模块相连之后最后再接上一个全连接层，神经元个数为类别个数**



![142051802f8de8513fe61601277f03c8](https://i.loli.net/2021/11/28/rdpGSFRzuKZeTs2.jpg)





# 8 批量归一化

- 我们一般在前向传播开始之前会对数据进行归一化，**使不同特征之间具有可比性，并且更快收敛**
- 通常来说，数据标准化预处理对于浅层模型就⾜够有效了，**但对深层神经网络来说，即使输⼊数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。**这种计算数值的不稳定性通常令我们难以训练出有效的深度模型
- 而批量归一化则是对每一层的输出都做一次归一化，**使均值永远为0，方差永远为1**



### 8.1 批量归一化层

- **通常，我们将批量归⼀化层置于全连接层中的仿射变换和激活函数之间**
- 首先要对小批量求均值和方差：

$$
\begin{array}{c}
\boldsymbol{\mu}_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} \boldsymbol{x}^{(i)} \\
\boldsymbol{\sigma}_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}\right)^{2}
\end{array}
$$

得到的均值和方差是两个向量，维度为特征个数

- 然后：

$$
\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}}{\sqrt{\boldsymbol{\sigma}_{\mathcal{B}}^{2}+\epsilon}}
$$

- $\epsilon$ 为一个很小的数，一般取$10^{-8}$ ，是为了保证分母大于0
- **但是我们不确定是否一定是所有层都进行批量归一化才是最好的情况，所以我们要给予一个能变回归一化前的值的可能性，所以引入拉伸参数（scale）$\gamma$ 和  偏移参数（shift）$\beta$**

$$
\boldsymbol{y}^{(i)} \leftarrow \gamma \odot \hat{\boldsymbol{x}}^{(i)}+\boldsymbol{\beta}
$$

将$\gamma$ 和 $\beta$ 作为两个可变化参数，然后通过学习来决定拉伸多少和偏移多少



### 8.2 对卷积层做批量归一化

- 批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前
- 如果卷积计算输出多个通道，我们需要**对这些通道的输出分别做批量归⼀化，且每个通道都拥有独立的拉伸和偏移参数**

- 设小批量中有m个样本。在单个通道上，假设卷积计算输出的⾼和宽分别为p和q。我们需要对该通道中m × p × q个元素同时做批量归⼀化



### 8.3 预测时的批量归一化

- **批量归一化在训练模式和预测模式的计算结果是不⼀样的**。确定均值和方差时，单个样本的输出不应取决于批量归⼀化所需要的随机小批量中的均值和⽅差。⼀种常⽤的⽅法是**通过移动平均（指数加权平均）估算整个训练数据集的样本均值和方差**，并在预测时使⽤它们得到确定的输出





# 9 残差网络（ResNet）

### 9.1 残差块

![image-20211125221657826](https://i.loli.net/2021/11/25/zADLUxHm7wS8Z9Q.png)

- 左图是一般形式的映射，右图为残差映射，是将输入x加权->激活->再加权后，再和原输入x相加，再送入激活函数
- 这样的结构中，输⼊可通过跨层的数据线路更快地向前传播
- 残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接⼀个批量归⼀化层和ReLU激活函数。然后我们**将输⼊跳过这2个卷积运算后直接加在最后的ReLU激活函数前**
- 这样的设计要求2个卷积层的输出与输⼊形状⼀样，从而可以相加。如果想改变通道数，就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算

![](https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg)



### 9.2 ResNet模型

- ResNet的前两层跟之前介绍的GoogLeNet中的⼀样：在输出通道数为64、步幅为2的7 × 7卷积层后接步幅为2的3 × 3的最⼤池化层。不同之处在于**ResNet每个卷积层（对应上图每一次加权运算）后增加的批量归⼀化层**



- **后接4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块**
- 第⼀个模块的通道数同输⼊通道数⼀致。由于之前已经使⽤了步幅为2的最⼤池化层，所以⽆须减小⾼和宽。**之后的每个模块在第⼀个残差块⾥将上⼀个模块的通道数翻倍，并将高和宽减半**



- 接着我们为ResNet加⼊所有残差块。这⾥每个模块的残差块个数可以自行定义



- 最后，与GoogLeNet⼀样，加⼊全局平均池化层后接上全连接层输出

![image-20211125223902681](https://i.loli.net/2021/11/25/9aypGjvX1ESYtrM.png)

其中10为类别个数

![](https://pic3.zhimg.com/80/v2-862e1c2dcb24f10d264544190ad38142_720w.jpg)



### 9.3 ResNet的作用

- 在深度神经网络中，如果我们进行反向传播，那么由链式求导法则可知，我们会**涉及到非常多参数和导数的连乘**，这时误差很容易产生消失或膨胀，通过实验也可以发现，层数更深的神经网络结果反而没有浅层的好

![image-20211125235804507](https://i.loli.net/2021/11/26/YecToJs5xNf6H24.png)

这种结果很大程度归结于深度神经网络的**梯度消失问题**

- 而ResNet的提出就是为了解决梯度消失的问题，**既然离输入近的神经网络层较难训练，那么我们可以将他短接到更接近输出的层**，并且这种方法并不会对结果产生影响，假设输入为X，我们在一般网络中想要拟合f(X)，放到残差网络中就只需要拟合f(X) - X 即可
- **残差网络的设计思想不仅仅局限于卷积网络，实际上很多其他的网络也用到了残差，也取得了很不错的效果，都是为了解决深度神经网络的梯度消失问题**





# 10 稠密连接网络（DenseNet） 

![image-20211125224307704](https://i.loli.net/2021/11/25/NTajCiqAydPvoVt.png)

- DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是**在通道维上连结**
- DenseNet的主要构建模块是**稠密块（dense block）**和**过渡层（transition layer）**。前者定义了输⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤



### 10.1 稠密块

- 我们将批量归⼀化、激活和卷积组合到一起形成一种块：

![image-20211125225431518](https://i.loli.net/2021/11/25/hfHF7BLySM1swZG.png)

- **稠密块由多个conv_block组成，每块使⽤相同的输出通道数。但在前向计算时，我们将每块的输⼊和输出在通道维上连结**



### 10.2 过渡层

- 由于每个稠密块都会带来通道数的增加，使⽤过多则会带来过于复杂的模型。过渡层⽤来控制模型复杂度。它**通过1 × 1卷积层来减小通道数，并使⽤步幅为2的平均池化层减半高和宽**，并且也要进行激活和BN运算



### 10.3 DenseNet模型

- DenseNet⾸先使⽤同ResNet⼀样的单卷积层和最⼤池化层

![image-20211125230039527](https://i.loli.net/2021/11/25/DqcGv6FtMioNez1.png)

- 接着使用4个稠密块，同ResNet⼀样，我们可以设置每个稠密块使⽤多少个卷积层



- 在稠密块之间我们使用过渡层来减半高宽，并减半通道数

  注意最后一层是不使用过渡层进行减半的

- 最后再和ResNet一样，接上全局平均池化层和全连接层来输出

![](https://pic2.zhimg.com/80/v2-c81da515c8fa9796601fde82e4d36f61_720w.jpg)
