---
title: KNN总结
math: true
date: 2022-3-25
---



# 1 基本思想

- KNN模型是一种非参、惰性的模型，即并不需要显式的训练过程，也不需要参数，可以基于训练集直接进行预测
- 给定训练集$$D = \{(x_1, y_1), ... (x_N, y_N)\}$$和要预测的输入$$x$$，在训练集中找到和$$x$$最近的k个样本，然后通过投票的方式来预测$$x$$的类别

$$
y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right), \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, K
$$

其中$$N_k$$为和$$x$$最近的k个样本

- 样本之间的距离度量常用**欧氏距离**，但也可以为更一般的$$L_p,\quad p=1,2,...,\infty$$

- 对于k值的选择具有很大的影响，在应用中，**k值一般取一个比较小的值**，通常用**交叉验证法**选取最优值





# 2 kd树

- 如果要把输入和每个样本都比较，复杂度是和训练及大小成正比的，所以采用kd树来降低复杂度



### 2.1 kd树构造

- 简单来说，就是**每次选取特征空间的一个维度，然后基于这个维度进行划分（一般选择当前样本在这个维度上的中位数进行划分）**，算法流程如下：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120149491.png" alt="image-20221103120149491" style="zoom: 50%;" />

- 举个栗子，给定特征空间$$T=\left\{(2,3)^{\mathrm{T}},(5,4)^{\mathrm{T}},(9,6)^{\mathrm{T}},(4,7)^{\mathrm{T}},(8,1)^{\mathrm{T}},(7,2)^{\mathrm{T}}\right\}$$，则会产生以下的kd树和空间划分：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120509325.png" alt="image-20221103120509325" style="zoom:50%;" />

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120523272.png" alt="image-20221103120523272" style="zoom:50%;" />



### 2.2 kd树搜索

- 构造好kd树后，就需要对kd树的结点进行搜索得到最近的k个样本，这里只说明搜索最近的样本（k=1），其他情况可以类推

- 可以从上面的栗子中看到，**每次划分会产生一个超矩阵空间，并且叶结点对应的超矩阵空间是最小的**

- 给定要预测的输入$$x$$，先根据kd树的划分依据，找到$$x$$对应的叶结点，然后将该叶节点对应的样本$$x'$$作为候选最近样本，那么**更近的样本，一定处于以x为球心，distance(x, x')为半径的超球面内部**，那么我们依次对每个超矩阵空间进行判定，**若超矩阵空间和超球面没有相交，那么最近邻样本一定不在该超矩阵空间中，则该空间中的所有样本就都不用判断了；反之若相交，则将该子树作为根节点，递归寻找**，算法流程如下：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103155306872.png" alt="image-20221103155306872" style="zoom:50%;" />

- 使用kd树使平均时间复杂度从$$O(N)$$降为了$$O(\log N)$$，但是需要很大的存储空间





# 3 一些结论

1. **KNN对回归、分类都可以用，对数据没有假设，并且对异常点不敏感，但是样本不平衡的时候准确率较低，可以通过投票时引入权值来改善**
2. **KNN每次选取最大方差的维度作为划分维度(方差越大，表示此维度上数据越分散)，这样一次划分的两组样本差别越大，搜索的时候可能会减少递归次数**

3. **KNN要提取中位数，可以在算法最开始的时候，在每个维度进行一次排序并存储下来，而不需要每次选了维度再排序，提升性能**

