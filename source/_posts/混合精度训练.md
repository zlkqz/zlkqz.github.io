---
title: 混合精度训练
math: true
date: 2023-08-01
---



# 1 浮点数精度

- 一个浮点数在存储时有3部分，**拿IEEE float32举例，总共32bit，其中包含1bit符号位，8bit指数位，23bit尾数位**

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230801193815438.png" alt="image-20230801193815438" style="zoom:50%;" />

- 以25.125举例，先转为二进制11001.001，然后移动位数后为$$1.1001001 \times 2^4$$，符号位为1表示正数，指数位为4，尾数位为1.1001001

- 所以容易得出结论：**指数位越多，表示范围越大；尾数位越多，表示精度越大（因为一些数的尾数过长，所以会对尾数位进行截断，所以尾数位越多精度损失越小）**

- 而深度学习里最常用的三种浮点精度为：FP32、FP16、BF16

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230801194506322.png" alt="image-20230801194506322" style="zoom:50%;" />

- 有三者的各位数可知：**表示范围FP32=BF16>FP16；表示精度FP32>FP16>BF16**

- **而在深度学习当中，不是很依赖于数字表示的精度（有微小误差是完全可以接受的），所以BF16是全面优于FP16的（并且两者吞吐量是相同的），但是部分硬件并不支持BF16**





# 2 混合精度训练

- 混合精度训练的是用FP16来代替FP32的计算，增大系统吞吐量和运算速度，并减少显存占用



### 2.1 训练流程

- 大致流程如下：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-3b81fa8af962c682b9c55ee48014af0e_1440w.webp" alt="img" style="zoom:67%;" />

1. 备份一份FP32的模型权重，然后将模型参数转为FP16进行前向传播，那么前向传播中的各数据，如激活值等，都是FP16
2. 得到loss后做loss-scaling，将loss乘某一个倍数进行放大
3. 然后进行反向传播，**得到FP16的梯度，然后将该梯度转为FP32，再除以刚刚所乘的倍数**
4. 使用得到的FP32梯度对备份的FP32权重进行更新，然后用新的参数进行下一次迭代



### 2.2 精度损失

- 用FP16进行运算，可表示范围减少了很多，肯定会造成模型精度的下降，作者对SSD网络在训练过程中的梯度和激活值中各精度的占比进行了统计：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230801205003340.png" alt="image-20230801205003340" style="zoom:55%;" />

- 统计发现，会有67%的值如果用FP16表示，会直接下溢变成0
- 在上文的训练流程中，发现得到loss之后会做scaling，然后在权重更新的时候又会unscaling回来。**这样其实是为了将对应的梯度放大到FP16的可表示范围之内，然后再在梯度更新的时候缩小回正确值进行更新。这样是可行的，因为训练到后面会发现参数基本都很小，所以靠前的位基本都是0，所以放大一般不会造成值的损失**



### 2.3 Tensor Core

- **在一些对精度较高的计算中（如BN、Softmax，因为涉及到一个大向量内部各个元素的相加，所以精度损失很严重），必须使用FP32**

- 而混合精度训练使用了Tensor Core，**将原本的FP32运算转为了两个FP16矩阵的乘积，再另用一个FP32矩阵来补精度，每次FP16溢出的值都会累计到这个FP32矩阵上**

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230801211604956.png" alt="image-20230801211604956" style="zoom: 50%;" />

- 在算法实现完之后再将这个FP32矩阵加到最终运算结果上，再转为FP16重新放入显存

- 这样可以同时得兼顾高运算速度和高精度



# 3 混合精度训练的缺点

- 在实际使用的时候，混合精度训练每次迭代会不断尝试不同的scaling值，如果不溢出才会使用这个scaling值。**但是有时候可能使用规定的最大的scaling值还是会溢出，所以可能训练失败**
- 其实也可以直接不用混合精度训练，直接用bf16来训练模型，也是完全可行