---
title: 激活函数的作用和比较
math: true
date: 2021-10-20
---



# 1 线性结构

如果满足：
$$
y = wx + b
$$
则可称y、x间具有线性关系

而对于神经网络，相邻两层之间的输出之间满足：
$$
X^{[l]} = w^{[l]}X^{[l - 1]} + b^{[l]}
$$
则可称其满足线性结构





# 2 激活函数的作用

- 我们先假设不用激活函数，假设神经网络有3层，每层的权重为$w^{[l]}$，偏差都为$b^{[l]}$，输入为$X$，输出为$Y$，则从头到尾的计算为：

$$
Y = w^{[3]}(w^{[2]}(w^{[1]}X + b^{[1]}) + b^{[2]}) + b^{[3]}
$$

就相当于做一次线性运算：
$$
Y = w'X + b'
$$
则从头到尾都是线性结构，这在某些情况是适用的，但是大多数时候输入和输出的关系是非线性的，**这时候我们就需要引入非线性因素，即激活函数，来使得神经网络有足够的capacity来抓取复杂的pattern**

所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。



- **能用线性拟合的情况：**

<img src="https://i.loli.net/2021/11/11/NdbYFTOkwyBj7xh.png" alt="image-20211111115632781" style="zoom:50%;" />

- **无法用线性拟合的情况：**

<img src="https://i.loli.net/2021/11/11/Zvo5O17LesqBbVF.png" alt="image-20211111115745462" style="zoom:50%;" />





# 3 常用激活函数的优缺点

### 3.1 Sigmoid函数

1. Sigmoid具有一个很好的特性就是能将输入值映射到$(0, 1)$，便于我们将值转化为概率，并且sigmoid平滑，便于求导

2. 但sigmoid还有三大缺点：

- **Gradient Vanishing：**

![image-20211111155149051](https://i.loli.net/2021/11/11/TWxNvl6CrD4kuAU.png)

**由图可以看出，在输入值较大或者较小时，其导数是趋于0，而在梯度下降时，其值就基本不会被更新**



- **函数输出并不是zero-centered**

我们以一个二维的情况举例：
$$
f(\vec{x} ; \vec{w}, b)=f\left(w_{0} x_{0}+w_{1} x_{1}+b\right)
$$
现在假设，参数 $w_0, w_1$ 的最优解 $w_0^∗, w_1^∗$ 满足条件:
$$
\left\{\begin{array}{l}
w_{0}<w_{0}^{*} \\
w_{1} \geqslant w_{1}^{*}
\end{array}\right.
$$
所以我们现在就是要让$w_0$变大，$w_1$变小，而：
$$
\frac{\partial L}{\partial w_0} = x_0 \frac{\partial L}{\partial f}, \\
\frac{\partial L}{\partial w_1} = x_1 \frac{\partial L}{\partial f}
$$
由于上一层sigmoid输出的$x_0, x_1$为正值，所以$w_0, w_1$的梯度一定同正或同负，所以我们要逼近最小值点，只能走下图红色线路：

<img src="https://i.loli.net/2021/11/11/IXqb8Brk9KOH7WG.png" alt="image-20211111160220315" style="zoom:50%;" />

而显然绿色线路才是最快的，但是由于绿色路线的$$\frac{\partial L}{\partial w_0},\frac{\partial L}{\partial w_1}$$符号相反，所以不能走，所以这会**影响梯度下降的速度**



- **幂运算相对来说比较耗时**



### 3.2 Tanh函数

- tanh一般优于sigmoid，就是因为tanh是zero-centered，但是tanh仍然没有解决sigmoid的其他两个问题



### 3.3 ReLu函数

1. 优点：

- 解决了gradient vanishing问题 (在正区间)
- 计算速度非常快，只需要判断输入是否大于0
- 收敛速度快

2. 缺点：

- ReLU的输出不是zero-centered
- **Dead ReLU Problem：**

指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。

在x<0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应

**产生原因：**

1. 参数初始化问题（比较少见）

2. learning rate太高导致在训练过程中参数更新太大



而为了防止这种情况，我们可以使用改进的ReLu函数，如Leaky ReLu：
$$
f(x) = max(0.01x, x)
$$

但是大部分时候我们还是使用ReLu，很多时候LReLu的表现和ReLu其实相差不多，只有神经元大量坏死，ReLu表现不好的时候，我们才会考虑使用LReLu，甚至更复杂的PReLu和ELU



### 3.4 GeLU函数

- 其实ReLU的非线性因素在于：**$$P=0.5$$的概率随机使一部分神经元失活**
- 而GeLU相当于把这个概率分布改为了标准正态分布：

$$
\operatorname{GELU}(\mathrm{x})=\mathrm{xP}(\mathrm{X}<=\mathrm{x})=\mathrm{x} \Phi(\mathrm{x})
$$

- GeLU的思想在于：**直观来说，因为一般更大的激活值更加重要，所以我们希望拥有更大激活值的神经元失活的概率小一些，所以采用了累积概率分布来体现这一点**

- 在具体使用中一般不使用标准正态分布，而使用其近似值：

$$
\operatorname{GELU}(\mathrm{x})=0.5 \mathrm{x}\left(1+\tanh \left[\sqrt{2 / \pi}\left(\mathrm{x}+0.044715 \mathrm{x}^{3}\right)\right]\right)
$$

- 而BERT的源码中的实现还要简便一些：

```python
def gelu(input_tensor):
	cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
	return input_tesnsor*cdf
```

