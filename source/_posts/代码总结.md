---
title: 代码总结（持续更新）
math: true
date: 2022-7-17
---

# NLTK用法

- nltk用于**英文**分词分句等应用

### 基本的预处理

- **分句：nltk.sent_tokenize(text, language="english"):**

> 输入：一个str段落
>
> 输出：一个list，每个元素是一个str句子

- **分词：nltk.word_tokenize(text, language="english", preserve_line=False):**

> 输入：一个str句子
>
> 输出：一个list，每个元素是一个str词

- **词性标注（POS_tag）：nltk.postag(tokens)**

> 输入：一个list，每个元素是一个str，一个句子分好词的结果
>
> 输入：一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[('football', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('family', 'NN')]
>
> 词性所对应的意义，大致来说**N开头对应名词NOUN，V开头对应动词VERB，J开头对应形容词ADJ，R开头对应副词ADV**
>
> 词性具体所对应意义如下：
>
> | 标记 | 含义 | 示例                            |
> | ---- | ---- | ------------------------------- |
> | CC   | 连词 | and, or,but, if, while,although |
> | CD   | 数词 | twenty-four, fourth, 1991,14:24 |
> |DT | 限定词|  the, a, some, most,every, no |
> |EX| 存在量词| there, there’s|
> |FW |外来词 |dolce, ersatz, esprit, quo,maitre|
> |IN |介词连词| on, of,at, with,by,into, under|
> |JJ |形容词 |new,good, high, special, big, local|
> |JJR| 比较级词语| bleaker braver breezier briefer brighter brisker|
> |JJS| 最高级词语| calmest cheapest choicest classiest cleanest clearest|
> |LS| 标记| A A. B B. C C. D E F First G H I J K|
> |MD| 情态动词| can cannot could couldn’t|
> |NN| 名词| year,home, costs, time, education|
> |NNS| 名词复数| undergraduates scotches|
> |NNP| 专有名词| Alison,Africa,April,Washington|
> |NNPS| 专有名词复数| Americans Americas Amharas Amityvilles|
> |PDT| 前限定词| all both half many|
> |POS| 所有格标记 ’| 's|
> |PRP| 人称代词| hers herself him himself hisself|
> |PRP$| 所有格| her his mine my our ours|
> |RB| 副词| occasionally unabatingly maddeningly|
> |RBR |副词比较级| further gloomier grander|
> |RBS| 副词最高级 |best biggest bluntest earliest|
> |RP |虚词| aboard about across along apart|
> |SYM |符号 |% & ’ ‘’ ‘’. ) )|
> |TO |词to| to|
> |UH| 感叹词| Goodbye Goody Gosh Wow|
> |VB |动词| ask assemble assess|
> |VBD| 动词过去式 |dipped pleaded swiped|
> |VBG| 动词现在分词| telegraphing stirring focusing|
> |VBN| 动词过去分词| multihulled dilapidated aerosolized|
> |VBP| 动词现在式非第三人称时态| predominate wrap resort sue|
> |VBZ| 动词现在式第三人称时态| bases reconstructs marks|
> |WDT| Wh限定词| who,which,when,what,where,how|
> |WP| WH代词| that what whatever|
> |WP$| WH代词所有格 |whose|
> |WRB| WH副词||
> 

- **词形还原Lemmatization：nltk.stem.WordNetLemmatizer().lemmatize(word, pos="n")：**

> 这是一个类的内置方法，首先需要创建WordNetLemmatizer对象（假设为wnl），则通过`wnl.lemmatize()`调用
>
> 输入：word是单个词，pos是其对应的单词词性（n对应noun，v对应verb，a对应adj，r对应adv，s对应satellite adjectives(不咋用)）
>
> 输出：word经过还原后的词，如cars还原为car



### NER

- 先介绍一下命名实体识别（Named Entity Recognition，NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。
- nltk中对NER类别的分类如下：

![img](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20191219223810610.png)

其中LOCATION和GPE有重合，GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。

- **进行NER：ne_chunk(tagged_tokens, binary=False)**

> 输入：tagged_tokens为`pos_tag()`函数的输出，一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[('football', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('family', 'NN')]
>
> 输出：同样的是一个list，每个元素变为了一个封装对象（Tree类），和输入一一对应
>
> - **对于该封装对象：**
>
> 输出的list中，有些元素是没有NER的结果的，其对应的封装对象没有label属性，**可使用`hasattr(ne_word, 'label')`函数判断是否有NER结果**
>
> 假设一个该对象命名为ne_word，调用`ne_word.leaves()`可返回一个list，每个元素为一个tuple，tuple[0]为token，tuple[1]为该token的pos_tag。对于NER，list中只有一个tuple，返回结果示例：[('FIFA', 'NNP')]
>
> 调用`ne_word.label()`函数可返回该token对应的NER结果，一定要先使用`hasattr()`函数才能使用`label()`函数



### 计算BLEU

- [BLEU的定义](https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#5-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91)

- **nltk.translate.bleu_score.sentence_bleu(references, hypothesis, ..., smoothing_function=None, ...)**

> references：参照序列，label。**type=list(list(str))，其中的每个str为词或字**
>
> hypothesis：候选序列，也就是预测出的序列。**type=list(str)，其中的每个str为词或字**
>
> smoothing_function，是论文中使用到的平滑技巧，一般输入`nltk.translate.bleu_score.SmoothingFunction().methodi()`，最后的i为0~7

- 另外还可以使用`corpus_bleu()`计算多个句子的BLEU；用`modified_precision()`计算修正的n-gram精确度



# Pyltp用法

- Pyltp主要用于中文的预处理等，包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注
- 使用Pyltp时注意其编码使用的都是utf-8，而Windows终端使用GBK编码，如果出现乱码可以将输出重定向到文件，然后用utf-8编码查看
- **分句：pyltp.SentenceSplitter()**

> 使用时先创建实例：`sp = SentenceSplitter()`，再进行分句：`sents = sp.split(doc)`

- **分词：pyltp.Segmentor()**

> Segmentor加载模型可以用`load()`也可以用`load_with_lexicon()`，后者还要加一个用户词典的参数

- **词性标注：pyltp.Postagger()**

> 直接举个栗子吧：
>
> ```python
> sent = "据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。"
> # 加载模型
> segmentor = Segmentor()
> segmentor.load_with_lexicon(cws_model_path, lexicon_path)
> postagger = Postagger()
> postagger.load(pos_model_path)
> # 分词和词性标注
> words = segmentor.segment(sent)
> postags = postagger.postag(words)
> print(list(postags))
> # 释放模型
> segmentor.release()
> postagger.release()
> ```
>
> 注意：进行这些处理后返回的都是VectorOfString类，是一个可迭代类，也可用list()转换为列表
>
> 词性标注的词性如下：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724143414199.png" alt="image-20220724143414199" style="zoom: 50%;" />
>
> 词性标注也可以添加用户词典

- **NER识别：pyltp.NamedEntityRecognizer()**

> ```python
> from pyltp import NamedEntityRecognizer
> recognizer = NamedEntityRecognizer()
> recognizer.load(ner_model_path)
> ner_results = recognizer.recognize(words, postags)
> ```
>
> LTP 采用 BIESO 标注体系。**B 表示实体开始词，I表示实体中间词，E表示实体结束词，S表示单独成实体，O表示不构成命名实体。**
>
> LTP 提供的命名实体类型为：**人名（Nh）、地名（Ns）、机构名（Ni）**。
>
> B、I、E、S位置标签和实体类型标签之间用一个横线 `-` 相连；O标签后没有类型标签。

- **依存句法分析：pyltp.Parser()**

> 依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系
>
> 首先也是创建实例和加载模型，然后`parse_results = parser.parse(words, postags)`，得到的结果是一个可迭代对象，其中的每一个元素也是个自定义类（假设parse_results[i]为result），调用`result.head`和`result.relation`可分别获得words[i]对应的关系词和关系，其中head是返回关系词所对应的索引+1，如果为0则为"Root"，表示无对应关系词
>
> 举个栗子：
>
> ```python
> relations = [result.relation for result in parse_results]
> heads = [words[result.head - 1] if result.head else "Root" for result in parse_results]
> for i in range(len(words)):
>  print(f"{relations[i]} : ({words[i]}, {heads[i]})")
> ```
>
> 输出结果：
> <div align=center><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211345194.png" alt="image-20220724211345194" style="zoom: 80%;" />
>
>
> 依存句法关系如下：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724153110907.png" alt="image-20220724153110907" style="zoom:50%;" />

- **语义角色标注：pyltp.SementicRoleLabeller()**

> 语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等
> 进行语义角色标注时首先同样是创建实例，再加载模型，然后调用方法：`roles = labeller.label(words, postags, parse_results)`。结果得到一个可迭代对象，**其中的每一个元素也是一个自定义类（假设每个元素为role），则`role.index`为谓语对应的索引，`role.arguments`又是一个可迭代对象，每个元素对应一个和该谓语对应的语义角色，也是一个自定义类（假设为argument），**那么可以通过`argument.name`获取角色和谓语的关系，`argument.range.start`和`argument.range.end`对应该角色的开始和结束索引（**end要算上的**）
>
> 给个示例：
>
> ```python
> roles = labeller.label(words, postags, parse_results)
> for role in roles:
>  arguments = role.arguments
>  index = role.index
>  print(f"谓语: {words[index]} (索引: {index})")
>  for argument in arguments:
>      start, end = argument.range.start, argument.range.end
>      obj = ""
>      for word in words[start : end+1]:
>          obj += word
>      print(f"{argument.name}: {obj} (索引: {start}:{end})")
>  print("\n")
> ```
>
> 输出结果：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211302209.png" alt="image-20220724211302209" style="zoom:67%;" />
>
> 语义角色如下：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211544134.png" alt="image-20220724211544134" style="zoom:60%;" />





# rouge用法

- rouge和bleu一样也是一种对序列质量的评估标准，但是相比于bleu更关注召回率而非精准率（通过计算F1_score时改变$$\beta$$参数），rouge-n计算公式如下：

$$
Rough-N=\frac{\sum_{S \in\{\text { ReferemceSummaries }\}} \sum_{\text {gram }_{n} \in S} \text { Count }_{\text {match }}\left(\text { gram }_{n}\right)}{\sum_{S \in\{\text { ReferenceSummaries }\}} \text { gram }_{n} \in S}
$$

- rouge-1和rouge-2都可通过上述公式计算出来，和bleu中的P1和P2计算公式很像。可以在下面看到，每个rough-N都会输出p和r，其实就是精准率和召回率，两者也就是分母不一样（一个分母是refs的n-gram个数，一个分母是hyps的n-gram个数）
- 而rouge-l是rouge-N的改进：

$$
\begin{array}{l}
R_{l c s}=\frac{L C S(X, Y)}{m} \\
P_{l c s}=\frac{L C S(X, Y)}{n} \\
F_{l c s}=\frac{\left(1+\beta^{2}\right) R_{l c s} P_{l c s}}{R_{l c s}+\beta^{2} P_{l c s}}
\end{array}
$$

其中X、Y为hyps和refs，m、n为他们的长度，LCS(X, Y)为X、Y的最长共同序列

- **rouge.Rouge.get_scores(hyps, refs, avg=False, ignore_empty=False)**

> **注意：输入的hyps和refs两个字符串，不管是中文英文，都需要每个字或词使用空格间隔**
>
> 输出是list(dict(dict))

- 直接给栗子吧：

```python
from rouge import Rouge 

hypothesis = "the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news"

reference = "this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news"

rouge = Rouge()
scores = rouge.get_scores(hypothesis, reference)
```

**output：**

```
[
  {
    "rouge-1": {
      "f": 0.4786324739396596,
      "p": 0.6363636363636364,
      "r": 0.3835616438356164
    },
    "rouge-2": {
      "f": 0.2608695605353498,
      "p": 0.3488372093023256,
      "r": 0.20833333333333334
    },
    "rouge-l": {
      "f": 0.44705881864636676,
      "p": 0.5277777777777778,
      "r": 0.3877551020408163
    }
  }
]
```

其中p、r、f分别为精准率、召回率、F1_score





# zhconv用法

- 该库用于中文简繁体转换，但是也可以用**OpenCC库，精准度更高、覆盖率更高、速度更快**

- **逐字转换：zhconv.convert(s, locale, update=None)**

- **基于MediaWiki的转换：zhconv.ocnvert_for_mw(s, locale, update=None)**

> 两个函数用法相同
>
> 示例：
>
> ```python
> sent2 = "計算機軟體"
> print(convert(sent2, "zh-hans"))
> ```
>
> locale可为以下值：
>
> `zh-cn` 大陆简体、`zh-tw` 台灣正體、`zh-hk` 香港繁體、` zh-sg` 马新简体、`zh-hans` 简体、`zh-hant` 繁體





# gensim用法

- gensim主要用于创建语料库，和计算各种特征（如相似度、tf-idf）等任务

### 创建语料库和计算相似度

- **创建语料库类：gensim.corpora.Dictionary(texts)**

> 输入：整个语料库，分好句和分好词的结果，一个list，每个元素又是一个list，里面的每个子元素是一个str词
>
> 输出：一个Dictionary对象

- **将句子转化为词袋形式：Dictionary.doc2bow(text)**

> 输入：分好词的一个句子，一个list，每个元素是一个str词
>
> 输入：一个list，每个元素是一个二元tuple，tuple[0]是句子中存在的词的索引，tuple[1]是其在句子中出现的次数（出现0次的不计入）
>
> 该类的用法和python自带的字典对象基本相同，values、key、item之类的

- **获取索引字典：Dictionary.token2id**

> 返回一个{token : id}的字典

- **创建相似度类：gensim.similarities.Similarity(output_prefix, corpus, num_features)**

> out_prefix是存储这个对象文件的名称
>
> corpus是整个语料库，但是必须先转化为词袋模型
>
> num_fuatures是整个词典的数量，一般使用len(dictionary)表示
>
> 对于得到的对象（取名为similarity），具体的用法为：similarity[test]，其中test为要比较的对象，可以是单个句子，也可以是整个语料库

**举个栗子：**

```python
from nltk import word_tokenize
from gensim.corpora import Dictionary
from gensim.similarities import Similarity

sent1 = "I love sky, I love sea."
sent2 = "I like running, I love reading."
sents = [sent1, sent2]
# 分词
texts = [word_tokenize(sent) for sent in sents]

# 创建字典对象
dictionary = Dictionary(texts)
# 获得词袋模型表示的词料库
corpus = [dictionary.doc2bow(text) for text in texts]
print(f"The corpus is : {corpus}")
# 创建相似度对象
similarity = Similarity("Similarity-excise1", corpus, num_features=len(dictionary))
print(f"Created class : {similarity}")
# 计算余弦相似度
test_corpus = dictionary.doc2bow(word_tokenize(sent1))
print(similarity[test_corpus])
```

输出：

![image-20220706160148320](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706160148320.png)



### 计算TF-IDF

- **首先介绍一下tf-idf：**

词频（term frequency，tf）指的是某一个给定的词语在该文件中对应的频率

逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量，可由文件总数除以包含该词的文件的总数求得

具体的计算公式如下：
$$
\mathrm{tf}_{\mathrm{i}, \mathrm{j}}=\frac{n_{i, j}}{\sum_{k} n_{k, j}} \\
\operatorname{idf}_{\mathrm{i}}=\lg \frac{|D|}{\left|\left\{j: t_{i} \in d_{j}\right\}\right|}\\
\operatorname{tfidf}_{i, j}=\mathrm{tf}_{\mathrm{i}, \mathrm{j}} \times \mathrm{idf}_{\mathrm{i}}
$$
其中$$tf_{i, j}$$为$$word_i$$在$$doc_j$$中出现的词频，$$n_{i, j}$$为$$word_i$$在$$doc_j$$中出现的个数

$$idf_i$$为$$word_i$$的逆向文件频率，$$|D|$$为文档总数，$$\left|\left\{j: t_{i} \in d_{j}\right\}\right|$$为包含$$word_i$$的文档个数

对于idf，底数可用2、e、10，并且分母可以+1，避免除以0

- **创建tfidf模型类：gensim.models.TfidfModel(corpus)**

> 输入为整个语料库，必须先转化为词袋模型
>
> 在具体使用时，和Similarity类似，tfidf_model[test]，其中test可以是单个句子也可以是整个语料库，返回一个列表，列表中每个元素为一个tuple，tuple[0]为词索引，tuple[1]为对应的tfidf值
>
> **gensim算出来的tf-idf是经过了规范化的，每个句子的tfidf值，是一个单位向量**

**举个栗子：**

```python
# texts的构建是每个段落分词，有三个段落，则len(texts)=3
texts = [text1, text2, text3]
texts = [get_tokens(text) for text in texts]
dictionary = Dictionary(texts)
id2token_dict = {v : k for k, v in dictionary.token2id.items()}
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf_model = TfidfModel(corpus)
result = tfidf_model[corpus]
for i in result:
    print(i)
```

输出：

![image-20220706210417422](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706210417422.png)





# Pandas用法

- **创建DataFrame对象：pd.DataFrame(data, columns, index)**

> 输入：data是一个list, 每个对象是一个tuple或list，tuple的长度和columns的个数对应。data也可以是numpy形式
>
> index是一个list，是行索引值，可以用于自行设置行索引**（索引没有规定一定是int）**
>
> 示例：pd.DataFrame([(1, 2), (3, 4), (5, 6)], columns=["name", "NER result"])，输出为：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220711120934316.png" alt="image-20220711120934316" style="zoom: 80%;" />

- **获取DataFrame特征：DataFrame.describe()**
- **获取DataFrame的某列：DataFrame["column_name"]**

>  这样可获取到column列，结果是一个Series类（**只有一个列则是Series，多个列则是DataFrame**）
>
>  如果不想出现重复结果，可调用`unique()`函数，示例：data["ner_result"].unique()，返回的结果是一个ndarray，也可使用`list()`将其转换为列表
>
>  如果想把这个Series类转换为ndarray，则可使用Series.values，这是Series类其中的一个属性，不是方法，类型是一个ndarrary，也可使用`list()`转换为列表

- **获取DataFrame列名：DataFrame.columns**

- **DataFrame根据某列，进行分组：DataFrame.groupby("column_name")**

> 将DataFrame通过column_name列进行分组
>
> 结果是一个可迭代的DataFrameGroupBy类，该类的用法和DataFrame几乎一样，比如可使用`DataFrameGroupBy["column_name"]`将其转换为SeriesGroupby类
>
> 也可转化为list，转换后，其中每一个元素为一个tuple，tuple[0]为column_name列对应的值，tuple[1]为原DataFrame的一部分，根据column_name的值进行截断。如果是将SeriesGroupby类转换为list，则tuple[1]变为一个Series类
>
> 举个例子：
> input_data为一个DataFrame，值如下：
>
> <div align=center><img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123006189.png" alt="image-20220716123006189" style="zoom:67%;" />
>
> 现在将input_data根据sent_order列进行分组：input_data.groupby("sent_order")，结果如下：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123836380.png" alt="image-20220716123836380" style="zoom: 67%;" />
>
> 只展示了前三个元组，每个元组，tuple[0]为sent_order的值，tuple[1]为对应的DataFrame

- **应用自定义方法：DaraFrame.apply(func, axis=0, ...)**

> Series、DataFrame和GroupbyDataFrame等均可使用
>
> func是一个自定义函数
>
> axis决定func的输入是什么，axis=0则输入为行，axis=1则输入为列
>
> Series无需指定axis，DataFrame可调节axis更换操作的维度，**DaraFrameGroupby也无需指定，func的输入为分组后的每个DataFrame，最后再将每个分组返回的结果总和起来**

- **DataFrame的count()方法：**

> `count()`有很多种用法，结果是可以对Series、DataFrame和GroupbyDataFrame等使用，结果是返回一个Series或DataFrame
>
> 下面举几个示例，其中使用的df如下：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725142611139.png" alt="image-20220725142611139" style="zoom: 60%;" />
>
> ```python
> df.count()    # DataFrame执行count()，对每一列分别执行count，结果返回一个Series
> df.data["length"].count()    # Series执行count()，直接返回length列的长度，类型为int
> df.groupy("length").count()   # 对DataFrameGroupby类执行，返回一个DataFrame，index为length的各指，colums为出length的其他列执行count的结果
> df.groupy("length")["evaluation"].count()  # 对SeriesGroupby类执行，则只返回evaluation列的count结果，类型为Series
> ```
>
> 上述四句语句的执行结果：
>
> - 第一句：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143928089.png" alt="image-20220725143928089" style="zoom:67%;" />
>
> - 第二句：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143939723.png" alt="image-20220725143939723" style="zoom:67%;" />
>
> - 第三句：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143953768.png" alt="image-20220725143953768" style="zoom:50%;" />
>
> - 第四句：
>
> <img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725144005108.png" alt="image-20220725144005108" style="zoom:50%;" />

- **读取/保存csv文件：**

```python
df.to_csv(file_name, index=False)    # 要加index=False
pd.read_csv(file_name)
```



### 处理缺失值

- **判断空值：DataFrame.isnull()或DataFrame.notnull()**

> DataFrame和Series都可用，结果也是返回一个DataFrame或Series，和输入一一对应，其中每个值为布尔值

- **删除缺失值：DataFrame.dropna(axis=0, inplace=False, how=None)**

> axis：删除行还是列，{0 or ‘index’, 1 or ‘columns’}
>
> how : 如果等于any则任何值为空都删除，如果等于all则所有值都为空才删除

- **填充缺失值：DataFrame.fillna(value, method, axis, ...)**

> value：用于填充的值，可以是单个值，或者字典（用于不同列填充不同值的情况，key是列名，value是值）
> method : 等于"ffill"使用前一个；不为空的值填充forword fill；等于"bfill"使用后一个不为空的值填充backword fill
> axis : 按行还是列填充，{0 or ‘index’, 1 or ‘columns’}



### Padas中DataFrame的增删查改

- **增加列：**

```python
df = DataFrame(...)    # 有三行数据
cities = ["成都", "上海", "北京"]
# 三种插入方式
df.insert(0, "city", citys)    # 参数分别为：插入列的位置、列名、插入内容
df["city"] = cities
df.loc[:, "city"] = cities
```

- **增加行：**

```python
df = DataFrame(...)  # 有两列 
df.loc[3] = ["1", "2"]       # 如果已经存在index=3的行，则修改值；反之直接添加该行
df = df.append(df_insert)    # 合成两个DataFrame，列要相同才行
```

- **loc[]和iloc[]的使用：**

> 两者功能是相同的，区别在于loc中对于列，只能使用列名（str类型）进行搜索，而iloc对于列只能使用整数索引进行搜索，DataFrame的查和改基本都是基于两者的

```python
df["column_name"]或df.loc[:, "column_name"]   # 查找column_name列
df.loc["index_name"]    # 查找index_name行
df.loc["index_name", "column_name"]     # 查找(index_name, column_name)处的值
# 还可以使用list或者切片来代替，一次操作多行或多列
df.loc[["index_name1", "index_name2"], ["column_name1", "column_name2"]]  # 同时操作index_name1&2行和column_name1&2列
df.loc[0:3, "column_name"]    # 进行切片，对0、1、2、3行操作
```

> `iloc[]`是使用整数列索引，比如：`df.iloc[:3, 2:6]`
>
> 注意`loc[]`中的切片，是包含了end所指的索引（和list的索引稍有不同），`iloc[]`是不包含end的索引的

- **删除：**

```python
df.drop(0, axis=0)   # 删除第0行
# 删除列的三种方法
df.drop("column_name", axis=1)
del df["column_name"]
ndf = df.pop("column_name")
# 使用drop的时候，第一个参数也可以是list，一次操作多行或多列
df.drop([0, 1, 2], axis=0)
df.drop(["column_name1", "column_name2"], axis=1)
```





# Pickle用法

- **储存pk文件：pickle.dump(obj, file)**

- **加载pk文件：pickle.load(file)**

> 可以存储和加载对象，其中`dump()`中的obj是要存的对象，file是`open()`函数返回的文件描述符
>
> `load()`返回值为所存储的对象
>
> 储存的文件后缀为.pk
>
> 举个栗子：
>
> ```python
> a = [1, 2]
> file_name = "test.pk"
> # 存
> with open(file_name, "wb") as f:
>     pickle.dump(a, f)
> # 取
> with open(file_name, "rb") as f:
>     b = pickle.load(f)
>     
> # a和b是一样的
> ```





# Json用法

- **json可进行Python对象和json格式之间的互换**

- **Python转json：json.dumps(obj, ensure_ascii=True, ..., indent=None, ...)**
- **json转Python：json.loads(s, ...)**

> obj为Python对象，s为json对象
>
> ensure_ascii：如果为False，则返回值可以包含非 ASCII
>
> indent：为json中每个元素的间隔数，如果为0或None，则每个元素之间紧凑排版
>
> **注意：**
>
> **1. 以上两个函数只是进行对象的转换，而不会进行保存，所以是一般配合`open()`、`f.write()`、`f.read()`使用，给两个示例：**
>
> ```python
> # 存储json对象
> with open(file_name, "w", encoding="utf-8") as f:
>  f.write(json.dumps(data, ensure_ascii=False, indent=2))
> # 读取json对象
> with open(file_name, 'r', encoding='utf-8') as f:
>  data = json.loads(f.read())
> ```
>





# Yaml

- **读取yaml文件：**`config = yaml.safe_load(open("config.yaml", "r", encoding="utf-8").read())`
- 在.yaml文件中，可用缩进表示层级关系，并且写浮点数不能用python的科学技术法的写法：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_25.png" alt="IMG_25" style="zoom:50%;" />





# Argparse

- **基本流程：**

```python
parser = argparse.ArgumentParser()
parser.add_argument("--batch_size", type=int, default=1) 
args = parser.parse_args()
```

- 其中`--`表示可选参数，在运行命令指定`--batch_size=xxx`即可





# Matplotlib用法

- **折线图：plt.plot(x, y)**
- **直方图：plt.bar(x, y)**
- **散点图：plt.scatter(x, y)**

> 上述函数还可以指定颜色、大小等参数：
>
> - color：颜色
>
> > 可为："b"：蓝色、"g"：绿色、"r"：红色、"c"：青色、"m"：品红、"y"：黄色、"k"：黑色、"w"：白色
> >
> > 也只直接用全名，如"blue"
>
> - s：点的大小
> - linewidth：线宽
> - linestyle：线样式，如设为"dashed"将线设为虚线
>
> - label：名字， 若要打印label，则使用`plt.legend()`

- **设置标题：plt.title()**

> 同样可以使用fontsize指定字体大小

- **设置x、y轴标题：plt.xlabel()和plt.ylabel()**
- **保存图片：plt.savefig()**
- **画完图之后记得关闭：plt.close()**
- **画一条水平的线：plt.hlines(y, xmin, xmax)**
- **画一条垂直的线：plt.vlines(x, ymin, ymax)**
- **在指定座标处加注释：plt.text(x, y, s)**





# Numpy用法

- **获取最大值的位置（索引）np.argmax(a, axis=None, ...)：**

> axis：如果为None, 则代表把整个数组当作一维的，然后返回最大的索引值。如果指定axis，则只在该维度搜索最大值，示例：
>
> ```python
> array([[10, 11, 12],
>        [13, 14, 15]])
> >>> np.argmax(a)
> 5
> >>> np.argmax(a, axis=0)
> array([1, 1, 1])
> >>> np.argmax(a, axis=1)
> array([2, 2])
> ```
>
> axis指定为哪个维度，则stack哪个维度

- **减少维度：np.squeeze(a, axis=None)**

> axis如果为None，则丢弃所有大小为1的维度



# Tensorflow用法

### 禁用GPU

```python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
```



### 数据预处理

- **填充序列：tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)**

> 输入：
> sequences是要填充的序列
>
> maxlen是要填充的最大长度，如果为None，则设为所有序列中最长的那个的长度
>
> padding和truncating可以设为"pre"或者"post"，选择是在序列前面还是后面填充/截断
>
> value为填充的值
>
> 返回值是一个ndarray

- **划分训练、测试集：sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)**

> arrays是要划分的数据，可以使用list、numpy array、matric、dataframe**（不能用Tensor）**，示例：
>
> `x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)`
>
> train_size和test_size指定一个就行
>
> random_state为随机种子
>
> shuffle决定是否打乱数据
>
> stratify决定是否进行数据分层，stratify不是bool，是直接指定分层的依据变量，即通过什么来分层，一般就是label或者Series（在输入数据是DataFrame时，stratify指定为Series，即根据那一列来分层）
>
> **输入的数据是什么类型，输出的数据就是什么类型，返回值依次为：x_train, x_test, y_train, y_test**



### 各种层

- **tf.keras.layers.Dense(units, activation=None, use_bias=True, ...)**
- **tf.keras.layers.Embedding(input_dim, output_dim,...., mask_zero=False, input_length=None)**

> input_dim：词汇表大小
>
> output_dim：词嵌入向量的维度
>
> mask_zero：决定输入值0是否是应被屏蔽的mask“填充”值。如果为True，则使用mask，相应的input_dim也应该为词汇表大小+1（因为增加了一个mask token，索引为0）
>
> intput_length：输入序列的长度，如果为None，则自动为最长序列的长度

- **tf.keras.layers.LSTM(units, activation="tanh", recurrent_activation="sigmoid", ..., return_sequences=False)**

> 其中return_sequences决定是否在最后返回所有时间步的输出，若为False只会返回最后一个时间步的输出，如果下一层需要使用到所有时间步的输出则需要设为True

- **tf.keras.layers.Bidirectional(layer, merge_mode="concat", ...)**

> 是RNN的双向包装器，可将单向RNN变为双向RNN
>
> layer是要包装的层，merge_mode决定前后信息汇总的 方式，可为："sum"、"mul"、"concat"、"ave"、None，如果为None，则不会合并，而是将他们作为列表输出

- **tf.keras.layers.Softmax(axis=-1)**

> axis：决定对那个维度执行softmax，默认为最后一个维度



### 模型的常用方法

- **序列容器：tf.keras.models.Sequential()**

> 可以和`add()`配合使用，也可以里面直接加个列表Sequential([...])

- **配置模型：model.compile(optimizer='rmsprop', loss=None, metrics=None,...)**

> optimizer：字符串或者Opitimizer实例
>
> loss：字符串或Loss示例或一个函数
>
> metrics：字符串或Metric示例或一个函数

- **训练模型：model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0., validation_data=None, ...)**

> x/y：可以是numpy数组或者Tensor或list或迭代器等，这样x/y应一一对应。x也可为DataSet、Generator、Sequence，但是这种情况下就不应该指定y，因为y已经在x中了
>
> batch_size如果未指定则默认32
>
> callbacks：一个列表，每个元素是一个callbacks中的实例，如EarlyStopping类
>
> validation_split和validaton_data只需要指定一个，前者是验证集所占比例，后者是直接指定验证集**（如果指定了该参数，则x/y只能Tensor或ndarray）**

- **画模型：tf.keras.utils.plot_model(model, to_file="model.png", show_shapes=False, show_dtype=False, show_layer_names=True, ...)**

> model是要打印的模型，一个keras model实例

- **测试模型：model.evaluate(x=None, y=None, batch_size=None, verbose=1, ...)**

> 用法和fit基本一样

- **采用另一种方法训练模型：**

> ```python
> with tf.GradientTape() as tape:
>     predictions = model(inputs, training = True)
>     loss = loss_func(labels, predictions)
> gradients = tape.gradient(loss, model.trainable_variables)
> optimizer.apply_gradients(zip(gradients, model.trainable_variables))
> ```



### 保存和加载模型

- 既有keras保存方式，也有tf原生方式保存方式，**但是前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署**。既可以直接保存整个模型，也可以分别保存模型的权重和结构

> ```python
> # 直接保存
> model.save("model.h5", save_format = "h5")
> model = tf.keras.models.load_model("model.h5")
> 
> # 分别保存模型的权重和结构
> json_str = model.to_json()
> model.save_weights("model_weights.h5", save_format = "h5")
> model = tf.keras.models.model_from_json(json_str)
> model.compile(...)    # 这种方法重新加载后需要重新配置优化器
> model.load_weights（"model_weights.h5"）
> 
> # 上面演示的是keras保存方式
> # 要使用tf原生方式，需要在model.save()和model.save_weights()时添加参数save_format="tf"，并且更改文件后缀
> # 演示一下：
> model.save_weights('tf_model_weights.ckpt',save_format="tf")
> model.save('tf_model_savedmodel', save_format="tf")
> ```



### 模型的反馈

- **设置Early Stop：tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, ..., restore_best_weights=False)**

> monitor：要监视的量，如："val_loss"、"val_accuracy"、"train_loss"等
>
> min_delta：若小于min_delta的绝对变化，将被视为没有改进
>
> patience：容忍可以没有提升的epochs数
>
> restore_best_weights：是否保存整个训练过程中最好的一个epoch

- **设置学习率衰减：tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.1, patience=10, verbose=0, ..., min_delta=0.0001, ..., min_lr=0)**

> 当经过patience个epochs仍没有改进，则降低学习率，学习率*factor来降低
>
> factor：每次降低的倍数
>
> min_lr：学习率可降低到的最小值



### Dataset用法

- 在`model.fit`中需要可以选择分别用list等类型指定x和y，也可以直接使用Dataset等类指定x而忽略y。对于前者，需要将所有数据一次性读入内存，内存负担较大。而后者可以使用TFRecordDataset类进行外存到内存的流式读取，只需要牺牲小部分IO性能，可大大减轻内存的负担
- 简单来说，一个.tfrecords文件包含多个样本**（官方建议每个文件大小100-200M）**，每个样本都对应着一个`tf.train.Example`类，每个样本可能有多个特征，每个特征用`tf.train.Feature()`封装
- tf.train.Featrue可接收以下三种类型，大多数其他通用类型也可以强制转换成下面的其中一种：

> 1. **tf.train.BytesList：**string、byte**（非标量数据也使用这个，但是需要先将Tensor序列化）**
> 2. **tf.train.FloatList：**float、double
> 3. **tf.train.Int64List：**bool、enum、int32、unit32、int64、uint64g

- **首先进行.tfrecords文件的存入：**

```python
def save_tfrecords(data, label, desfile):
    with tf.io.TFRecordWriter(desfile) as writer:
        for i in range(len(data)):
            features = tf.train.Features(
                feature = {
                    "data":tf.train.Feature(bytes_list = tf.train.BytesList(value =[tf.io.serialize_tensor(data[i]).numpy()])),
                    "label":tf.train.Feature(float_list = tf.train.FloatList(value = label[i])),
                }
            )
            example = tf.train.Example(features = features)
            serialized = example.SerializeToString()
            writer.write(serialized)
   

save_tfrecords(x_in_sample1, y_in_sample1, "path1.tfrecords")
save_tfrecords(x_in_sample2, y_in_sample2, "path2.tfrecords")
```

> - 首先要用`tf.io.TFRecordWriter()`打开写入器，data和label是两个列表，每个元素对应一个Example的特征，desfile是文件名
> - 对于每个Example的特征，使用一个字典表示，字典还需要被`tf.train.Features()`封装起来，字典的每个值可以是上述三种类型之一，需要使用`tf.train.Feature()`封装，其中value的值必须为一个list**（注意前者是Features，后者是Feature）**。并且**由于data是非标量的高维数据，需要先使用`tf.io.serialize_tensor()`进行序列化，然后再使用其numpy值**
> - 最后通过`tf.train.Example`封装feature，然后调用`SerializeToString()`函数，将编码后的字符串写入

- **接下来进行数据读取和使用：**

```python
# TFR数据反编译
def map_func(example):
    feature_description = {
        'data': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.float32),
    }
    parsed_example = tf.io.parse_single_example(example, features=feature_description)
    
    x_sample = tf.io.parse_tensor(parsed_example['data'], tf.float32)
    y_sample = parsed_example['label']
    
    return x_sample, y_sample

# 加载数据集
def load_dataset(filepaths):
    shuffle_buffer_size = 3000
    batch_size = 256

    dataset = tf.data.TFRecordDataset(filepaths)
    dataset = dataset.shuffle(shuffle_buffer_size)
    dataset = dataset.map(map_func=map_func, num_parallel_calls=tf.data.AOTOTUNE)
    dataset = dataset.batch(batch_size).prefetch(64)
    
    return dataset


train_set = load_dataset(["path1.tfrecords","path2.tfrecords"])
valid_set = load_dataset(["path3.tfrecords","path4.tfrecords"])
model.fit(train_set,epochs=model_epochs, validation_data=valid_set, callbacks=[early_stopping])
```

> - 首先要生成`tf.data.TFRecordDataset`类，传入参数是一个list，其中元素是.tfrecords文件名
> - 每次读取后要先打乱
> - 之后调用`map()`，其中指定的函数是**对每个样本操作的**，由于储存时序列化为了二进制字符串，所以数据是什么类型是的信息是丢失了的，解码时需要自己制定数据类型，所以在`map()`中首先需要指定feature_description，说明每个数据的类型
> - 然后使用`tf.io.parse_single_example()`解码单个样本，结果返回一个字典，字典的每个元素对应一个特征，其中**由于data是string类型，由于是编码后的非标量数据，所以还需要使用`tf.io.parse_tensor`解码一次（非标量数据的shape的编码解码的过程中是会丢失的，所以需要把shape一起储存）**
> - 如果储存的是列表，需要在`tf.io.FixedLenFeature()`指定shape，也就是(len(list), )





# keras_bert用法

- **加载预训练模型：keras_bert.load_trained_model_from_checkpoint(config_file, checkpoint_file, training=False, trainable=None, output_layer_num=1, seq_len=int(1e9), kwargs)**

> config_file和checkpoint_file分别为bert_config.json文件和bert_model.ckpt文件
>
> training：如果为True，则将返回带有 MLM 和 NSP输出的模型；否则，将返回输入层和最后一个特征提取层。
>
> trainable：模型是否可训练（也可通过for循环分别制定每层是否可训练）
>
> seq_len为最大序列长度

- **编码输入：keras_bert.Tokenizer(token_dict, ...)**

> bert模型的输入有两个，一个是语义token，一个是序列token，需要创建Tokenizer实例，再对句子进行编码，产生两个token
>
> token_dict：是输入的映射字典，在vocab.txt中可以获取所有token，但是需要自行建立token -> id的字典
>
> 创建Tokenizer实例后，调用`Tokenizer.encode(first, second=None, max_len=None)`进行编码，结果返回一个list，元素分别为两个list，分别对应两个token

- **将数据输入模型：**

> 在`model.fit()`中，由于是多输入，采用`x=[train_token_embeddings, train_seq_embeddings]`和`validation_data=([test_token_embeddings, test_seq_embeddings], test_labels)`这种形式进行输入

- **加载模型时所需要注意的：**

> 加载保存好的模型时同样是使用`tf.keras.models.load_model()`，但是会出现如下报错：
>
> **ValueError: Unknown layer: TokenEmbedding**
>
> 这种情况需要：先`from keras_bert import get_custom_objects`，再在`load_model()`时添加参数`custom_objects=get_custom_objects()`





# bert4keras用法

- **由于keras和tf.keras的兼容问题，所以最开始需要添加环境变量TF_KERAS=1：**

```python
import os
os.environ["TF_KERAS"] = "1"
```

- 直接看[民间的API文档](https://github.com/Sniper970119/bert4keras_document)，写的还可以





# Pytorch

### 基本操作

- **读取数据**

> - 自建一个类，继承`torch.utils.data.Dataset`，还要实现`__init__()`、`__getitem__(index)`、`__len__()`三个函数
> - 实例化Dataset，并用`torch.utils.data.DataLoader`封装：`DataLoader(data, batch_size=1)`
> - 直接for循环取每个batch：`for input_ids, attention_mask, labels in data: `

- **搭建模型：**

> 1. 继承`torch.nn.Module `，并实现`__init__()`、`forward()`

- **损失函数：**

> - 以交叉熵举例
> - 创建：`loss = torch.nn.CrossEntropyLoss(reduction="none").cuda() `
> - 其中`reduction=Union[“mean”, “sum”, “none”]`，表示对每个sample的loss怎么处理
> - **踩个坑：如果loss的输入（模型的输出置信度）超过两维，如token级别的分类，`shape=[batch_size, max_seq_len, num_class]`，需要把num_class移到第二维，即执行`output.transpose(1, 2) `，现在`shape=[batch_size, num_class, max_seq_len]`**
>
> ![IMG_27](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_27.png)

- **优化器：**`torch.optim.Adam(model.parameters(), lr=0.01`
- **反向传播：**

> ```python
> model.zero_grad()
> loss.backward()
> optimizer.step()
> ```

- **模型保存和加载：**

```python
# 可以选择保存整个模型，也可以选择只保存权重
# 保存整个模型：
torch.save(model, file_path)   # file_path以.pt .pth结尾
# 加载整个模型
model = torch.load(file_path)

# 保存权重
torch.save(model.state_dict(), file_path)
# 加载权重
model = MyModel()
model.load_state_dict(torch.load(file_path))
```

- **模型训练、测试模式转换：**`model.train()`、`model.eval()`



### 多卡并行

- **运行命令：**`python3 -m torch.distributed.launch --nproc_per_node=2 {file_name}`

> 其中nproc_per_node代表GPU数量

- **命令行参数**

> 1. 进程编号：`parser.add_argument("--local_rank", type=int, default=-1) `，local_rank为0代表主进程
> 2. GPU数量：`gpu_num = int(os.environ['WORLD_SIZE']) `
> 3. 需要手动设置环境变量：`os.environ["CUDA_VISIBLE_DEVICES"] = “0, ..., gpu_num-1”`

- **初始化：**

> ```python
> torch.cuda.set_device(local_rank)
> if gpu_num > 1 :
>  	torch.distributed.init_process_group(backend="nccl”)
> ```

- **读取数据：**

> ```python 
> data = Dataset()
> data_sampler = torch.utils.data.distributed.DistributedSampler(data)
> data = DataLoader(data, batch_size=1, sampler=data_sampler)   # 需要在DataLoader封装时指定sampler
> ```

- **模型：**

> - 模型需要用`torch.nn.parallel.DistributedDataParallel `封装：
> ```python
> model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=False) `
> ```
> - 其中find_unused_parameters默认为False，为False效率更高，如果报错则改为True

- **打乱数据：**

> - 在每个epoch开始的时候需要`data.sampler.set_epoch(epoch) `
> - 是为了让sample打乱数据，使每个epoch的训练数据顺序不同

- **同步每个进程的数据：**

> - 每个进程中的数据是不同步的，如train_loss、val_loss等
> - 通过`torch.distributed.all_reduce(value)`来对value进行同步
> - 同步之后是所有进程该值的和，所以一般需要除以gpu_num
> - value必须是Tensor，不能是标量

- **模型保存和加载：**

> - 如果model用DDP封装，保存时保存`model.module`或者`model.module.state_dict()`
> - **一般只在主进程进行模型保存、打印、tensorboard记录**



### Huggingface Transformers

- **Tokenizer加载：**`AutoTokenizer.from_pretrained(checkpoint)`
- **Tokenizer编码：**

> - `tokenizer.encode(text, return_tensors="pt") `：只返回词典映射后的编码结果，`text=str`
> - `tokenizer(texts, padding="max_length", truncation=True,max_length=150, return_tensors="pt”)`：返回一个字典，包括模型所需要的所有输入，一般为input_ids和attention_mask。`texts=Union[str, list(str)]`，`padding=Union[“max_length”, True, False]`
> - **tokenizer()的结果如果需要组成batch，字典中的每个Tensor需要将第一维去掉：**
>
> ```python
> for key, val in inputs.items():
>     inputs[key] = val.squeeze(0)
> ```

- **Tokenizer解码：**`tokenizer.decode(ids, skip_special_tokens=True) `

> 其中ids为input_ids

- **指定Tokenizer填充和截断方向：**
> - `tokenizer.padding_side=Union[“left”, “right”]`
> -  `tokenizer.truncation_side=Union[“left”, “right”] `
- **模型：**

> - 加载：`AutoModel.from_pretrained(checkpoint) `
> - 前向计算：
>
> ```python
> inputs = tokenizer().    # inputs是一个dict
> output = model(**inputs)。   # 输出的output是一个dict
> ```

- **生成式模型进行生成预测：**`model.generate(ids, max_length=None, num_beams=1,do_sample=False, top_k=1, early_stopping=False, num_return_sequences=1)`

> - 一些生成模型如BloomForCausalModel自带生成函数
> - 其中ids为encode之后的结果，每次generate的ids只对应一个句子，无法并行
> - 需要do_sanple=True，才能启用top_k、top_p



### Tensorboard

- 直接给个🌰：

> ```python
> # 初始化
> time = “{0:%Y-%m-%d-%H:%M:%S/}".format(datetime.datetime.now())
> tb_path = config["tb_root_path"] + time
> print(f"Start Tensorboard with 'tensorboard --logdir={tb_path} --bind_all', view at http://localhost:6006/")
> writer = SummaryWriter(tb_path)
> # 保存标量
> writer.add_scalar("train/loss", loss, step)
> ```
>
> - 运行完之后直接运行`tensorboard --logdir={tb_path} --bind_all`，然后访问本地6006端口即可
> - 在多卡时，一般只在主进程采用tensorboard

