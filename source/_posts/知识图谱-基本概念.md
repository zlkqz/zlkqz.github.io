---
title: 知识图谱-基本概念
math: true
date: 2022-12-30
---



# 1 知识图谱的构成

- 作为一种只是表示形式，知识图谱是一种大规模语义网络，包含实体（Entity）、概念（Concept）及其之间的各种语义关系，如下图：

![image-20230108190012895](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190012895.png)

- 而语义网络是以图形化的形式通过点和边表达知识的方式，其中点包括：

> 1. **实体：**实体是属性赖以存在的基础，并且必须是自在的，即独立的、不依附于其他东西而存在的。比如身高，仅仅说身高是没有意义的，说“哲学家”这个类别的身高也是没有意义的，而必须说某个具体的哲学家的身高，这才是有明确所指且有意义的。
> 2. **概念：**是指一类实体，比如“哲学家”，不是指某个特定的哲学家，而是指一类人
> 3. **值：**每个实体都有一定的属性值，一般是数值、日期或文本
>
> ![image-20230108190222968](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190222968.png)

- 知识图谱中的边可以分为**属性（Property）**和**关系（Relation）**两类。属性描述实体某方面的特性，而关系则可以认为是一类特殊的属性，当实体的某个属性值也是一个实体时，这个属性就是关系

- 知识图谱和传统语义网络很像，但是两者的根本区别在于**前者是大规模自动化知识获取，而后者过于依赖于专家知识，导致规模等受限**






# 2 知识图谱中的分类

- 可以根据所包含的不同知识对知识图谱进行分类：

> 1. **事实知识（Factual Knowledge）：**关于某个特定实体的基本事实。如（柏拉图，出生地，雅典）
> 2. **概念知识（Taxonomy Knowledge）：**概念知识分为两类，一类是实体与概念之间的类属关系（isA关系），如（柏拉图，isA，哲学家），另一类是子概念与父概念之间的子类关系（subclassOf ），如（唯心主义哲学家 subclassOf 哲学家）
> 3. **词汇知识（Lexical Knowledge）：**主要包括实体与词汇之间的关系（比如，实体的命名、称谓、英文名等）以及词汇之间的关系(包括同义关系、反义关系、缩略词关系、上下位词关系等)。例如﹐(“Plato”，中文名，柏拉图)、(赵匡胤，谥号，宋太祖)、(妻子，同义，老婆）
> 4. **常识知识（Commonsense Knowledge）：**是人类通过身体与世界交互而积累的经验与知识，是人们在交流时无需言明就能理解的知识。如我们都知道鸟有翅膀，能飞，但是这种信息很少出现在文本里，所以常识知识的提取是十分困难的

- 知识图谱还可以通过其他方式来分类，总结一下：

![image-20230108194127845](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194127845.png)





# 3 知识图谱的数值表示

- 一个三元组包括：主题（Subject）、谓词（Predicate）以及客体（Object）。而一个知识图谱可以视作许多个三元组的集合，如下图：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194532703.png" alt="image-20230108194532703" style="zoom:80%;" />

- 知识图谱的表示学习旨在将知识图谱中的元素（包括实体、属性、概念等）表示为低维稠密实值向量。而其关键是合理定义知识图谱中**关于事实（即三元组$$<h, r, t>$$）的损失函数$$f_r(h,t)$$，当事实$$<h, r, t>$$成立时，我们期望$$f_r(h,t)$$越小。考虑整个知识图谱的事实，就可以通过最小化$$\sum_{\langle h, r, t\rangle \in O} f_{r}(\boldsymbol{h}, \boldsymbol{t})$$来学习向量化表示**

- 那么现在问题就变成了如何定义$$f_r(h,t)$$，一般思路有基于距离和基于翻译两种



### 3.1 SE模型

- 基于距离的代表模型，基本思想是当两个实体属于同一个三元组$$<h,r,t >$$时，它们的向量表示在投影后的空间中也应该彼此靠近
- 因此定义损失函数为向量投影后的距离：

$$
f_{r}(\boldsymbol{h}, \boldsymbol{t})=\left\|\boldsymbol{W}_{r, 1} \boldsymbol{h}-\boldsymbol{W}_{r, 2} \boldsymbol{t}\right\|_{l_{1}}
$$

其中的两个矩阵$$W_{r, 1}, W_{r, 2}$$分别用于头实体向量$$h$$和尾实体向量$$t$$的投影操作，**但SE难以捕捉实体和关系之间的语义相关性**



### 3.2 TransE模型

- 除了基于距离，还有基于翻译的模型，如TransE认为在知识库中，三元组$$<h,r,t >$$可以堪称头实体h到尾实体t利用关系r所进行的翻译。
- 比如，对于三元组<柏拉图，老师，苏格拉底>来说，头实体“柏拉图”的向量加上关系“老师”的向量，应该尽可能和尾实体“苏格拉底”的向量接近，**也就是$$h+r \approx t$$**
- 基于这一思想可以得到损失函数：

$$
f_{r}(\boldsymbol{h}, \boldsymbol{t})=\|\boldsymbol{h}+\boldsymbol{r}-\boldsymbol{t}\|_{l_{1} / l_{2}}
$$

- 在实际应用中，**为了使习得的表示更有区分度，使用Hinge Loss目标函数，使得正负例尽可能分开：**

$$
L=\sum_{(h, r, t) \in S} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in S^{\prime}}\left[\gamma+f_{r}(\boldsymbol{h}, \boldsymbol{t})-f_{r}\left(\boldsymbol{h}^{\prime}, \boldsymbol{t}^{\prime}\right)\right]_{+}
$$

其中，$$\gamma$$是间隔参数，$$S$$是正例集合（知识库中已有的三元组)，$$S'$$是负例集合（知识库中不存在的三元组，通常通过对关系r的三元组头尾实体进行随机替换来构造)。$$[x]_+ = \max(0, x)$$



### 3.3 TransH模型

- **TransE模型中的$$h+r \approx t$$假设太强**，导致在自反一对多、多对一等关系下实体向量学习的错误。
- 比如，对于自反关系r，$$<h,r,t >$$和$$<t,r,h >$$同时成立，导致h =t。对于多对一关系。又比如<柏拉图，性别，男>、<特朗普，性别，男>两个三元组有着相同的关系和尾实体，导致柏拉图和特朗普向量接近。但是柏拉图与特朗普除了在性别上相同，在其他方面显然完全不同。
- 为解决上述问题，TransH放宽了假设，**只要求头尾实体在关系r相对应的超平面上的投影彼此接近即可**。设r所对应的超平面的法向量为$$W_r$$，那么$$h, t$$映射到超平面上为：

$$
\begin{aligned}
\boldsymbol{h}_{\perp} & =\boldsymbol{h}-\boldsymbol{W}_{r}^{\mathrm{T}} \boldsymbol{h} \boldsymbol{W}_{r} \\
\boldsymbol{t}_{\perp} & =\boldsymbol{t}-\boldsymbol{W}_{r}^{\mathrm{T}} \boldsymbol{t} \boldsymbol{W}_{r}
\end{aligned}
$$

- 那么损失函数为：

$$
f_{r}(\boldsymbol{h}, \boldsymbol{t})=\left\|\left(\boldsymbol{h}-\boldsymbol{W}_{r}^{\mathrm{T}} \boldsymbol{h} \boldsymbol{W}_{r}\right)+r-\left(\boldsymbol{t}-\boldsymbol{W}_{r}^{\mathrm{T}} \boldsymbol{t} \boldsymbol{W}_{r}\right)\right\|_{l_{1} / l_{2}}
$$



### 3.4 TransR模型

- 在TransE模型和TransH模型中，实体和关系都在相同的空间中进行表示。这种做法无法区分两个语义相近的实体在某些特定方面（关系）上的不同
- 而TransR模型基本思想和TransH相似，**但是要求将头尾实体映射到关系r所对应的向量空间中，并且彼此接近**
- 每个关系r维护一个映射矩阵$$M_r$$，那么头尾实体映射到该向量空间为：

$$
\begin{aligned}
h_r = M_rh \\
t_r = M_rt
\end{aligned}
$$

- 那么损失函数为：

$$
f_{r}(\boldsymbol{h}, \boldsymbol{t})=\left\|\boldsymbol{h}_{r}+\boldsymbol{r}-\boldsymbol{t}_{r}\right\|_{l_{1} / l_{2}}
$$



### 3.5 TransD模型

- 在TransR中，一个关系r对应一个映射矩阵$$M_r$$，**该矩阵是和头尾实体无关的**。对于一个三元组，头尾实体可能不是一类实体，比如<柏拉图，出生地，希腊>，**头尾实体不是一类实体，但是却使用了相同的映射矩阵$$M_r$$**，明显不合理

- **所以TransD令映射矩阵和实体、关系同时相关**。在TransD中，**每个实体或关系都拥有两个向量**，对于三元组$$<h,r,t >$$，需要用6个向量$$h,h_p,t,t_p \in R^n, r, r_p \in R^m$$，其中**没有下标p的向量是用来捕捉语义信息的，而有下标p的向量是用于构造映射矩阵$$M_{rh}, M_{rt}$$的**，映射函数为：

$$
\begin{aligned}
\boldsymbol{M}_{r h}=r_{p} \boldsymbol{h}_{p}^{\mathrm{T}}+\boldsymbol{I}^{m \times n}\\\quad \boldsymbol{M}_{r t}=r_{p} \boldsymbol{t}_{p}^{\mathrm{T}}+\boldsymbol{I}^{m \times n}
\end{aligned}
$$

> - 为什么构造映射矩阵的时候要加单位矩阵？
>
> 每次构造映射矩阵时，是先将矩阵初始化为单位矩阵，然后再通过向量内积来改变这个矩阵。**个人猜测是为了不要让映射后的空间和原空间相差过大**

- 得到映射矩阵后，进行空间变换$$\boldsymbol{h}_{\perp}=\boldsymbol{M}_{r h} \boldsymbol{h}, \quad \boldsymbol{t}_{\perp}=\boldsymbol{M}_{r t} \boldsymbol{t}$$，那么损失函数变为：

$$
f_{r}(\boldsymbol{h}+\boldsymbol{t})=\left\|\boldsymbol{h}_{\perp}+\boldsymbol{r}-\boldsymbol{t}_{\perp}\right\|_{l_{1} / l_{2}}
$$





# 4 知识表示形式

- 最常见的知识表示形式即前面的三元组形式，但是也有其他的表示形式，如谓词逻辑、产生式规则、框架（Frame）、树形知识表示、概率图等：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215032003.png" alt="image-20230108215032003" style="zoom:80%;" />

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215042336.png" alt="image-20230108215042336" style="zoom:80%;" />

- 这里主要介绍一下用马尔可夫随机场（MRF）和马尔可夫逻辑网（MLN）进行表示
- **马尔可夫逻辑网是通过软化一阶逻辑实现的**。传统的一阶逻辑知识库（由一阶逻辑命题所组成的知识库）被视作在一系列可能世界 （Possible World）上所施加的一组硬约束（Hard Constraint)，**但是这样的约束太过生硬，有时观察到的规则可能和知识库中的规则冲突**。而MLN旨在软化这些约束，**每条规则都与一个反应其约束强度的权重关联，权重越高，满足和不满足此规则的对数概率差就越大，即这条规则的置信度就越高**

![image-20230108215936859](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215936859.png)

其中，$$Fr(x, y)$$表示x，y是朋友，$$Sm(x)$$表示x抽烟。综上所述，**MLN就是一个（规则，权重）对的集合**

- 而MLN可以视作定义具体的MRF（马尔可夫随机场）的模板。给定一个MLN（记作$$L=\{F_i, w_i\}$$）以及一个常量集合（如$$C=\{Anna, Bob\}$$），就可以定义一个相应的MRF（记作$$M_{L, C}$$）。构筑规则如下：

> - **MRF的每个节点对应将MLN规则中经过给定常量实例化（Grounding）而得到的一个谓词**。比如规则$$\forall x \forall y \forall z \operatorname{Fr}(x, y) \wedge \operatorname{Fr}(y, z) \Rightarrow \operatorname{Fr}(x, z)$$中包含$$Fr(x, y)$$，由于给定了$$C=\{Anna, Bob\}$$，所以可以将其实例化为$$Fr(Anna, Bob)$$，这就是一个谓词实例，也是一个二元随机变量（要么为真要么为假），其就对应了MRF中的一个节点
> - 现在要进一步需要明确MRF中的边：**两个谓词实例之间存在一条边，当且仅当它们至少在一个规则中同时出现。一条规则中的谓词之间形成了马尔可夫随机场中的一个团（不一定是最大团）**

- 通过上图的最后两条规则，并给定$$C=\{Anna, Bob\}$$，可以构建出如下的MRF：

<img src="https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108221652749.png" alt="image-20230108221652749"  />

- 根据得到的$$M_{L,C}$$可以进行概率推理，推理所要回答的问题形式是：**一直一条规则成立，则另一条规则成立的概率是多少**。比如已知$$F_2$$为$$Fr(Anna, Bob) \vee Sm(Anna)$$，求$$F_1 = Sm(Bob)$$的概率：

$$
\begin{aligned}
P(F_1 \mid F_2, M_{L, C}) & =\frac{P\left(F_{1} \wedge F_{2} \mid M_{L, C}\right)}{P\left(F_{2} \mid M_{L, C}\right)} \\
& =\frac{\sum_{x \in \chi_{F_{1}} \cap \chi_{F_{2}}} P\left(X=x \mid M_{L, C}\right)}{\sum_{x \in \chi_{F_{2}}} P\left(X=x \mid M_{L, C}\right)}
\end{aligned}
$$

其中$$\chi_{F_i}$$表示$$F_i$$成立的世界的集合

