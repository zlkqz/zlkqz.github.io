---
title: 不同batch size梯度下降的影响
math: true
date: 2021-11-23
---



- 梯度下降法作为机器学习中较常使用的优化算法，针对不同的batch size，有着3种不同的形式：**批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）、小批量梯度下降（Mini-Batch Gradient Descent）**



# 1 批量梯度下降（BGD）

使用整个训练集的优化算法被称为**批量**(batch)或**确定性**(deterministic)梯度算法，因为它们会**在一个大批量中同时处理所有样本**

**批量梯度下降法**是最原始的形式，它是指在**每一次迭代时**使用**所有样本**来进行梯度的更新

- **优点：**

1. 在训练过程中，使用固定的学习率
2. 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向
3. 一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行

- **缺点：**

1. **尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了**
2. 不能投入新数据实时更新模型
3. 对非凸函数可能只能收敛到局部最小点，而非全局最小点

![BGD](https://i.loli.net/2021/11/11/mCkfeDIPyh3a5iY.png)





# 2 随机梯度下降（SGD）

**随机梯度下降法**不同于批量梯度下降，随机梯度下降是在**每次迭代时**使用**一个样本**来对参数进行更新（mini-batch size =1）

- **优点：**

1. 在学习过程中加入了噪声，提高了泛化误差
2. **噪声造成的扰动可能可以使其脱离局部最小点**
3. SGD一次只遍历一个样本就可以进行更新 ，所以收敛很快，并且可以新增样本

- **缺点：**

1. **不收敛，在最小值附近波动**
2. **不能在一个样本中使用并行化计算，学习过程变得很慢**
3. 单个样本并不能代表全体样本的趋势，所以学习过程可能变得特别的曲折，**虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的**，如下图

<img src="https://i.loli.net/2021/11/11/GLBkox67lvnmsOA.png" alt="image-20211111201918007" style="zoom: 67%;" />

- **对上面期望的证明，SGD的梯度是BGD梯度的无偏估计：**

$$
E(\nabla f_i(x)) = \frac{1}{n}\sum_{i = 1}^n\nabla f_i(x) = \nabla f(x)
$$

**这说明了SGD的总体优化方向仍然是对的**





# 3 Mini-batch梯度下降

大多数用于深度学习的梯度下降算法介于以上两者之间，**使用一个以上而又不是全部的训练样本**

- 在一次取样本的时候我们需要在所有样本中**随机**取batch-size个样本

- **优点：**

1. 收敛速度比BGD快，因为只遍历部分样例就可执行更新
2. 随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例
3. 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果
4. 因为有噪音（但是噪音比SGD小），所以**可能脱离局部最小点**

- **缺点：**

1. 在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛
2. 学习过程会有更多的振荡，为更接近最小值，需要增加**学习率衰减项**，以降低学习率，避免过度振荡

- **小批量大小**（mini-batch size）的选择：

1. 更大的批量会计算更精确的梯度，但是回报却是小于线性的
2. 极小的批量通常难以充分利用多核结构。当批量低于某个数值时，计算时间不会减少
3. 批量处理中的所有样本可以并行处理，**内存消耗和批量大小会成正比**。对于很多硬件设备，这是批量大小的限制因素
4. 在使用**GPU**时，通常使用**2的幂数作为批量大小**可以获得更少的运行时间。一般，2的幂数取值范围是**32~256**。16有时在尝试大模型时使用





使用三种梯度下降的收敛过程：

<img src="https://i.loli.net/2021/11/11/paxvbFh9fRiDKNg.png" alt="image-20211111204100788" style="zoom: 80%;" />

- **Mini-batch梯度下降和SGD一样，其梯度也是BGD梯度的无偏估计**

- **从无偏估计可以看出，Mini-batch和SGD其实就是在用部分样本梯度来代替总体梯度**