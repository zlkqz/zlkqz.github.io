{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/fluid/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/fluid/source/css/gitalk.css","path":"css/gitalk.css","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/default.png","path":"img/default.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/boot.js","path":"js/boot.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/police_beian.png","path":"img/police_beian.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/events.js","path":"js/events.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/color-schema.js","path":"js/color-schema.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/leancloud.js","path":"js/leancloud.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/img-lazyload.js","path":"js/img-lazyload.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/xml/local-search.xml","path":"xml/local-search.xml","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/plugins.js","path":"js/plugins.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/lib/hint/hint.min.css","path":"lib/hint/hint.min.css","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"a2cb84b762c567313022337e01ff48f37e2c7fa3","modified":1666061652000},{"_id":"source/_posts/Bert总结.md","hash":"111e0a5fce3c8c092956dd43178408f41e02500a","modified":1671517077983},{"_id":"source/_posts/BART总结.md","hash":"479746af1e488516515b0e63c2bf354576ddf5ea","modified":1671517469773},{"_id":"source/_posts/EDA和AEDA.md","hash":"6bdb75d8a354612d613f73f278401f3dfdd96157","modified":1671517490218},{"_id":"source/_posts/CNN.md","hash":"4f253e65979e4927750362d4a82569fbc0cd470d","modified":1671516978204},{"_id":"source/_posts/CV基础.md","hash":"86cea0e39bb3eb7565673ff23b092998afbbceb5","modified":1671517011882},{"_id":"source/_posts/HMM和CRF.md","hash":"9318e6fc03b5fd31d9e94f2d9cce7573525d7b9d","modified":1671517353900},{"_id":"source/_posts/KNN.md","hash":"0b687fb34f85a7c617ffe1618841ab89f012235f","modified":1671517162075},{"_id":"source/_posts/GPT.md","hash":"347be615ec4c5830aafd84a508257aa37f33512e","modified":1680106231623},{"_id":"source/_posts/RoBERTa总结.md","hash":"bccb75de165ac579dd6442497f1a3725aa781f60","modified":1671517457680},{"_id":"source/_posts/NLP基础.md","hash":"55160f7875803879cf10742c6255e34166535eb9","modified":1675673039278},{"_id":"source/_posts/RNN.md","hash":"3c1892cb2f603135fbffbbf6bcf750d5919bb4cb","modified":1680106413246},{"_id":"source/_posts/SVM.md","hash":"c31a8922f4f21f476428b3b7b34bf11d06a990df","modified":1671517204493},{"_id":"source/_posts/SimCSE总结.md","hash":"a00d943034c4751ef888d99753dc6d433e15af0d","modified":1671517500657},{"_id":"source/_posts/Transformer总结.md","hash":"9756d55c8f3c0718ebdb34eb40b5983b079c037a","modified":1671517058438},{"_id":"source/_posts/T5 && mT5.md","hash":"357f5638e6315823d7cbd1e4ef4403cd864a4757","modified":1675749117996},{"_id":"source/_posts/不同batch size梯度下降的影响.md","hash":"829bab9c61fd4b87cc38b20e2490012b83fbafef","modified":1671516957186},{"_id":"source/_posts/代码总结.md","hash":"ea945d71b378adfb749ab0798ec45277e50dbaf3","modified":1680106623144},{"_id":"source/_posts/决策树.md","hash":"72ff81065976d9e12fe2acac218cf40f31660625","modified":1671517249282},{"_id":"source/_posts/常用损失函数和评估模型的指标.md","hash":"1ee20427c11d7863530c6ae1f7a62ce73d18e34f","modified":1673159165500},{"_id":"source/_posts/优化算法.md","hash":"b44a27df140479cddde907ff89e4b515c0efedb4","modified":1671517603798},{"_id":"source/_posts/激活函数的作用和比较.md","hash":"b21fb97f632dab784b4fb2867e0d81f6170ca467","modified":1671516918573},{"_id":"source/_posts/最大熵模型.md","hash":"8b9a51cbb9426a78503843ab50a8c2176ea55131","modified":1671517336792},{"_id":"source/_posts/知识图谱-基本概念.md","hash":"1fb8b0c25a0695c2f13a19d3e8c3e29368cb0192","modified":1673244073403},{"_id":"source/_posts/深度学习基础总结.md","hash":"2d814825372fb858971c34dc4ba6b96561fb5038","modified":1671516893043},{"_id":"source/_posts/语法.md","hash":"cef6e3b15eea2a0bb2a17bc90aa4f0c12f6013ac","modified":1673187850300},{"_id":"source/_posts/贝叶斯分类器.md","hash":"23ecdf29df1a39f58c8c6e4c173d186673855de1","modified":1671517217751},{"_id":"source/_posts/集成学习.md","hash":"f7085fa6b8559ff75472400aa434e70d5c7c6349","modified":1671517258835},{"_id":"themes/fluid/source/css/_pages/_category/category.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"themes/fluid/.editorconfig","hash":"33218fbd623feb43edf5f99f15965392cecc44a6","modified":499162500000},{"_id":"themes/fluid/.eslintrc","hash":"4bc2b19ce2b8c4d242f97d4ccf2d741e68ab0097","modified":499162500000},{"_id":"themes/fluid/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":499162500000},{"_id":"themes/fluid/LICENSE","hash":"df5b54be535593d5442cebafbea34eb9bd69b987","modified":499162500000},{"_id":"themes/fluid/README_en.md","hash":"ca8fd19a4948de1f253616a62c0e8a7d81f692f5","modified":499162500000},{"_id":"themes/fluid/README.md","hash":"03cfa8e5f149514b57ef80dcb84eb7fea261370d","modified":499162500000},{"_id":"themes/fluid/_config.yml","hash":"4af7e42fd7ba44056098d0d46d7dd1dd1bb41236","modified":1659332454000},{"_id":"themes/fluid/package.json","hash":"20d4128b364a38f8b963a8f43ec84041b0914a4e","modified":1634643244000},{"_id":"themes/fluid/languages/en.yml","hash":"a85dcc5cc21f9cab50df31e5001b8818ee62d1e2","modified":499162500000},{"_id":"themes/fluid/languages/eo.yml","hash":"a0c7984495d4f2d33b64adfa33adebbf768a5ac3","modified":499162500000},{"_id":"themes/fluid/languages/zh-CN.yml","hash":"21307b4137c3d9b04bb58243747e75af0abc5a71","modified":499162500000},{"_id":"themes/fluid/languages/de.yml","hash":"13a6a799415fc2f6f69ebd1a399fb44426a5d641","modified":499162500000},{"_id":"themes/fluid/languages/zh-TW.yml","hash":"1a6d415446da11dee5c5f400e7d67544fbe743ea","modified":499162500000},{"_id":"themes/fluid/languages/ja.yml","hash":"91020031a847c0361a6fd7ab990c7be4bf17529b","modified":499162500000},{"_id":"themes/fluid/layout/about.ejs","hash":"ad6fed7b646d3ca961db83db0fbe020e3a5d42ad","modified":499162500000},{"_id":"themes/fluid/layout/archive.ejs","hash":"472d0813ca5b88000a7bc6039f33b7e27b5a3216","modified":499162500000},{"_id":"themes/fluid/layout/index.ejs","hash":"666476260a2cead2cc2928d51977c4a7ba8de6bb","modified":499162500000},{"_id":"themes/fluid/layout/category.ejs","hash":"58291dfec65c36889dfce0ddc603540b67e4c598","modified":499162500000},{"_id":"themes/fluid/layout/404.ejs","hash":"689d9f4efd2a7f5edfd9b24561a7ade69d46617c","modified":499162500000},{"_id":"themes/fluid/layout/categories.ejs","hash":"20c2a195a109d2a263b5fa6e79cbcc62932508ad","modified":499162500000},{"_id":"themes/fluid/layout/layout.ejs","hash":"7f566edf750241e62d7c54abfbb0c504fdab850a","modified":499162500000},{"_id":"themes/fluid/layout/links.ejs","hash":"2a7b49f0f9aecf07550b5a0b99242aab5654cf2b","modified":499162500000},{"_id":"themes/fluid/layout/page.ejs","hash":"1014b901d396f4fc445cb1ffc938d5380d894d71","modified":499162500000},{"_id":"themes/fluid/layout/post.ejs","hash":"79e3679a7069351a6172c281b9d09f59d7580484","modified":499162500000},{"_id":"themes/fluid/layout/tag.ejs","hash":"0ad89eb7c92a822980fa9a85285e6d94ad845d1d","modified":499162500000},{"_id":"themes/fluid/layout/tags.ejs","hash":"1d06af34b6cf1d8a20d2eb565e309326ceba309f","modified":499162500000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/bug_report.md","hash":"16d33eb89ecf90f4046720fde5395d972c7ba1fd","modified":499162500000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/question.md","hash":"ab5eab9e3ff889c4ba7fd82846e7f5b7ae15bebc","modified":499162500000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/feature_request.md","hash":"c134dd57ffd269b93402ccfffe7dbe0f0b583bec","modified":499162500000},{"_id":"themes/fluid/layout/_partial/archive-list.ejs","hash":"53a4f6029373a40394a87aba9284696a71610f90","modified":499162500000},{"_id":"themes/fluid/layout/_partial/beian.ejs","hash":"58b4bbe36386de4305a8da5ffd7d56802df23049","modified":499162500000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/feature_request_zh.md","hash":"ed08574b196447376dd74411cca664ac9227a5d4","modified":499162500000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/bug_report_zh.md","hash":"af977ed0792508bb0766ea8afe82d34ef1e8fb3c","modified":499162500000},{"_id":"themes/fluid/layout/_partial/css.ejs","hash":"c363829a4b80f74fc1c565e41f6dab41c95006ea","modified":499162500000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/question_zh.md","hash":"e24b470f7aa8044499a4f5e39634e5dc43899011","modified":499162500000},{"_id":"themes/fluid/layout/_partial/footer.ejs","hash":"a62278c38a310da495d96c39abacacef266945cb","modified":499162500000},{"_id":"themes/fluid/layout/_partial/head.ejs","hash":"7d0cf31f2595cfe6d5ad31d569654f21a00dfd77","modified":499162500000},{"_id":"themes/fluid/layout/_partial/nav.ejs","hash":"e71b3c4aa263163597d31b1f91e5a1a877084cfd","modified":499162500000},{"_id":"themes/fluid/layout/_partial/post-meta.ejs","hash":"32a17edadeed40da6db21d2d8031bd47d2fc9bf4","modified":499162500000},{"_id":"themes/fluid/layout/_partial/paginator.ejs","hash":"0f38a2c238169edcb63fc46c23bfc529ff3859b7","modified":499162500000},{"_id":"themes/fluid/layout/_partial/scripts.ejs","hash":"0ee838b6fcd895d21a29d9d67dbb99b752d623d1","modified":499162500000},{"_id":"themes/fluid/layout/_partial/statistics.ejs","hash":"920bc618d357d48d2b96f8758f6ae8f9488fc4d8","modified":499162500000},{"_id":"themes/fluid/scripts/events/index.js","hash":"44faef3e77ab08b91e4c5c6f1cd9087a9faff443","modified":499162500000},{"_id":"themes/fluid/scripts/generators/local-search.js","hash":"fc2c50405b771b06b7f6cfc4e9de97b992691555","modified":499162500000},{"_id":"themes/fluid/.github/workflows/limit.yaml","hash":"f8bd2edeb4424ee7a055b31583445d5d5dff91a4","modified":499162500000},{"_id":"themes/fluid/layout/_partial/toc.ejs","hash":"3d2fb5552f373e5a0c56bc356702d807bcbcb411","modified":499162500000},{"_id":"themes/fluid/layout/_partial/search.ejs","hash":"cdd7919fa01f6ef7ccc09938d662ff3d77f5d999","modified":499162500000},{"_id":"themes/fluid/scripts/filters/locals.js","hash":"58d0fec976f6b1d35e7ea03edc45414088acf05c","modified":499162500000},{"_id":"themes/fluid/scripts/generators/pages.js","hash":"d9971f15fbb6b775e3d31a1b9b45011959395010","modified":499162500000},{"_id":"themes/fluid/scripts/filters/post-filter.js","hash":"6c37e9f1ac1d6d00b3c32794e02e244dba942cd9","modified":499162500000},{"_id":"themes/fluid/scripts/helpers/export-config.js","hash":"cde964c8cd3217268a231de5e018a62c53c2e047","modified":499162500000},{"_id":"themes/fluid/source/css/main.styl","hash":"d5a8a59c8d1fd17d699a951e59c4ce9ae44c419d","modified":499162500000},{"_id":"themes/fluid/scripts/helpers/url.js","hash":"2a6a8288176d0e0f6ec008056bf2745a86e8943e","modified":499162500000},{"_id":"themes/fluid/scripts/helpers/page.js","hash":"4607607445233b3029ef20ed5e91de0da0a7f9c5","modified":499162500000},{"_id":"themes/fluid/source/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":499162500000},{"_id":"themes/fluid/scripts/helpers/utils.js","hash":"3aa5b4ea879cd34d3a32468d88da18d72cbcc8e0","modified":499162500000},{"_id":"themes/fluid/scripts/helpers/wordcount.js","hash":"8e33f915028ac56258f6999d19b1ad8d800cecfe","modified":499162500000},{"_id":"themes/fluid/scripts/utils/compare-versions.js","hash":"dbbc928c914fc2bd242cd66aa0c45971aec13a5d","modified":499162500000},{"_id":"themes/fluid/scripts/utils/object.js","hash":"649457796374c79e49a19bd541e4ad8e78fe8995","modified":499162500000},{"_id":"themes/fluid/scripts/tags/button.js","hash":"3eb43a8cdea0a64576ad6b31b4df6c2bf5698d4c","modified":499162500000},{"_id":"themes/fluid/scripts/utils/url-join.js","hash":"718aab5e7b2059a06b093ca738de420d9afa44ba","modified":499162500000},{"_id":"themes/fluid/scripts/tags/checkbox.js","hash":"63468f7875c09d9557fe8315afc97175745d9087","modified":499162500000},{"_id":"themes/fluid/scripts/tags/label.js","hash":"f05a6d32cca79535b22907dc03edb9d3fa2d8176","modified":499162500000},{"_id":"themes/fluid/scripts/tags/mermaid.js","hash":"75160561e1ef3603b6d2ad2938464ab1cb77fd38","modified":499162500000},{"_id":"themes/fluid/source/img/favicon.png","hash":"fe1094697fe564c9e1d22f26c6bb73eb0f70217c","modified":1634647284000},{"_id":"themes/fluid/scripts/tags/note.js","hash":"f52f3a005b41f48b4da274ac64710177c8d4502f","modified":499162500000},{"_id":"themes/fluid/scripts/tags/group-image.js","hash":"4aeebb797026f1df25646a5d69f7fde79b1bcd26","modified":499162500000},{"_id":"themes/fluid/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":499162500000},{"_id":"themes/fluid/source/img/loading.gif","hash":"af8ba90e5a40164dfa52125c5889d1ad2ce62caf","modified":1634645106000},{"_id":"themes/fluid/source/js/boot.js","hash":"3de344ee619da989f6dccf7c2ae459fe91075983","modified":499162500000},{"_id":"themes/fluid/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":499162500000},{"_id":"themes/fluid/source/js/events.js","hash":"4b9d2676c9544db9cc40a8c7d18456792299ba86","modified":499162500000},{"_id":"themes/fluid/source/js/color-schema.js","hash":"2520c6459b544a4e965b838f8095e2a84390a43c","modified":499162500000},{"_id":"themes/fluid/source/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":499162500000},{"_id":"themes/fluid/source/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":499162500000},{"_id":"themes/fluid/source/js/local-search.js","hash":"633f0142c657805359b0197f287e12ae4bcde731","modified":499162500000},{"_id":"themes/fluid/source/js/leancloud.js","hash":"4a787cfce27045aa0a92ec22e84f2ccf30cabc4c","modified":499162500000},{"_id":"themes/fluid/source/js/plugins.js","hash":"d058f30bd09b28769c4d8313428ff23dfc8d52dd","modified":499162500000},{"_id":"themes/fluid/source/js/utils.js","hash":"4a43f2700e91937650bef511fd438825b001c4c6","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/cusdis.ejs","hash":"5f9dc012be27040bbe874d0c093c0d53958cc987","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/disqus.ejs","hash":"aab4a4d24c55231a37db308ae94414319cecdd9b","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/gitalk.ejs","hash":"843bc141a4545eb20d1c92fb63c85d459b4271ec","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/livere.ejs","hash":"2264758fed57542a7389c7aa9f00f1aefa17eb87","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/twikoo.ejs","hash":"1af53bc0be642610a3a4d4e7c05287854a821508","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/changyan.ejs","hash":"c9b2d68ed3d375f1953e7007307d2a3f75ed6249","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/remark42.ejs","hash":"d4e9532feeb02aed61bd15eda536b5b631454dac","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/valine.ejs","hash":"4052ab2a8f78efa92f0fe17abe8f66135943390a","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/utterances.ejs","hash":"e1ed6530dfd7310f91060a75766a93ac3c39be3a","modified":499162500000},{"_id":"themes/fluid/layout/_partial/comments/waline.ejs","hash":"21e00443054802e893aac1f668b69d5bb4b39b3a","modified":499162500000},{"_id":"themes/fluid/layout/_partial/plugins/mermaid.ejs","hash":"10ed1f9a611449d37736e17c4e251127b38b3772","modified":499162500000},{"_id":"themes/fluid/layout/_partial/plugins/math.ejs","hash":"a7ed1d3079c32497c8955ca75f5fb6992e5ffb8b","modified":499162500000},{"_id":"themes/fluid/layout/_partial/plugins/nprogress.ejs","hash":"4c2d39ce816b8a6dcd6b53113c8695f8bd650a23","modified":499162500000},{"_id":"themes/fluid/layout/_partial/plugins/analytics.ejs","hash":"557077a8825fffc0a2c7fe2b29f319287950244f","modified":499162500000},{"_id":"themes/fluid/layout/_partial/plugins/typed.ejs","hash":"ece659572cf4e12638a1607fca512c25098bbd82","modified":499162500000},{"_id":"themes/fluid/scripts/events/lib/compatible-configs.js","hash":"b5fd5a2d9c463eb59318af0f47c591c485b6ad27","modified":499162500000},{"_id":"themes/fluid/scripts/events/lib/highlight.js","hash":"deed966f38cf0c8dee3f72e5b1f2e878510db0e1","modified":499162500000},{"_id":"themes/fluid/scripts/events/lib/merge-configs.js","hash":"c1db1a4f9eca6e36b660530641e3a4fb6a30c8d8","modified":499162500000},{"_id":"themes/fluid/scripts/events/lib/hello.js","hash":"28e186c32576eb3d5d923273471a001c47fe8071","modified":499162500000},{"_id":"themes/fluid/scripts/events/lib/lazyload.js","hash":"9ba0d4bc224e22af8a5a48d6ff13e5a0fcfee2a4","modified":499162500000},{"_id":"themes/fluid/scripts/events/lib/footnote.js","hash":"3b2abc5f5e3b681874637e98e047dc4969eb1983","modified":499162500000},{"_id":"themes/fluid/source/css/_variables/base.styl","hash":"63aedd67d90d641cc672540db20ef615f528c9f1","modified":499162500000},{"_id":"themes/fluid/source/lib/hint/hint.min.css","hash":"b38df228460ebfb4c0b6085336ee2878fe85aafe","modified":499162500000},{"_id":"themes/fluid/source/css/_functions/base.styl","hash":"2e46f3f4e2c9fe34c1ff1c598738fc7349ae8188","modified":499162500000},{"_id":"themes/fluid/source/css/_mixins/base.styl","hash":"542e306ee9494e8a78e44d6d7d409605d94caeb3","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/pages.styl","hash":"b8e887bc7fb3b765a1f8ec9448eff8603a41984f","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_archive/archive.styl","hash":"6e6f22b664199772370b59ce1678b0c148b5849f","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_about/about.styl","hash":"97fe42516ea531fdad771489b68aa8b2a7f6ae46","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/base.styl","hash":"cd55a2dce6b9d3e165a0a26d0b5bf7f649006bcd","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/color-schema.styl","hash":"32fb938d72b2d86159cb315a98b086bd17fa4415","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/inline.styl","hash":"d547ab0b91f84eb0acd0bc0c5d716ce17c30361a","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_index/index.styl","hash":"616c1f7147078c3d532dd1cfd2af09c0c3a816f0","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/rewrite.styl","hash":"c66e0da2c0d05e76a686a77ab4e74f0d2e89777d","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/keyframes.styl","hash":"94065ea50f5bef7566d184f2422f6ac20866ba22","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_category/categories.styl","hash":"1ab7db37c2f7dc7ccdb994dcb41c16a4c8920397","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_post/post.styl","hash":"3a6b4f8a29648d9d2c1e99b52a7b42df3f15cf62","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_post/tag_plugin.styl","hash":"b89b96c8a6a433a6f372b42710554b05cab85a24","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_tag/tags.styl","hash":"65bfc01c76abc927fa1a23bf2422892b0d566c3f","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_links/links.styl","hash":"7e32a3268accf3d524209c213e15e2d5d5e2e1a6","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/banner.styl","hash":"30f8fab95a5214d79df0ccc02b937df8bd885676","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/copy-btn.styl","hash":"9f932ca3f9625c13aa5353f58319881e62c0c653","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/board.styl","hash":"32d90bcc8bf2fd5d8d78e86a567973d4b69bcfa1","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/footer.styl","hash":"0ce7552dc4993926426019398d73e817cfd841a1","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/header.styl","hash":"d8011325756eb6e4ce619b3e7b4d6d80c2de8a57","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/footnote.styl","hash":"ae9289cc89649af2042907f8a003303b987f3404","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/search.styl","hash":"10f7e91a91e681fb9fe46f9df7707b9ef78707c8","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/scroll-btn.styl","hash":"55e10a6965462f8f62f85e75fd5e143af02a4b44","modified":499162500000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/qrcode.styl","hash":"78704a94c0436097abfb0e0a57abeb3429c749b7","modified":499162500000},{"_id":"themes/fluid/source/img/default.png","hash":"20911d7b80d93d259083de2ab505a2d85c06f14d","modified":1638010726000},{"_id":"public/local-search.xml","hash":"e6d2a5f11f12344d5222bea8e0bbf66da3a25c58","modified":1680106794041},{"_id":"public/2023/03/10/GPT/index.html","hash":"8f9ba0570b5fb7ecbd6fce6ff01ee0acc9b62c5e","modified":1680106794041},{"_id":"public/2022/12/30/知识图谱-基本概念/index.html","hash":"79ab7bde224ea7d5f78ec4543341accab1add96a","modified":1680106794041},{"_id":"public/2022/11/22/T5 && mT5/index.html","hash":"e40bc9dac60eafc4a8c62d628fda4adc200919e3","modified":1680106794041},{"_id":"public/2022/10/21/SimCSE总结/index.html","hash":"7a853a4cfc54a9c7051d2afcc38d86b55f18007f","modified":1680106794041},{"_id":"public/2022/09/24/EDA和AEDA/index.html","hash":"30f3c664d372b8e58cd120190ab712a752be86dc","modified":1680106794041},{"_id":"public/2022/08/29/BART总结/index.html","hash":"5c689a4c7c907e42cbb579582c59816605f6b6af","modified":1680106794041},{"_id":"public/2022/08/09/RoBERTa总结/index.html","hash":"5550d91ca966b19de286f761ff6f874d69c06c42","modified":1680106794041},{"_id":"public/2022/07/17/代码总结/index.html","hash":"c63cad610c7f007fd094e27d30f6885236418206","modified":1680106794041},{"_id":"public/2022/06/14/HMM和CRF/index.html","hash":"a29dfd44dcdf347e72aa346a4cb2fd0351716b10","modified":1680106794041},{"_id":"public/2022/05/26/最大熵模型/index.html","hash":"eae60e8f6f294dc3a97a7ffe1d9843a34d21d187","modified":1680106794041},{"_id":"public/2022/05/17/集成学习/index.html","hash":"a11a5067ed497e5404d03b68343f53994cea6a7d","modified":1680106794041},{"_id":"public/2022/05/07/决策树/index.html","hash":"ba3b46471001835719f9d76f7188364410ebbf35","modified":1680106794041},{"_id":"public/2022/04/26/贝叶斯分类器/index.html","hash":"8d11fece8a24ef68d91fc763cb2ccfa255294fd0","modified":1680106794041},{"_id":"public/2022/04/06/SVM/index.html","hash":"9d1d8e5c80860d87447b747d6c0629bdf53fde2e","modified":1680106794041},{"_id":"public/2022/03/25/KNN/index.html","hash":"8f4f39364065acbc6ca39e2314647abcd39d262c","modified":1680106794041},{"_id":"public/2022/02/25/Bert总结/index.html","hash":"b7cb5f0031f9f3127015b6e394e1aa773d447204","modified":1680106794041},{"_id":"public/2022/01/25/Transformer总结/index.html","hash":"f1066f5c471f90c3c38088029d557c590d4223af","modified":1680106794041},{"_id":"public/2022/01/08/NLP基础/index.html","hash":"f55a8bfdda699f4cb84067c9929e512a566bdad9","modified":1680106794041},{"_id":"public/2021/12/16/CV基础/index.html","hash":"67bd6dedb989f6fce183f1d3e25b2d81fb7fdab5","modified":1680106794041},{"_id":"public/2021/12/07/优化算法/index.html","hash":"52a7858c4aa6784a2b431116dd187024748cd463","modified":1680106794041},{"_id":"public/2021/12/06/RNN/index.html","hash":"8ebe2f5e1071677078dc81f168db39490b51c37c","modified":1680106794041},{"_id":"public/archives/index.html","hash":"a26a5e3a1f3596b5e8a47a3c95ccc825d07c9542","modified":1680106794041},{"_id":"public/archives/page/2/index.html","hash":"41a1762460259f543d5e613a13a29c3f33920f64","modified":1680106794041},{"_id":"public/archives/page/3/index.html","hash":"cd816994b2c190ebdc29cbacea2d75c2d7f1d53c","modified":1680106794041},{"_id":"public/archives/2021/index.html","hash":"d394ec754c36e4a2508728f183738edf90c922c4","modified":1680106794041},{"_id":"public/archives/2021/09/index.html","hash":"d59fd6a73bb02c10ddf6a4d028dafb860709e3ba","modified":1680106794041},{"_id":"public/archives/2021/11/index.html","hash":"5677e818fb14a2260ccf5c8e176260de2902faec","modified":1680106794041},{"_id":"public/archives/2021/10/index.html","hash":"f0feb9e1e9fcd852efbad7093348eb0032bd35c8","modified":1680106794041},{"_id":"public/archives/2021/12/index.html","hash":"fb7dfcd26fe43d203f6df135ec605cc0061d8c11","modified":1680106794041},{"_id":"public/archives/2022/index.html","hash":"d628055d99227e7398d30ae5eabd0e960708e429","modified":1680106794041},{"_id":"public/archives/2022/page/2/index.html","hash":"1abdb716d5f9e4a6730b86fd34f6771aa6c45478","modified":1680106794041},{"_id":"public/archives/2022/01/index.html","hash":"04104164c386810043089035cd431e338fa18251","modified":1680106794041},{"_id":"public/archives/2022/02/index.html","hash":"af5307f7706f0f14de98cdb007ec939a6fef9de0","modified":1680106794041},{"_id":"public/archives/2022/03/index.html","hash":"a7f3af3729e85c93d5cb8c2ed4be263a474c4b8d","modified":1680106794041},{"_id":"public/archives/2022/04/index.html","hash":"522719d47d2e4135656e781d4cd5991eb5d5a7bc","modified":1680106794041},{"_id":"public/archives/2022/05/index.html","hash":"fec5475af34236a7602ced0b4b148f9879cceac3","modified":1680106794041},{"_id":"public/archives/2022/06/index.html","hash":"942d9d59c749b7c970d8e589bf47a65527f8e4ef","modified":1680106794041},{"_id":"public/archives/2022/07/index.html","hash":"b5ef444ceff656a6e24e17ef5ce9e53a133f113f","modified":1680106794041},{"_id":"public/archives/2022/08/index.html","hash":"c3d5c6427895ab13c8a2278c101a46bc2fe04d47","modified":1680106794041},{"_id":"public/archives/2022/09/index.html","hash":"53a2e117924c03dea0c512a406f6ff9bd133f210","modified":1680106794041},{"_id":"public/archives/2022/10/index.html","hash":"416ce714bc9d36544b1ba6be483cf700b8d2f297","modified":1680106794041},{"_id":"public/archives/2022/11/index.html","hash":"4d9fe7d5085c4c4afb8ca7d9f4c58e2092c05f17","modified":1680106794041},{"_id":"public/archives/2022/12/index.html","hash":"5ec7f081f664df27a55bb59bb57272c04187cdba","modified":1680106794041},{"_id":"public/archives/2023/index.html","hash":"772e0ebe485d3d34be401de0d03e757a15c883f1","modified":1680106794041},{"_id":"public/archives/2023/03/index.html","hash":"f8dad008b3e3352dd3fcfdf17034f8a92a076cff","modified":1680106794041},{"_id":"public/404.html","hash":"e6f216e7c59f38af744bb5159a3d1a98e1b31812","modified":1680106794041},{"_id":"public/page/3/index.html","hash":"7352c9cc68416fa4cb6043a6188ebf73a24f4d00","modified":1680106794041},{"_id":"public/tags/index.html","hash":"63ddc6216a15ade14c19dc200f246da717aecd47","modified":1680106794041},{"_id":"public/links/index.html","hash":"c41335b3a164257bb020d642a9059e9e295c8846","modified":1680106794041},{"_id":"public/categories/index.html","hash":"0aafe2f5411fd3d6305d578700f43e20f7e2ece4","modified":1680106794041},{"_id":"public/2021/11/26/CNN/index.html","hash":"a85e8fc53b8a0d2c20f9c50540d7c5bd3a374efd","modified":1680106794041},{"_id":"public/2021/11/23/不同batch size梯度下降的影响/index.html","hash":"6446a1f71da16c744f5cb2b100d87b5ae4defc80","modified":1680106794041},{"_id":"public/2021/11/13/常用损失函数和评估模型的指标/index.html","hash":"0b92ea9a93605d4b8a777f7bda630fdc7f2aeee6","modified":1680106794041},{"_id":"public/2021/10/15/深度学习基础总结/index.html","hash":"2e46ef9146d122b625360053e05081644d395579","modified":1680106794041},{"_id":"public/2021/09/15/语法/index.html","hash":"39d29cff1dfe46c0b031509da59ed8d2956567d7","modified":1680106794041},{"_id":"public/2021/10/20/激活函数的作用和比较/index.html","hash":"de48b5d1fc427a6ea8f8b1cc56d9e86633e2b55a","modified":1680106794041},{"_id":"public/index.html","hash":"d20cf1f578a4043540bce9590ae7cbc934dc77a0","modified":1680106794041},{"_id":"public/page/2/index.html","hash":"cb7000d97b17fddf8c1e6b970725a5cefa99678a","modified":1680106794041},{"_id":"public/CNAME","hash":"a2cb84b762c567313022337e01ff48f37e2c7fa3","modified":1680106794041},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1680106794041},{"_id":"public/img/favicon.png","hash":"fe1094697fe564c9e1d22f26c6bb73eb0f70217c","modified":1680106794041},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1680106794041},{"_id":"public/img/loading.gif","hash":"af8ba90e5a40164dfa52125c5889d1ad2ce62caf","modified":1680106794041},{"_id":"public/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1680106794041},{"_id":"public/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1680106794041},{"_id":"public/js/boot.js","hash":"3de344ee619da989f6dccf7c2ae459fe91075983","modified":1680106794041},{"_id":"public/js/leancloud.js","hash":"4a787cfce27045aa0a92ec22e84f2ccf30cabc4c","modified":1680106794041},{"_id":"public/js/events.js","hash":"4b9d2676c9544db9cc40a8c7d18456792299ba86","modified":1680106794041},{"_id":"public/js/color-schema.js","hash":"2520c6459b544a4e965b838f8095e2a84390a43c","modified":1680106794041},{"_id":"public/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1680106794041},{"_id":"public/js/utils.js","hash":"4a43f2700e91937650bef511fd438825b001c4c6","modified":1680106794041},{"_id":"public/js/plugins.js","hash":"d058f30bd09b28769c4d8313428ff23dfc8d52dd","modified":1680106794041},{"_id":"public/lib/hint/hint.min.css","hash":"b38df228460ebfb4c0b6085336ee2878fe85aafe","modified":1680106794041},{"_id":"public/js/local-search.js","hash":"633f0142c657805359b0197f287e12ae4bcde731","modified":1680106794041},{"_id":"public/css/main.css","hash":"80b7c3dc3f47db8b700b4437bdf45706fe504f66","modified":1680106794041},{"_id":"public/img/default.png","hash":"20911d7b80d93d259083de2ab505a2d85c06f14d","modified":1680106794041}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"BART总结","math":true,"date":"2022-08-28T16:00:00.000Z","_content":"\n\n\n- 作者提出了一种seq2seq的模型，是一种去噪自编码器，大体的设计思路是：**使用随机的噪音破坏文本，然后使用该模型将模型恢复回来**，模型的**结构是BERT的Encoder+GPT的Decoder**，取名叫做BART（Bidirectional and Auto-Regressive Transformers）\n\n- 由于BART是seq2seq的模型，所以相比于BERT，可以拿来做翻译任务。并且通过实验发现，**BART在文本生成和理解任务等方面是优于BERT的**\n\n- 这种去噪自编码器的优点是：**在无监督预训练时，可以学得更加鲁棒的特征**\n\n\n\n# 1 BART的结构\n\n- BART的结构就是BERT的Encoder+GPT的Decoder，**对于Decoder，将原本的ReLu改为了GeLu。并且参数初始化改为服从$$N(0, 0.02)$$**\n- base model分别有6个Encoder和Decoder，large model分别有12个\n- 同等的规模，BART比BERT的参数量多10%\n- **BERT和GPT和BART的对比：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809212642716.png\" alt=\"image-20220809212642716\" style=\"zoom:90%;\" />\n\nBERT适合具有**双向表征和可并行化的优点**，但是由于其不是自回归的， 并且每个词是各自独立进行预测的，所以并**不适合文本生成领域**。而GPT由于其自回归性，可以用于文本生成。所以BART就将两者结合，结合两者有点，生成一个seq2seq模型，**使输入和输出不需要对齐**，可以用于文本生成、翻译等任务。\n\n\n\n\n\n# 2 BART的Pre-training\n\n- BART的预训练通过引入噪音破坏文本再恢复文本的方式进行学习，损失采用Decoder的输出和原文本的交叉熵\n- **BART相较于其他去噪自编码器最大的优点就是：它可以应用任何文本破坏方式，而不是特定的方法**\n\n> Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption\n\n\n\n### 2.1 BART中使用的破坏文本方式\n\n- **Token Masking：**BERT的Mask策略\n\n- **Token Deletion：**随机删除词\n\n- **Text Infilling：**采样多个文本片段，每个文本片段长度服从$$\\lambda = 3$$的泊松分布**（长度也可为0）**，每个文本片段用**单个**[MASK] token替换，替换成单个[MASK]能够迫使模型学习到一个片段中所缺失的token数量\n\n- **Sentence Permutation：**按句号将文档分割成多个句子，然后随机打乱这些句子。\n- **Document Rotation：**随机均匀地选择一个token，再旋转文档使文档以该token作为起始。该任务的目的是训练模型识别文档开头\n\n- 举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809214445327.png\" alt=\"image-20220809214445327\" style=\"zoom:67%;\" />\n\n- BART的一个关键优势是噪声的随意性，可以动用任何方式(包括改变长度)对原始文本进行破坏。**这种方式让模型学习过程中更多地考虑句子的整体长度，并对输入进行更大范围的转换，从而将BERT中MLM和NSP目标统一起来。**\n\n> This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input  \n\n\n\n\n\n# 3 BART的Fine-tuning\n\n### 3.1 句子分类任务\n\n- 方法类似于使用BERT中的[CLS]。**将相同的句子同时输入Encoder和Decoder，取Decoder最后一个时间步的输出**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809230327711.png\" alt=\"image-20220809230327711\" style=\"zoom:75%;\" />\n\n- 这种方法很像seq2seq模型翻译任务中的做法，以上图为例，区别在于翻译任务只在Decoder中输入A、B、C、D，而不输入E，然后期望输出A、B、C、D、E。而在此句子分类任务中，输入A、B、C、D、E，期望输出A、B、C、D、E、Label，只取最后一个时间步的Label，用作分类。\n\n\n\n### 3.2 Token分类和序列生成\n\n- **Token分类：**将整个文档输入encoder和decoder，每个token用其对应的最上方的decoder输出值用以分类\n\n- **序列生成：**由于Decoder的自回归性，所以很适合序列生成，直接把数据输入进Encoder和Decoder（Decoder中输入的是label数据）即可\n\n \n\n### 3.3 翻译任务\n\n- 翻译任务有所不同，**在原本的Encoder前面又额外增加了一个随机初始化的Encoder**，结构如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809232721147.png\" alt=\"image-20220809232721147\" style=\"zoom: 85%;\" />\n\n- **新加的Encoder作用是：先将外语输入进去，然后通过该Encoder将其编码成带噪音的目标端语言，然后再通过BART降噪，作用和pre-training类似**\n\n> These layers are trained to essentially translate the foreign language to noised English, by propagation through BART, thereby using BART as a pre-trained target-side language model\n\n- 步骤：\n\n> 1. 冻结BART的大部分参数，仅更新新增加的encoder、BART位置嵌入和BART每个encoder第一层的自注意力输入投影矩阵\n> 2. 将所有模型参数进行少量迭代训练\n\n\n\n\n\n# 4 对比试验\n\n- 文章对比了不同预训练目标之间的影响，包括：\n\n> 1. **Language Model：**与GPT类似，训练一个从左到右的Transformer语言模型。该模型相当于BART的decoder，只是没有交叉注意(cross-attention)\n> 2. **Permuted Language Model：**该模型基于XLNet，采样1/6的token，并以自回归的随机顺序生成。为了与其他模型保持一致，这里没有引入相对位置编码和XLNet中的片段级的循环注意力机制\n> 3. **Masked Language Model：**与BERT相同，15%的token用 [MASK] token替换，训练模型重建出这些被遮蔽掉的token\n>\n> 4. **Multitask Masked Language Model：**与 UniLM 一样，使用额外self-attention mask训练带遮蔽的语言模型。自注意力遮蔽按如下比例随机选择:1/6从左到右；1/6从右到左；1/3未遮蔽；剩余的1/3中前50%的未遮蔽，其余的从左到右遮蔽\n> 5. **Masked Seq-to-Seq：**与MASS模型类似，遮蔽一个片段中50%的token，并训练一个序列到序列模型预测被遮蔽的tokens\n\n- 实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809235547649.png\" alt=\"image-20220809235547649\" style=\"zoom: 67%;\" />\n\n- 通过实验对比，总结出如下结果：\n\n> 1. 在不同的任务中，预训练方法的表现有显著差异。换句话说，预训练方法的有效性高度依赖于任务本身。比如，一个简单的语言模型在ELI5数据集上可以夺冠，但是在SQUAD上的结果却是最差的\n> 2. 遮蔽Token至关重要。只使用旋转文档或句子组合的预训练目标则效果较差，效果较好的都是使用了token的删除或遮蔽作为预训练目标。此外，在生成任务上，删除token似乎比遮蔽token更胜一筹\n> 3. 从左到右的预训练目标有助于文本生成任务。Masked Language Model和Permuted Language Model在文本生成任务上不如其他模型。而这两种模型在预训练阶段都没有用到从左到右的自回归语言模型\n> 4. 对于SQuAD而言双向的encoder至关重要。因为上下文在分类决策中至关重要\n> 5. 预训练目标并不是唯一重要的因素。这里的Permuted Language Model略逊于XLNet，其中一些差异可能是由于没有使用XLNet架构中的其他的改进，如相对位置编码和片段级的循环机制\n> 6. Language Model在ELI5数据集上技压群雄，其困惑度远优于其他模型。这表明当输出仅受到输入的松散约束时，BART较为低效\n\n- 同时实验还对比了几种文本破坏方法对任务的贡献到底有多少，发现**使用Text Infilling或Text Infilling + Sentence Shuffling得到的效果最好**\n\n\n\n\n\n# 5 在各种下游任务上的表现\n\n- 在此实验中，使用large规模的模型，预训练使用RoBerta的batch size=8000和steps=500000，以及使用BPE。预处理使用了text infilling和sentence permutation，并且mask掉了30%的token，重排所有句子  \n\n\n\n### 5.1 自然语言理解任务\n- 结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810000923604.png\" alt=\"image-20220810000923604\" style=\"zoom:80%;\" />\n\n- BART在自然语言理解任务上与其他先进模型不相上下。这表明**BART在生成任务上的进一步突破并不是以牺牲自然语言理解性能为代价**\n\n\n\n### 5.2 自然语言生成任务\n\n- 在微调时，使用了label smooth的交叉熵损失，平滑参数为0.1。并在生成时使用大小为5的束搜索\n\n- 文本摘要任务结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810001908759.png\" alt=\"image-20220810001908759\" style=\"zoom:80%;\" />\n\n- 在这两个摘要任务上，BART 在所有度量指标上均优于之前的模型，但与人类的摘要结果相比仍然有差距\n\n- 对话生成任务结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002027643.png\" alt=\"image-20220810002027643\" style=\"zoom:80%;\" />\n\n- BART 在对话生成任务上的性能同样优于之前的模型\n- 抽象QA任务结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002302848.png\" alt=\"image-20220810002302848\" style=\"zoom:80%;\" />\n\n\n\n### 5.3 翻译任务\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002422747.png\" alt=\"image-20220810002422747\" style=\"zoom:80%;\" />","source":"_posts/BART总结.md","raw":"---\ntitle: BART总结\nmath: true\ndate: 2022-8-29\n---\n\n\n\n- 作者提出了一种seq2seq的模型，是一种去噪自编码器，大体的设计思路是：**使用随机的噪音破坏文本，然后使用该模型将模型恢复回来**，模型的**结构是BERT的Encoder+GPT的Decoder**，取名叫做BART（Bidirectional and Auto-Regressive Transformers）\n\n- 由于BART是seq2seq的模型，所以相比于BERT，可以拿来做翻译任务。并且通过实验发现，**BART在文本生成和理解任务等方面是优于BERT的**\n\n- 这种去噪自编码器的优点是：**在无监督预训练时，可以学得更加鲁棒的特征**\n\n\n\n# 1 BART的结构\n\n- BART的结构就是BERT的Encoder+GPT的Decoder，**对于Decoder，将原本的ReLu改为了GeLu。并且参数初始化改为服从$$N(0, 0.02)$$**\n- base model分别有6个Encoder和Decoder，large model分别有12个\n- 同等的规模，BART比BERT的参数量多10%\n- **BERT和GPT和BART的对比：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809212642716.png\" alt=\"image-20220809212642716\" style=\"zoom:90%;\" />\n\nBERT适合具有**双向表征和可并行化的优点**，但是由于其不是自回归的， 并且每个词是各自独立进行预测的，所以并**不适合文本生成领域**。而GPT由于其自回归性，可以用于文本生成。所以BART就将两者结合，结合两者有点，生成一个seq2seq模型，**使输入和输出不需要对齐**，可以用于文本生成、翻译等任务。\n\n\n\n\n\n# 2 BART的Pre-training\n\n- BART的预训练通过引入噪音破坏文本再恢复文本的方式进行学习，损失采用Decoder的输出和原文本的交叉熵\n- **BART相较于其他去噪自编码器最大的优点就是：它可以应用任何文本破坏方式，而不是特定的方法**\n\n> Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption\n\n\n\n### 2.1 BART中使用的破坏文本方式\n\n- **Token Masking：**BERT的Mask策略\n\n- **Token Deletion：**随机删除词\n\n- **Text Infilling：**采样多个文本片段，每个文本片段长度服从$$\\lambda = 3$$的泊松分布**（长度也可为0）**，每个文本片段用**单个**[MASK] token替换，替换成单个[MASK]能够迫使模型学习到一个片段中所缺失的token数量\n\n- **Sentence Permutation：**按句号将文档分割成多个句子，然后随机打乱这些句子。\n- **Document Rotation：**随机均匀地选择一个token，再旋转文档使文档以该token作为起始。该任务的目的是训练模型识别文档开头\n\n- 举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809214445327.png\" alt=\"image-20220809214445327\" style=\"zoom:67%;\" />\n\n- BART的一个关键优势是噪声的随意性，可以动用任何方式(包括改变长度)对原始文本进行破坏。**这种方式让模型学习过程中更多地考虑句子的整体长度，并对输入进行更大范围的转换，从而将BERT中MLM和NSP目标统一起来。**\n\n> This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input  \n\n\n\n\n\n# 3 BART的Fine-tuning\n\n### 3.1 句子分类任务\n\n- 方法类似于使用BERT中的[CLS]。**将相同的句子同时输入Encoder和Decoder，取Decoder最后一个时间步的输出**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809230327711.png\" alt=\"image-20220809230327711\" style=\"zoom:75%;\" />\n\n- 这种方法很像seq2seq模型翻译任务中的做法，以上图为例，区别在于翻译任务只在Decoder中输入A、B、C、D，而不输入E，然后期望输出A、B、C、D、E。而在此句子分类任务中，输入A、B、C、D、E，期望输出A、B、C、D、E、Label，只取最后一个时间步的Label，用作分类。\n\n\n\n### 3.2 Token分类和序列生成\n\n- **Token分类：**将整个文档输入encoder和decoder，每个token用其对应的最上方的decoder输出值用以分类\n\n- **序列生成：**由于Decoder的自回归性，所以很适合序列生成，直接把数据输入进Encoder和Decoder（Decoder中输入的是label数据）即可\n\n \n\n### 3.3 翻译任务\n\n- 翻译任务有所不同，**在原本的Encoder前面又额外增加了一个随机初始化的Encoder**，结构如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809232721147.png\" alt=\"image-20220809232721147\" style=\"zoom: 85%;\" />\n\n- **新加的Encoder作用是：先将外语输入进去，然后通过该Encoder将其编码成带噪音的目标端语言，然后再通过BART降噪，作用和pre-training类似**\n\n> These layers are trained to essentially translate the foreign language to noised English, by propagation through BART, thereby using BART as a pre-trained target-side language model\n\n- 步骤：\n\n> 1. 冻结BART的大部分参数，仅更新新增加的encoder、BART位置嵌入和BART每个encoder第一层的自注意力输入投影矩阵\n> 2. 将所有模型参数进行少量迭代训练\n\n\n\n\n\n# 4 对比试验\n\n- 文章对比了不同预训练目标之间的影响，包括：\n\n> 1. **Language Model：**与GPT类似，训练一个从左到右的Transformer语言模型。该模型相当于BART的decoder，只是没有交叉注意(cross-attention)\n> 2. **Permuted Language Model：**该模型基于XLNet，采样1/6的token，并以自回归的随机顺序生成。为了与其他模型保持一致，这里没有引入相对位置编码和XLNet中的片段级的循环注意力机制\n> 3. **Masked Language Model：**与BERT相同，15%的token用 [MASK] token替换，训练模型重建出这些被遮蔽掉的token\n>\n> 4. **Multitask Masked Language Model：**与 UniLM 一样，使用额外self-attention mask训练带遮蔽的语言模型。自注意力遮蔽按如下比例随机选择:1/6从左到右；1/6从右到左；1/3未遮蔽；剩余的1/3中前50%的未遮蔽，其余的从左到右遮蔽\n> 5. **Masked Seq-to-Seq：**与MASS模型类似，遮蔽一个片段中50%的token，并训练一个序列到序列模型预测被遮蔽的tokens\n\n- 实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809235547649.png\" alt=\"image-20220809235547649\" style=\"zoom: 67%;\" />\n\n- 通过实验对比，总结出如下结果：\n\n> 1. 在不同的任务中，预训练方法的表现有显著差异。换句话说，预训练方法的有效性高度依赖于任务本身。比如，一个简单的语言模型在ELI5数据集上可以夺冠，但是在SQUAD上的结果却是最差的\n> 2. 遮蔽Token至关重要。只使用旋转文档或句子组合的预训练目标则效果较差，效果较好的都是使用了token的删除或遮蔽作为预训练目标。此外，在生成任务上，删除token似乎比遮蔽token更胜一筹\n> 3. 从左到右的预训练目标有助于文本生成任务。Masked Language Model和Permuted Language Model在文本生成任务上不如其他模型。而这两种模型在预训练阶段都没有用到从左到右的自回归语言模型\n> 4. 对于SQuAD而言双向的encoder至关重要。因为上下文在分类决策中至关重要\n> 5. 预训练目标并不是唯一重要的因素。这里的Permuted Language Model略逊于XLNet，其中一些差异可能是由于没有使用XLNet架构中的其他的改进，如相对位置编码和片段级的循环机制\n> 6. Language Model在ELI5数据集上技压群雄，其困惑度远优于其他模型。这表明当输出仅受到输入的松散约束时，BART较为低效\n\n- 同时实验还对比了几种文本破坏方法对任务的贡献到底有多少，发现**使用Text Infilling或Text Infilling + Sentence Shuffling得到的效果最好**\n\n\n\n\n\n# 5 在各种下游任务上的表现\n\n- 在此实验中，使用large规模的模型，预训练使用RoBerta的batch size=8000和steps=500000，以及使用BPE。预处理使用了text infilling和sentence permutation，并且mask掉了30%的token，重排所有句子  \n\n\n\n### 5.1 自然语言理解任务\n- 结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810000923604.png\" alt=\"image-20220810000923604\" style=\"zoom:80%;\" />\n\n- BART在自然语言理解任务上与其他先进模型不相上下。这表明**BART在生成任务上的进一步突破并不是以牺牲自然语言理解性能为代价**\n\n\n\n### 5.2 自然语言生成任务\n\n- 在微调时，使用了label smooth的交叉熵损失，平滑参数为0.1。并在生成时使用大小为5的束搜索\n\n- 文本摘要任务结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810001908759.png\" alt=\"image-20220810001908759\" style=\"zoom:80%;\" />\n\n- 在这两个摘要任务上，BART 在所有度量指标上均优于之前的模型，但与人类的摘要结果相比仍然有差距\n\n- 对话生成任务结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002027643.png\" alt=\"image-20220810002027643\" style=\"zoom:80%;\" />\n\n- BART 在对话生成任务上的性能同样优于之前的模型\n- 抽象QA任务结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002302848.png\" alt=\"image-20220810002302848\" style=\"zoom:80%;\" />\n\n\n\n### 5.3 翻译任务\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002422747.png\" alt=\"image-20220810002422747\" style=\"zoom:80%;\" />","slug":"BART总结","published":1,"updated":"2022-12-20T06:24:29.773Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1200007csz0gx9fvlt","content":"<ul>\n<li><p>作者提出了一种seq2seq的模型，是一种去噪自编码器，大体的设计思路是：<strong>使用随机的噪音破坏文本，然后使用该模型将模型恢复回来</strong>，模型的<strong>结构是BERT的Encoder+GPT的Decoder</strong>，取名叫做BART（Bidirectional and Auto-Regressive Transformers）</p>\n</li>\n<li><p>由于BART是seq2seq的模型，所以相比于BERT，可以拿来做翻译任务。并且通过实验发现，<strong>BART在文本生成和理解任务等方面是优于BERT的</strong></p>\n</li>\n<li><p>这种去噪自编码器的优点是：<strong>在无监督预训练时，可以学得更加鲁棒的特征</strong></p>\n</li>\n</ul>\n<h1 id=\"1-BART的结构\"><a href=\"#1-BART的结构\" class=\"headerlink\" title=\"1 BART的结构\"></a>1 BART的结构</h1><ul>\n<li>BART的结构就是BERT的Encoder+GPT的Decoder，<strong>对于Decoder，将原本的ReLu改为了GeLu。并且参数初始化改为服从<script type=\"math/tex\">N(0, 0.02)</script></strong></li>\n<li>base model分别有6个Encoder和Decoder，large model分别有12个</li>\n<li>同等的规模，BART比BERT的参数量多10%</li>\n<li><strong>BERT和GPT和BART的对比：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809212642716.png\" alt=\"image-20220809212642716\" style=\"zoom:90%;\" /></p>\n<p>BERT适合具有<strong>双向表征和可并行化的优点</strong>，但是由于其不是自回归的， 并且每个词是各自独立进行预测的，所以并<strong>不适合文本生成领域</strong>。而GPT由于其自回归性，可以用于文本生成。所以BART就将两者结合，结合两者有点，生成一个seq2seq模型，<strong>使输入和输出不需要对齐</strong>，可以用于文本生成、翻译等任务。</p>\n<h1 id=\"2-BART的Pre-training\"><a href=\"#2-BART的Pre-training\" class=\"headerlink\" title=\"2 BART的Pre-training\"></a>2 BART的Pre-training</h1><ul>\n<li>BART的预训练通过引入噪音破坏文本再恢复文本的方式进行学习，损失采用Decoder的输出和原文本的交叉熵</li>\n<li><strong>BART相较于其他去噪自编码器最大的优点就是：它可以应用任何文本破坏方式，而不是特定的方法</strong></li>\n</ul>\n<blockquote>\n<p>Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption</p>\n</blockquote>\n<h3 id=\"2-1-BART中使用的破坏文本方式\"><a href=\"#2-1-BART中使用的破坏文本方式\" class=\"headerlink\" title=\"2.1 BART中使用的破坏文本方式\"></a>2.1 BART中使用的破坏文本方式</h3><ul>\n<li><p><strong>Token Masking：</strong>BERT的Mask策略</p>\n</li>\n<li><p><strong>Token Deletion：</strong>随机删除词</p>\n</li>\n<li><p><strong>Text Infilling：</strong>采样多个文本片段，每个文本片段长度服从<script type=\"math/tex\">\\lambda = 3</script>的泊松分布<strong>（长度也可为0）</strong>，每个文本片段用<strong>单个</strong>[MASK] token替换，替换成单个[MASK]能够迫使模型学习到一个片段中所缺失的token数量</p>\n</li>\n<li><p><strong>Sentence Permutation：</strong>按句号将文档分割成多个句子，然后随机打乱这些句子。</p>\n</li>\n<li><p><strong>Document Rotation：</strong>随机均匀地选择一个token，再旋转文档使文档以该token作为起始。该任务的目的是训练模型识别文档开头</p>\n</li>\n<li><p>举个栗子：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809214445327.png\" alt=\"image-20220809214445327\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>BART的一个关键优势是噪声的随意性，可以动用任何方式(包括改变长度)对原始文本进行破坏。<strong>这种方式让模型学习过程中更多地考虑句子的整体长度，并对输入进行更大范围的转换，从而将BERT中MLM和NSP目标统一起来。</strong></li>\n</ul>\n<blockquote>\n<p>This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input  </p>\n</blockquote>\n<h1 id=\"3-BART的Fine-tuning\"><a href=\"#3-BART的Fine-tuning\" class=\"headerlink\" title=\"3 BART的Fine-tuning\"></a>3 BART的Fine-tuning</h1><h3 id=\"3-1-句子分类任务\"><a href=\"#3-1-句子分类任务\" class=\"headerlink\" title=\"3.1 句子分类任务\"></a>3.1 句子分类任务</h3><ul>\n<li>方法类似于使用BERT中的[CLS]。<strong>将相同的句子同时输入Encoder和Decoder，取Decoder最后一个时间步的输出</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809230327711.png\" alt=\"image-20220809230327711\" style=\"zoom:75%;\" /></p>\n<ul>\n<li>这种方法很像seq2seq模型翻译任务中的做法，以上图为例，区别在于翻译任务只在Decoder中输入A、B、C、D，而不输入E，然后期望输出A、B、C、D、E。而在此句子分类任务中，输入A、B、C、D、E，期望输出A、B、C、D、E、Label，只取最后一个时间步的Label，用作分类。</li>\n</ul>\n<h3 id=\"3-2-Token分类和序列生成\"><a href=\"#3-2-Token分类和序列生成\" class=\"headerlink\" title=\"3.2 Token分类和序列生成\"></a>3.2 Token分类和序列生成</h3><ul>\n<li><p><strong>Token分类：</strong>将整个文档输入encoder和decoder，每个token用其对应的最上方的decoder输出值用以分类</p>\n</li>\n<li><p><strong>序列生成：</strong>由于Decoder的自回归性，所以很适合序列生成，直接把数据输入进Encoder和Decoder（Decoder中输入的是label数据）即可</p>\n</li>\n</ul>\n<h3 id=\"3-3-翻译任务\"><a href=\"#3-3-翻译任务\" class=\"headerlink\" title=\"3.3 翻译任务\"></a>3.3 翻译任务</h3><ul>\n<li>翻译任务有所不同，<strong>在原本的Encoder前面又额外增加了一个随机初始化的Encoder</strong>，结构如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809232721147.png\" alt=\"image-20220809232721147\" style=\"zoom: 85%;\" /></p>\n<ul>\n<li><strong>新加的Encoder作用是：先将外语输入进去，然后通过该Encoder将其编码成带噪音的目标端语言，然后再通过BART降噪，作用和pre-training类似</strong></li>\n</ul>\n<blockquote>\n<p>These layers are trained to essentially translate the foreign language to noised English, by propagation through BART, thereby using BART as a pre-trained target-side language model</p>\n</blockquote>\n<ul>\n<li>步骤：</li>\n</ul>\n<blockquote>\n<ol>\n<li>冻结BART的大部分参数，仅更新新增加的encoder、BART位置嵌入和BART每个encoder第一层的自注意力输入投影矩阵</li>\n<li>将所有模型参数进行少量迭代训练</li>\n</ol>\n</blockquote>\n<h1 id=\"4-对比试验\"><a href=\"#4-对比试验\" class=\"headerlink\" title=\"4 对比试验\"></a>4 对比试验</h1><ul>\n<li>文章对比了不同预训练目标之间的影响，包括：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Language Model：</strong>与GPT类似，训练一个从左到右的Transformer语言模型。该模型相当于BART的decoder，只是没有交叉注意(cross-attention)</li>\n<li><strong>Permuted Language Model：</strong>该模型基于XLNet，采样1/6的token，并以自回归的随机顺序生成。为了与其他模型保持一致，这里没有引入相对位置编码和XLNet中的片段级的循环注意力机制</li>\n<li><p><strong>Masked Language Model：</strong>与BERT相同，15%的token用 [MASK] token替换，训练模型重建出这些被遮蔽掉的token</p>\n</li>\n<li><p><strong>Multitask Masked Language Model：</strong>与 UniLM 一样，使用额外self-attention mask训练带遮蔽的语言模型。自注意力遮蔽按如下比例随机选择:1/6从左到右；1/6从右到左；1/3未遮蔽；剩余的1/3中前50%的未遮蔽，其余的从左到右遮蔽</p>\n</li>\n<li><strong>Masked Seq-to-Seq：</strong>与MASS模型类似，遮蔽一个片段中50%的token，并训练一个序列到序列模型预测被遮蔽的tokens</li>\n</ol>\n</blockquote>\n<ul>\n<li>实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809235547649.png\" alt=\"image-20220809235547649\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>通过实验对比，总结出如下结果：</li>\n</ul>\n<blockquote>\n<ol>\n<li>在不同的任务中，预训练方法的表现有显著差异。换句话说，预训练方法的有效性高度依赖于任务本身。比如，一个简单的语言模型在ELI5数据集上可以夺冠，但是在SQUAD上的结果却是最差的</li>\n<li>遮蔽Token至关重要。只使用旋转文档或句子组合的预训练目标则效果较差，效果较好的都是使用了token的删除或遮蔽作为预训练目标。此外，在生成任务上，删除token似乎比遮蔽token更胜一筹</li>\n<li>从左到右的预训练目标有助于文本生成任务。Masked Language Model和Permuted Language Model在文本生成任务上不如其他模型。而这两种模型在预训练阶段都没有用到从左到右的自回归语言模型</li>\n<li>对于SQuAD而言双向的encoder至关重要。因为上下文在分类决策中至关重要</li>\n<li>预训练目标并不是唯一重要的因素。这里的Permuted Language Model略逊于XLNet，其中一些差异可能是由于没有使用XLNet架构中的其他的改进，如相对位置编码和片段级的循环机制</li>\n<li>Language Model在ELI5数据集上技压群雄，其困惑度远优于其他模型。这表明当输出仅受到输入的松散约束时，BART较为低效</li>\n</ol>\n</blockquote>\n<ul>\n<li>同时实验还对比了几种文本破坏方法对任务的贡献到底有多少，发现<strong>使用Text Infilling或Text Infilling + Sentence Shuffling得到的效果最好</strong></li>\n</ul>\n<h1 id=\"5-在各种下游任务上的表现\"><a href=\"#5-在各种下游任务上的表现\" class=\"headerlink\" title=\"5 在各种下游任务上的表现\"></a>5 在各种下游任务上的表现</h1><ul>\n<li>在此实验中，使用large规模的模型，预训练使用RoBerta的batch size=8000和steps=500000，以及使用BPE。预处理使用了text infilling和sentence permutation，并且mask掉了30%的token，重排所有句子  </li>\n</ul>\n<h3 id=\"5-1-自然语言理解任务\"><a href=\"#5-1-自然语言理解任务\" class=\"headerlink\" title=\"5.1 自然语言理解任务\"></a>5.1 自然语言理解任务</h3><ul>\n<li>结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810000923604.png\" alt=\"image-20220810000923604\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>BART在自然语言理解任务上与其他先进模型不相上下。这表明<strong>BART在生成任务上的进一步突破并不是以牺牲自然语言理解性能为代价</strong></li>\n</ul>\n<h3 id=\"5-2-自然语言生成任务\"><a href=\"#5-2-自然语言生成任务\" class=\"headerlink\" title=\"5.2 自然语言生成任务\"></a>5.2 自然语言生成任务</h3><ul>\n<li><p>在微调时，使用了label smooth的交叉熵损失，平滑参数为0.1。并在生成时使用大小为5的束搜索</p>\n</li>\n<li><p>文本摘要任务结果：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810001908759.png\" alt=\"image-20220810001908759\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><p>在这两个摘要任务上，BART 在所有度量指标上均优于之前的模型，但与人类的摘要结果相比仍然有差距</p>\n</li>\n<li><p>对话生成任务结果：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002027643.png\" alt=\"image-20220810002027643\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>BART 在对话生成任务上的性能同样优于之前的模型</li>\n<li>抽象QA任务结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002302848.png\" alt=\"image-20220810002302848\" style=\"zoom:80%;\" /></p>\n<h3 id=\"5-3-翻译任务\"><a href=\"#5-3-翻译任务\" class=\"headerlink\" title=\"5.3 翻译任务\"></a>5.3 翻译任务</h3><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002422747.png\" alt=\"image-20220810002422747\" style=\"zoom:80%;\" /></p>\n","site":{"data":{}},"wordcount":3626,"excerpt":"","more":"<ul>\n<li><p>作者提出了一种seq2seq的模型，是一种去噪自编码器，大体的设计思路是：<strong>使用随机的噪音破坏文本，然后使用该模型将模型恢复回来</strong>，模型的<strong>结构是BERT的Encoder+GPT的Decoder</strong>，取名叫做BART（Bidirectional and Auto-Regressive Transformers）</p>\n</li>\n<li><p>由于BART是seq2seq的模型，所以相比于BERT，可以拿来做翻译任务。并且通过实验发现，<strong>BART在文本生成和理解任务等方面是优于BERT的</strong></p>\n</li>\n<li><p>这种去噪自编码器的优点是：<strong>在无监督预训练时，可以学得更加鲁棒的特征</strong></p>\n</li>\n</ul>\n<h1 id=\"1-BART的结构\"><a href=\"#1-BART的结构\" class=\"headerlink\" title=\"1 BART的结构\"></a>1 BART的结构</h1><ul>\n<li>BART的结构就是BERT的Encoder+GPT的Decoder，<strong>对于Decoder，将原本的ReLu改为了GeLu。并且参数初始化改为服从<script type=\"math/tex\">N(0, 0.02)</script></strong></li>\n<li>base model分别有6个Encoder和Decoder，large model分别有12个</li>\n<li>同等的规模，BART比BERT的参数量多10%</li>\n<li><strong>BERT和GPT和BART的对比：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809212642716.png\" alt=\"image-20220809212642716\" style=\"zoom:90%;\" /></p>\n<p>BERT适合具有<strong>双向表征和可并行化的优点</strong>，但是由于其不是自回归的， 并且每个词是各自独立进行预测的，所以并<strong>不适合文本生成领域</strong>。而GPT由于其自回归性，可以用于文本生成。所以BART就将两者结合，结合两者有点，生成一个seq2seq模型，<strong>使输入和输出不需要对齐</strong>，可以用于文本生成、翻译等任务。</p>\n<h1 id=\"2-BART的Pre-training\"><a href=\"#2-BART的Pre-training\" class=\"headerlink\" title=\"2 BART的Pre-training\"></a>2 BART的Pre-training</h1><ul>\n<li>BART的预训练通过引入噪音破坏文本再恢复文本的方式进行学习，损失采用Decoder的输出和原文本的交叉熵</li>\n<li><strong>BART相较于其他去噪自编码器最大的优点就是：它可以应用任何文本破坏方式，而不是特定的方法</strong></li>\n</ul>\n<blockquote>\n<p>Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption</p>\n</blockquote>\n<h3 id=\"2-1-BART中使用的破坏文本方式\"><a href=\"#2-1-BART中使用的破坏文本方式\" class=\"headerlink\" title=\"2.1 BART中使用的破坏文本方式\"></a>2.1 BART中使用的破坏文本方式</h3><ul>\n<li><p><strong>Token Masking：</strong>BERT的Mask策略</p>\n</li>\n<li><p><strong>Token Deletion：</strong>随机删除词</p>\n</li>\n<li><p><strong>Text Infilling：</strong>采样多个文本片段，每个文本片段长度服从<script type=\"math/tex\">\\lambda = 3</script>的泊松分布<strong>（长度也可为0）</strong>，每个文本片段用<strong>单个</strong>[MASK] token替换，替换成单个[MASK]能够迫使模型学习到一个片段中所缺失的token数量</p>\n</li>\n<li><p><strong>Sentence Permutation：</strong>按句号将文档分割成多个句子，然后随机打乱这些句子。</p>\n</li>\n<li><p><strong>Document Rotation：</strong>随机均匀地选择一个token，再旋转文档使文档以该token作为起始。该任务的目的是训练模型识别文档开头</p>\n</li>\n<li><p>举个栗子：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809214445327.png\" alt=\"image-20220809214445327\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>BART的一个关键优势是噪声的随意性，可以动用任何方式(包括改变长度)对原始文本进行破坏。<strong>这种方式让模型学习过程中更多地考虑句子的整体长度，并对输入进行更大范围的转换，从而将BERT中MLM和NSP目标统一起来。</strong></li>\n</ul>\n<blockquote>\n<p>This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input  </p>\n</blockquote>\n<h1 id=\"3-BART的Fine-tuning\"><a href=\"#3-BART的Fine-tuning\" class=\"headerlink\" title=\"3 BART的Fine-tuning\"></a>3 BART的Fine-tuning</h1><h3 id=\"3-1-句子分类任务\"><a href=\"#3-1-句子分类任务\" class=\"headerlink\" title=\"3.1 句子分类任务\"></a>3.1 句子分类任务</h3><ul>\n<li>方法类似于使用BERT中的[CLS]。<strong>将相同的句子同时输入Encoder和Decoder，取Decoder最后一个时间步的输出</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809230327711.png\" alt=\"image-20220809230327711\" style=\"zoom:75%;\" /></p>\n<ul>\n<li>这种方法很像seq2seq模型翻译任务中的做法，以上图为例，区别在于翻译任务只在Decoder中输入A、B、C、D，而不输入E，然后期望输出A、B、C、D、E。而在此句子分类任务中，输入A、B、C、D、E，期望输出A、B、C、D、E、Label，只取最后一个时间步的Label，用作分类。</li>\n</ul>\n<h3 id=\"3-2-Token分类和序列生成\"><a href=\"#3-2-Token分类和序列生成\" class=\"headerlink\" title=\"3.2 Token分类和序列生成\"></a>3.2 Token分类和序列生成</h3><ul>\n<li><p><strong>Token分类：</strong>将整个文档输入encoder和decoder，每个token用其对应的最上方的decoder输出值用以分类</p>\n</li>\n<li><p><strong>序列生成：</strong>由于Decoder的自回归性，所以很适合序列生成，直接把数据输入进Encoder和Decoder（Decoder中输入的是label数据）即可</p>\n</li>\n</ul>\n<h3 id=\"3-3-翻译任务\"><a href=\"#3-3-翻译任务\" class=\"headerlink\" title=\"3.3 翻译任务\"></a>3.3 翻译任务</h3><ul>\n<li>翻译任务有所不同，<strong>在原本的Encoder前面又额外增加了一个随机初始化的Encoder</strong>，结构如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809232721147.png\" alt=\"image-20220809232721147\" style=\"zoom: 85%;\" /></p>\n<ul>\n<li><strong>新加的Encoder作用是：先将外语输入进去，然后通过该Encoder将其编码成带噪音的目标端语言，然后再通过BART降噪，作用和pre-training类似</strong></li>\n</ul>\n<blockquote>\n<p>These layers are trained to essentially translate the foreign language to noised English, by propagation through BART, thereby using BART as a pre-trained target-side language model</p>\n</blockquote>\n<ul>\n<li>步骤：</li>\n</ul>\n<blockquote>\n<ol>\n<li>冻结BART的大部分参数，仅更新新增加的encoder、BART位置嵌入和BART每个encoder第一层的自注意力输入投影矩阵</li>\n<li>将所有模型参数进行少量迭代训练</li>\n</ol>\n</blockquote>\n<h1 id=\"4-对比试验\"><a href=\"#4-对比试验\" class=\"headerlink\" title=\"4 对比试验\"></a>4 对比试验</h1><ul>\n<li>文章对比了不同预训练目标之间的影响，包括：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Language Model：</strong>与GPT类似，训练一个从左到右的Transformer语言模型。该模型相当于BART的decoder，只是没有交叉注意(cross-attention)</li>\n<li><strong>Permuted Language Model：</strong>该模型基于XLNet，采样1/6的token，并以自回归的随机顺序生成。为了与其他模型保持一致，这里没有引入相对位置编码和XLNet中的片段级的循环注意力机制</li>\n<li><p><strong>Masked Language Model：</strong>与BERT相同，15%的token用 [MASK] token替换，训练模型重建出这些被遮蔽掉的token</p>\n</li>\n<li><p><strong>Multitask Masked Language Model：</strong>与 UniLM 一样，使用额外self-attention mask训练带遮蔽的语言模型。自注意力遮蔽按如下比例随机选择:1/6从左到右；1/6从右到左；1/3未遮蔽；剩余的1/3中前50%的未遮蔽，其余的从左到右遮蔽</p>\n</li>\n<li><strong>Masked Seq-to-Seq：</strong>与MASS模型类似，遮蔽一个片段中50%的token，并训练一个序列到序列模型预测被遮蔽的tokens</li>\n</ol>\n</blockquote>\n<ul>\n<li>实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809235547649.png\" alt=\"image-20220809235547649\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>通过实验对比，总结出如下结果：</li>\n</ul>\n<blockquote>\n<ol>\n<li>在不同的任务中，预训练方法的表现有显著差异。换句话说，预训练方法的有效性高度依赖于任务本身。比如，一个简单的语言模型在ELI5数据集上可以夺冠，但是在SQUAD上的结果却是最差的</li>\n<li>遮蔽Token至关重要。只使用旋转文档或句子组合的预训练目标则效果较差，效果较好的都是使用了token的删除或遮蔽作为预训练目标。此外，在生成任务上，删除token似乎比遮蔽token更胜一筹</li>\n<li>从左到右的预训练目标有助于文本生成任务。Masked Language Model和Permuted Language Model在文本生成任务上不如其他模型。而这两种模型在预训练阶段都没有用到从左到右的自回归语言模型</li>\n<li>对于SQuAD而言双向的encoder至关重要。因为上下文在分类决策中至关重要</li>\n<li>预训练目标并不是唯一重要的因素。这里的Permuted Language Model略逊于XLNet，其中一些差异可能是由于没有使用XLNet架构中的其他的改进，如相对位置编码和片段级的循环机制</li>\n<li>Language Model在ELI5数据集上技压群雄，其困惑度远优于其他模型。这表明当输出仅受到输入的松散约束时，BART较为低效</li>\n</ol>\n</blockquote>\n<ul>\n<li>同时实验还对比了几种文本破坏方法对任务的贡献到底有多少，发现<strong>使用Text Infilling或Text Infilling + Sentence Shuffling得到的效果最好</strong></li>\n</ul>\n<h1 id=\"5-在各种下游任务上的表现\"><a href=\"#5-在各种下游任务上的表现\" class=\"headerlink\" title=\"5 在各种下游任务上的表现\"></a>5 在各种下游任务上的表现</h1><ul>\n<li>在此实验中，使用large规模的模型，预训练使用RoBerta的batch size=8000和steps=500000，以及使用BPE。预处理使用了text infilling和sentence permutation，并且mask掉了30%的token，重排所有句子  </li>\n</ul>\n<h3 id=\"5-1-自然语言理解任务\"><a href=\"#5-1-自然语言理解任务\" class=\"headerlink\" title=\"5.1 自然语言理解任务\"></a>5.1 自然语言理解任务</h3><ul>\n<li>结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810000923604.png\" alt=\"image-20220810000923604\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>BART在自然语言理解任务上与其他先进模型不相上下。这表明<strong>BART在生成任务上的进一步突破并不是以牺牲自然语言理解性能为代价</strong></li>\n</ul>\n<h3 id=\"5-2-自然语言生成任务\"><a href=\"#5-2-自然语言生成任务\" class=\"headerlink\" title=\"5.2 自然语言生成任务\"></a>5.2 自然语言生成任务</h3><ul>\n<li><p>在微调时，使用了label smooth的交叉熵损失，平滑参数为0.1。并在生成时使用大小为5的束搜索</p>\n</li>\n<li><p>文本摘要任务结果：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810001908759.png\" alt=\"image-20220810001908759\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><p>在这两个摘要任务上，BART 在所有度量指标上均优于之前的模型，但与人类的摘要结果相比仍然有差距</p>\n</li>\n<li><p>对话生成任务结果：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002027643.png\" alt=\"image-20220810002027643\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>BART 在对话生成任务上的性能同样优于之前的模型</li>\n<li>抽象QA任务结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002302848.png\" alt=\"image-20220810002302848\" style=\"zoom:80%;\" /></p>\n<h3 id=\"5-3-翻译任务\"><a href=\"#5-3-翻译任务\" class=\"headerlink\" title=\"5.3 翻译任务\"></a>5.3 翻译任务</h3><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810002422747.png\" alt=\"image-20220810002422747\" style=\"zoom:80%;\" /></p>\n"},{"title":"BERT总结","math":"ture","date":"2022-02-24T16:00:00.000Z","_content":"\n- BERT是一种通过**在预训练时使用无监督**方法能在**每一层**实现**双向**表征的语言模型，并且使用微调的方法，在**具体的下游任务时不需要task-specific architecture**，只需要添加一两层和少部分参数，十分易于迁移\n\n- BERT带来的主要提升是解决了双向性的问题。如OpenAI GPT使用的是left-to-right（LTR）Transformer结构，失去了双向性。又比如ELMo使用简单的将left-to-right（LTR）的LSTM和right-to-left（RTL）的LSTM在最后简单的连结来实现双向性，而BERT能在每一层都实现双向，并且相比于在最后简单的连结更具有直观性和可解释性。\n\n\n\n\n\n# 1 BERT的结构\n\n### 1.1 结构和规模\n\n- BERT的结构十分简单，就是由**多个Transformer的encoder组合而成**\n- 我们将encoder的数量设为L，隐藏层的单元数设为H，自注意力头的个数设为A，则BERT可分为$$BERT_{BASE}$$（L=12，H=768，A=12，总参数量=110M  ）和$$BERT_{LARGE}$$（L=24，H=1024，A=16，总参数量=340M）两个版本\n\n- $$BERT_{LARGE}$$在几乎所有的任务上都是优于$$BERT_{BASE}$$的，特别是特别小的数据集上\n\n\n\n### 1.2 BERT的输入输出\n\n- BERT使用WordPiece embeddings  \n\n- BERT的**输入可以是一个句子也可以是两个句子**，每个输入的**最开始都需要加一个[CLS] token**，如果输入包含两个句子（sentence A and sentence B），则**中间需要加入一个[SEP] token来做分隔**\n- **总的输入为**：对应的token embedding+segment embedding+position embedding的总和：\n\n![image-20220701114959234](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701114959234.png)\n\n其中segment embedding是用以区分sentence A和sentence B（**第一个句子的segment embedding都是0，第二个的都是1**），而position embedding和Transformer中的不一样，Transformer是采用三角函数，而**BERT采用learned position embedding**\n\n- 输入输出的形式大致如下：\n\n![image-20220701115457939](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701115457939.png)\n\n**其中C为[CLS]对应的最终的embedding，在分类任务时作为整个序列的总表征（但是C在微调之前是没有具体意义的向量，因为他是通过NSP预训练出来的）**。$$T_i$$为第$$i$$个token所对应的embedding\n\n\n\n\n\n# 2 BERT的Pre-training\n\n- BERT的Pre-training可分为MLM和NSP，分别对应token级的任务和sentence级的任务\n- Pre-training采用的是**无监督的方法**\n- 在Pre-training数据的选择上，使用document-level corpus要优于shuffled sentence-level corpus\n\n\n\n### 2.1 Masked Language Model（MLM）\n\n#### 2.1.1 MLM的输入\n\n- 每个输入的sequence会**随机mask掉15%的token**，并且在最后预测mask掉的地方是什么词（通过将该token最后对应的embedding送入softmax层并采用交叉熵损失，分类个数为整个词典的token数）\n- **其中mask的策略为**，对于一个要mask的token：\n\n1. 80%的概率变为[MASK]\n2. 10%的概率变为随机词\n3. 10%的概率不变\n\n举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701122513870.png\" alt=\"image-20220701122513870\" style=\"zoom:80%;\" />\n\n\n\n#### 2.2 采用此mask策略的原因\n\n- 预训练时是有[MASK]的，但是微调时是没有的，那么**微调时模型就只能根据其他token的信息和语序结构来预测当前词，而无法利用到这个词本身的信息**（因为它们从未出现在训练过程中，等于模型从未接触到它们的信息，等于整个语义空间损失了部分信息），所以会**产生预训练和微调的mismatch**\n- 而保留下来的信息**如果全部使用原始token，那么模型在预训练的时候可能会偷懒，直接照抄当前的token**，所以需要随机换成其他词，会让模型不能去死记硬背当前的token，而去**尽力学习单词周边的语义表达和远距离的信息依赖**，尝试建模完整的语言信息\n- 但是随机替换不能太多，要不然肯定会对模型产生误导，以下是经过多次实验的数据：\n\n![image-20220701123511287](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123511287.png)\n\n可以看到只使用随机替换，会对结果产生极大的影响\n\n\n\n#### 2.3 MLM的问题\n\n- 由于MLM每次只mask掉15%的词，所以只预测15%的词，所以需要更多的steps才能收敛，以下是MLM和LTR模型的对比：\n\n![image-20220701123837335](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123837335.png)\n\n可以看到MLM收敛速度更慢，需要更多的steps，但是所获得的改进远大于所增加的成本，所以问题不大\n\n\n\n### 2.2 Next Sentence Predictoin（NSP）\n\n- NSP的输入为两个句子，有50%的概率sentence B是sentence A的下一句，有50%的概率不是，举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701124711635.png\" alt=\"image-20220701124711635\" style=\"zoom:90%;\" />\n\n- 在最后使用C向量送入下一层，判断为IsNext or NotNext\n\n\n\n### 2.3 Pre-training的细节\n\n- **优化：**\n> 1. Adam（learning rate = $$10^{-4}$$，$$\\beta_1 = 0.9$$， $$\\beta_2 = 0.999$$）\n> 2. learning rate在前10000个steps采用warmup，并且还应用了线性衰减\n> 3. 0.01的L2权重衰减和0.1的Dropout\n> 4. batch size = 256 sequences / batch\n\n- 激活函数采用gelu而非relu\n- 损失为MLM的最大似然和NSP的最大似然的和\n\n- 由于attention是随序列长度进行平方增长的，所以为了提高预训练速度，在实验时，**先在90%的steps应用应用128的序列长，然后在剩下的10%的steps中改为512序列长度，来学习position embedding**\n\n\n\n\n\n\n# 3 BERT的Fine-tuning\n\n### 3.1 Fine-tuning的一般做法\n\n- 都是在最后加上一两层，来进行微调。对于Transformer Encoder的输出：\n1. 如果是token级的下游任务，如sequence tagging和question answering，是直接将对应的token输出的embedding送入下一层。\n2. 如果是sentence级的下游任务，如sentiment analysis，需要将[CLS]对应的输出，也就是C，送入下一层用以分类\n\n\n\n### 3.2 Fine-tuning的细节\n\n- 大多数超参数和pre-training时是一样的，除了batch size、learning rate和epochs\n- dropout的概率还是保持为0.1\n- 在实验中发现，以下几个超参的选择，适用于大多数的任务：\n\n> **Batch size：16， 32**\n>\n> **Learning rate (Adam)：$$5 \\times 10^{-5}, 3 \\times 10^{-5}, 2 \\times 10^{-5}$$  **\n>\n> **Number of epochs：2，3，4  **\n\n- 并且还发现大数据集相比小数据集对于超参的选择是不那么敏感的\n\n\n\n\n\n# 4 BERT实践\n\n- 下面介绍BERT在各种下游任务上的表现：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701233158369.png\" alt=\"image-20220701233158369\" style=\"zoom:70%;\" />\n\n### 4.1 GLUE\n\n- GLUE全称为The General Language Understanding Evaluation，包含了各种而样的自然语言理解任务\n- 在BERT中我们只添加了一个多分类输出层，将[CLS]对用的输出C，送入该层，再使用softmax，计算损失\n- 采用的超参：batch size = 32，epochs = 3，learning rate =  $$5 \\times 10^{-5}，4 \\times 10^{-5}，3 \\times 10^{-5}，2 \\times 10^{-5}$$\n\n- 在微调时，**$$BERT_{LARGE}$$在小数据集上的结果是不稳定的**，所以采取了**多次随机重启**（不一样的数据重洗和分类层的参数初始化），并且选择了在验证集上结果最好的模型\n\n\n\n### 4.2 SQuAD \n\n- SQuAD全称The Stanford Question Answering Dataset，收录了100k的QA对，其中每个Query的Answer是在对应的Passage中的一段连续文本（answer span）\n\n#### 4.2.1 SQuAD v1.1\n\n- 首先设$$S \\in R^H$$和$$E \\in R^H$$分别为answer span中的第一个词和最后一个词的embedding\n- 那么$$word_i$$作为第一个词的概率，可以使用点积+softmax的求得：（其中$$T_i$$是$$word_i$$对应的output）\n\n$$\nP_{i}=\\frac{e^{S \\cdot T_{i}}}{\\sum_{j} e^{S \\cdot T_{j}}}\n$$\n\n将$$word_i$$作为最后一个词的概率也是一样的算法，只是把S替换成E\n\n- 在训练时的损失为正确的开始和结束位置的最大似然\n\n- 在预测时，每个候选位置，即将$$word_i$$到$$word_j$$作为answer的score为：\n\n$$\nS \\cdot T_i + E \\cdot T_j \\quad (j \\geq i)\n$$\n\n然后取最大score的侯选位置作为输出\n\n- 超参：batch size = 32，epochs = 3，learning rate = $$5 \\times 10^{-5}$$\n\n- 在具体实验中，应用于SQuAD数据集上前，先在TriviaQA上微调，进行适当的数据增强\n\n\n\n#### 4.2.2 SQuAD v2.0\n\n- SQuAD v2.0相对于SQuAD v1.1增加了一个No Answer的输出，因为一个问题的答案并不总是出现在passage中的，No Answer的的具体形式为start和end都是[CLS]的answer span，预测为No Answer的score为：\n\n$$\ns_{null} = S \\cdot C + E \\cdot C\n$$\n\n当满足下式时，则不预测为No Answer：\n$$\n\\hat{s_{i, j}}>s_{\\mathrm{null}}+\\tau\n$$\n其中$$\\hat{s_{i, j}}=\\max _{j \\geq i} S \\cdot T_{i}+E \\cdot T_{j}$$，而$$\\tau$$是通过实验所得，使在验证集上获得最大的F1\n\n- 在本次实验中并未使用TriviaQA data set\n- 超参：batch size = 48，epochs = 2，learning rate = $$5 \\times 10^{-5}$$\n\n\n\n### 4.3 SWAG\n\n- 全称The Situations With Adversarial Generations，用于常识推断任务，具体任务是给定一个sentence，然后需要在4个选择中选出最合适的答案\n- 任务可建模为：每次有4个输入序列，每个输出是给定的sentence+4个可能的选择之一，最后得到C向量，再加一层全连接层，用sotfmax计算概率\n- 超参：batch size = 16，epochs = 3，learning rate = $$2 \\times 10^{-5}$$\n\n\n\n\n\n# 5 BERT和其他模型的对比\n\n- 实验进行了ELMo，OpenAI GPT和BERT之间的对比\n- 首先介绍大致做法和结构：\n\n> 1. BERT使用双向Transformer，OpenAI GPT使用LTR Transformer，而ELMo使用LTR和RTL的LSTM在最后的简单连结\n> 2. BERT和OpenAI GPT使用fine-tuning approaches，而ELMo使用feature-based approach\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220702121317344.png\" alt=\"image-20220702121317344\" style=\"zoom:80%;\" />\n\n- 另外，ELMo使用在最后将LTR和TRL简单的连结，有以下缺点：\n\n> 1. 两倍的工作量\n> 2. 对于有些任务是不直观的，如QA\n> 3. BERT在每层都可以实现双向，而ELMo只会在最后连结\n\n\n\n\n\n# 6 BERT的变体\n\n- [RoBERTa](https://zlkqz.top/2022/08/09/RoBERTa%E6%80%BB%E7%BB%93/)\n\n- [BART](https://zlkqz.top/2022/08/10/BART%E6%80%BB%E7%BB%93/)","source":"_posts/Bert总结.md","raw":"---\ntitle: BERT总结\nmath: ture\ndate: 2022-2-25\n---\n\n- BERT是一种通过**在预训练时使用无监督**方法能在**每一层**实现**双向**表征的语言模型，并且使用微调的方法，在**具体的下游任务时不需要task-specific architecture**，只需要添加一两层和少部分参数，十分易于迁移\n\n- BERT带来的主要提升是解决了双向性的问题。如OpenAI GPT使用的是left-to-right（LTR）Transformer结构，失去了双向性。又比如ELMo使用简单的将left-to-right（LTR）的LSTM和right-to-left（RTL）的LSTM在最后简单的连结来实现双向性，而BERT能在每一层都实现双向，并且相比于在最后简单的连结更具有直观性和可解释性。\n\n\n\n\n\n# 1 BERT的结构\n\n### 1.1 结构和规模\n\n- BERT的结构十分简单，就是由**多个Transformer的encoder组合而成**\n- 我们将encoder的数量设为L，隐藏层的单元数设为H，自注意力头的个数设为A，则BERT可分为$$BERT_{BASE}$$（L=12，H=768，A=12，总参数量=110M  ）和$$BERT_{LARGE}$$（L=24，H=1024，A=16，总参数量=340M）两个版本\n\n- $$BERT_{LARGE}$$在几乎所有的任务上都是优于$$BERT_{BASE}$$的，特别是特别小的数据集上\n\n\n\n### 1.2 BERT的输入输出\n\n- BERT使用WordPiece embeddings  \n\n- BERT的**输入可以是一个句子也可以是两个句子**，每个输入的**最开始都需要加一个[CLS] token**，如果输入包含两个句子（sentence A and sentence B），则**中间需要加入一个[SEP] token来做分隔**\n- **总的输入为**：对应的token embedding+segment embedding+position embedding的总和：\n\n![image-20220701114959234](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701114959234.png)\n\n其中segment embedding是用以区分sentence A和sentence B（**第一个句子的segment embedding都是0，第二个的都是1**），而position embedding和Transformer中的不一样，Transformer是采用三角函数，而**BERT采用learned position embedding**\n\n- 输入输出的形式大致如下：\n\n![image-20220701115457939](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701115457939.png)\n\n**其中C为[CLS]对应的最终的embedding，在分类任务时作为整个序列的总表征（但是C在微调之前是没有具体意义的向量，因为他是通过NSP预训练出来的）**。$$T_i$$为第$$i$$个token所对应的embedding\n\n\n\n\n\n# 2 BERT的Pre-training\n\n- BERT的Pre-training可分为MLM和NSP，分别对应token级的任务和sentence级的任务\n- Pre-training采用的是**无监督的方法**\n- 在Pre-training数据的选择上，使用document-level corpus要优于shuffled sentence-level corpus\n\n\n\n### 2.1 Masked Language Model（MLM）\n\n#### 2.1.1 MLM的输入\n\n- 每个输入的sequence会**随机mask掉15%的token**，并且在最后预测mask掉的地方是什么词（通过将该token最后对应的embedding送入softmax层并采用交叉熵损失，分类个数为整个词典的token数）\n- **其中mask的策略为**，对于一个要mask的token：\n\n1. 80%的概率变为[MASK]\n2. 10%的概率变为随机词\n3. 10%的概率不变\n\n举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701122513870.png\" alt=\"image-20220701122513870\" style=\"zoom:80%;\" />\n\n\n\n#### 2.2 采用此mask策略的原因\n\n- 预训练时是有[MASK]的，但是微调时是没有的，那么**微调时模型就只能根据其他token的信息和语序结构来预测当前词，而无法利用到这个词本身的信息**（因为它们从未出现在训练过程中，等于模型从未接触到它们的信息，等于整个语义空间损失了部分信息），所以会**产生预训练和微调的mismatch**\n- 而保留下来的信息**如果全部使用原始token，那么模型在预训练的时候可能会偷懒，直接照抄当前的token**，所以需要随机换成其他词，会让模型不能去死记硬背当前的token，而去**尽力学习单词周边的语义表达和远距离的信息依赖**，尝试建模完整的语言信息\n- 但是随机替换不能太多，要不然肯定会对模型产生误导，以下是经过多次实验的数据：\n\n![image-20220701123511287](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123511287.png)\n\n可以看到只使用随机替换，会对结果产生极大的影响\n\n\n\n#### 2.3 MLM的问题\n\n- 由于MLM每次只mask掉15%的词，所以只预测15%的词，所以需要更多的steps才能收敛，以下是MLM和LTR模型的对比：\n\n![image-20220701123837335](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123837335.png)\n\n可以看到MLM收敛速度更慢，需要更多的steps，但是所获得的改进远大于所增加的成本，所以问题不大\n\n\n\n### 2.2 Next Sentence Predictoin（NSP）\n\n- NSP的输入为两个句子，有50%的概率sentence B是sentence A的下一句，有50%的概率不是，举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701124711635.png\" alt=\"image-20220701124711635\" style=\"zoom:90%;\" />\n\n- 在最后使用C向量送入下一层，判断为IsNext or NotNext\n\n\n\n### 2.3 Pre-training的细节\n\n- **优化：**\n> 1. Adam（learning rate = $$10^{-4}$$，$$\\beta_1 = 0.9$$， $$\\beta_2 = 0.999$$）\n> 2. learning rate在前10000个steps采用warmup，并且还应用了线性衰减\n> 3. 0.01的L2权重衰减和0.1的Dropout\n> 4. batch size = 256 sequences / batch\n\n- 激活函数采用gelu而非relu\n- 损失为MLM的最大似然和NSP的最大似然的和\n\n- 由于attention是随序列长度进行平方增长的，所以为了提高预训练速度，在实验时，**先在90%的steps应用应用128的序列长，然后在剩下的10%的steps中改为512序列长度，来学习position embedding**\n\n\n\n\n\n\n# 3 BERT的Fine-tuning\n\n### 3.1 Fine-tuning的一般做法\n\n- 都是在最后加上一两层，来进行微调。对于Transformer Encoder的输出：\n1. 如果是token级的下游任务，如sequence tagging和question answering，是直接将对应的token输出的embedding送入下一层。\n2. 如果是sentence级的下游任务，如sentiment analysis，需要将[CLS]对应的输出，也就是C，送入下一层用以分类\n\n\n\n### 3.2 Fine-tuning的细节\n\n- 大多数超参数和pre-training时是一样的，除了batch size、learning rate和epochs\n- dropout的概率还是保持为0.1\n- 在实验中发现，以下几个超参的选择，适用于大多数的任务：\n\n> **Batch size：16， 32**\n>\n> **Learning rate (Adam)：$$5 \\times 10^{-5}, 3 \\times 10^{-5}, 2 \\times 10^{-5}$$  **\n>\n> **Number of epochs：2，3，4  **\n\n- 并且还发现大数据集相比小数据集对于超参的选择是不那么敏感的\n\n\n\n\n\n# 4 BERT实践\n\n- 下面介绍BERT在各种下游任务上的表现：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701233158369.png\" alt=\"image-20220701233158369\" style=\"zoom:70%;\" />\n\n### 4.1 GLUE\n\n- GLUE全称为The General Language Understanding Evaluation，包含了各种而样的自然语言理解任务\n- 在BERT中我们只添加了一个多分类输出层，将[CLS]对用的输出C，送入该层，再使用softmax，计算损失\n- 采用的超参：batch size = 32，epochs = 3，learning rate =  $$5 \\times 10^{-5}，4 \\times 10^{-5}，3 \\times 10^{-5}，2 \\times 10^{-5}$$\n\n- 在微调时，**$$BERT_{LARGE}$$在小数据集上的结果是不稳定的**，所以采取了**多次随机重启**（不一样的数据重洗和分类层的参数初始化），并且选择了在验证集上结果最好的模型\n\n\n\n### 4.2 SQuAD \n\n- SQuAD全称The Stanford Question Answering Dataset，收录了100k的QA对，其中每个Query的Answer是在对应的Passage中的一段连续文本（answer span）\n\n#### 4.2.1 SQuAD v1.1\n\n- 首先设$$S \\in R^H$$和$$E \\in R^H$$分别为answer span中的第一个词和最后一个词的embedding\n- 那么$$word_i$$作为第一个词的概率，可以使用点积+softmax的求得：（其中$$T_i$$是$$word_i$$对应的output）\n\n$$\nP_{i}=\\frac{e^{S \\cdot T_{i}}}{\\sum_{j} e^{S \\cdot T_{j}}}\n$$\n\n将$$word_i$$作为最后一个词的概率也是一样的算法，只是把S替换成E\n\n- 在训练时的损失为正确的开始和结束位置的最大似然\n\n- 在预测时，每个候选位置，即将$$word_i$$到$$word_j$$作为answer的score为：\n\n$$\nS \\cdot T_i + E \\cdot T_j \\quad (j \\geq i)\n$$\n\n然后取最大score的侯选位置作为输出\n\n- 超参：batch size = 32，epochs = 3，learning rate = $$5 \\times 10^{-5}$$\n\n- 在具体实验中，应用于SQuAD数据集上前，先在TriviaQA上微调，进行适当的数据增强\n\n\n\n#### 4.2.2 SQuAD v2.0\n\n- SQuAD v2.0相对于SQuAD v1.1增加了一个No Answer的输出，因为一个问题的答案并不总是出现在passage中的，No Answer的的具体形式为start和end都是[CLS]的answer span，预测为No Answer的score为：\n\n$$\ns_{null} = S \\cdot C + E \\cdot C\n$$\n\n当满足下式时，则不预测为No Answer：\n$$\n\\hat{s_{i, j}}>s_{\\mathrm{null}}+\\tau\n$$\n其中$$\\hat{s_{i, j}}=\\max _{j \\geq i} S \\cdot T_{i}+E \\cdot T_{j}$$，而$$\\tau$$是通过实验所得，使在验证集上获得最大的F1\n\n- 在本次实验中并未使用TriviaQA data set\n- 超参：batch size = 48，epochs = 2，learning rate = $$5 \\times 10^{-5}$$\n\n\n\n### 4.3 SWAG\n\n- 全称The Situations With Adversarial Generations，用于常识推断任务，具体任务是给定一个sentence，然后需要在4个选择中选出最合适的答案\n- 任务可建模为：每次有4个输入序列，每个输出是给定的sentence+4个可能的选择之一，最后得到C向量，再加一层全连接层，用sotfmax计算概率\n- 超参：batch size = 16，epochs = 3，learning rate = $$2 \\times 10^{-5}$$\n\n\n\n\n\n# 5 BERT和其他模型的对比\n\n- 实验进行了ELMo，OpenAI GPT和BERT之间的对比\n- 首先介绍大致做法和结构：\n\n> 1. BERT使用双向Transformer，OpenAI GPT使用LTR Transformer，而ELMo使用LTR和RTL的LSTM在最后的简单连结\n> 2. BERT和OpenAI GPT使用fine-tuning approaches，而ELMo使用feature-based approach\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220702121317344.png\" alt=\"image-20220702121317344\" style=\"zoom:80%;\" />\n\n- 另外，ELMo使用在最后将LTR和TRL简单的连结，有以下缺点：\n\n> 1. 两倍的工作量\n> 2. 对于有些任务是不直观的，如QA\n> 3. BERT在每层都可以实现双向，而ELMo只会在最后连结\n\n\n\n\n\n# 6 BERT的变体\n\n- [RoBERTa](https://zlkqz.top/2022/08/09/RoBERTa%E6%80%BB%E7%BB%93/)\n\n- [BART](https://zlkqz.top/2022/08/10/BART%E6%80%BB%E7%BB%93/)","slug":"Bert总结","published":1,"updated":"2022-12-20T06:17:57.983Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1700017cszgz2qakn3","content":"<ul>\n<li><p>BERT是一种通过<strong>在预训练时使用无监督</strong>方法能在<strong>每一层</strong>实现<strong>双向</strong>表征的语言模型，并且使用微调的方法，在<strong>具体的下游任务时不需要task-specific architecture</strong>，只需要添加一两层和少部分参数，十分易于迁移</p>\n</li>\n<li><p>BERT带来的主要提升是解决了双向性的问题。如OpenAI GPT使用的是left-to-right（LTR）Transformer结构，失去了双向性。又比如ELMo使用简单的将left-to-right（LTR）的LSTM和right-to-left（RTL）的LSTM在最后简单的连结来实现双向性，而BERT能在每一层都实现双向，并且相比于在最后简单的连结更具有直观性和可解释性。</p>\n</li>\n</ul>\n<h1 id=\"1-BERT的结构\"><a href=\"#1-BERT的结构\" class=\"headerlink\" title=\"1 BERT的结构\"></a>1 BERT的结构</h1><h3 id=\"1-1-结构和规模\"><a href=\"#1-1-结构和规模\" class=\"headerlink\" title=\"1.1 结构和规模\"></a>1.1 结构和规模</h3><ul>\n<li>BERT的结构十分简单，就是由<strong>多个Transformer的encoder组合而成</strong></li>\n<li><p>我们将encoder的数量设为L，隐藏层的单元数设为H，自注意力头的个数设为A，则BERT可分为<script type=\"math/tex\">BERT_{BASE}</script>（L=12，H=768，A=12，总参数量=110M  ）和<script type=\"math/tex\">BERT_{LARGE}</script>（L=24，H=1024，A=16，总参数量=340M）两个版本</p>\n</li>\n<li><p><script type=\"math/tex\">BERT_{LARGE}</script>在几乎所有的任务上都是优于<script type=\"math/tex\">BERT_{BASE}</script>的，特别是特别小的数据集上</p>\n</li>\n</ul>\n<h3 id=\"1-2-BERT的输入输出\"><a href=\"#1-2-BERT的输入输出\" class=\"headerlink\" title=\"1.2 BERT的输入输出\"></a>1.2 BERT的输入输出</h3><ul>\n<li><p>BERT使用WordPiece embeddings  </p>\n</li>\n<li><p>BERT的<strong>输入可以是一个句子也可以是两个句子</strong>，每个输入的<strong>最开始都需要加一个[CLS] token</strong>，如果输入包含两个句子（sentence A and sentence B），则<strong>中间需要加入一个[SEP] token来做分隔</strong></p>\n</li>\n<li><strong>总的输入为</strong>：对应的token embedding+segment embedding+position embedding的总和：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701114959234.png\" alt=\"image-20220701114959234\"></p>\n<p>其中segment embedding是用以区分sentence A和sentence B（<strong>第一个句子的segment embedding都是0，第二个的都是1</strong>），而position embedding和Transformer中的不一样，Transformer是采用三角函数，而<strong>BERT采用learned position embedding</strong></p>\n<ul>\n<li>输入输出的形式大致如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701115457939.png\" alt=\"image-20220701115457939\"></p>\n<p><strong>其中C为[CLS]对应的最终的embedding，在分类任务时作为整个序列的总表征（但是C在微调之前是没有具体意义的向量，因为他是通过NSP预训练出来的）</strong>。<script type=\"math/tex\">T_i</script>为第<script type=\"math/tex\">i</script>个token所对应的embedding</p>\n<h1 id=\"2-BERT的Pre-training\"><a href=\"#2-BERT的Pre-training\" class=\"headerlink\" title=\"2 BERT的Pre-training\"></a>2 BERT的Pre-training</h1><ul>\n<li>BERT的Pre-training可分为MLM和NSP，分别对应token级的任务和sentence级的任务</li>\n<li>Pre-training采用的是<strong>无监督的方法</strong></li>\n<li>在Pre-training数据的选择上，使用document-level corpus要优于shuffled sentence-level corpus</li>\n</ul>\n<h3 id=\"2-1-Masked-Language-Model（MLM）\"><a href=\"#2-1-Masked-Language-Model（MLM）\" class=\"headerlink\" title=\"2.1 Masked Language Model（MLM）\"></a>2.1 Masked Language Model（MLM）</h3><h4 id=\"2-1-1-MLM的输入\"><a href=\"#2-1-1-MLM的输入\" class=\"headerlink\" title=\"2.1.1 MLM的输入\"></a>2.1.1 MLM的输入</h4><ul>\n<li>每个输入的sequence会<strong>随机mask掉15%的token</strong>，并且在最后预测mask掉的地方是什么词（通过将该token最后对应的embedding送入softmax层并采用交叉熵损失，分类个数为整个词典的token数）</li>\n<li><strong>其中mask的策略为</strong>，对于一个要mask的token：</li>\n</ul>\n<ol>\n<li>80%的概率变为[MASK]</li>\n<li>10%的概率变为随机词</li>\n<li>10%的概率不变</li>\n</ol>\n<p>举个栗子：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701122513870.png\" alt=\"image-20220701122513870\" style=\"zoom:80%;\" /></p>\n<h4 id=\"2-2-采用此mask策略的原因\"><a href=\"#2-2-采用此mask策略的原因\" class=\"headerlink\" title=\"2.2 采用此mask策略的原因\"></a>2.2 采用此mask策略的原因</h4><ul>\n<li>预训练时是有[MASK]的，但是微调时是没有的，那么<strong>微调时模型就只能根据其他token的信息和语序结构来预测当前词，而无法利用到这个词本身的信息</strong>（因为它们从未出现在训练过程中，等于模型从未接触到它们的信息，等于整个语义空间损失了部分信息），所以会<strong>产生预训练和微调的mismatch</strong></li>\n<li>而保留下来的信息<strong>如果全部使用原始token，那么模型在预训练的时候可能会偷懒，直接照抄当前的token</strong>，所以需要随机换成其他词，会让模型不能去死记硬背当前的token，而去<strong>尽力学习单词周边的语义表达和远距离的信息依赖</strong>，尝试建模完整的语言信息</li>\n<li>但是随机替换不能太多，要不然肯定会对模型产生误导，以下是经过多次实验的数据：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123511287.png\" alt=\"image-20220701123511287\"></p>\n<p>可以看到只使用随机替换，会对结果产生极大的影响</p>\n<h4 id=\"2-3-MLM的问题\"><a href=\"#2-3-MLM的问题\" class=\"headerlink\" title=\"2.3 MLM的问题\"></a>2.3 MLM的问题</h4><ul>\n<li>由于MLM每次只mask掉15%的词，所以只预测15%的词，所以需要更多的steps才能收敛，以下是MLM和LTR模型的对比：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123837335.png\" alt=\"image-20220701123837335\"></p>\n<p>可以看到MLM收敛速度更慢，需要更多的steps，但是所获得的改进远大于所增加的成本，所以问题不大</p>\n<h3 id=\"2-2-Next-Sentence-Predictoin（NSP）\"><a href=\"#2-2-Next-Sentence-Predictoin（NSP）\" class=\"headerlink\" title=\"2.2 Next Sentence Predictoin（NSP）\"></a>2.2 Next Sentence Predictoin（NSP）</h3><ul>\n<li>NSP的输入为两个句子，有50%的概率sentence B是sentence A的下一句，有50%的概率不是，举个栗子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701124711635.png\" alt=\"image-20220701124711635\" style=\"zoom:90%;\" /></p>\n<ul>\n<li>在最后使用C向量送入下一层，判断为IsNext or NotNext</li>\n</ul>\n<h3 id=\"2-3-Pre-training的细节\"><a href=\"#2-3-Pre-training的细节\" class=\"headerlink\" title=\"2.3 Pre-training的细节\"></a>2.3 Pre-training的细节</h3><ul>\n<li><p><strong>优化：</strong></p>\n<blockquote>\n<ol>\n<li>Adam（learning rate = <script type=\"math/tex\">10^{-4}</script>，<script type=\"math/tex\">\\beta_1 = 0.9</script>， <script type=\"math/tex\">\\beta_2 = 0.999</script>）</li>\n<li>learning rate在前10000个steps采用warmup，并且还应用了线性衰减</li>\n<li>0.01的L2权重衰减和0.1的Dropout</li>\n<li>batch size = 256 sequences / batch</li>\n</ol>\n</blockquote>\n</li>\n<li><p>激活函数采用gelu而非relu</p>\n</li>\n<li><p>损失为MLM的最大似然和NSP的最大似然的和</p>\n</li>\n<li><p>由于attention是随序列长度进行平方增长的，所以为了提高预训练速度，在实验时，<strong>先在90%的steps应用应用128的序列长，然后在剩下的10%的steps中改为512序列长度，来学习position embedding</strong></p>\n</li>\n</ul>\n<h1 id=\"3-BERT的Fine-tuning\"><a href=\"#3-BERT的Fine-tuning\" class=\"headerlink\" title=\"3 BERT的Fine-tuning\"></a>3 BERT的Fine-tuning</h1><h3 id=\"3-1-Fine-tuning的一般做法\"><a href=\"#3-1-Fine-tuning的一般做法\" class=\"headerlink\" title=\"3.1 Fine-tuning的一般做法\"></a>3.1 Fine-tuning的一般做法</h3><ul>\n<li>都是在最后加上一两层，来进行微调。对于Transformer Encoder的输出：</li>\n</ul>\n<ol>\n<li>如果是token级的下游任务，如sequence tagging和question answering，是直接将对应的token输出的embedding送入下一层。</li>\n<li>如果是sentence级的下游任务，如sentiment analysis，需要将[CLS]对应的输出，也就是C，送入下一层用以分类</li>\n</ol>\n<h3 id=\"3-2-Fine-tuning的细节\"><a href=\"#3-2-Fine-tuning的细节\" class=\"headerlink\" title=\"3.2 Fine-tuning的细节\"></a>3.2 Fine-tuning的细节</h3><ul>\n<li>大多数超参数和pre-training时是一样的，除了batch size、learning rate和epochs</li>\n<li>dropout的概率还是保持为0.1</li>\n<li>在实验中发现，以下几个超参的选择，适用于大多数的任务：</li>\n</ul>\n<blockquote>\n<p><strong>Batch size：16， 32</strong></p>\n<p><strong>Learning rate (Adam)：<script type=\"math/tex\">5 \\times 10^{-5}, 3 \\times 10^{-5}, 2 \\times 10^{-5}</script>  </strong></p>\n<p><strong>Number of epochs：2，3，4  </strong></p>\n</blockquote>\n<ul>\n<li>并且还发现大数据集相比小数据集对于超参的选择是不那么敏感的</li>\n</ul>\n<h1 id=\"4-BERT实践\"><a href=\"#4-BERT实践\" class=\"headerlink\" title=\"4 BERT实践\"></a>4 BERT实践</h1><ul>\n<li>下面介绍BERT在各种下游任务上的表现：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701233158369.png\" alt=\"image-20220701233158369\" style=\"zoom:70%;\" /></p>\n<h3 id=\"4-1-GLUE\"><a href=\"#4-1-GLUE\" class=\"headerlink\" title=\"4.1 GLUE\"></a>4.1 GLUE</h3><ul>\n<li>GLUE全称为The General Language Understanding Evaluation，包含了各种而样的自然语言理解任务</li>\n<li>在BERT中我们只添加了一个多分类输出层，将[CLS]对用的输出C，送入该层，再使用softmax，计算损失</li>\n<li><p>采用的超参：batch size = 32，epochs = 3，learning rate =  <script type=\"math/tex\">5 \\times 10^{-5}，4 \\times 10^{-5}，3 \\times 10^{-5}，2 \\times 10^{-5}</script></p>\n</li>\n<li><p>在微调时，<strong><script type=\"math/tex\">BERT_{LARGE}</script>在小数据集上的结果是不稳定的</strong>，所以采取了<strong>多次随机重启</strong>（不一样的数据重洗和分类层的参数初始化），并且选择了在验证集上结果最好的模型</p>\n</li>\n</ul>\n<h3 id=\"4-2-SQuAD\"><a href=\"#4-2-SQuAD\" class=\"headerlink\" title=\"4.2 SQuAD\"></a>4.2 SQuAD</h3><ul>\n<li>SQuAD全称The Stanford Question Answering Dataset，收录了100k的QA对，其中每个Query的Answer是在对应的Passage中的一段连续文本（answer span）</li>\n</ul>\n<h4 id=\"4-2-1-SQuAD-v1-1\"><a href=\"#4-2-1-SQuAD-v1-1\" class=\"headerlink\" title=\"4.2.1 SQuAD v1.1\"></a>4.2.1 SQuAD v1.1</h4><ul>\n<li>首先设<script type=\"math/tex\">S \\in R^H</script>和<script type=\"math/tex\">E \\in R^H</script>分别为answer span中的第一个词和最后一个词的embedding</li>\n<li>那么<script type=\"math/tex\">word_i</script>作为第一个词的概率，可以使用点积+softmax的求得：（其中<script type=\"math/tex\">T_i</script>是<script type=\"math/tex\">word_i</script>对应的output）</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP_{i}=\\frac{e^{S \\cdot T_{i}}}{\\sum_{j} e^{S \\cdot T_{j}}}</script><p>将<script type=\"math/tex\">word_i</script>作为最后一个词的概率也是一样的算法，只是把S替换成E</p>\n<ul>\n<li><p>在训练时的损失为正确的开始和结束位置的最大似然</p>\n</li>\n<li><p>在预测时，每个候选位置，即将<script type=\"math/tex\">word_i</script>到<script type=\"math/tex\">word_j</script>作为answer的score为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nS \\cdot T_i + E \\cdot T_j \\quad (j \\geq i)</script><p>然后取最大score的侯选位置作为输出</p>\n<ul>\n<li><p>超参：batch size = 32，epochs = 3，learning rate = <script type=\"math/tex\">5 \\times 10^{-5}</script></p>\n</li>\n<li><p>在具体实验中，应用于SQuAD数据集上前，先在TriviaQA上微调，进行适当的数据增强</p>\n</li>\n</ul>\n<h4 id=\"4-2-2-SQuAD-v2-0\"><a href=\"#4-2-2-SQuAD-v2-0\" class=\"headerlink\" title=\"4.2.2 SQuAD v2.0\"></a>4.2.2 SQuAD v2.0</h4><ul>\n<li>SQuAD v2.0相对于SQuAD v1.1增加了一个No Answer的输出，因为一个问题的答案并不总是出现在passage中的，No Answer的的具体形式为start和end都是[CLS]的answer span，预测为No Answer的score为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ns_{null} = S \\cdot C + E \\cdot C</script><p>当满足下式时，则不预测为No Answer：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{s_{i, j}}>s_{\\mathrm{null}}+\\tau</script><p>其中<script type=\"math/tex\">\\hat{s_{i, j}}=\\max _{j \\geq i} S \\cdot T_{i}+E \\cdot T_{j}</script>，而<script type=\"math/tex\">\\tau</script>是通过实验所得，使在验证集上获得最大的F1</p>\n<ul>\n<li>在本次实验中并未使用TriviaQA data set</li>\n<li>超参：batch size = 48，epochs = 2，learning rate = <script type=\"math/tex\">5 \\times 10^{-5}</script></li>\n</ul>\n<h3 id=\"4-3-SWAG\"><a href=\"#4-3-SWAG\" class=\"headerlink\" title=\"4.3 SWAG\"></a>4.3 SWAG</h3><ul>\n<li>全称The Situations With Adversarial Generations，用于常识推断任务，具体任务是给定一个sentence，然后需要在4个选择中选出最合适的答案</li>\n<li>任务可建模为：每次有4个输入序列，每个输出是给定的sentence+4个可能的选择之一，最后得到C向量，再加一层全连接层，用sotfmax计算概率</li>\n<li>超参：batch size = 16，epochs = 3，learning rate = <script type=\"math/tex\">2 \\times 10^{-5}</script></li>\n</ul>\n<h1 id=\"5-BERT和其他模型的对比\"><a href=\"#5-BERT和其他模型的对比\" class=\"headerlink\" title=\"5 BERT和其他模型的对比\"></a>5 BERT和其他模型的对比</h1><ul>\n<li>实验进行了ELMo，OpenAI GPT和BERT之间的对比</li>\n<li>首先介绍大致做法和结构：</li>\n</ul>\n<blockquote>\n<ol>\n<li>BERT使用双向Transformer，OpenAI GPT使用LTR Transformer，而ELMo使用LTR和RTL的LSTM在最后的简单连结</li>\n<li>BERT和OpenAI GPT使用fine-tuning approaches，而ELMo使用feature-based approach</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220702121317344.png\" alt=\"image-20220702121317344\" style=\"zoom:80%;\" /></p>\n</blockquote>\n<ul>\n<li>另外，ELMo使用在最后将LTR和TRL简单的连结，有以下缺点：</li>\n</ul>\n<blockquote>\n<ol>\n<li>两倍的工作量</li>\n<li>对于有些任务是不直观的，如QA</li>\n<li>BERT在每层都可以实现双向，而ELMo只会在最后连结</li>\n</ol>\n</blockquote>\n<h1 id=\"6-BERT的变体\"><a href=\"#6-BERT的变体\" class=\"headerlink\" title=\"6 BERT的变体\"></a>6 BERT的变体</h1><ul>\n<li><p><a href=\"https://zlkqz.top/2022/08/09/RoBERTa%E6%80%BB%E7%BB%93/\">RoBERTa</a></p>\n</li>\n<li><p><a href=\"https://zlkqz.top/2022/08/10/BART%E6%80%BB%E7%BB%93/\">BART</a></p>\n</li>\n</ul>\n","site":{"data":{}},"wordcount":4516,"excerpt":"","more":"<ul>\n<li><p>BERT是一种通过<strong>在预训练时使用无监督</strong>方法能在<strong>每一层</strong>实现<strong>双向</strong>表征的语言模型，并且使用微调的方法，在<strong>具体的下游任务时不需要task-specific architecture</strong>，只需要添加一两层和少部分参数，十分易于迁移</p>\n</li>\n<li><p>BERT带来的主要提升是解决了双向性的问题。如OpenAI GPT使用的是left-to-right（LTR）Transformer结构，失去了双向性。又比如ELMo使用简单的将left-to-right（LTR）的LSTM和right-to-left（RTL）的LSTM在最后简单的连结来实现双向性，而BERT能在每一层都实现双向，并且相比于在最后简单的连结更具有直观性和可解释性。</p>\n</li>\n</ul>\n<h1 id=\"1-BERT的结构\"><a href=\"#1-BERT的结构\" class=\"headerlink\" title=\"1 BERT的结构\"></a>1 BERT的结构</h1><h3 id=\"1-1-结构和规模\"><a href=\"#1-1-结构和规模\" class=\"headerlink\" title=\"1.1 结构和规模\"></a>1.1 结构和规模</h3><ul>\n<li>BERT的结构十分简单，就是由<strong>多个Transformer的encoder组合而成</strong></li>\n<li><p>我们将encoder的数量设为L，隐藏层的单元数设为H，自注意力头的个数设为A，则BERT可分为<script type=\"math/tex\">BERT_{BASE}</script>（L=12，H=768，A=12，总参数量=110M  ）和<script type=\"math/tex\">BERT_{LARGE}</script>（L=24，H=1024，A=16，总参数量=340M）两个版本</p>\n</li>\n<li><p><script type=\"math/tex\">BERT_{LARGE}</script>在几乎所有的任务上都是优于<script type=\"math/tex\">BERT_{BASE}</script>的，特别是特别小的数据集上</p>\n</li>\n</ul>\n<h3 id=\"1-2-BERT的输入输出\"><a href=\"#1-2-BERT的输入输出\" class=\"headerlink\" title=\"1.2 BERT的输入输出\"></a>1.2 BERT的输入输出</h3><ul>\n<li><p>BERT使用WordPiece embeddings  </p>\n</li>\n<li><p>BERT的<strong>输入可以是一个句子也可以是两个句子</strong>，每个输入的<strong>最开始都需要加一个[CLS] token</strong>，如果输入包含两个句子（sentence A and sentence B），则<strong>中间需要加入一个[SEP] token来做分隔</strong></p>\n</li>\n<li><strong>总的输入为</strong>：对应的token embedding+segment embedding+position embedding的总和：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701114959234.png\" alt=\"image-20220701114959234\"></p>\n<p>其中segment embedding是用以区分sentence A和sentence B（<strong>第一个句子的segment embedding都是0，第二个的都是1</strong>），而position embedding和Transformer中的不一样，Transformer是采用三角函数，而<strong>BERT采用learned position embedding</strong></p>\n<ul>\n<li>输入输出的形式大致如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701115457939.png\" alt=\"image-20220701115457939\"></p>\n<p><strong>其中C为[CLS]对应的最终的embedding，在分类任务时作为整个序列的总表征（但是C在微调之前是没有具体意义的向量，因为他是通过NSP预训练出来的）</strong>。<script type=\"math/tex\">T_i</script>为第<script type=\"math/tex\">i</script>个token所对应的embedding</p>\n<h1 id=\"2-BERT的Pre-training\"><a href=\"#2-BERT的Pre-training\" class=\"headerlink\" title=\"2 BERT的Pre-training\"></a>2 BERT的Pre-training</h1><ul>\n<li>BERT的Pre-training可分为MLM和NSP，分别对应token级的任务和sentence级的任务</li>\n<li>Pre-training采用的是<strong>无监督的方法</strong></li>\n<li>在Pre-training数据的选择上，使用document-level corpus要优于shuffled sentence-level corpus</li>\n</ul>\n<h3 id=\"2-1-Masked-Language-Model（MLM）\"><a href=\"#2-1-Masked-Language-Model（MLM）\" class=\"headerlink\" title=\"2.1 Masked Language Model（MLM）\"></a>2.1 Masked Language Model（MLM）</h3><h4 id=\"2-1-1-MLM的输入\"><a href=\"#2-1-1-MLM的输入\" class=\"headerlink\" title=\"2.1.1 MLM的输入\"></a>2.1.1 MLM的输入</h4><ul>\n<li>每个输入的sequence会<strong>随机mask掉15%的token</strong>，并且在最后预测mask掉的地方是什么词（通过将该token最后对应的embedding送入softmax层并采用交叉熵损失，分类个数为整个词典的token数）</li>\n<li><strong>其中mask的策略为</strong>，对于一个要mask的token：</li>\n</ul>\n<ol>\n<li>80%的概率变为[MASK]</li>\n<li>10%的概率变为随机词</li>\n<li>10%的概率不变</li>\n</ol>\n<p>举个栗子：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701122513870.png\" alt=\"image-20220701122513870\" style=\"zoom:80%;\" /></p>\n<h4 id=\"2-2-采用此mask策略的原因\"><a href=\"#2-2-采用此mask策略的原因\" class=\"headerlink\" title=\"2.2 采用此mask策略的原因\"></a>2.2 采用此mask策略的原因</h4><ul>\n<li>预训练时是有[MASK]的，但是微调时是没有的，那么<strong>微调时模型就只能根据其他token的信息和语序结构来预测当前词，而无法利用到这个词本身的信息</strong>（因为它们从未出现在训练过程中，等于模型从未接触到它们的信息，等于整个语义空间损失了部分信息），所以会<strong>产生预训练和微调的mismatch</strong></li>\n<li>而保留下来的信息<strong>如果全部使用原始token，那么模型在预训练的时候可能会偷懒，直接照抄当前的token</strong>，所以需要随机换成其他词，会让模型不能去死记硬背当前的token，而去<strong>尽力学习单词周边的语义表达和远距离的信息依赖</strong>，尝试建模完整的语言信息</li>\n<li>但是随机替换不能太多，要不然肯定会对模型产生误导，以下是经过多次实验的数据：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123511287.png\" alt=\"image-20220701123511287\"></p>\n<p>可以看到只使用随机替换，会对结果产生极大的影响</p>\n<h4 id=\"2-3-MLM的问题\"><a href=\"#2-3-MLM的问题\" class=\"headerlink\" title=\"2.3 MLM的问题\"></a>2.3 MLM的问题</h4><ul>\n<li>由于MLM每次只mask掉15%的词，所以只预测15%的词，所以需要更多的steps才能收敛，以下是MLM和LTR模型的对比：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701123837335.png\" alt=\"image-20220701123837335\"></p>\n<p>可以看到MLM收敛速度更慢，需要更多的steps，但是所获得的改进远大于所增加的成本，所以问题不大</p>\n<h3 id=\"2-2-Next-Sentence-Predictoin（NSP）\"><a href=\"#2-2-Next-Sentence-Predictoin（NSP）\" class=\"headerlink\" title=\"2.2 Next Sentence Predictoin（NSP）\"></a>2.2 Next Sentence Predictoin（NSP）</h3><ul>\n<li>NSP的输入为两个句子，有50%的概率sentence B是sentence A的下一句，有50%的概率不是，举个栗子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701124711635.png\" alt=\"image-20220701124711635\" style=\"zoom:90%;\" /></p>\n<ul>\n<li>在最后使用C向量送入下一层，判断为IsNext or NotNext</li>\n</ul>\n<h3 id=\"2-3-Pre-training的细节\"><a href=\"#2-3-Pre-training的细节\" class=\"headerlink\" title=\"2.3 Pre-training的细节\"></a>2.3 Pre-training的细节</h3><ul>\n<li><p><strong>优化：</strong></p>\n<blockquote>\n<ol>\n<li>Adam（learning rate = <script type=\"math/tex\">10^{-4}</script>，<script type=\"math/tex\">\\beta_1 = 0.9</script>， <script type=\"math/tex\">\\beta_2 = 0.999</script>）</li>\n<li>learning rate在前10000个steps采用warmup，并且还应用了线性衰减</li>\n<li>0.01的L2权重衰减和0.1的Dropout</li>\n<li>batch size = 256 sequences / batch</li>\n</ol>\n</blockquote>\n</li>\n<li><p>激活函数采用gelu而非relu</p>\n</li>\n<li><p>损失为MLM的最大似然和NSP的最大似然的和</p>\n</li>\n<li><p>由于attention是随序列长度进行平方增长的，所以为了提高预训练速度，在实验时，<strong>先在90%的steps应用应用128的序列长，然后在剩下的10%的steps中改为512序列长度，来学习position embedding</strong></p>\n</li>\n</ul>\n<h1 id=\"3-BERT的Fine-tuning\"><a href=\"#3-BERT的Fine-tuning\" class=\"headerlink\" title=\"3 BERT的Fine-tuning\"></a>3 BERT的Fine-tuning</h1><h3 id=\"3-1-Fine-tuning的一般做法\"><a href=\"#3-1-Fine-tuning的一般做法\" class=\"headerlink\" title=\"3.1 Fine-tuning的一般做法\"></a>3.1 Fine-tuning的一般做法</h3><ul>\n<li>都是在最后加上一两层，来进行微调。对于Transformer Encoder的输出：</li>\n</ul>\n<ol>\n<li>如果是token级的下游任务，如sequence tagging和question answering，是直接将对应的token输出的embedding送入下一层。</li>\n<li>如果是sentence级的下游任务，如sentiment analysis，需要将[CLS]对应的输出，也就是C，送入下一层用以分类</li>\n</ol>\n<h3 id=\"3-2-Fine-tuning的细节\"><a href=\"#3-2-Fine-tuning的细节\" class=\"headerlink\" title=\"3.2 Fine-tuning的细节\"></a>3.2 Fine-tuning的细节</h3><ul>\n<li>大多数超参数和pre-training时是一样的，除了batch size、learning rate和epochs</li>\n<li>dropout的概率还是保持为0.1</li>\n<li>在实验中发现，以下几个超参的选择，适用于大多数的任务：</li>\n</ul>\n<blockquote>\n<p><strong>Batch size：16， 32</strong></p>\n<p><strong>Learning rate (Adam)：<script type=\"math/tex\">5 \\times 10^{-5}, 3 \\times 10^{-5}, 2 \\times 10^{-5}</script>  </strong></p>\n<p><strong>Number of epochs：2，3，4  </strong></p>\n</blockquote>\n<ul>\n<li>并且还发现大数据集相比小数据集对于超参的选择是不那么敏感的</li>\n</ul>\n<h1 id=\"4-BERT实践\"><a href=\"#4-BERT实践\" class=\"headerlink\" title=\"4 BERT实践\"></a>4 BERT实践</h1><ul>\n<li>下面介绍BERT在各种下游任务上的表现：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220701233158369.png\" alt=\"image-20220701233158369\" style=\"zoom:70%;\" /></p>\n<h3 id=\"4-1-GLUE\"><a href=\"#4-1-GLUE\" class=\"headerlink\" title=\"4.1 GLUE\"></a>4.1 GLUE</h3><ul>\n<li>GLUE全称为The General Language Understanding Evaluation，包含了各种而样的自然语言理解任务</li>\n<li>在BERT中我们只添加了一个多分类输出层，将[CLS]对用的输出C，送入该层，再使用softmax，计算损失</li>\n<li><p>采用的超参：batch size = 32，epochs = 3，learning rate =  <script type=\"math/tex\">5 \\times 10^{-5}，4 \\times 10^{-5}，3 \\times 10^{-5}，2 \\times 10^{-5}</script></p>\n</li>\n<li><p>在微调时，<strong><script type=\"math/tex\">BERT_{LARGE}</script>在小数据集上的结果是不稳定的</strong>，所以采取了<strong>多次随机重启</strong>（不一样的数据重洗和分类层的参数初始化），并且选择了在验证集上结果最好的模型</p>\n</li>\n</ul>\n<h3 id=\"4-2-SQuAD\"><a href=\"#4-2-SQuAD\" class=\"headerlink\" title=\"4.2 SQuAD\"></a>4.2 SQuAD</h3><ul>\n<li>SQuAD全称The Stanford Question Answering Dataset，收录了100k的QA对，其中每个Query的Answer是在对应的Passage中的一段连续文本（answer span）</li>\n</ul>\n<h4 id=\"4-2-1-SQuAD-v1-1\"><a href=\"#4-2-1-SQuAD-v1-1\" class=\"headerlink\" title=\"4.2.1 SQuAD v1.1\"></a>4.2.1 SQuAD v1.1</h4><ul>\n<li>首先设<script type=\"math/tex\">S \\in R^H</script>和<script type=\"math/tex\">E \\in R^H</script>分别为answer span中的第一个词和最后一个词的embedding</li>\n<li>那么<script type=\"math/tex\">word_i</script>作为第一个词的概率，可以使用点积+softmax的求得：（其中<script type=\"math/tex\">T_i</script>是<script type=\"math/tex\">word_i</script>对应的output）</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP_{i}=\\frac{e^{S \\cdot T_{i}}}{\\sum_{j} e^{S \\cdot T_{j}}}</script><p>将<script type=\"math/tex\">word_i</script>作为最后一个词的概率也是一样的算法，只是把S替换成E</p>\n<ul>\n<li><p>在训练时的损失为正确的开始和结束位置的最大似然</p>\n</li>\n<li><p>在预测时，每个候选位置，即将<script type=\"math/tex\">word_i</script>到<script type=\"math/tex\">word_j</script>作为answer的score为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nS \\cdot T_i + E \\cdot T_j \\quad (j \\geq i)</script><p>然后取最大score的侯选位置作为输出</p>\n<ul>\n<li><p>超参：batch size = 32，epochs = 3，learning rate = <script type=\"math/tex\">5 \\times 10^{-5}</script></p>\n</li>\n<li><p>在具体实验中，应用于SQuAD数据集上前，先在TriviaQA上微调，进行适当的数据增强</p>\n</li>\n</ul>\n<h4 id=\"4-2-2-SQuAD-v2-0\"><a href=\"#4-2-2-SQuAD-v2-0\" class=\"headerlink\" title=\"4.2.2 SQuAD v2.0\"></a>4.2.2 SQuAD v2.0</h4><ul>\n<li>SQuAD v2.0相对于SQuAD v1.1增加了一个No Answer的输出，因为一个问题的答案并不总是出现在passage中的，No Answer的的具体形式为start和end都是[CLS]的answer span，预测为No Answer的score为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ns_{null} = S \\cdot C + E \\cdot C</script><p>当满足下式时，则不预测为No Answer：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{s_{i, j}}>s_{\\mathrm{null}}+\\tau</script><p>其中<script type=\"math/tex\">\\hat{s_{i, j}}=\\max _{j \\geq i} S \\cdot T_{i}+E \\cdot T_{j}</script>，而<script type=\"math/tex\">\\tau</script>是通过实验所得，使在验证集上获得最大的F1</p>\n<ul>\n<li>在本次实验中并未使用TriviaQA data set</li>\n<li>超参：batch size = 48，epochs = 2，learning rate = <script type=\"math/tex\">5 \\times 10^{-5}</script></li>\n</ul>\n<h3 id=\"4-3-SWAG\"><a href=\"#4-3-SWAG\" class=\"headerlink\" title=\"4.3 SWAG\"></a>4.3 SWAG</h3><ul>\n<li>全称The Situations With Adversarial Generations，用于常识推断任务，具体任务是给定一个sentence，然后需要在4个选择中选出最合适的答案</li>\n<li>任务可建模为：每次有4个输入序列，每个输出是给定的sentence+4个可能的选择之一，最后得到C向量，再加一层全连接层，用sotfmax计算概率</li>\n<li>超参：batch size = 16，epochs = 3，learning rate = <script type=\"math/tex\">2 \\times 10^{-5}</script></li>\n</ul>\n<h1 id=\"5-BERT和其他模型的对比\"><a href=\"#5-BERT和其他模型的对比\" class=\"headerlink\" title=\"5 BERT和其他模型的对比\"></a>5 BERT和其他模型的对比</h1><ul>\n<li>实验进行了ELMo，OpenAI GPT和BERT之间的对比</li>\n<li>首先介绍大致做法和结构：</li>\n</ul>\n<blockquote>\n<ol>\n<li>BERT使用双向Transformer，OpenAI GPT使用LTR Transformer，而ELMo使用LTR和RTL的LSTM在最后的简单连结</li>\n<li>BERT和OpenAI GPT使用fine-tuning approaches，而ELMo使用feature-based approach</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220702121317344.png\" alt=\"image-20220702121317344\" style=\"zoom:80%;\" /></p>\n</blockquote>\n<ul>\n<li>另外，ELMo使用在最后将LTR和TRL简单的连结，有以下缺点：</li>\n</ul>\n<blockquote>\n<ol>\n<li>两倍的工作量</li>\n<li>对于有些任务是不直观的，如QA</li>\n<li>BERT在每层都可以实现双向，而ELMo只会在最后连结</li>\n</ol>\n</blockquote>\n<h1 id=\"6-BERT的变体\"><a href=\"#6-BERT的变体\" class=\"headerlink\" title=\"6 BERT的变体\"></a>6 BERT的变体</h1><ul>\n<li><p><a href=\"https://zlkqz.top/2022/08/09/RoBERTa%E6%80%BB%E7%BB%93/\">RoBERTa</a></p>\n</li>\n<li><p><a href=\"https://zlkqz.top/2022/08/10/BART%E6%80%BB%E7%BB%93/\">BART</a></p>\n</li>\n</ul>\n"},{"title":"CNN基本概念","math":true,"date":"2021-11-25T16:00:00.000Z","_content":"\n\n\n# 1 卷积层\n\n### 1.1 互相关运算\n\n- **在⼆维卷积层中，⼀个⼆维输⼊数组和⼀个⼆维核（kernel）数组通过互相 关运算输出⼀个⼆维数组**，如下图：\n\n![image-20211124131936578](https://i.loli.net/2021/11/24/8amf1EYbuU7JPKd.png)\n\n卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依 次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘 并求和，得到输出数组中相应位置的元素\n\n- **⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出**\n- **使用互相关运算做边缘检测：**\n\n比如我们把kernel设为[1, -1]，要识别的图像为单通道，只有黑白：![image-20211124132335469](https://i.loli.net/2021/11/24/Uro3neFEKigbCd2.png)\n\n则进行互相关运算之后可以变为：![image-20211124132405678](https://i.loli.net/2021/11/24/kuH17Ef3p8Rbjdi.png)\n\n由此我们可以看出边缘是在第2和第6列\n\n- **卷积运算：**其实就是把核数组左右翻转并 上下翻转，再与输⼊数组做互相关运算。但因为我们的参数都是学习出来的，而不是一开始设定好的，**所以卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出**，而我们一般使用的也是互相关运算\n\n\n\n### 1.2 特征图和感受野\n\n- **⼆维卷积层输出的⼆维数组可以看作输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）**\n- **影响元素x的前向计算的所有可能输⼊区域叫做x的感受野（receptive field）**，用上图距离，阴影中19的感受野就是输入的四个阴影，如果输入的元素又是另一个卷积运算的输出，则我们还可以将19的感受野扩大\n- 可⻅，我们可以通过更深的卷积神经⽹络使特征 图中单个元素的感受野变得更加⼴阔，从而捕捉输⼊上更⼤尺⼨的特征\n\n\n\n### 1.3 填充和步幅\n\n- **填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）**，如下图就是对3$\\times$3数组进行填充，填充为1：\n\n![image-20211124133849360](https://i.loli.net/2021/11/24/YE4p8vg31IaWbuK.png)\n\n- **我们将每次滑动的⾏数和列数称为步幅（stride）**\n\n- **一般来说输出的高宽为：**\n  $$\n  [(n_h - k_h + p_h + s_h) / s_h] \\times [(n_w - k_w + p_w + s_w) / s_w]\n  $$\n  其中：$n为输入，k为核，p为填充，s为步幅$\n\n- 步幅为1时，很多情况下，我们会设置$p_h = k_h−1$和$p_w = k_w −1$来使输⼊和输出具有相同的⾼和宽\n\n\n\n### 1.4 多输入通道和多输出通道\n\n- 当输入通道 $c_i > 1$ 时，我们为每个输入通道都分配一个核数组，则得到一个形状为$c_i \\times k_h \\times k_w$的卷积核，最后再将每个通道上的卷积结果相加，就得到输出，如下图：\n\n![image-20211124140843742](https://i.loli.net/2021/11/24/iNGt96JXUlSAEKZ.png)\n\n- 可是按上面的方法，无论输入通道是多少，输出通道总为1，我们设输入、输出通道分别为$c_i, c_o$，如果我们想要多通道的输出，则可以给每个输出通道都创建一个形状为$c_i \\times k_h \\times k_w$的核，则可以得到形状为$c_o \\times c_i \\times k_h \\times k_w$的卷积核\n\n\n\n### 1.5 1$\\times$1卷积层\n\n- 卷积窗口形状为1 × 1的多通道卷积层。我们通常称之为1 × 1卷 积层，并将其中的卷积运算称为1 × 1卷积\n- 因为使⽤了最小窗口，1 × 1卷积失去了卷积层可以 识别⾼和宽维度上相邻元素构成的模式的功能。**实际上，1 × 1卷积的主要计算发⽣在通道维上， 1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度**\n- **假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那 么1 × 1卷积层的作⽤与全连接层等价**\n\n\n\n### 1.6 卷积层相对于全连接层的优点\n\n- **卷积层使图像的像素在⾼和宽两个⽅向上的相关性均可能被有效识别**，用全连接层举例，全连接层就相当于一个很大的卷积核，但是过大的卷积核带来的问题是我们可能无法提取到有效特征，并且像素在高宽方向上的相关性可能提取不到，如果选择合适的卷积核大小才更可能提取到\n- 卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重复计算，同一输入通道使用同一卷积核，**实现参数共享**，从而**避免参数过多**。同时参数共享也具有物理意义，他使卷积层**具有平移等特性**，比如图像中有一只猫，那么无论他在图像的任何位置，我们都能提取到他的主要特征，将他识别为猫\n- 对于全连接层，**任意一对输入和输出之间都会产生交互**，形成稠密的连接结构\n\n![image-20211125234024022](https://i.loli.net/2021/11/25/BuGFLnhli2NJPCv.png)\n\n而在卷积神经网络中，卷积核尺度远小于输入的维度，**这样每个输出神经元仅与前一层部分神经元产生交互**\n\n![image-20211125234143051](https://i.loli.net/2021/11/26/HBjoOyvEcqxSna6.png)\n\n我们将这种特性称为**稀疏交互**，这样我们可以将**优化过程的时间复杂度减少好几个数量级，并且缓解过拟合**\n\n稀疏交互的物理意义是**许多现实中的数据都具有局部的特征结构，我们可以先学习局部的特征，然后再将局部的特征组合起来形成更复杂和抽象的特征**\n\n\n\n\n\n# 2 池化层\n\n- **池化层的提出是为了缓解卷积层对位置的过度敏感性**。不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均值。该运算也分别叫做**最⼤池化**或**平均池化**\n- **池化层没有参数**\n- Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，这个妥协会越来越小，下面介绍几种常用的池化层\n\n\n\n### 2.1 平均池化层（mean-pooling）\n\n- 即对邻域内特征点只求平均\n- 优缺点：**抑制邻域大小受限造成的估计值方差增大**，能很好的保留背景，但容易使得图片变模糊\n\n\n\n### 2.2 最大池化层（max-pooling）\n\n- 即对邻域内特征点取最大\n- 优缺点：**抑制积层参数误差造成估计均值的偏移**，能很好的保留纹理特征，一般现在都用max-pooling而很少用mean-pooling\n\n\n\n### 2.3 全局平均池化（global average pooling）\n\n- 对每个通道中所有元素求平均并直接⽤于分类\n- 优点：大幅度减少网络参数，理所当然的减少了过拟合现象\n\n\n\n### 2.4 池化层的作用\n\n1. **对特征进行压缩，保留主要特征的同时减少参数和计算量，减少模型复杂度，防止过拟合。**用池化层做互相关运算，本身就能减少特征的个数。并且对于全局平均优化，如果我们要进行图像分类，我们需要使用参数很多的全连接层，最后再导入Softmax，而如果我们使用全局平均优化，则可以规避全连接庞大的参数量，减少过拟合\n2. **实现不变性**，包括平移不变性、旋转不变性和尺度不变性。\n\n\n\n### 2.5 池化层的多通道\n\n- 和卷积层有区别， 在处理多通道输⼊数据时，**池化层对每个输⼊通道分别池化，而不是像卷积层那样将各通道的输 ⼊按通道相加。这意味着池化层的输出通道数与输⼊通道数相等**\n\n\n\n\n\n# 3 LeNet\n\n- 分为卷积层块和全连接层块两个部分\n\n### 3.1 卷积层块\n\n- 卷积层块⾥的基本单位是卷积层后接最⼤池化层，卷积层⽤来识别图像⾥的空间模式，如线条和 物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性\n- 在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似\n- 卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2\n\n\n\n### 3.2 全连接层块\n\n- 当卷积层块的输出传⼊全连接层块时，全连接 层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输⼊形状将变成⼆维，其中第⼀维是小批量中的样本，第⼆维是每个样本变平后的向量表⽰，且向量⻓度为通道、⾼和宽的乘积\n- 全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数\n\n\n\n![20150903212346407](https://i.loli.net/2021/11/28/NH5zDmge7RiwaGA.png)\n\n\n\n\n\n# 4 AlexNet（相对于LeNet较深）\n\n- 相对于较小的LeNet，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层\n- AlexNet第⼀层中的卷积窗口形状是11 × 11，步幅为4。因为ImageNet中绝⼤多数图像的⾼和宽均 ⽐MNIST图像的⾼和宽⼤10倍以上，ImageNet图像的物体占⽤更多的像素，所以需要更⼤的卷积窗口来捕获物体。第⼆层中的卷积窗口形状减小到5 × 5，之后全采⽤3 × 3\n- 第⼀、第⼆和第五个卷积层之后都使⽤了窗口形状为3 × 3、步幅为2的最⼤池化层\n- AlexNet使⽤的卷积通道数也⼤于LeNet中的卷积通道数数⼗倍，5层的输出通道数分别为96，256，384，384，256\n- 紧接着最后⼀个卷积层的是两个输出个数为4096的全连接层\n- AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数\n- AlexNet使用了Dropout和图像增广\n- AlexNet相比于LeNet有更小的学习率\n- **AlexNet的一个很大的意义就是在图像识别上AI超越人工，并且首次使用了GPU加速**\n\n![v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b](https://pic2.zhimg.com/v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b.jpg)\n\n\n\n\n\n# 5 VGG（使用重复元素）\n\n### 5.1 VGG块\n\n- 连续使⽤**数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层**。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。可以指定卷积层的数量num_convs和输出通道数num_channels\n\n\n\n### 5.2 VGG网络\n\n- 我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积层和3个全连接层，所以经常被称为VGG-11\n- 之后的3个全连接层神经元个数分别为：4096，4096，10（最后一层为类别个数）\n- 使用VGG后，每次输⼊的⾼和宽将减半，直到最终⾼和宽变成7后传⼊全连接层。与此同时，输出通道数每次翻倍，直到变成512。VGG这种⾼和宽减半以及通道翻倍的设计使多数卷积层都有相同的模型参数尺⼨和计算复杂度\n\n\n\n- **VGG的一个很大的意义就是VGG用数个小卷积核代替大卷积核，使参数量减少**\n\n![20180205192403250](https://i.loli.net/2021/11/28/tTvmMekgd81PJX6.png)\n\n\n\n\n\n# 6 NiN（网络中的网络）\n\n- 的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深\n- 但是NiN有所不同，由于1$\\times$1卷积层可以替代全连接层，所以NiN中就进行了替换：\n- **NiN之所以被称为网络中的网络是因为：NiN串联多个由卷积层和“准全连接层”（1X1的网络）的小网络来构成一个深层网络**\n\n![image-20211125152432130](https://i.loli.net/2021/11/25/NjfabGRcVe8ysZk.png)\n\n### 6.1 NiN块\n\n- NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 × 1卷积层串联而成。其中**第⼀个卷积层的超参数可以⾃⾏设置，而第⼆和第三个卷积层的超参数⼀般是固定的**\n- 三个卷积层的通道数是相同的\n\n\n\n### 6.2 NiN模型\n\n- NiN使⽤卷积窗口形状分别为11 × 11、5 × 5和3 × 3的卷积层，相应的输出通道数也与AlexNet中的⼀致。每个NiN块后接⼀ 个步幅为2、窗口形状为3 × 3的最⼤池化层\n- 除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤**全局平均池化层**对每个通道中所有元素求平均并直接⽤于分类。**这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合，然而，该设计有时会造成获得有效模型的训练时间的增加**\n- NiN的学习率一般比AlexNet和VGG大\n\n![image-20211128200750143](https://i.loli.net/2021/11/28/2eiOxQsw4XoBFN9.png)\n\n\n\n\n\n# 7 GoogLeNet（含并行连接的网络）\n\n### 7.1 Inception块\n\n![image-20211125155525157](https://i.loli.net/2021/11/25/Q4sfSlhjntDaXFM.png)\n\n- Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 × 3和5 × 5的卷积层来**抽取不同空间尺⼨下的信息**\n\n- 其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，**以降低模型复杂度**。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数\n\n- 4条线路都使⽤了合适的填充来使**输⼊与输出的⾼和宽⼀致**。最后我们将每条线路的输出**在通道维上连结**，并输⼊接下来的层中去\n\n\n\n### 7.2 GoogLeNet模型\n\n- 在主体卷积部分中使⽤5个模块（block），**每个模块之间使⽤步幅为2的3× 3最⼤池化层来减小输出⾼宽**\n- 第⼀模块使⽤⼀个64通道的7 × 7卷积层，步幅为2\n\n\n\n- 第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层，然后是将通道增⼤3倍（变为192）的3 × 3卷积层\n\n\n\n- 第三模块串联2个完整的Inception块。第⼀个Inception块的输出通道数为64+128+32+32 = 256（4个加数对应4条线路的通道数），其中第⼆、第三条线路由于有两个卷积层，所以两条线路的第一个$$1 \\times 1$$卷积层先将输出通道减少为96和16，再接上第⼆层卷积层。\n\n  第⼆个Inception块 输出通道数增⾄128 + 192 + 96 + 64 = 480。其中第⼆、第三条线路$$1 \\times 1$$卷积层的输出通道分别为128和32\n\n\n\n- 第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + (96,208) + (16,48) + 64 = 512、 160+(112,224)+(24,64)+64 = 512、128+(128,256)+(24,64)+64 = 512、112+(144,288)+(32,64)+64 = 528和 256+(160,320)+(32,128)+128 = 832\n\n  其中括号里的第一个数字为二三条通道的第一个卷积核输出通道数\n\n\n\n- 第五模块有输出通道数为256 + (160,320) + (32,128) + 128 = 832和384 + (192,384) + (48,128) + 128 = 1024的两个Inception块，**然后再接上一个全局平均池化层**\n\n\n\n- **五个模块相连之后最后再接上一个全连接层，神经元个数为类别个数**\n\n\n\n![142051802f8de8513fe61601277f03c8](https://i.loli.net/2021/11/28/rdpGSFRzuKZeTs2.jpg)\n\n\n\n\n\n# 8 批量归一化\n\n- 我们一般在前向传播开始之前会对数据进行归一化，**使不同特征之间具有可比性，并且更快收敛**\n- 通常来说，数据标准化预处理对于浅层模型就⾜够有效了，**但对深层神经网络来说，即使输⼊数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。**这种计算数值的不稳定性通常令我们难以训练出有效的深度模型\n- 而批量归一化则是对每一层的输出都做一次归一化，**使均值永远为0，方差永远为1**\n\n\n\n### 8.1 批量归一化层\n\n- **通常，我们将批量归⼀化层置于全连接层中的仿射变换和激活函数之间**\n- 首先要对小批量求均值和方差：\n\n$$\n\\begin{array}{c}\n\\boldsymbol{\\mu}_{\\mathcal{B}} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} \\boldsymbol{x}^{(i)} \\\\\n\\boldsymbol{\\sigma}_{\\mathcal{B}}^{2} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{\\mathcal{B}}\\right)^{2}\n\\end{array}\n$$\n\n得到的均值和方差是两个向量，维度为特征个数\n\n- 然后：\n\n$$\n\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{\\mathcal{B}}}{\\sqrt{\\boldsymbol{\\sigma}_{\\mathcal{B}}^{2}+\\epsilon}}\n$$\n\n- $\\epsilon$ 为一个很小的数，一般取$10^{-8}$ ，是为了保证分母大于0\n- **但是我们不确定是否一定是所有层都进行批量归一化才是最好的情况，所以我们要给予一个能变回归一化前的值的可能性，所以引入拉伸参数（scale）$\\gamma$ 和  偏移参数（shift）$\\beta$**\n\n$$\n\\boldsymbol{y}^{(i)} \\leftarrow \\gamma \\odot \\hat{\\boldsymbol{x}}^{(i)}+\\boldsymbol{\\beta}\n$$\n\n将$\\gamma$ 和 $\\beta$ 作为两个可变化参数，然后通过学习来决定拉伸多少和偏移多少\n\n\n\n### 8.2 对卷积层做批量归一化\n\n- 批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前\n- 如果卷积计算输出多个通道，我们需要**对这些通道的输出分别做批量归⼀化，且每个通道都拥有独立的拉伸和偏移参数**\n\n- 设小批量中有m个样本。在单个通道上，假设卷积计算输出的⾼和宽分别为p和q。我们需要对该通道中m × p × q个元素同时做批量归⼀化\n\n\n\n### 8.3 预测时的批量归一化\n\n- **批量归一化在训练模式和预测模式的计算结果是不⼀样的**。确定均值和方差时，单个样本的输出不应取决于批量归⼀化所需要的随机小批量中的均值和⽅差。⼀种常⽤的⽅法是**通过移动平均（指数加权平均）估算整个训练数据集的样本均值和方差**，并在预测时使⽤它们得到确定的输出\n\n\n\n\n\n# 9 残差网络（ResNet）\n\n### 9.1 残差块\n\n![image-20211125221657826](https://i.loli.net/2021/11/25/zADLUxHm7wS8Z9Q.png)\n\n- 左图是一般形式的映射，右图为残差映射，是将输入x加权->激活->再加权后，再和原输入x相加，再送入激活函数\n- 这样的结构中，输⼊可通过跨层的数据线路更快地向前传播\n- 残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接⼀个批量归⼀化层和ReLU激活函数。然后我们**将输⼊跳过这2个卷积运算后直接加在最后的ReLU激活函数前**\n- 这样的设计要求2个卷积层的输出与输⼊形状⼀样，从而可以相加。如果想改变通道数，就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算\n\n![](https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg)\n\n\n\n### 9.2 ResNet模型\n\n- ResNet的前两层跟之前介绍的GoogLeNet中的⼀样：在输出通道数为64、步幅为2的7 × 7卷积层后接步幅为2的3 × 3的最⼤池化层。不同之处在于**ResNet每个卷积层（对应上图每一次加权运算）后增加的批量归⼀化层**\n\n\n\n- **后接4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块**\n- 第⼀个模块的通道数同输⼊通道数⼀致。由于之前已经使⽤了步幅为2的最⼤池化层，所以⽆须减小⾼和宽。**之后的每个模块在第⼀个残差块⾥将上⼀个模块的通道数翻倍，并将高和宽减半**\n\n\n\n- 接着我们为ResNet加⼊所有残差块。这⾥每个模块的残差块个数可以自行定义\n\n\n\n- 最后，与GoogLeNet⼀样，加⼊全局平均池化层后接上全连接层输出\n\n![image-20211125223902681](https://i.loli.net/2021/11/25/9aypGjvX1ESYtrM.png)\n\n其中10为类别个数\n\n![](https://pic3.zhimg.com/80/v2-862e1c2dcb24f10d264544190ad38142_720w.jpg)\n\n\n\n### 9.3 ResNet的作用\n\n- 在深度神经网络中，如果我们进行反向传播，那么由链式求导法则可知，我们会**涉及到非常多参数和导数的连乘**，这时误差很容易产生消失或膨胀，通过实验也可以发现，层数更深的神经网络结果反而没有浅层的好\n\n![image-20211125235804507](https://i.loli.net/2021/11/26/YecToJs5xNf6H24.png)\n\n这种结果很大程度归结于深度神经网络的**梯度消失问题**\n\n- 而ResNet的提出就是为了解决梯度消失的问题，**既然离输入近的神经网络层较难训练，那么我们可以将他短接到更接近输出的层**，并且这种方法并不会对结果产生影响，假设输入为X，我们在一般网络中想要拟合f(X)，放到残差网络中就只需要拟合f(X) - X 即可\n- **残差网络的设计思想不仅仅局限于卷积网络，实际上很多其他的网络也用到了残差，也取得了很不错的效果，都是为了解决深度神经网络的梯度消失问题**\n\n\n\n\n\n# 10 稠密连接网络（DenseNet） \n\n![image-20211125224307704](https://i.loli.net/2021/11/25/NTajCiqAydPvoVt.png)\n\n- DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是**在通道维上连结**\n- DenseNet的主要构建模块是**稠密块（dense block）**和**过渡层（transition layer）**。前者定义了输⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤\n\n\n\n### 10.1 稠密块\n\n- 我们将批量归⼀化、激活和卷积组合到一起形成一种块：\n\n![image-20211125225431518](https://i.loli.net/2021/11/25/hfHF7BLySM1swZG.png)\n\n- **稠密块由多个conv_block组成，每块使⽤相同的输出通道数。但在前向计算时，我们将每块的输⼊和输出在通道维上连结**\n\n\n\n### 10.2 过渡层\n\n- 由于每个稠密块都会带来通道数的增加，使⽤过多则会带来过于复杂的模型。过渡层⽤来控制模型复杂度。它**通过1 × 1卷积层来减小通道数，并使⽤步幅为2的平均池化层减半高和宽**，并且也要进行激活和BN运算\n\n\n\n### 10.3 DenseNet模型\n\n- DenseNet⾸先使⽤同ResNet⼀样的单卷积层和最⼤池化层\n\n![image-20211125230039527](https://i.loli.net/2021/11/25/DqcGv6FtMioNez1.png)\n\n- 接着使用4个稠密块，同ResNet⼀样，我们可以设置每个稠密块使⽤多少个卷积层\n\n\n\n- 在稠密块之间我们使用过渡层来减半高宽，并减半通道数\n\n  注意最后一层是不使用过渡层进行减半的\n\n- 最后再和ResNet一样，接上全局平均池化层和全连接层来输出\n\n![](https://pic2.zhimg.com/80/v2-c81da515c8fa9796601fde82e4d36f61_720w.jpg)\n","source":"_posts/CNN.md","raw":"---\ntitle: CNN基本概念\nmath: true\ndate: 2021-11-26\n---\n\n\n\n# 1 卷积层\n\n### 1.1 互相关运算\n\n- **在⼆维卷积层中，⼀个⼆维输⼊数组和⼀个⼆维核（kernel）数组通过互相 关运算输出⼀个⼆维数组**，如下图：\n\n![image-20211124131936578](https://i.loli.net/2021/11/24/8amf1EYbuU7JPKd.png)\n\n卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依 次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘 并求和，得到输出数组中相应位置的元素\n\n- **⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出**\n- **使用互相关运算做边缘检测：**\n\n比如我们把kernel设为[1, -1]，要识别的图像为单通道，只有黑白：![image-20211124132335469](https://i.loli.net/2021/11/24/Uro3neFEKigbCd2.png)\n\n则进行互相关运算之后可以变为：![image-20211124132405678](https://i.loli.net/2021/11/24/kuH17Ef3p8Rbjdi.png)\n\n由此我们可以看出边缘是在第2和第6列\n\n- **卷积运算：**其实就是把核数组左右翻转并 上下翻转，再与输⼊数组做互相关运算。但因为我们的参数都是学习出来的，而不是一开始设定好的，**所以卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出**，而我们一般使用的也是互相关运算\n\n\n\n### 1.2 特征图和感受野\n\n- **⼆维卷积层输出的⼆维数组可以看作输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）**\n- **影响元素x的前向计算的所有可能输⼊区域叫做x的感受野（receptive field）**，用上图距离，阴影中19的感受野就是输入的四个阴影，如果输入的元素又是另一个卷积运算的输出，则我们还可以将19的感受野扩大\n- 可⻅，我们可以通过更深的卷积神经⽹络使特征 图中单个元素的感受野变得更加⼴阔，从而捕捉输⼊上更⼤尺⼨的特征\n\n\n\n### 1.3 填充和步幅\n\n- **填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）**，如下图就是对3$\\times$3数组进行填充，填充为1：\n\n![image-20211124133849360](https://i.loli.net/2021/11/24/YE4p8vg31IaWbuK.png)\n\n- **我们将每次滑动的⾏数和列数称为步幅（stride）**\n\n- **一般来说输出的高宽为：**\n  $$\n  [(n_h - k_h + p_h + s_h) / s_h] \\times [(n_w - k_w + p_w + s_w) / s_w]\n  $$\n  其中：$n为输入，k为核，p为填充，s为步幅$\n\n- 步幅为1时，很多情况下，我们会设置$p_h = k_h−1$和$p_w = k_w −1$来使输⼊和输出具有相同的⾼和宽\n\n\n\n### 1.4 多输入通道和多输出通道\n\n- 当输入通道 $c_i > 1$ 时，我们为每个输入通道都分配一个核数组，则得到一个形状为$c_i \\times k_h \\times k_w$的卷积核，最后再将每个通道上的卷积结果相加，就得到输出，如下图：\n\n![image-20211124140843742](https://i.loli.net/2021/11/24/iNGt96JXUlSAEKZ.png)\n\n- 可是按上面的方法，无论输入通道是多少，输出通道总为1，我们设输入、输出通道分别为$c_i, c_o$，如果我们想要多通道的输出，则可以给每个输出通道都创建一个形状为$c_i \\times k_h \\times k_w$的核，则可以得到形状为$c_o \\times c_i \\times k_h \\times k_w$的卷积核\n\n\n\n### 1.5 1$\\times$1卷积层\n\n- 卷积窗口形状为1 × 1的多通道卷积层。我们通常称之为1 × 1卷 积层，并将其中的卷积运算称为1 × 1卷积\n- 因为使⽤了最小窗口，1 × 1卷积失去了卷积层可以 识别⾼和宽维度上相邻元素构成的模式的功能。**实际上，1 × 1卷积的主要计算发⽣在通道维上， 1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度**\n- **假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那 么1 × 1卷积层的作⽤与全连接层等价**\n\n\n\n### 1.6 卷积层相对于全连接层的优点\n\n- **卷积层使图像的像素在⾼和宽两个⽅向上的相关性均可能被有效识别**，用全连接层举例，全连接层就相当于一个很大的卷积核，但是过大的卷积核带来的问题是我们可能无法提取到有效特征，并且像素在高宽方向上的相关性可能提取不到，如果选择合适的卷积核大小才更可能提取到\n- 卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重复计算，同一输入通道使用同一卷积核，**实现参数共享**，从而**避免参数过多**。同时参数共享也具有物理意义，他使卷积层**具有平移等特性**，比如图像中有一只猫，那么无论他在图像的任何位置，我们都能提取到他的主要特征，将他识别为猫\n- 对于全连接层，**任意一对输入和输出之间都会产生交互**，形成稠密的连接结构\n\n![image-20211125234024022](https://i.loli.net/2021/11/25/BuGFLnhli2NJPCv.png)\n\n而在卷积神经网络中，卷积核尺度远小于输入的维度，**这样每个输出神经元仅与前一层部分神经元产生交互**\n\n![image-20211125234143051](https://i.loli.net/2021/11/26/HBjoOyvEcqxSna6.png)\n\n我们将这种特性称为**稀疏交互**，这样我们可以将**优化过程的时间复杂度减少好几个数量级，并且缓解过拟合**\n\n稀疏交互的物理意义是**许多现实中的数据都具有局部的特征结构，我们可以先学习局部的特征，然后再将局部的特征组合起来形成更复杂和抽象的特征**\n\n\n\n\n\n# 2 池化层\n\n- **池化层的提出是为了缓解卷积层对位置的过度敏感性**。不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均值。该运算也分别叫做**最⼤池化**或**平均池化**\n- **池化层没有参数**\n- Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，这个妥协会越来越小，下面介绍几种常用的池化层\n\n\n\n### 2.1 平均池化层（mean-pooling）\n\n- 即对邻域内特征点只求平均\n- 优缺点：**抑制邻域大小受限造成的估计值方差增大**，能很好的保留背景，但容易使得图片变模糊\n\n\n\n### 2.2 最大池化层（max-pooling）\n\n- 即对邻域内特征点取最大\n- 优缺点：**抑制积层参数误差造成估计均值的偏移**，能很好的保留纹理特征，一般现在都用max-pooling而很少用mean-pooling\n\n\n\n### 2.3 全局平均池化（global average pooling）\n\n- 对每个通道中所有元素求平均并直接⽤于分类\n- 优点：大幅度减少网络参数，理所当然的减少了过拟合现象\n\n\n\n### 2.4 池化层的作用\n\n1. **对特征进行压缩，保留主要特征的同时减少参数和计算量，减少模型复杂度，防止过拟合。**用池化层做互相关运算，本身就能减少特征的个数。并且对于全局平均优化，如果我们要进行图像分类，我们需要使用参数很多的全连接层，最后再导入Softmax，而如果我们使用全局平均优化，则可以规避全连接庞大的参数量，减少过拟合\n2. **实现不变性**，包括平移不变性、旋转不变性和尺度不变性。\n\n\n\n### 2.5 池化层的多通道\n\n- 和卷积层有区别， 在处理多通道输⼊数据时，**池化层对每个输⼊通道分别池化，而不是像卷积层那样将各通道的输 ⼊按通道相加。这意味着池化层的输出通道数与输⼊通道数相等**\n\n\n\n\n\n# 3 LeNet\n\n- 分为卷积层块和全连接层块两个部分\n\n### 3.1 卷积层块\n\n- 卷积层块⾥的基本单位是卷积层后接最⼤池化层，卷积层⽤来识别图像⾥的空间模式，如线条和 物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性\n- 在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似\n- 卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2\n\n\n\n### 3.2 全连接层块\n\n- 当卷积层块的输出传⼊全连接层块时，全连接 层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输⼊形状将变成⼆维，其中第⼀维是小批量中的样本，第⼆维是每个样本变平后的向量表⽰，且向量⻓度为通道、⾼和宽的乘积\n- 全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数\n\n\n\n![20150903212346407](https://i.loli.net/2021/11/28/NH5zDmge7RiwaGA.png)\n\n\n\n\n\n# 4 AlexNet（相对于LeNet较深）\n\n- 相对于较小的LeNet，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层\n- AlexNet第⼀层中的卷积窗口形状是11 × 11，步幅为4。因为ImageNet中绝⼤多数图像的⾼和宽均 ⽐MNIST图像的⾼和宽⼤10倍以上，ImageNet图像的物体占⽤更多的像素，所以需要更⼤的卷积窗口来捕获物体。第⼆层中的卷积窗口形状减小到5 × 5，之后全采⽤3 × 3\n- 第⼀、第⼆和第五个卷积层之后都使⽤了窗口形状为3 × 3、步幅为2的最⼤池化层\n- AlexNet使⽤的卷积通道数也⼤于LeNet中的卷积通道数数⼗倍，5层的输出通道数分别为96，256，384，384，256\n- 紧接着最后⼀个卷积层的是两个输出个数为4096的全连接层\n- AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数\n- AlexNet使用了Dropout和图像增广\n- AlexNet相比于LeNet有更小的学习率\n- **AlexNet的一个很大的意义就是在图像识别上AI超越人工，并且首次使用了GPU加速**\n\n![v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b](https://pic2.zhimg.com/v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b.jpg)\n\n\n\n\n\n# 5 VGG（使用重复元素）\n\n### 5.1 VGG块\n\n- 连续使⽤**数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层**。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。可以指定卷积层的数量num_convs和输出通道数num_channels\n\n\n\n### 5.2 VGG网络\n\n- 我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积层和3个全连接层，所以经常被称为VGG-11\n- 之后的3个全连接层神经元个数分别为：4096，4096，10（最后一层为类别个数）\n- 使用VGG后，每次输⼊的⾼和宽将减半，直到最终⾼和宽变成7后传⼊全连接层。与此同时，输出通道数每次翻倍，直到变成512。VGG这种⾼和宽减半以及通道翻倍的设计使多数卷积层都有相同的模型参数尺⼨和计算复杂度\n\n\n\n- **VGG的一个很大的意义就是VGG用数个小卷积核代替大卷积核，使参数量减少**\n\n![20180205192403250](https://i.loli.net/2021/11/28/tTvmMekgd81PJX6.png)\n\n\n\n\n\n# 6 NiN（网络中的网络）\n\n- 的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深\n- 但是NiN有所不同，由于1$\\times$1卷积层可以替代全连接层，所以NiN中就进行了替换：\n- **NiN之所以被称为网络中的网络是因为：NiN串联多个由卷积层和“准全连接层”（1X1的网络）的小网络来构成一个深层网络**\n\n![image-20211125152432130](https://i.loli.net/2021/11/25/NjfabGRcVe8ysZk.png)\n\n### 6.1 NiN块\n\n- NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 × 1卷积层串联而成。其中**第⼀个卷积层的超参数可以⾃⾏设置，而第⼆和第三个卷积层的超参数⼀般是固定的**\n- 三个卷积层的通道数是相同的\n\n\n\n### 6.2 NiN模型\n\n- NiN使⽤卷积窗口形状分别为11 × 11、5 × 5和3 × 3的卷积层，相应的输出通道数也与AlexNet中的⼀致。每个NiN块后接⼀ 个步幅为2、窗口形状为3 × 3的最⼤池化层\n- 除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤**全局平均池化层**对每个通道中所有元素求平均并直接⽤于分类。**这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合，然而，该设计有时会造成获得有效模型的训练时间的增加**\n- NiN的学习率一般比AlexNet和VGG大\n\n![image-20211128200750143](https://i.loli.net/2021/11/28/2eiOxQsw4XoBFN9.png)\n\n\n\n\n\n# 7 GoogLeNet（含并行连接的网络）\n\n### 7.1 Inception块\n\n![image-20211125155525157](https://i.loli.net/2021/11/25/Q4sfSlhjntDaXFM.png)\n\n- Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 × 3和5 × 5的卷积层来**抽取不同空间尺⼨下的信息**\n\n- 其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，**以降低模型复杂度**。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数\n\n- 4条线路都使⽤了合适的填充来使**输⼊与输出的⾼和宽⼀致**。最后我们将每条线路的输出**在通道维上连结**，并输⼊接下来的层中去\n\n\n\n### 7.2 GoogLeNet模型\n\n- 在主体卷积部分中使⽤5个模块（block），**每个模块之间使⽤步幅为2的3× 3最⼤池化层来减小输出⾼宽**\n- 第⼀模块使⽤⼀个64通道的7 × 7卷积层，步幅为2\n\n\n\n- 第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层，然后是将通道增⼤3倍（变为192）的3 × 3卷积层\n\n\n\n- 第三模块串联2个完整的Inception块。第⼀个Inception块的输出通道数为64+128+32+32 = 256（4个加数对应4条线路的通道数），其中第⼆、第三条线路由于有两个卷积层，所以两条线路的第一个$$1 \\times 1$$卷积层先将输出通道减少为96和16，再接上第⼆层卷积层。\n\n  第⼆个Inception块 输出通道数增⾄128 + 192 + 96 + 64 = 480。其中第⼆、第三条线路$$1 \\times 1$$卷积层的输出通道分别为128和32\n\n\n\n- 第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + (96,208) + (16,48) + 64 = 512、 160+(112,224)+(24,64)+64 = 512、128+(128,256)+(24,64)+64 = 512、112+(144,288)+(32,64)+64 = 528和 256+(160,320)+(32,128)+128 = 832\n\n  其中括号里的第一个数字为二三条通道的第一个卷积核输出通道数\n\n\n\n- 第五模块有输出通道数为256 + (160,320) + (32,128) + 128 = 832和384 + (192,384) + (48,128) + 128 = 1024的两个Inception块，**然后再接上一个全局平均池化层**\n\n\n\n- **五个模块相连之后最后再接上一个全连接层，神经元个数为类别个数**\n\n\n\n![142051802f8de8513fe61601277f03c8](https://i.loli.net/2021/11/28/rdpGSFRzuKZeTs2.jpg)\n\n\n\n\n\n# 8 批量归一化\n\n- 我们一般在前向传播开始之前会对数据进行归一化，**使不同特征之间具有可比性，并且更快收敛**\n- 通常来说，数据标准化预处理对于浅层模型就⾜够有效了，**但对深层神经网络来说，即使输⼊数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。**这种计算数值的不稳定性通常令我们难以训练出有效的深度模型\n- 而批量归一化则是对每一层的输出都做一次归一化，**使均值永远为0，方差永远为1**\n\n\n\n### 8.1 批量归一化层\n\n- **通常，我们将批量归⼀化层置于全连接层中的仿射变换和激活函数之间**\n- 首先要对小批量求均值和方差：\n\n$$\n\\begin{array}{c}\n\\boldsymbol{\\mu}_{\\mathcal{B}} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} \\boldsymbol{x}^{(i)} \\\\\n\\boldsymbol{\\sigma}_{\\mathcal{B}}^{2} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{\\mathcal{B}}\\right)^{2}\n\\end{array}\n$$\n\n得到的均值和方差是两个向量，维度为特征个数\n\n- 然后：\n\n$$\n\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{\\mathcal{B}}}{\\sqrt{\\boldsymbol{\\sigma}_{\\mathcal{B}}^{2}+\\epsilon}}\n$$\n\n- $\\epsilon$ 为一个很小的数，一般取$10^{-8}$ ，是为了保证分母大于0\n- **但是我们不确定是否一定是所有层都进行批量归一化才是最好的情况，所以我们要给予一个能变回归一化前的值的可能性，所以引入拉伸参数（scale）$\\gamma$ 和  偏移参数（shift）$\\beta$**\n\n$$\n\\boldsymbol{y}^{(i)} \\leftarrow \\gamma \\odot \\hat{\\boldsymbol{x}}^{(i)}+\\boldsymbol{\\beta}\n$$\n\n将$\\gamma$ 和 $\\beta$ 作为两个可变化参数，然后通过学习来决定拉伸多少和偏移多少\n\n\n\n### 8.2 对卷积层做批量归一化\n\n- 批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前\n- 如果卷积计算输出多个通道，我们需要**对这些通道的输出分别做批量归⼀化，且每个通道都拥有独立的拉伸和偏移参数**\n\n- 设小批量中有m个样本。在单个通道上，假设卷积计算输出的⾼和宽分别为p和q。我们需要对该通道中m × p × q个元素同时做批量归⼀化\n\n\n\n### 8.3 预测时的批量归一化\n\n- **批量归一化在训练模式和预测模式的计算结果是不⼀样的**。确定均值和方差时，单个样本的输出不应取决于批量归⼀化所需要的随机小批量中的均值和⽅差。⼀种常⽤的⽅法是**通过移动平均（指数加权平均）估算整个训练数据集的样本均值和方差**，并在预测时使⽤它们得到确定的输出\n\n\n\n\n\n# 9 残差网络（ResNet）\n\n### 9.1 残差块\n\n![image-20211125221657826](https://i.loli.net/2021/11/25/zADLUxHm7wS8Z9Q.png)\n\n- 左图是一般形式的映射，右图为残差映射，是将输入x加权->激活->再加权后，再和原输入x相加，再送入激活函数\n- 这样的结构中，输⼊可通过跨层的数据线路更快地向前传播\n- 残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接⼀个批量归⼀化层和ReLU激活函数。然后我们**将输⼊跳过这2个卷积运算后直接加在最后的ReLU激活函数前**\n- 这样的设计要求2个卷积层的输出与输⼊形状⼀样，从而可以相加。如果想改变通道数，就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算\n\n![](https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg)\n\n\n\n### 9.2 ResNet模型\n\n- ResNet的前两层跟之前介绍的GoogLeNet中的⼀样：在输出通道数为64、步幅为2的7 × 7卷积层后接步幅为2的3 × 3的最⼤池化层。不同之处在于**ResNet每个卷积层（对应上图每一次加权运算）后增加的批量归⼀化层**\n\n\n\n- **后接4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块**\n- 第⼀个模块的通道数同输⼊通道数⼀致。由于之前已经使⽤了步幅为2的最⼤池化层，所以⽆须减小⾼和宽。**之后的每个模块在第⼀个残差块⾥将上⼀个模块的通道数翻倍，并将高和宽减半**\n\n\n\n- 接着我们为ResNet加⼊所有残差块。这⾥每个模块的残差块个数可以自行定义\n\n\n\n- 最后，与GoogLeNet⼀样，加⼊全局平均池化层后接上全连接层输出\n\n![image-20211125223902681](https://i.loli.net/2021/11/25/9aypGjvX1ESYtrM.png)\n\n其中10为类别个数\n\n![](https://pic3.zhimg.com/80/v2-862e1c2dcb24f10d264544190ad38142_720w.jpg)\n\n\n\n### 9.3 ResNet的作用\n\n- 在深度神经网络中，如果我们进行反向传播，那么由链式求导法则可知，我们会**涉及到非常多参数和导数的连乘**，这时误差很容易产生消失或膨胀，通过实验也可以发现，层数更深的神经网络结果反而没有浅层的好\n\n![image-20211125235804507](https://i.loli.net/2021/11/26/YecToJs5xNf6H24.png)\n\n这种结果很大程度归结于深度神经网络的**梯度消失问题**\n\n- 而ResNet的提出就是为了解决梯度消失的问题，**既然离输入近的神经网络层较难训练，那么我们可以将他短接到更接近输出的层**，并且这种方法并不会对结果产生影响，假设输入为X，我们在一般网络中想要拟合f(X)，放到残差网络中就只需要拟合f(X) - X 即可\n- **残差网络的设计思想不仅仅局限于卷积网络，实际上很多其他的网络也用到了残差，也取得了很不错的效果，都是为了解决深度神经网络的梯度消失问题**\n\n\n\n\n\n# 10 稠密连接网络（DenseNet） \n\n![image-20211125224307704](https://i.loli.net/2021/11/25/NTajCiqAydPvoVt.png)\n\n- DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是**在通道维上连结**\n- DenseNet的主要构建模块是**稠密块（dense block）**和**过渡层（transition layer）**。前者定义了输⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤\n\n\n\n### 10.1 稠密块\n\n- 我们将批量归⼀化、激活和卷积组合到一起形成一种块：\n\n![image-20211125225431518](https://i.loli.net/2021/11/25/hfHF7BLySM1swZG.png)\n\n- **稠密块由多个conv_block组成，每块使⽤相同的输出通道数。但在前向计算时，我们将每块的输⼊和输出在通道维上连结**\n\n\n\n### 10.2 过渡层\n\n- 由于每个稠密块都会带来通道数的增加，使⽤过多则会带来过于复杂的模型。过渡层⽤来控制模型复杂度。它**通过1 × 1卷积层来减小通道数，并使⽤步幅为2的平均池化层减半高和宽**，并且也要进行激活和BN运算\n\n\n\n### 10.3 DenseNet模型\n\n- DenseNet⾸先使⽤同ResNet⼀样的单卷积层和最⼤池化层\n\n![image-20211125230039527](https://i.loli.net/2021/11/25/DqcGv6FtMioNez1.png)\n\n- 接着使用4个稠密块，同ResNet⼀样，我们可以设置每个稠密块使⽤多少个卷积层\n\n\n\n- 在稠密块之间我们使用过渡层来减半高宽，并减半通道数\n\n  注意最后一层是不使用过渡层进行减半的\n\n- 最后再和ResNet一样，接上全局平均池化层和全连接层来输出\n\n![](https://pic2.zhimg.com/80/v2-c81da515c8fa9796601fde82e4d36f61_720w.jpg)\n","slug":"CNN","published":1,"updated":"2022-12-20T06:16:18.204Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1900027csz95o1e83m","content":"<h1 id=\"1-卷积层\"><a href=\"#1-卷积层\" class=\"headerlink\" title=\"1 卷积层\"></a>1 卷积层</h1><h3 id=\"1-1-互相关运算\"><a href=\"#1-1-互相关运算\" class=\"headerlink\" title=\"1.1 互相关运算\"></a>1.1 互相关运算</h3><ul>\n<li><strong>在⼆维卷积层中，⼀个⼆维输⼊数组和⼀个⼆维核（kernel）数组通过互相 关运算输出⼀个⼆维数组</strong>，如下图：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/24/8amf1EYbuU7JPKd.png\" alt=\"image-20211124131936578\"></p>\n<p>卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依 次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘 并求和，得到输出数组中相应位置的元素</p>\n<ul>\n<li><strong>⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出</strong></li>\n<li><strong>使用互相关运算做边缘检测：</strong></li>\n</ul>\n<p>比如我们把kernel设为[1, -1]，要识别的图像为单通道，只有黑白：<img src=\"https://i.loli.net/2021/11/24/Uro3neFEKigbCd2.png\" alt=\"image-20211124132335469\"></p>\n<p>则进行互相关运算之后可以变为：<img src=\"https://i.loli.net/2021/11/24/kuH17Ef3p8Rbjdi.png\" alt=\"image-20211124132405678\"></p>\n<p>由此我们可以看出边缘是在第2和第6列</p>\n<ul>\n<li><strong>卷积运算：</strong>其实就是把核数组左右翻转并 上下翻转，再与输⼊数组做互相关运算。但因为我们的参数都是学习出来的，而不是一开始设定好的，<strong>所以卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出</strong>，而我们一般使用的也是互相关运算</li>\n</ul>\n<h3 id=\"1-2-特征图和感受野\"><a href=\"#1-2-特征图和感受野\" class=\"headerlink\" title=\"1.2 特征图和感受野\"></a>1.2 特征图和感受野</h3><ul>\n<li><strong>⼆维卷积层输出的⼆维数组可以看作输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）</strong></li>\n<li><strong>影响元素x的前向计算的所有可能输⼊区域叫做x的感受野（receptive field）</strong>，用上图距离，阴影中19的感受野就是输入的四个阴影，如果输入的元素又是另一个卷积运算的输出，则我们还可以将19的感受野扩大</li>\n<li>可⻅，我们可以通过更深的卷积神经⽹络使特征 图中单个元素的感受野变得更加⼴阔，从而捕捉输⼊上更⼤尺⼨的特征</li>\n</ul>\n<h3 id=\"1-3-填充和步幅\"><a href=\"#1-3-填充和步幅\" class=\"headerlink\" title=\"1.3 填充和步幅\"></a>1.3 填充和步幅</h3><ul>\n<li><strong>填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）</strong>，如下图就是对3$\\times$3数组进行填充，填充为1：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/24/YE4p8vg31IaWbuK.png\" alt=\"image-20211124133849360\"></p>\n<ul>\n<li><p><strong>我们将每次滑动的⾏数和列数称为步幅（stride）</strong></p>\n</li>\n<li><p><strong>一般来说输出的高宽为：</strong></p>\n<script type=\"math/tex; mode=display\">\n[(n_h - k_h + p_h + s_h) / s_h] \\times [(n_w - k_w + p_w + s_w) / s_w]</script><p>其中：$n为输入，k为核，p为填充，s为步幅$</p>\n</li>\n<li><p>步幅为1时，很多情况下，我们会设置$p_h = k_h−1$和$p_w = k_w −1$来使输⼊和输出具有相同的⾼和宽</p>\n</li>\n</ul>\n<h3 id=\"1-4-多输入通道和多输出通道\"><a href=\"#1-4-多输入通道和多输出通道\" class=\"headerlink\" title=\"1.4 多输入通道和多输出通道\"></a>1.4 多输入通道和多输出通道</h3><ul>\n<li>当输入通道 $c_i &gt; 1$ 时，我们为每个输入通道都分配一个核数组，则得到一个形状为$c_i \\times k_h \\times k_w$的卷积核，最后再将每个通道上的卷积结果相加，就得到输出，如下图：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/24/iNGt96JXUlSAEKZ.png\" alt=\"image-20211124140843742\"></p>\n<ul>\n<li>可是按上面的方法，无论输入通道是多少，输出通道总为1，我们设输入、输出通道分别为$c_i, c_o$，如果我们想要多通道的输出，则可以给每个输出通道都创建一个形状为$c_i \\times k_h \\times k_w$的核，则可以得到形状为$c_o \\times c_i \\times k_h \\times k_w$的卷积核</li>\n</ul>\n<h3 id=\"1-5-1-times-1卷积层\"><a href=\"#1-5-1-times-1卷积层\" class=\"headerlink\" title=\"1.5 1$\\times$1卷积层\"></a>1.5 1$\\times$1卷积层</h3><ul>\n<li>卷积窗口形状为1 × 1的多通道卷积层。我们通常称之为1 × 1卷 积层，并将其中的卷积运算称为1 × 1卷积</li>\n<li>因为使⽤了最小窗口，1 × 1卷积失去了卷积层可以 识别⾼和宽维度上相邻元素构成的模式的功能。<strong>实际上，1 × 1卷积的主要计算发⽣在通道维上， 1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度</strong></li>\n<li><strong>假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那 么1 × 1卷积层的作⽤与全连接层等价</strong></li>\n</ul>\n<h3 id=\"1-6-卷积层相对于全连接层的优点\"><a href=\"#1-6-卷积层相对于全连接层的优点\" class=\"headerlink\" title=\"1.6 卷积层相对于全连接层的优点\"></a>1.6 卷积层相对于全连接层的优点</h3><ul>\n<li><strong>卷积层使图像的像素在⾼和宽两个⽅向上的相关性均可能被有效识别</strong>，用全连接层举例，全连接层就相当于一个很大的卷积核，但是过大的卷积核带来的问题是我们可能无法提取到有效特征，并且像素在高宽方向上的相关性可能提取不到，如果选择合适的卷积核大小才更可能提取到</li>\n<li>卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重复计算，同一输入通道使用同一卷积核，<strong>实现参数共享</strong>，从而<strong>避免参数过多</strong>。同时参数共享也具有物理意义，他使卷积层<strong>具有平移等特性</strong>，比如图像中有一只猫，那么无论他在图像的任何位置，我们都能提取到他的主要特征，将他识别为猫</li>\n<li>对于全连接层，<strong>任意一对输入和输出之间都会产生交互</strong>，形成稠密的连接结构</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/BuGFLnhli2NJPCv.png\" alt=\"image-20211125234024022\"></p>\n<p>而在卷积神经网络中，卷积核尺度远小于输入的维度，<strong>这样每个输出神经元仅与前一层部分神经元产生交互</strong></p>\n<p><img src=\"https://i.loli.net/2021/11/26/HBjoOyvEcqxSna6.png\" alt=\"image-20211125234143051\"></p>\n<p>我们将这种特性称为<strong>稀疏交互</strong>，这样我们可以将<strong>优化过程的时间复杂度减少好几个数量级，并且缓解过拟合</strong></p>\n<p>稀疏交互的物理意义是<strong>许多现实中的数据都具有局部的特征结构，我们可以先学习局部的特征，然后再将局部的特征组合起来形成更复杂和抽象的特征</strong></p>\n<h1 id=\"2-池化层\"><a href=\"#2-池化层\" class=\"headerlink\" title=\"2 池化层\"></a>2 池化层</h1><ul>\n<li><strong>池化层的提出是为了缓解卷积层对位置的过度敏感性</strong>。不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均值。该运算也分别叫做<strong>最⼤池化</strong>或<strong>平均池化</strong></li>\n<li><strong>池化层没有参数</strong></li>\n<li>Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，这个妥协会越来越小，下面介绍几种常用的池化层</li>\n</ul>\n<h3 id=\"2-1-平均池化层（mean-pooling）\"><a href=\"#2-1-平均池化层（mean-pooling）\" class=\"headerlink\" title=\"2.1 平均池化层（mean-pooling）\"></a>2.1 平均池化层（mean-pooling）</h3><ul>\n<li>即对邻域内特征点只求平均</li>\n<li>优缺点：<strong>抑制邻域大小受限造成的估计值方差增大</strong>，能很好的保留背景，但容易使得图片变模糊</li>\n</ul>\n<h3 id=\"2-2-最大池化层（max-pooling）\"><a href=\"#2-2-最大池化层（max-pooling）\" class=\"headerlink\" title=\"2.2 最大池化层（max-pooling）\"></a>2.2 最大池化层（max-pooling）</h3><ul>\n<li>即对邻域内特征点取最大</li>\n<li>优缺点：<strong>抑制积层参数误差造成估计均值的偏移</strong>，能很好的保留纹理特征，一般现在都用max-pooling而很少用mean-pooling</li>\n</ul>\n<h3 id=\"2-3-全局平均池化（global-average-pooling）\"><a href=\"#2-3-全局平均池化（global-average-pooling）\" class=\"headerlink\" title=\"2.3 全局平均池化（global average pooling）\"></a>2.3 全局平均池化（global average pooling）</h3><ul>\n<li>对每个通道中所有元素求平均并直接⽤于分类</li>\n<li>优点：大幅度减少网络参数，理所当然的减少了过拟合现象</li>\n</ul>\n<h3 id=\"2-4-池化层的作用\"><a href=\"#2-4-池化层的作用\" class=\"headerlink\" title=\"2.4 池化层的作用\"></a>2.4 池化层的作用</h3><ol>\n<li><strong>对特征进行压缩，保留主要特征的同时减少参数和计算量，减少模型复杂度，防止过拟合。</strong>用池化层做互相关运算，本身就能减少特征的个数。并且对于全局平均优化，如果我们要进行图像分类，我们需要使用参数很多的全连接层，最后再导入Softmax，而如果我们使用全局平均优化，则可以规避全连接庞大的参数量，减少过拟合</li>\n<li><strong>实现不变性</strong>，包括平移不变性、旋转不变性和尺度不变性。</li>\n</ol>\n<h3 id=\"2-5-池化层的多通道\"><a href=\"#2-5-池化层的多通道\" class=\"headerlink\" title=\"2.5 池化层的多通道\"></a>2.5 池化层的多通道</h3><ul>\n<li>和卷积层有区别， 在处理多通道输⼊数据时，<strong>池化层对每个输⼊通道分别池化，而不是像卷积层那样将各通道的输 ⼊按通道相加。这意味着池化层的输出通道数与输⼊通道数相等</strong></li>\n</ul>\n<h1 id=\"3-LeNet\"><a href=\"#3-LeNet\" class=\"headerlink\" title=\"3 LeNet\"></a>3 LeNet</h1><ul>\n<li>分为卷积层块和全连接层块两个部分</li>\n</ul>\n<h3 id=\"3-1-卷积层块\"><a href=\"#3-1-卷积层块\" class=\"headerlink\" title=\"3.1 卷积层块\"></a>3.1 卷积层块</h3><ul>\n<li>卷积层块⾥的基本单位是卷积层后接最⼤池化层，卷积层⽤来识别图像⾥的空间模式，如线条和 物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性</li>\n<li>在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似</li>\n<li>卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2</li>\n</ul>\n<h3 id=\"3-2-全连接层块\"><a href=\"#3-2-全连接层块\" class=\"headerlink\" title=\"3.2 全连接层块\"></a>3.2 全连接层块</h3><ul>\n<li>当卷积层块的输出传⼊全连接层块时，全连接 层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输⼊形状将变成⼆维，其中第⼀维是小批量中的样本，第⼆维是每个样本变平后的向量表⽰，且向量⻓度为通道、⾼和宽的乘积</li>\n<li>全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/28/NH5zDmge7RiwaGA.png\" alt=\"20150903212346407\"></p>\n<h1 id=\"4-AlexNet（相对于LeNet较深）\"><a href=\"#4-AlexNet（相对于LeNet较深）\" class=\"headerlink\" title=\"4 AlexNet（相对于LeNet较深）\"></a>4 AlexNet（相对于LeNet较深）</h1><ul>\n<li>相对于较小的LeNet，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层</li>\n<li>AlexNet第⼀层中的卷积窗口形状是11 × 11，步幅为4。因为ImageNet中绝⼤多数图像的⾼和宽均 ⽐MNIST图像的⾼和宽⼤10倍以上，ImageNet图像的物体占⽤更多的像素，所以需要更⼤的卷积窗口来捕获物体。第⼆层中的卷积窗口形状减小到5 × 5，之后全采⽤3 × 3</li>\n<li>第⼀、第⼆和第五个卷积层之后都使⽤了窗口形状为3 × 3、步幅为2的最⼤池化层</li>\n<li>AlexNet使⽤的卷积通道数也⼤于LeNet中的卷积通道数数⼗倍，5层的输出通道数分别为96，256，384，384，256</li>\n<li>紧接着最后⼀个卷积层的是两个输出个数为4096的全连接层</li>\n<li>AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数</li>\n<li>AlexNet使用了Dropout和图像增广</li>\n<li>AlexNet相比于LeNet有更小的学习率</li>\n<li><strong>AlexNet的一个很大的意义就是在图像识别上AI超越人工，并且首次使用了GPU加速</strong></li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b.jpg\" alt=\"v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b\"></p>\n<h1 id=\"5-VGG（使用重复元素）\"><a href=\"#5-VGG（使用重复元素）\" class=\"headerlink\" title=\"5 VGG（使用重复元素）\"></a>5 VGG（使用重复元素）</h1><h3 id=\"5-1-VGG块\"><a href=\"#5-1-VGG块\" class=\"headerlink\" title=\"5.1 VGG块\"></a>5.1 VGG块</h3><ul>\n<li>连续使⽤<strong>数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层</strong>。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。可以指定卷积层的数量num_convs和输出通道数num_channels</li>\n</ul>\n<h3 id=\"5-2-VGG网络\"><a href=\"#5-2-VGG网络\" class=\"headerlink\" title=\"5.2 VGG网络\"></a>5.2 VGG网络</h3><ul>\n<li>我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积层和3个全连接层，所以经常被称为VGG-11</li>\n<li>之后的3个全连接层神经元个数分别为：4096，4096，10（最后一层为类别个数）</li>\n<li>使用VGG后，每次输⼊的⾼和宽将减半，直到最终⾼和宽变成7后传⼊全连接层。与此同时，输出通道数每次翻倍，直到变成512。VGG这种⾼和宽减半以及通道翻倍的设计使多数卷积层都有相同的模型参数尺⼨和计算复杂度</li>\n</ul>\n<ul>\n<li><strong>VGG的一个很大的意义就是VGG用数个小卷积核代替大卷积核，使参数量减少</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/28/tTvmMekgd81PJX6.png\" alt=\"20180205192403250\"></p>\n<h1 id=\"6-NiN（网络中的网络）\"><a href=\"#6-NiN（网络中的网络）\" class=\"headerlink\" title=\"6 NiN（网络中的网络）\"></a>6 NiN（网络中的网络）</h1><ul>\n<li>的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深</li>\n<li>但是NiN有所不同，由于1$\\times$1卷积层可以替代全连接层，所以NiN中就进行了替换：</li>\n<li><strong>NiN之所以被称为网络中的网络是因为：NiN串联多个由卷积层和“准全连接层”（1X1的网络）的小网络来构成一个深层网络</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/NjfabGRcVe8ysZk.png\" alt=\"image-20211125152432130\"></p>\n<h3 id=\"6-1-NiN块\"><a href=\"#6-1-NiN块\" class=\"headerlink\" title=\"6.1 NiN块\"></a>6.1 NiN块</h3><ul>\n<li>NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 × 1卷积层串联而成。其中<strong>第⼀个卷积层的超参数可以⾃⾏设置，而第⼆和第三个卷积层的超参数⼀般是固定的</strong></li>\n<li>三个卷积层的通道数是相同的</li>\n</ul>\n<h3 id=\"6-2-NiN模型\"><a href=\"#6-2-NiN模型\" class=\"headerlink\" title=\"6.2 NiN模型\"></a>6.2 NiN模型</h3><ul>\n<li>NiN使⽤卷积窗口形状分别为11 × 11、5 × 5和3 × 3的卷积层，相应的输出通道数也与AlexNet中的⼀致。每个NiN块后接⼀ 个步幅为2、窗口形状为3 × 3的最⼤池化层</li>\n<li>除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤<strong>全局平均池化层</strong>对每个通道中所有元素求平均并直接⽤于分类。<strong>这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合，然而，该设计有时会造成获得有效模型的训练时间的增加</strong></li>\n<li>NiN的学习率一般比AlexNet和VGG大</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/28/2eiOxQsw4XoBFN9.png\" alt=\"image-20211128200750143\"></p>\n<h1 id=\"7-GoogLeNet（含并行连接的网络）\"><a href=\"#7-GoogLeNet（含并行连接的网络）\" class=\"headerlink\" title=\"7 GoogLeNet（含并行连接的网络）\"></a>7 GoogLeNet（含并行连接的网络）</h1><h3 id=\"7-1-Inception块\"><a href=\"#7-1-Inception块\" class=\"headerlink\" title=\"7.1 Inception块\"></a>7.1 Inception块</h3><p><img src=\"https://i.loli.net/2021/11/25/Q4sfSlhjntDaXFM.png\" alt=\"image-20211125155525157\"></p>\n<ul>\n<li><p>Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 × 3和5 × 5的卷积层来<strong>抽取不同空间尺⼨下的信息</strong></p>\n</li>\n<li><p>其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，<strong>以降低模型复杂度</strong>。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数</p>\n</li>\n<li><p>4条线路都使⽤了合适的填充来使<strong>输⼊与输出的⾼和宽⼀致</strong>。最后我们将每条线路的输出<strong>在通道维上连结</strong>，并输⼊接下来的层中去</p>\n</li>\n</ul>\n<h3 id=\"7-2-GoogLeNet模型\"><a href=\"#7-2-GoogLeNet模型\" class=\"headerlink\" title=\"7.2 GoogLeNet模型\"></a>7.2 GoogLeNet模型</h3><ul>\n<li>在主体卷积部分中使⽤5个模块（block），<strong>每个模块之间使⽤步幅为2的3× 3最⼤池化层来减小输出⾼宽</strong></li>\n<li>第⼀模块使⽤⼀个64通道的7 × 7卷积层，步幅为2</li>\n</ul>\n<ul>\n<li>第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层，然后是将通道增⼤3倍（变为192）的3 × 3卷积层</li>\n</ul>\n<ul>\n<li><p>第三模块串联2个完整的Inception块。第⼀个Inception块的输出通道数为64+128+32+32 = 256（4个加数对应4条线路的通道数），其中第⼆、第三条线路由于有两个卷积层，所以两条线路的第一个<script type=\"math/tex\">1 \\times 1</script>卷积层先将输出通道减少为96和16，再接上第⼆层卷积层。</p>\n<p>第⼆个Inception块 输出通道数增⾄128 + 192 + 96 + 64 = 480。其中第⼆、第三条线路<script type=\"math/tex\">1 \\times 1</script>卷积层的输出通道分别为128和32</p>\n</li>\n</ul>\n<ul>\n<li><p>第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + (96,208) + (16,48) + 64 = 512、 160+(112,224)+(24,64)+64 = 512、128+(128,256)+(24,64)+64 = 512、112+(144,288)+(32,64)+64 = 528和 256+(160,320)+(32,128)+128 = 832</p>\n<p>其中括号里的第一个数字为二三条通道的第一个卷积核输出通道数</p>\n</li>\n</ul>\n<ul>\n<li>第五模块有输出通道数为256 + (160,320) + (32,128) + 128 = 832和384 + (192,384) + (48,128) + 128 = 1024的两个Inception块，<strong>然后再接上一个全局平均池化层</strong></li>\n</ul>\n<ul>\n<li><strong>五个模块相连之后最后再接上一个全连接层，神经元个数为类别个数</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/28/rdpGSFRzuKZeTs2.jpg\" alt=\"142051802f8de8513fe61601277f03c8\"></p>\n<h1 id=\"8-批量归一化\"><a href=\"#8-批量归一化\" class=\"headerlink\" title=\"8 批量归一化\"></a>8 批量归一化</h1><ul>\n<li>我们一般在前向传播开始之前会对数据进行归一化，<strong>使不同特征之间具有可比性，并且更快收敛</strong></li>\n<li>通常来说，数据标准化预处理对于浅层模型就⾜够有效了，<strong>但对深层神经网络来说，即使输⼊数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。</strong>这种计算数值的不稳定性通常令我们难以训练出有效的深度模型</li>\n<li>而批量归一化则是对每一层的输出都做一次归一化，<strong>使均值永远为0，方差永远为1</strong></li>\n</ul>\n<h3 id=\"8-1-批量归一化层\"><a href=\"#8-1-批量归一化层\" class=\"headerlink\" title=\"8.1 批量归一化层\"></a>8.1 批量归一化层</h3><ul>\n<li><strong>通常，我们将批量归⼀化层置于全连接层中的仿射变换和激活函数之间</strong></li>\n<li>首先要对小批量求均值和方差：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\boldsymbol{\\mu}_{\\mathcal{B}} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} \\boldsymbol{x}^{(i)} \\\\\n\\boldsymbol{\\sigma}_{\\mathcal{B}}^{2} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{\\mathcal{B}}\\right)^{2}\n\\end{array}</script><p>得到的均值和方差是两个向量，维度为特征个数</p>\n<ul>\n<li>然后：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{\\mathcal{B}}}{\\sqrt{\\boldsymbol{\\sigma}_{\\mathcal{B}}^{2}+\\epsilon}}</script><ul>\n<li>$\\epsilon$ 为一个很小的数，一般取$10^{-8}$ ，是为了保证分母大于0</li>\n<li><strong>但是我们不确定是否一定是所有层都进行批量归一化才是最好的情况，所以我们要给予一个能变回归一化前的值的可能性，所以引入拉伸参数（scale）$\\gamma$ 和  偏移参数（shift）$\\beta$</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{y}^{(i)} \\leftarrow \\gamma \\odot \\hat{\\boldsymbol{x}}^{(i)}+\\boldsymbol{\\beta}</script><p>将$\\gamma$ 和 $\\beta$ 作为两个可变化参数，然后通过学习来决定拉伸多少和偏移多少</p>\n<h3 id=\"8-2-对卷积层做批量归一化\"><a href=\"#8-2-对卷积层做批量归一化\" class=\"headerlink\" title=\"8.2 对卷积层做批量归一化\"></a>8.2 对卷积层做批量归一化</h3><ul>\n<li>批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前</li>\n<li><p>如果卷积计算输出多个通道，我们需要<strong>对这些通道的输出分别做批量归⼀化，且每个通道都拥有独立的拉伸和偏移参数</strong></p>\n</li>\n<li><p>设小批量中有m个样本。在单个通道上，假设卷积计算输出的⾼和宽分别为p和q。我们需要对该通道中m × p × q个元素同时做批量归⼀化</p>\n</li>\n</ul>\n<h3 id=\"8-3-预测时的批量归一化\"><a href=\"#8-3-预测时的批量归一化\" class=\"headerlink\" title=\"8.3 预测时的批量归一化\"></a>8.3 预测时的批量归一化</h3><ul>\n<li><strong>批量归一化在训练模式和预测模式的计算结果是不⼀样的</strong>。确定均值和方差时，单个样本的输出不应取决于批量归⼀化所需要的随机小批量中的均值和⽅差。⼀种常⽤的⽅法是<strong>通过移动平均（指数加权平均）估算整个训练数据集的样本均值和方差</strong>，并在预测时使⽤它们得到确定的输出</li>\n</ul>\n<h1 id=\"9-残差网络（ResNet）\"><a href=\"#9-残差网络（ResNet）\" class=\"headerlink\" title=\"9 残差网络（ResNet）\"></a>9 残差网络（ResNet）</h1><h3 id=\"9-1-残差块\"><a href=\"#9-1-残差块\" class=\"headerlink\" title=\"9.1 残差块\"></a>9.1 残差块</h3><p><img src=\"https://i.loli.net/2021/11/25/zADLUxHm7wS8Z9Q.png\" alt=\"image-20211125221657826\"></p>\n<ul>\n<li>左图是一般形式的映射，右图为残差映射，是将输入x加权-&gt;激活-&gt;再加权后，再和原输入x相加，再送入激活函数</li>\n<li>这样的结构中，输⼊可通过跨层的数据线路更快地向前传播</li>\n<li>残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接⼀个批量归⼀化层和ReLU激活函数。然后我们<strong>将输⼊跳过这2个卷积运算后直接加在最后的ReLU激活函数前</strong></li>\n<li>这样的设计要求2个卷积层的输出与输⼊形状⼀样，从而可以相加。如果想改变通道数，就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算</li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg\" alt=\"\"></p>\n<h3 id=\"9-2-ResNet模型\"><a href=\"#9-2-ResNet模型\" class=\"headerlink\" title=\"9.2 ResNet模型\"></a>9.2 ResNet模型</h3><ul>\n<li>ResNet的前两层跟之前介绍的GoogLeNet中的⼀样：在输出通道数为64、步幅为2的7 × 7卷积层后接步幅为2的3 × 3的最⼤池化层。不同之处在于<strong>ResNet每个卷积层（对应上图每一次加权运算）后增加的批量归⼀化层</strong></li>\n</ul>\n<ul>\n<li><strong>后接4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块</strong></li>\n<li>第⼀个模块的通道数同输⼊通道数⼀致。由于之前已经使⽤了步幅为2的最⼤池化层，所以⽆须减小⾼和宽。<strong>之后的每个模块在第⼀个残差块⾥将上⼀个模块的通道数翻倍，并将高和宽减半</strong></li>\n</ul>\n<ul>\n<li>接着我们为ResNet加⼊所有残差块。这⾥每个模块的残差块个数可以自行定义</li>\n</ul>\n<ul>\n<li>最后，与GoogLeNet⼀样，加⼊全局平均池化层后接上全连接层输出</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/9aypGjvX1ESYtrM.png\" alt=\"image-20211125223902681\"></p>\n<p>其中10为类别个数</p>\n<p><img src=\"https://pic3.zhimg.com/80/v2-862e1c2dcb24f10d264544190ad38142_720w.jpg\" alt=\"\"></p>\n<h3 id=\"9-3-ResNet的作用\"><a href=\"#9-3-ResNet的作用\" class=\"headerlink\" title=\"9.3 ResNet的作用\"></a>9.3 ResNet的作用</h3><ul>\n<li>在深度神经网络中，如果我们进行反向传播，那么由链式求导法则可知，我们会<strong>涉及到非常多参数和导数的连乘</strong>，这时误差很容易产生消失或膨胀，通过实验也可以发现，层数更深的神经网络结果反而没有浅层的好</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/26/YecToJs5xNf6H24.png\" alt=\"image-20211125235804507\"></p>\n<p>这种结果很大程度归结于深度神经网络的<strong>梯度消失问题</strong></p>\n<ul>\n<li>而ResNet的提出就是为了解决梯度消失的问题，<strong>既然离输入近的神经网络层较难训练，那么我们可以将他短接到更接近输出的层</strong>，并且这种方法并不会对结果产生影响，假设输入为X，我们在一般网络中想要拟合f(X)，放到残差网络中就只需要拟合f(X) - X 即可</li>\n<li><strong>残差网络的设计思想不仅仅局限于卷积网络，实际上很多其他的网络也用到了残差，也取得了很不错的效果，都是为了解决深度神经网络的梯度消失问题</strong></li>\n</ul>\n<h1 id=\"10-稠密连接网络（DenseNet）\"><a href=\"#10-稠密连接网络（DenseNet）\" class=\"headerlink\" title=\"10 稠密连接网络（DenseNet）\"></a>10 稠密连接网络（DenseNet）</h1><p><img src=\"https://i.loli.net/2021/11/25/NTajCiqAydPvoVt.png\" alt=\"image-20211125224307704\"></p>\n<ul>\n<li>DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是<strong>在通道维上连结</strong></li>\n<li>DenseNet的主要构建模块是<strong>稠密块（dense block）</strong>和<strong>过渡层（transition layer）</strong>。前者定义了输⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤</li>\n</ul>\n<h3 id=\"10-1-稠密块\"><a href=\"#10-1-稠密块\" class=\"headerlink\" title=\"10.1 稠密块\"></a>10.1 稠密块</h3><ul>\n<li>我们将批量归⼀化、激活和卷积组合到一起形成一种块：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/hfHF7BLySM1swZG.png\" alt=\"image-20211125225431518\"></p>\n<ul>\n<li><strong>稠密块由多个conv_block组成，每块使⽤相同的输出通道数。但在前向计算时，我们将每块的输⼊和输出在通道维上连结</strong></li>\n</ul>\n<h3 id=\"10-2-过渡层\"><a href=\"#10-2-过渡层\" class=\"headerlink\" title=\"10.2 过渡层\"></a>10.2 过渡层</h3><ul>\n<li>由于每个稠密块都会带来通道数的增加，使⽤过多则会带来过于复杂的模型。过渡层⽤来控制模型复杂度。它<strong>通过1 × 1卷积层来减小通道数，并使⽤步幅为2的平均池化层减半高和宽</strong>，并且也要进行激活和BN运算</li>\n</ul>\n<h3 id=\"10-3-DenseNet模型\"><a href=\"#10-3-DenseNet模型\" class=\"headerlink\" title=\"10.3 DenseNet模型\"></a>10.3 DenseNet模型</h3><ul>\n<li>DenseNet⾸先使⽤同ResNet⼀样的单卷积层和最⼤池化层</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/DqcGv6FtMioNez1.png\" alt=\"image-20211125230039527\"></p>\n<ul>\n<li>接着使用4个稠密块，同ResNet⼀样，我们可以设置每个稠密块使⽤多少个卷积层</li>\n</ul>\n<ul>\n<li><p>在稠密块之间我们使用过渡层来减半高宽，并减半通道数</p>\n<p>注意最后一层是不使用过渡层进行减半的</p>\n</li>\n<li><p>最后再和ResNet一样，接上全局平均池化层和全连接层来输出</p>\n</li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/80/v2-c81da515c8fa9796601fde82e4d36f61_720w.jpg\" alt=\"\"></p>\n","site":{"data":{}},"wordcount":7930,"excerpt":"","more":"<h1 id=\"1-卷积层\"><a href=\"#1-卷积层\" class=\"headerlink\" title=\"1 卷积层\"></a>1 卷积层</h1><h3 id=\"1-1-互相关运算\"><a href=\"#1-1-互相关运算\" class=\"headerlink\" title=\"1.1 互相关运算\"></a>1.1 互相关运算</h3><ul>\n<li><strong>在⼆维卷积层中，⼀个⼆维输⼊数组和⼀个⼆维核（kernel）数组通过互相 关运算输出⼀个⼆维数组</strong>，如下图：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/24/8amf1EYbuU7JPKd.png\" alt=\"image-20211124131936578\"></p>\n<p>卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依 次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘 并求和，得到输出数组中相应位置的元素</p>\n<ul>\n<li><strong>⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出</strong></li>\n<li><strong>使用互相关运算做边缘检测：</strong></li>\n</ul>\n<p>比如我们把kernel设为[1, -1]，要识别的图像为单通道，只有黑白：<img src=\"https://i.loli.net/2021/11/24/Uro3neFEKigbCd2.png\" alt=\"image-20211124132335469\"></p>\n<p>则进行互相关运算之后可以变为：<img src=\"https://i.loli.net/2021/11/24/kuH17Ef3p8Rbjdi.png\" alt=\"image-20211124132405678\"></p>\n<p>由此我们可以看出边缘是在第2和第6列</p>\n<ul>\n<li><strong>卷积运算：</strong>其实就是把核数组左右翻转并 上下翻转，再与输⼊数组做互相关运算。但因为我们的参数都是学习出来的，而不是一开始设定好的，<strong>所以卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出</strong>，而我们一般使用的也是互相关运算</li>\n</ul>\n<h3 id=\"1-2-特征图和感受野\"><a href=\"#1-2-特征图和感受野\" class=\"headerlink\" title=\"1.2 特征图和感受野\"></a>1.2 特征图和感受野</h3><ul>\n<li><strong>⼆维卷积层输出的⼆维数组可以看作输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）</strong></li>\n<li><strong>影响元素x的前向计算的所有可能输⼊区域叫做x的感受野（receptive field）</strong>，用上图距离，阴影中19的感受野就是输入的四个阴影，如果输入的元素又是另一个卷积运算的输出，则我们还可以将19的感受野扩大</li>\n<li>可⻅，我们可以通过更深的卷积神经⽹络使特征 图中单个元素的感受野变得更加⼴阔，从而捕捉输⼊上更⼤尺⼨的特征</li>\n</ul>\n<h3 id=\"1-3-填充和步幅\"><a href=\"#1-3-填充和步幅\" class=\"headerlink\" title=\"1.3 填充和步幅\"></a>1.3 填充和步幅</h3><ul>\n<li><strong>填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）</strong>，如下图就是对3$\\times$3数组进行填充，填充为1：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/24/YE4p8vg31IaWbuK.png\" alt=\"image-20211124133849360\"></p>\n<ul>\n<li><p><strong>我们将每次滑动的⾏数和列数称为步幅（stride）</strong></p>\n</li>\n<li><p><strong>一般来说输出的高宽为：</strong></p>\n<script type=\"math/tex; mode=display\">\n[(n_h - k_h + p_h + s_h) / s_h] \\times [(n_w - k_w + p_w + s_w) / s_w]</script><p>其中：$n为输入，k为核，p为填充，s为步幅$</p>\n</li>\n<li><p>步幅为1时，很多情况下，我们会设置$p_h = k_h−1$和$p_w = k_w −1$来使输⼊和输出具有相同的⾼和宽</p>\n</li>\n</ul>\n<h3 id=\"1-4-多输入通道和多输出通道\"><a href=\"#1-4-多输入通道和多输出通道\" class=\"headerlink\" title=\"1.4 多输入通道和多输出通道\"></a>1.4 多输入通道和多输出通道</h3><ul>\n<li>当输入通道 $c_i &gt; 1$ 时，我们为每个输入通道都分配一个核数组，则得到一个形状为$c_i \\times k_h \\times k_w$的卷积核，最后再将每个通道上的卷积结果相加，就得到输出，如下图：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/24/iNGt96JXUlSAEKZ.png\" alt=\"image-20211124140843742\"></p>\n<ul>\n<li>可是按上面的方法，无论输入通道是多少，输出通道总为1，我们设输入、输出通道分别为$c_i, c_o$，如果我们想要多通道的输出，则可以给每个输出通道都创建一个形状为$c_i \\times k_h \\times k_w$的核，则可以得到形状为$c_o \\times c_i \\times k_h \\times k_w$的卷积核</li>\n</ul>\n<h3 id=\"1-5-1-times-1卷积层\"><a href=\"#1-5-1-times-1卷积层\" class=\"headerlink\" title=\"1.5 1$\\times$1卷积层\"></a>1.5 1$\\times$1卷积层</h3><ul>\n<li>卷积窗口形状为1 × 1的多通道卷积层。我们通常称之为1 × 1卷 积层，并将其中的卷积运算称为1 × 1卷积</li>\n<li>因为使⽤了最小窗口，1 × 1卷积失去了卷积层可以 识别⾼和宽维度上相邻元素构成的模式的功能。<strong>实际上，1 × 1卷积的主要计算发⽣在通道维上， 1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度</strong></li>\n<li><strong>假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那 么1 × 1卷积层的作⽤与全连接层等价</strong></li>\n</ul>\n<h3 id=\"1-6-卷积层相对于全连接层的优点\"><a href=\"#1-6-卷积层相对于全连接层的优点\" class=\"headerlink\" title=\"1.6 卷积层相对于全连接层的优点\"></a>1.6 卷积层相对于全连接层的优点</h3><ul>\n<li><strong>卷积层使图像的像素在⾼和宽两个⽅向上的相关性均可能被有效识别</strong>，用全连接层举例，全连接层就相当于一个很大的卷积核，但是过大的卷积核带来的问题是我们可能无法提取到有效特征，并且像素在高宽方向上的相关性可能提取不到，如果选择合适的卷积核大小才更可能提取到</li>\n<li>卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重复计算，同一输入通道使用同一卷积核，<strong>实现参数共享</strong>，从而<strong>避免参数过多</strong>。同时参数共享也具有物理意义，他使卷积层<strong>具有平移等特性</strong>，比如图像中有一只猫，那么无论他在图像的任何位置，我们都能提取到他的主要特征，将他识别为猫</li>\n<li>对于全连接层，<strong>任意一对输入和输出之间都会产生交互</strong>，形成稠密的连接结构</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/BuGFLnhli2NJPCv.png\" alt=\"image-20211125234024022\"></p>\n<p>而在卷积神经网络中，卷积核尺度远小于输入的维度，<strong>这样每个输出神经元仅与前一层部分神经元产生交互</strong></p>\n<p><img src=\"https://i.loli.net/2021/11/26/HBjoOyvEcqxSna6.png\" alt=\"image-20211125234143051\"></p>\n<p>我们将这种特性称为<strong>稀疏交互</strong>，这样我们可以将<strong>优化过程的时间复杂度减少好几个数量级，并且缓解过拟合</strong></p>\n<p>稀疏交互的物理意义是<strong>许多现实中的数据都具有局部的特征结构，我们可以先学习局部的特征，然后再将局部的特征组合起来形成更复杂和抽象的特征</strong></p>\n<h1 id=\"2-池化层\"><a href=\"#2-池化层\" class=\"headerlink\" title=\"2 池化层\"></a>2 池化层</h1><ul>\n<li><strong>池化层的提出是为了缓解卷积层对位置的过度敏感性</strong>。不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均值。该运算也分别叫做<strong>最⼤池化</strong>或<strong>平均池化</strong></li>\n<li><strong>池化层没有参数</strong></li>\n<li>Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，这个妥协会越来越小，下面介绍几种常用的池化层</li>\n</ul>\n<h3 id=\"2-1-平均池化层（mean-pooling）\"><a href=\"#2-1-平均池化层（mean-pooling）\" class=\"headerlink\" title=\"2.1 平均池化层（mean-pooling）\"></a>2.1 平均池化层（mean-pooling）</h3><ul>\n<li>即对邻域内特征点只求平均</li>\n<li>优缺点：<strong>抑制邻域大小受限造成的估计值方差增大</strong>，能很好的保留背景，但容易使得图片变模糊</li>\n</ul>\n<h3 id=\"2-2-最大池化层（max-pooling）\"><a href=\"#2-2-最大池化层（max-pooling）\" class=\"headerlink\" title=\"2.2 最大池化层（max-pooling）\"></a>2.2 最大池化层（max-pooling）</h3><ul>\n<li>即对邻域内特征点取最大</li>\n<li>优缺点：<strong>抑制积层参数误差造成估计均值的偏移</strong>，能很好的保留纹理特征，一般现在都用max-pooling而很少用mean-pooling</li>\n</ul>\n<h3 id=\"2-3-全局平均池化（global-average-pooling）\"><a href=\"#2-3-全局平均池化（global-average-pooling）\" class=\"headerlink\" title=\"2.3 全局平均池化（global average pooling）\"></a>2.3 全局平均池化（global average pooling）</h3><ul>\n<li>对每个通道中所有元素求平均并直接⽤于分类</li>\n<li>优点：大幅度减少网络参数，理所当然的减少了过拟合现象</li>\n</ul>\n<h3 id=\"2-4-池化层的作用\"><a href=\"#2-4-池化层的作用\" class=\"headerlink\" title=\"2.4 池化层的作用\"></a>2.4 池化层的作用</h3><ol>\n<li><strong>对特征进行压缩，保留主要特征的同时减少参数和计算量，减少模型复杂度，防止过拟合。</strong>用池化层做互相关运算，本身就能减少特征的个数。并且对于全局平均优化，如果我们要进行图像分类，我们需要使用参数很多的全连接层，最后再导入Softmax，而如果我们使用全局平均优化，则可以规避全连接庞大的参数量，减少过拟合</li>\n<li><strong>实现不变性</strong>，包括平移不变性、旋转不变性和尺度不变性。</li>\n</ol>\n<h3 id=\"2-5-池化层的多通道\"><a href=\"#2-5-池化层的多通道\" class=\"headerlink\" title=\"2.5 池化层的多通道\"></a>2.5 池化层的多通道</h3><ul>\n<li>和卷积层有区别， 在处理多通道输⼊数据时，<strong>池化层对每个输⼊通道分别池化，而不是像卷积层那样将各通道的输 ⼊按通道相加。这意味着池化层的输出通道数与输⼊通道数相等</strong></li>\n</ul>\n<h1 id=\"3-LeNet\"><a href=\"#3-LeNet\" class=\"headerlink\" title=\"3 LeNet\"></a>3 LeNet</h1><ul>\n<li>分为卷积层块和全连接层块两个部分</li>\n</ul>\n<h3 id=\"3-1-卷积层块\"><a href=\"#3-1-卷积层块\" class=\"headerlink\" title=\"3.1 卷积层块\"></a>3.1 卷积层块</h3><ul>\n<li>卷积层块⾥的基本单位是卷积层后接最⼤池化层，卷积层⽤来识别图像⾥的空间模式，如线条和 物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性</li>\n<li>在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似</li>\n<li>卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2</li>\n</ul>\n<h3 id=\"3-2-全连接层块\"><a href=\"#3-2-全连接层块\" class=\"headerlink\" title=\"3.2 全连接层块\"></a>3.2 全连接层块</h3><ul>\n<li>当卷积层块的输出传⼊全连接层块时，全连接 层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输⼊形状将变成⼆维，其中第⼀维是小批量中的样本，第⼆维是每个样本变平后的向量表⽰，且向量⻓度为通道、⾼和宽的乘积</li>\n<li>全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/28/NH5zDmge7RiwaGA.png\" alt=\"20150903212346407\"></p>\n<h1 id=\"4-AlexNet（相对于LeNet较深）\"><a href=\"#4-AlexNet（相对于LeNet较深）\" class=\"headerlink\" title=\"4 AlexNet（相对于LeNet较深）\"></a>4 AlexNet（相对于LeNet较深）</h1><ul>\n<li>相对于较小的LeNet，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层</li>\n<li>AlexNet第⼀层中的卷积窗口形状是11 × 11，步幅为4。因为ImageNet中绝⼤多数图像的⾼和宽均 ⽐MNIST图像的⾼和宽⼤10倍以上，ImageNet图像的物体占⽤更多的像素，所以需要更⼤的卷积窗口来捕获物体。第⼆层中的卷积窗口形状减小到5 × 5，之后全采⽤3 × 3</li>\n<li>第⼀、第⼆和第五个卷积层之后都使⽤了窗口形状为3 × 3、步幅为2的最⼤池化层</li>\n<li>AlexNet使⽤的卷积通道数也⼤于LeNet中的卷积通道数数⼗倍，5层的输出通道数分别为96，256，384，384，256</li>\n<li>紧接着最后⼀个卷积层的是两个输出个数为4096的全连接层</li>\n<li>AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数</li>\n<li>AlexNet使用了Dropout和图像增广</li>\n<li>AlexNet相比于LeNet有更小的学习率</li>\n<li><strong>AlexNet的一个很大的意义就是在图像识别上AI超越人工，并且首次使用了GPU加速</strong></li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b.jpg\" alt=\"v2-3f5a7ab9bcb15004d5a08fdf71e6a775_b\"></p>\n<h1 id=\"5-VGG（使用重复元素）\"><a href=\"#5-VGG（使用重复元素）\" class=\"headerlink\" title=\"5 VGG（使用重复元素）\"></a>5 VGG（使用重复元素）</h1><h3 id=\"5-1-VGG块\"><a href=\"#5-1-VGG块\" class=\"headerlink\" title=\"5.1 VGG块\"></a>5.1 VGG块</h3><ul>\n<li>连续使⽤<strong>数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层</strong>。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。可以指定卷积层的数量num_convs和输出通道数num_channels</li>\n</ul>\n<h3 id=\"5-2-VGG网络\"><a href=\"#5-2-VGG网络\" class=\"headerlink\" title=\"5.2 VGG网络\"></a>5.2 VGG网络</h3><ul>\n<li>我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积层和3个全连接层，所以经常被称为VGG-11</li>\n<li>之后的3个全连接层神经元个数分别为：4096，4096，10（最后一层为类别个数）</li>\n<li>使用VGG后，每次输⼊的⾼和宽将减半，直到最终⾼和宽变成7后传⼊全连接层。与此同时，输出通道数每次翻倍，直到变成512。VGG这种⾼和宽减半以及通道翻倍的设计使多数卷积层都有相同的模型参数尺⼨和计算复杂度</li>\n</ul>\n<ul>\n<li><strong>VGG的一个很大的意义就是VGG用数个小卷积核代替大卷积核，使参数量减少</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/28/tTvmMekgd81PJX6.png\" alt=\"20180205192403250\"></p>\n<h1 id=\"6-NiN（网络中的网络）\"><a href=\"#6-NiN（网络中的网络）\" class=\"headerlink\" title=\"6 NiN（网络中的网络）\"></a>6 NiN（网络中的网络）</h1><ul>\n<li>的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深</li>\n<li>但是NiN有所不同，由于1$\\times$1卷积层可以替代全连接层，所以NiN中就进行了替换：</li>\n<li><strong>NiN之所以被称为网络中的网络是因为：NiN串联多个由卷积层和“准全连接层”（1X1的网络）的小网络来构成一个深层网络</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/NjfabGRcVe8ysZk.png\" alt=\"image-20211125152432130\"></p>\n<h3 id=\"6-1-NiN块\"><a href=\"#6-1-NiN块\" class=\"headerlink\" title=\"6.1 NiN块\"></a>6.1 NiN块</h3><ul>\n<li>NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 × 1卷积层串联而成。其中<strong>第⼀个卷积层的超参数可以⾃⾏设置，而第⼆和第三个卷积层的超参数⼀般是固定的</strong></li>\n<li>三个卷积层的通道数是相同的</li>\n</ul>\n<h3 id=\"6-2-NiN模型\"><a href=\"#6-2-NiN模型\" class=\"headerlink\" title=\"6.2 NiN模型\"></a>6.2 NiN模型</h3><ul>\n<li>NiN使⽤卷积窗口形状分别为11 × 11、5 × 5和3 × 3的卷积层，相应的输出通道数也与AlexNet中的⼀致。每个NiN块后接⼀ 个步幅为2、窗口形状为3 × 3的最⼤池化层</li>\n<li>除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤<strong>全局平均池化层</strong>对每个通道中所有元素求平均并直接⽤于分类。<strong>这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合，然而，该设计有时会造成获得有效模型的训练时间的增加</strong></li>\n<li>NiN的学习率一般比AlexNet和VGG大</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/28/2eiOxQsw4XoBFN9.png\" alt=\"image-20211128200750143\"></p>\n<h1 id=\"7-GoogLeNet（含并行连接的网络）\"><a href=\"#7-GoogLeNet（含并行连接的网络）\" class=\"headerlink\" title=\"7 GoogLeNet（含并行连接的网络）\"></a>7 GoogLeNet（含并行连接的网络）</h1><h3 id=\"7-1-Inception块\"><a href=\"#7-1-Inception块\" class=\"headerlink\" title=\"7.1 Inception块\"></a>7.1 Inception块</h3><p><img src=\"https://i.loli.net/2021/11/25/Q4sfSlhjntDaXFM.png\" alt=\"image-20211125155525157\"></p>\n<ul>\n<li><p>Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 × 3和5 × 5的卷积层来<strong>抽取不同空间尺⼨下的信息</strong></p>\n</li>\n<li><p>其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，<strong>以降低模型复杂度</strong>。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数</p>\n</li>\n<li><p>4条线路都使⽤了合适的填充来使<strong>输⼊与输出的⾼和宽⼀致</strong>。最后我们将每条线路的输出<strong>在通道维上连结</strong>，并输⼊接下来的层中去</p>\n</li>\n</ul>\n<h3 id=\"7-2-GoogLeNet模型\"><a href=\"#7-2-GoogLeNet模型\" class=\"headerlink\" title=\"7.2 GoogLeNet模型\"></a>7.2 GoogLeNet模型</h3><ul>\n<li>在主体卷积部分中使⽤5个模块（block），<strong>每个模块之间使⽤步幅为2的3× 3最⼤池化层来减小输出⾼宽</strong></li>\n<li>第⼀模块使⽤⼀个64通道的7 × 7卷积层，步幅为2</li>\n</ul>\n<ul>\n<li>第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层，然后是将通道增⼤3倍（变为192）的3 × 3卷积层</li>\n</ul>\n<ul>\n<li><p>第三模块串联2个完整的Inception块。第⼀个Inception块的输出通道数为64+128+32+32 = 256（4个加数对应4条线路的通道数），其中第⼆、第三条线路由于有两个卷积层，所以两条线路的第一个<script type=\"math/tex\">1 \\times 1</script>卷积层先将输出通道减少为96和16，再接上第⼆层卷积层。</p>\n<p>第⼆个Inception块 输出通道数增⾄128 + 192 + 96 + 64 = 480。其中第⼆、第三条线路<script type=\"math/tex\">1 \\times 1</script>卷积层的输出通道分别为128和32</p>\n</li>\n</ul>\n<ul>\n<li><p>第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + (96,208) + (16,48) + 64 = 512、 160+(112,224)+(24,64)+64 = 512、128+(128,256)+(24,64)+64 = 512、112+(144,288)+(32,64)+64 = 528和 256+(160,320)+(32,128)+128 = 832</p>\n<p>其中括号里的第一个数字为二三条通道的第一个卷积核输出通道数</p>\n</li>\n</ul>\n<ul>\n<li>第五模块有输出通道数为256 + (160,320) + (32,128) + 128 = 832和384 + (192,384) + (48,128) + 128 = 1024的两个Inception块，<strong>然后再接上一个全局平均池化层</strong></li>\n</ul>\n<ul>\n<li><strong>五个模块相连之后最后再接上一个全连接层，神经元个数为类别个数</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/28/rdpGSFRzuKZeTs2.jpg\" alt=\"142051802f8de8513fe61601277f03c8\"></p>\n<h1 id=\"8-批量归一化\"><a href=\"#8-批量归一化\" class=\"headerlink\" title=\"8 批量归一化\"></a>8 批量归一化</h1><ul>\n<li>我们一般在前向传播开始之前会对数据进行归一化，<strong>使不同特征之间具有可比性，并且更快收敛</strong></li>\n<li>通常来说，数据标准化预处理对于浅层模型就⾜够有效了，<strong>但对深层神经网络来说，即使输⼊数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。</strong>这种计算数值的不稳定性通常令我们难以训练出有效的深度模型</li>\n<li>而批量归一化则是对每一层的输出都做一次归一化，<strong>使均值永远为0，方差永远为1</strong></li>\n</ul>\n<h3 id=\"8-1-批量归一化层\"><a href=\"#8-1-批量归一化层\" class=\"headerlink\" title=\"8.1 批量归一化层\"></a>8.1 批量归一化层</h3><ul>\n<li><strong>通常，我们将批量归⼀化层置于全连接层中的仿射变换和激活函数之间</strong></li>\n<li>首先要对小批量求均值和方差：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\boldsymbol{\\mu}_{\\mathcal{B}} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} \\boldsymbol{x}^{(i)} \\\\\n\\boldsymbol{\\sigma}_{\\mathcal{B}}^{2} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{\\mathcal{B}}\\right)^{2}\n\\end{array}</script><p>得到的均值和方差是两个向量，维度为特征个数</p>\n<ul>\n<li>然后：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{\\mathcal{B}}}{\\sqrt{\\boldsymbol{\\sigma}_{\\mathcal{B}}^{2}+\\epsilon}}</script><ul>\n<li>$\\epsilon$ 为一个很小的数，一般取$10^{-8}$ ，是为了保证分母大于0</li>\n<li><strong>但是我们不确定是否一定是所有层都进行批量归一化才是最好的情况，所以我们要给予一个能变回归一化前的值的可能性，所以引入拉伸参数（scale）$\\gamma$ 和  偏移参数（shift）$\\beta$</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{y}^{(i)} \\leftarrow \\gamma \\odot \\hat{\\boldsymbol{x}}^{(i)}+\\boldsymbol{\\beta}</script><p>将$\\gamma$ 和 $\\beta$ 作为两个可变化参数，然后通过学习来决定拉伸多少和偏移多少</p>\n<h3 id=\"8-2-对卷积层做批量归一化\"><a href=\"#8-2-对卷积层做批量归一化\" class=\"headerlink\" title=\"8.2 对卷积层做批量归一化\"></a>8.2 对卷积层做批量归一化</h3><ul>\n<li>批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前</li>\n<li><p>如果卷积计算输出多个通道，我们需要<strong>对这些通道的输出分别做批量归⼀化，且每个通道都拥有独立的拉伸和偏移参数</strong></p>\n</li>\n<li><p>设小批量中有m个样本。在单个通道上，假设卷积计算输出的⾼和宽分别为p和q。我们需要对该通道中m × p × q个元素同时做批量归⼀化</p>\n</li>\n</ul>\n<h3 id=\"8-3-预测时的批量归一化\"><a href=\"#8-3-预测时的批量归一化\" class=\"headerlink\" title=\"8.3 预测时的批量归一化\"></a>8.3 预测时的批量归一化</h3><ul>\n<li><strong>批量归一化在训练模式和预测模式的计算结果是不⼀样的</strong>。确定均值和方差时，单个样本的输出不应取决于批量归⼀化所需要的随机小批量中的均值和⽅差。⼀种常⽤的⽅法是<strong>通过移动平均（指数加权平均）估算整个训练数据集的样本均值和方差</strong>，并在预测时使⽤它们得到确定的输出</li>\n</ul>\n<h1 id=\"9-残差网络（ResNet）\"><a href=\"#9-残差网络（ResNet）\" class=\"headerlink\" title=\"9 残差网络（ResNet）\"></a>9 残差网络（ResNet）</h1><h3 id=\"9-1-残差块\"><a href=\"#9-1-残差块\" class=\"headerlink\" title=\"9.1 残差块\"></a>9.1 残差块</h3><p><img src=\"https://i.loli.net/2021/11/25/zADLUxHm7wS8Z9Q.png\" alt=\"image-20211125221657826\"></p>\n<ul>\n<li>左图是一般形式的映射，右图为残差映射，是将输入x加权-&gt;激活-&gt;再加权后，再和原输入x相加，再送入激活函数</li>\n<li>这样的结构中，输⼊可通过跨层的数据线路更快地向前传播</li>\n<li>残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接⼀个批量归⼀化层和ReLU激活函数。然后我们<strong>将输⼊跳过这2个卷积运算后直接加在最后的ReLU激活函数前</strong></li>\n<li>这样的设计要求2个卷积层的输出与输⼊形状⼀样，从而可以相加。如果想改变通道数，就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算</li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg\" alt=\"\"></p>\n<h3 id=\"9-2-ResNet模型\"><a href=\"#9-2-ResNet模型\" class=\"headerlink\" title=\"9.2 ResNet模型\"></a>9.2 ResNet模型</h3><ul>\n<li>ResNet的前两层跟之前介绍的GoogLeNet中的⼀样：在输出通道数为64、步幅为2的7 × 7卷积层后接步幅为2的3 × 3的最⼤池化层。不同之处在于<strong>ResNet每个卷积层（对应上图每一次加权运算）后增加的批量归⼀化层</strong></li>\n</ul>\n<ul>\n<li><strong>后接4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块</strong></li>\n<li>第⼀个模块的通道数同输⼊通道数⼀致。由于之前已经使⽤了步幅为2的最⼤池化层，所以⽆须减小⾼和宽。<strong>之后的每个模块在第⼀个残差块⾥将上⼀个模块的通道数翻倍，并将高和宽减半</strong></li>\n</ul>\n<ul>\n<li>接着我们为ResNet加⼊所有残差块。这⾥每个模块的残差块个数可以自行定义</li>\n</ul>\n<ul>\n<li>最后，与GoogLeNet⼀样，加⼊全局平均池化层后接上全连接层输出</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/9aypGjvX1ESYtrM.png\" alt=\"image-20211125223902681\"></p>\n<p>其中10为类别个数</p>\n<p><img src=\"https://pic3.zhimg.com/80/v2-862e1c2dcb24f10d264544190ad38142_720w.jpg\" alt=\"\"></p>\n<h3 id=\"9-3-ResNet的作用\"><a href=\"#9-3-ResNet的作用\" class=\"headerlink\" title=\"9.3 ResNet的作用\"></a>9.3 ResNet的作用</h3><ul>\n<li>在深度神经网络中，如果我们进行反向传播，那么由链式求导法则可知，我们会<strong>涉及到非常多参数和导数的连乘</strong>，这时误差很容易产生消失或膨胀，通过实验也可以发现，层数更深的神经网络结果反而没有浅层的好</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/26/YecToJs5xNf6H24.png\" alt=\"image-20211125235804507\"></p>\n<p>这种结果很大程度归结于深度神经网络的<strong>梯度消失问题</strong></p>\n<ul>\n<li>而ResNet的提出就是为了解决梯度消失的问题，<strong>既然离输入近的神经网络层较难训练，那么我们可以将他短接到更接近输出的层</strong>，并且这种方法并不会对结果产生影响，假设输入为X，我们在一般网络中想要拟合f(X)，放到残差网络中就只需要拟合f(X) - X 即可</li>\n<li><strong>残差网络的设计思想不仅仅局限于卷积网络，实际上很多其他的网络也用到了残差，也取得了很不错的效果，都是为了解决深度神经网络的梯度消失问题</strong></li>\n</ul>\n<h1 id=\"10-稠密连接网络（DenseNet）\"><a href=\"#10-稠密连接网络（DenseNet）\" class=\"headerlink\" title=\"10 稠密连接网络（DenseNet）\"></a>10 稠密连接网络（DenseNet）</h1><p><img src=\"https://i.loli.net/2021/11/25/NTajCiqAydPvoVt.png\" alt=\"image-20211125224307704\"></p>\n<ul>\n<li>DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是<strong>在通道维上连结</strong></li>\n<li>DenseNet的主要构建模块是<strong>稠密块（dense block）</strong>和<strong>过渡层（transition layer）</strong>。前者定义了输⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤</li>\n</ul>\n<h3 id=\"10-1-稠密块\"><a href=\"#10-1-稠密块\" class=\"headerlink\" title=\"10.1 稠密块\"></a>10.1 稠密块</h3><ul>\n<li>我们将批量归⼀化、激活和卷积组合到一起形成一种块：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/hfHF7BLySM1swZG.png\" alt=\"image-20211125225431518\"></p>\n<ul>\n<li><strong>稠密块由多个conv_block组成，每块使⽤相同的输出通道数。但在前向计算时，我们将每块的输⼊和输出在通道维上连结</strong></li>\n</ul>\n<h3 id=\"10-2-过渡层\"><a href=\"#10-2-过渡层\" class=\"headerlink\" title=\"10.2 过渡层\"></a>10.2 过渡层</h3><ul>\n<li>由于每个稠密块都会带来通道数的增加，使⽤过多则会带来过于复杂的模型。过渡层⽤来控制模型复杂度。它<strong>通过1 × 1卷积层来减小通道数，并使⽤步幅为2的平均池化层减半高和宽</strong>，并且也要进行激活和BN运算</li>\n</ul>\n<h3 id=\"10-3-DenseNet模型\"><a href=\"#10-3-DenseNet模型\" class=\"headerlink\" title=\"10.3 DenseNet模型\"></a>10.3 DenseNet模型</h3><ul>\n<li>DenseNet⾸先使⽤同ResNet⼀样的单卷积层和最⼤池化层</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/25/DqcGv6FtMioNez1.png\" alt=\"image-20211125230039527\"></p>\n<ul>\n<li>接着使用4个稠密块，同ResNet⼀样，我们可以设置每个稠密块使⽤多少个卷积层</li>\n</ul>\n<ul>\n<li><p>在稠密块之间我们使用过渡层来减半高宽，并减半通道数</p>\n<p>注意最后一层是不使用过渡层进行减半的</p>\n</li>\n<li><p>最后再和ResNet一样，接上全局平均池化层和全连接层来输出</p>\n</li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/80/v2-c81da515c8fa9796601fde82e4d36f61_720w.jpg\" alt=\"\"></p>\n"},{"title":"CV基础","math":true,"date":"2021-12-15T16:00:00.000Z","_content":"\n\n\n# 1 图像增广\n\n- 图像增⼴（image augmentation）技术通过对训练图像做⼀系列随机改变，来产⽣相似但⼜不同的训练样本，从而**扩大训练数据集的规模**\n- 随机改变训练样本可以**降低模型对某些属性的依赖，从而提高模型的泛化能力**。例如，我们可以对图像进⾏不同 方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性\n\n- 常用的方法有：**翻转**、**裁剪**、**变换颜色**。对于翻转，大部分情况左右翻转比上下翻转更通用一些。对于颜色，我们可以从亮度、对比度、饱和度和⾊调四方面进行改变\n- 实际应⽤中我们会将多个图像增⼴方法叠加使⽤\n\n\n\n\n\n# 2 微调\n\n- **迁移学习（transfer learning）**：**将从源数据集学到的知识迁移到目标数据集上**。例如，虽然ImageNet数据集的图像大多跟椅⼦⽆关，但在该数据集上训练的模型可以抽 取较通⽤的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于 识别椅⼦也可能同样有效。\n- 迁移学习中的一种常用技术“微调（**fine tuning）**”，由一下四步构成：\n\n> 1. 在源数据集（如ImageNet数据集）上预训练⼀个神经⽹络模型，即源模型\n> 2. 创建⼀个新的神经⽹络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设 计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适⽤于目标数据集。我们还假设源模型的输出层与源数据集的标签紧密相关，因此在目标模 型中不予采⽤。\n> 3. 为目标模型添加⼀个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。\n> 4. 在目标数据集（如椅⼦数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123170934001.png\" alt=\"image-20220123170934001\" style=\"zoom: 67%;\" />\n>\n> - 可以选择保留除输出层以外的所有层，也可以保留除临近输出层的几层以外的所有层\n> - 由于预训练模型是比较接近正确结果的，而新添加的层是随机初始化。**所以两者的学习率是不同的，预训练的学习率更小**\n\n\n\n\n\n# 3 目标检测和边界框\n\n- 很多时候图像⾥有多个我们感兴趣的目标，我们 不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉⾥，我们将这类任务 称为目标检测（object detection）或物体检测\n- 在目标检测⾥，我们通常使⽤**边界框（bounding box）**来描述目标位置。边界框是⼀个矩形框， 可以由矩形左上⻆的x和y轴坐标与右下⻆的x和y轴坐标确定\n- 它以每个像素为中⼼⽣ 成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为**锚框（anchor box）**\n\n\n\n### 3.1 锚框的生成\n\n- 假设输⼊图像高为h，宽为w。我们分别以图像的每个像素为中⼼⽣成不同形状的锚框。设大小为s $$\\in$$ (0, 1]且宽高比为r > 0，那么锚框的宽和高将分别为$$ws\\sqrt{r}$$和$$hs/\\sqrt{r}$$。当中⼼位置给定时，已 知宽和高的锚框是确定的。⾯我们分别设定好⼀组大小$$s_1, . . . , s_n$$和⼀组宽高比$$r_1, . . . , r_m$$，s和r两两配对能覆盖所有的真实边界框，但是计算复杂度容易更高，所以我们通常只对包含$$s_1$$或$$r_1$$的大小与宽高比的组合感兴趣，即：\n\n$$\n\\left(s_{1}, r_{1}\\right),\\left(s_{1}, r_{2}\\right), \\ldots,\\left(s_{1}, r_{m}\\right),\\left(s_{2}, r_{1}\\right),\\left(s_{3}, r_{1}\\right), \\ldots,\\left(s_{n}, r_{1}\\right)\n$$\n\n\n\n### 3.2 交并比\n\n- Jaccard系数 （Jaccard index）可以衡量两个集合的相似度。给定集合A和B，它们的Jaccard系数即⼆者交集大小除以⼆者并集大小：\n\n$$\nJ(\\mathcal{A}, \\mathcal{B})=\\frac{|\\mathcal{A} \\cap \\mathcal{B}|}{|\\mathcal{A} \\cup \\mathcal{B}|}\n$$\n\n- 我们通 常将Jaccard系数称为交并比（intersection over union，IoU），即两个边界框相交⾯积与相并⾯积之比\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123173756721.png\" alt=\"image-20220123173756721\" style=\"zoom: 80%;\" />\n\n\n\n### 3.3 标注训练集的锚框\n\n- 在训练集中，我们**将每个锚框视为⼀个训练样本**。为了训练目标检测模型，我们需要为每个锚框标注两类标签：**⼀是锚框所含目标的类别，简称类别；⼆是真实边界框相对锚框的偏移量，简称偏移量（offset)**\n- 在目标检测时，我们⾸先⽣成多个锚框，然后为每个锚框预测类别以及偏移量， 接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框\n\n\n\n- 假设图像中锚框分别为$$A_1, A_2, . . . , A_{n_a}$$，真实边界框分别为$$B_1, B_2, . . . , B_{n_b}$$，且$$n_a$$ ≥ $$n_b$$。定义矩 阵$$X \\in R^{n_a\\times n_b}$$，其中第i⾏第j列的元素$$x_{ij}$$为锚框$$A_i$$与真实边界框$$B_j$$的交并比\n> 1. 找到矩阵中最大元素，并将该值对应的真实边界框赋值给锚框，然后从矩阵中去除该行和该列所有元素。然后继续找最大元素，重复上述操作，直到所有真实边界框都被分配完\n> 2. 这个时候，我们已为$$n_b$$个锚框各分配了⼀个真实边界框。接下来，我们 只遍历剩余的$$n_a − n_b$$个锚框：给定其中的锚框$$A_i$$，根据矩阵$$X$$的第i⾏找到与$$A_i$$交并比最大的真实边界框$$B_j$$，**且只有当该交并比大于预先设定的阈值时，才为锚框$$A_i$$分配真实边界框$$B_j$$**\n\n- **如果⼀个锚框A被分配了真实边界框B，将锚框A的类别设为B的类别，并根据B和A的中心坐标的相对位置以及两个框的相对大小为锚框A标注偏 移量。**\n- **由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要⼀些特殊变换，才能使偏移量的分布更均匀从而更容易拟合**。设锚框A及其被分配的真实边界框B的中 ⼼坐标分别为$$(x_a, y_a)$$和$$(x_b, y_b)$$，A和B的宽分别为$$w_a$$和$$w_b$$，高分别为$$h_a$$和$$h_b$$，⼀个常⽤的技巧是将A的偏移量标注为：\n\n$$\n\\left(\\frac{\\frac{x_{b}-x_{a}}{w_{a}}-\\mu_{x}}{\\sigma_{x}}, \\frac{\\frac{y_{b}-y_{a}}{h_{a}}-\\mu_{y}}{\\sigma_{y}}, \\frac{\\log \\frac{w_{b}}{w_{a}}-\\mu_{w}}{\\sigma_{w}}, \\frac{\\log \\frac{h_{b}}{h_{a}}-\\mu_{h}}{\\sigma_{h}}\\right)\n$$\n\n其中常数的默认值为$$µ_x = µ_y = µ_w = µ_h = 0, σ_x = σ_y = 0.1, σ_w = σ_h = 0.2$$\n\n\n\n- 如果一个锚框没有被分配真实边界框，我们只需将该锚框的类别设为**背景**。类别为背景的锚框通常被称为**负类锚框**，其余则被称为**正类锚框**\n\n- **掩码（mask）：** **形状为(批量大小, 锚框个数的四倍)**。掩码变量中的元素 与每个锚框的4个偏移量⼀⼀对应。由于我们不关⼼对背景的检测，有关负类的偏移量不应影响 目标函数。**通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量**\n\n\n\n### 3.4 非极大值抑制\n\n- 为了使结果更加简洁，我们可以移除相似的预测边界框。常⽤的方法叫作非极大值抑制（non-maximum suppression，NMS)：\n\n> 1. 在同⼀图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序， 得到列表L\n> 2. 从L中选取置信度最高的预测边界框$$B_1$$作为基准，将所有与$$B_1$$的交并比大于某阈值 的非基准预测边界框从L中移除。这⾥的阈值是预先设定的超参数\n> 3. 继续重复上述操作，直到L中所有的预测边界框都曾作为基准\n\n- 实践中，我们可以在执⾏非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。\n\n\n\n### 3.5 多尺度目标检测\n\n- 在不同尺度下，我们可以⽣成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多\n- 因此，当使⽤较小 锚框来检测较小目标时，我们可以采样较多的区域；而当使⽤较大锚框来检测较大目标时，我们可以采样较少的区域。\n- 我们可以通过**控制特征图的大小来控制尺度（特征图每个单元在输入图像上对应的感受野可大可小）**。本质上，我们⽤输⼊图像在某个感 受野区域内的信息来预测输⼊图像上与该区域位置相近的锚框的类别和偏移量\n\n\n\n### 3.6 单发多框检测（SSD）\n\n- 它主要由**⼀个基础⽹络块和若干个多尺度特征块串联而成**。其中**基础⽹络块⽤来从原始图像中抽取特征**，因此⼀般会选择常⽤的深度卷积神经⽹络。单 发多框检测论⽂中选⽤了在分类层之前截断的VGG，现在也常⽤ResNet替代\n- 我们可以设计 基础⽹络，使它输出的高和宽较大。这样⼀来，基于该特征图⽣成的锚框数量较多，可以⽤来检 测尺⼨较小的目标\n- 接下来的每个多尺度特征块将上⼀层提供的特征图的**高和宽缩小（如减半）**， 并使**特征图中每个单元在输⼊图像上的感受野变得更⼴阔**。如此⼀来，下图中**越靠近顶部的多 尺度特征块输出的特征图越小，故而基于特征图⽣成的锚框也越少，加之特征图中每个单元感受 野越大，因此更适合检测尺⼨较大的目标**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123182624112.png\" alt=\"image-20220123182624112\" style=\"zoom:80%;\" />\n\n##### 3.6.1 类别预测层\n\n- 设目标的类别个数为q。每个锚框的类别个数将是q + 1，其中类别0表⽰锚框只包含背景\n- 设特征图的高和宽分别为h和w，如果以其中每个单元为中⼼⽣成a个锚框，那么我们需 要对hwa个锚框进⾏分类。如果使⽤全连接层作为输出，**很容易导致模型参数过多。所以我们可以通过通道输出类别**\n- 类别预测层使⽤⼀个**保持输⼊高和宽的卷积层**。这样⼀来，输出和输⼊在特征图宽和高上的空间坐标⼀⼀对应。考虑输出和输⼊同⼀空间坐标(x, y)：**输出特征图上(x, y)坐标的通道里包含了以输⼊特征图(x, y)坐标为中心生成的所有锚框的类别预测**。因此**输出通道数为a(q + 1)， 其中索引为i(q + 1) + j（0 ≤ j ≤ q）的通道代表了索引为i的锚框有关类别索引为j的预测**\n\n\n\n##### 3.6.2 边界框预测层\n\n- 边界框预测层的设计与类别预测层的设计类似。唯⼀不同的是，这⾥需要为每个锚框预测4个偏移量，而不是q + 1个类别\n\n\n\n##### 3.6.3 连结多尺度的预测\n\n- 每个尺度的输出，除了批量大小一样，其他维度的大小均不一样。我们需要将它们变形成统⼀的格式并将多尺度的预测连结，从而让后续计算更简单\n- 所以我们需要将为(批量大小, 通道数, 高, 宽)格式转换成⼆维的(批量大小, 高×宽×通道数)的格式，以方便之后在维度1上的连结。\n\n\n\n##### 3.6.4 损失函数和评价函数\n\n- 目标检测有两个损失：**⼀是有关锚框类别的损失**，我们可以重⽤之前图像分类问题⾥⼀直使⽤ 的交叉熵损失函数；**⼆是有关正类锚框偏移量的损失**\n- 。预测偏移量是⼀个回归问题，但这⾥不 使⽤前⾯介绍过的平方损失，而使⽤L1范数损失，即预测值与真实值之间差的绝对值（其中 使用掩码变量令负类锚框和填充锚框不参与损失的计算）\n- 最后，我们将有关锚框类别和偏移量的损失相加得到模型的最终损失函数。\n- 可以将$$L_1$$损失换成**平滑的$$L_1$$范数损失**，它在零点附近使⽤平方函数从而更加平滑，这是通过⼀个超参数$$\\sigma$$来控制平滑区域的：\n\n$$\nf(x)=\\left\\{\\begin{array}{ll}\n(\\sigma x)^{2} / 2, & \\text { if }|x|<1 / \\sigma^{2} \\\\\n|x|-0.5 / \\sigma^{2}, & \\text { otherwise }\n\\end{array}\\right.\n$$\n\n当$$\\sigma$$很⼤时该损失类似于$$L_1$$范数损失。当它较小时，损失函数较平滑。\n\n- 还可以将交叉熵损失换成**焦点损失（focal loss）**：\n\n$$\n-\\alpha\\left(1-p_{j}\\right)^{\\gamma} \\log p_{j}\n$$\n\n焦点损失适用于比较密集的目标检测，即要判定的类别比较多的情况。我们将一个锚框标签的类别作为正类，其余都作为负类（包括背景），那么这就转换成了一个二分类问题，回顾二分类的交叉熵损失函数：\n$$\n\\mathrm{L}=-\\mathrm{ylog} y^{\\prime}-(1-y) \\log \\left(1-y^{\\prime}\\right)=\\left\\{\\begin{array}{ll}\n-\\log y^{\\prime} & , \\quad y=1 \\\\\n-\\log \\left(1-y^{\\prime}\\right), & y=0\n\\end{array}\\right.\n$$\n可以看到当标签为负类（y=0），且正类预测概率$$y'$$较大时，会产生较大的损失。而由于正负样本的极其不均衡（比如有1000个类别，正类只有1种，负类则有999种），所以负样本会主导梯度的更新方向，使得整体学习方向跑偏\n\n上述的负类因为正负样本的不均衡，所以负类是是易分类的样本（$$p_j > 0.5$$），而焦点损失中的$$(1-p_j)^{\\gamma}$$就是为了减轻易分类样本的权重，让对象检测器更关注难分类的样本（即正样本）\n\n\n\n\n\n# 4 区域卷积神经网络（R-CNN）\n\n### 4.1 R-CNN\n\n- R-CNN⾸先对图像**选取若干提议区域**（如锚框也是⼀种选取方法）并标注它们的类别和边界框 （如偏移量）。然后，**⽤卷积神经⽹络对每个提议区域做前向计算抽取特征。之后，我们⽤每个提议区域的特征预测类别和边界框**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123184832784.png\" alt=\"image-20220123184832784\" style=\"zoom:80%;\" />\n\n- 具体来说，R-CNN主要由以下4步构成：\n\n  > 1. 对输⼊图像使⽤选择性搜索（selective search）来**选取多个高质量的提议区域**。这些提议区域通常是在**多个尺度下**选取的，并具有不同的形状和大小。**每个提议区域将被标注类 别和真实边界框。**\n  > 2. 选取⼀个预训练的卷积神经⽹络，并将其在输出层之前截断\n  > 3. 将每个提议区域的特征连同其标注的类别作为⼀个样本，训练多个⽀持向量机对目标分类。 其中每个⽀持向量机⽤来判断样本是否属于某⼀个类别\n  > 4. 将每个提议区域的特征连同其标注的边界框作为⼀个样本，训练线性回归模型来预测真实边界框\n\n- 一张图片中可能会有很多个提议区域，每个区域都要进行卷积运算。这个**巨大的计算量令R-CNN难以在实际应⽤中被⼴泛采⽤**\n\n\n\n### 4.2 Fast R-CNN\n\n- R-CNN提议区域通常**有大量重叠， 独⽴的特征抽取会导致大量的重复计算**。Fast R-CNN对R-CNN的⼀个主要改进在于**只对整个图像 做卷积神经⽹络的前向计算。**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123185433036.png\" alt=\"image-20220123185433036\" style=\"zoom:80%;\" />\n\n- 主要步骤：\n\n> 1. 与R-CNN相比，Fast R-CNN⽤来提取特征的卷积神经⽹络的输⼊是整个图像，而不是各个提议区域。而且，**这个⽹络通常会参与训练**，即更新模型参数。设输⼊为⼀张图像，将卷积神经⽹络的输出的形状记为$$1 × c × h_1 × w_1$$\n> 2. 假设选择性搜索⽣成n个提议区域。**这些形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域**。这些兴趣区域需要抽取出**形状相同**的特征（假设高和宽均分别指定为$$h_2$$和$$w_2$$）以便于连结后输出（使用**兴趣区域池化（region of interest pooling，RoI池化）层，将卷积神经⽹络的输出和提议区域作为输⼊，输出连结后的各个提议区域抽取的特征**）\n> 3. 通过全连接层将输出形状变换为n × d，其中超参数d取决于模型设计\n> 4. 预测类别时，将全连接层的输出的形状再变换为n × q并使⽤softmax回归（q为类别个数）。 预测边界框时，将全连接层的输出的形状变换为n × 4。也就是说，我们为每个提议区域预 测类别和边界框。\n\n- 兴趣区域池化层：兴趣区域池化层对每个区域的输 出形状是可以直接指定的如下图：\n\n![image-20220123191333584](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191333584.png)\n\n第一张图的蓝色区域是一个提议区域，将其经过2x2兴趣区域池化层后，划分成了4个区域，分别含有元素0、1、4、5（5最 大），2、6（6最大），8、9（9最大），10。输出每个区域的最大元素\n\n\n\n### 4.3 Faster R-CNN\n\n- Fast R-CNN通常需要在选择性搜索中⽣成较多的提议区域，以获得较精确的目标检测结果。Faster R-CNN提出**将选择性搜索替换成区域提议⽹络（region proposal network）**，从而**减少提议区域 的⽣成数量**，并保证目标检测的精度\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191545146.png\" alt=\"image-20220123191545146\" style=\"zoom:67%;\" />\n\n\n\n- 与Fast R-CNN相比，只有⽣成提议区域的方法从选择性搜索变成了区域提议⽹络，而其他部分均保持不变。具体来说，区域提议⽹络的计算步骤如下：\n\n> 1. 使⽤填充为1的3 × 3卷积层变换卷积神经⽹络的输出，并将输出通道数记为c\n> 2. . 以特征图每个单元为中⼼，⽣成多个不同大小和宽高比的锚框并标注它们\n> 3. . ⽤锚框中⼼单元⻓度为c的特征分别预测该锚框的**二元类别（含目标还是背景，需要reshape）**和边界框\n> 4.  使⽤非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测 边界框即兴趣区域池化层所需要的提议区域。\n\n- 区域提议⽹络作为Faster R-CNN的⼀部分，是和整个模型⼀起训练得到的。也就 是说，**Faster R-CNN的目标函数既包括目标检测中的类别和边界框预测，⼜包括区域提议⽹络中 锚框的⼆元类别和边界框预测**\n\n\n\n### 4.4 Mask R-CNN\n\n- 如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利⽤这些详尽 的标注信息进⼀步提升目标检测的精度。\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123193610616.png\" alt=\"image-20220123193610616\" style=\"zoom: 80%;\" />\n\n- Mask R-CNN将兴趣区域池化层 替换成了兴趣区域对⻬层，即通过**双线性插值（bilinear interpolation）（一种常用的上采样方法，目的是将下一层的特征图的单元于上一层特征图单元对齐）**来保留特征图上的空间信息，从而更适于像素级预测\n\n\n\n\n\n# 5 语义分割\n\n- 语义分割（semantic segmentation）关注如何将图像分割成属于不同语义类别的区域。值得⼀提的是，这些语义区域的标注和预测都是像素级的\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123194304346.png\" alt=\"image-20220123194304346\" style=\"zoom: 67%;\" />\n\n- 计算机视觉领域还有2个与语义分割相似的重要问题，即**图像分割（image segmentation）**和**实例分割（instance segmentation）**\n- **图像分割**将图像分割成若干组成区域。**这类问题的方法通常利⽤图像中像素之间的相关性**。 它在训练时不需要有关图像像素的标签信息，在预测时也⽆法保证分割出的区域具有我们希望得到的语义。如上图图像分割可能将狗分割成两个区域：⼀个覆盖以⿊⾊为主的嘴巴和眼睛，而另⼀个覆盖以⻩⾊为主的其余部分⾝体\n- **实例分割**研究如何 识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割**不仅需要区分语 义，还要区分不同的目标实例**。如果图像中有两只狗，实例分割需要区分像素属于这两只 狗中的哪⼀只。\n\n\n\n- 如果我们通过缩放图像使其符合模型的输⼊形状。然而在语义分割⾥，**需要将预测的像素类别重新映射回原始尺⼨的输⼊图像**。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，**我们将图像裁剪成固定尺⼨而不是缩放**（对于实际尺寸小于规定尺寸的图像，需要移除）\n\n\n\n\n\n# 6 全卷积网络（FCN）\n\n- 全卷积⽹络（fully convolutional network，FCN）采⽤卷积神经⽹络实现了从图像像素到像素类别的变换。\n- 全卷积⽹络通过**转置卷积（transposed convolution）层将中间层特征图的高和宽变换回输⼊图像的尺⼨**\n\n\n\n- 因为卷积运算可以用矩阵乘法来实现，假设input进行卷积运算相当于input矩阵乘一个$$W$$矩阵得到特征图featrue。那么可以通过featrue乘一个$$W^T$$来变回input的形状。**所以可以通过转置卷积层来交换卷积层输入和输出的形状**\n\n\n\n### 6.1 模型构造\n\n- 全卷积⽹络先使⽤卷积神经⽹络抽取图像特征，然后通过1 × 1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的⾼和宽变换为输⼊图像的尺⼨\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124154145643.png\" alt=\"image-20220124154145643\" style=\"zoom: 67%;\" />\n\n\n\n### 6.2 初始化转置卷积层\n\n- **双线性插值：**首先介绍一下单线性插值（一维）：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190819135319360.jpg\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\" />\n\n我们知道$$x_0,x_1,y_0,y_1,x$$的值，要求$$y$$的值：\n$$\ny = y_0 + \\frac{y_1 - y_0}{x_1 - x_0}(x - x_0)\n$$\n而**双线性插值其实就是在不同的维度上单线性插值两次**，已知$$Q_{11}(x_1,y_1),Q_{12}(x_1,y_2),Q_{21}(x_2,y_1),Q_{22}(x_2,y_2)$$，求其中点P(x,y)的函数值：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-ad3d95548a97aa47ca85867cd0f2e161_720w.jpg\" alt=\"img\" style=\"zoom:67%;\" />\n\n首先在x方向单线性插值两次：\n$$\n\\begin{array}{l}\nf\\left(R_{1}\\right)=\\frac{x_{2}-x}{x_{2}-x_{1}} f\\left(Q_{11}\\right)+\\frac{x-x_{1}}{x_{2}-x_{1}} f\\left(Q_{21}\\right) \\\\\nf\\left(R_{2}\\right)=\\frac{x_{2}-x}{x_{2}-x_{1}} f\\left(Q_{12}\\right)+\\frac{x-x_{1}}{x_{2}-x_{1}} f\\left(Q_{22}\\right)\n\\end{array}\n$$\n然后再在y方向单线性插值一次：\n$$\nf(P)=\\frac{y_{2}-y}{y_{2}-y_{1}} f\\left(R_{1}\\right)+\\frac{y-y_{1}}{y_{2}-y_{1}} f\\left(R_{2}\\right)\n$$\n即可得到结果。\n\n\n\n- 在全卷积⽹络中，我们一般**将转置卷积层初始化为双线性插值的上采样**，具体方法是：\n\n> 1. 为了得到输出图像在坐 标(x, y)上的像素，先将该坐标映射到输⼊图像的坐标(x ′ , y′ )，例如，根据输⼊与输出的尺⼨之⽐来映射。\n> 2. 映射后的x ′和y ′通常是实数。然后，在输⼊图像上找到与坐标(x ′ , y′ )最近的4像素。最后， 输出图像在坐标(x, y)上的像素依据输⼊图像上这4像素及其与(x ′ , y′ )的相对距离来计算（用双线性插值）\n\n\n\n- 如果步幅为s、填充为s/2（假设s/2为整数）、卷积核的⾼和宽为2s，转置卷积核将输⼊的⾼和宽分别放⼤s倍\n\n- 转置卷积层的输出形状可能会不一样，这是因为当**输入图像的高宽无法整除步幅时，输出的高宽会有所偏差**。为了解决这个问题，我们可以在图像中**截取多块⾼和宽为步幅的整数倍的矩形区域**，并分别对这些区域中的像素做前向计算。**这些区域的并集需要完整覆盖输⼊图像**。当⼀个像素被多个区域所覆盖时，它在不同区域 前向计算中转置卷积层输出的**平均值可以作为softmax运算的输⼊**，从而预测类别\n\n\n\n\n\n# 7 样式迁移\n\n- 使⽤卷积神经⽹络⾃动将某图像中的样式应⽤在另⼀图像之上，即样式迁移（style transfer）。需要两张输⼊图像，⼀张是**内容图像**，另⼀张是**样式图像**， 我们将使⽤神经⽹络**修改内容图像使其在样式上接近样式图像**\n\n- 主要步骤：\n\n> 1. ⾸先，我们初始化合成图像，例如 将其**初始化成内容图像**。该合成图像是样式迁移过程中**唯⼀需要更新的变量**，\n> 2. 我们选择⼀个预训练的卷积神经⽹络来**抽取图像的特征**，其中的**模型参数在训练中⽆须更新**。深度卷积神经⽹络**凭借多个层逐级抽取图像的特征。我们可以选择其中某些 层的输出作为内容特征或样式特征**，例如下图，这⾥选取的预训练的神经⽹络含有3个卷积 层，其中第⼆层输出图像的内容特征，而第⼀层和第三层的输出被作为图像的样式特征\n> 3. 样式迁移常⽤的损失函数由3部分组成：**内容损失（content loss）**使合成图像与内容图像在内容特征上接近，**样式损失（style loss）**令合成图像与样式图像 在样式特征上接近，而**总变差损失（total variation loss）**则有助于减少合成图像中的噪点\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124175546385.png\" alt=\"image-20220124175546385\" style=\"zoom:50%;\" />\n\n\n\n### 7.1 内容层和样式层的选择\n\n- **⼀般来说，越靠近输⼊层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成 图像过多保留内容图像的细节，我们选择较靠近输出的层来输出图像的内容特征**\n- **我们还可以选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层**\n- 例如VGG-19,使⽤了5个卷积块。我们可以选择第四卷积块的最后⼀个卷积层作为内容层，以及每个卷积块的第⼀个卷积层作为样式层\n\n\n\n### 7.2 损失函数\n\n- **内容损失：**与线性回归中的损失函数类似，内容损失通过平⽅误差函数衡量合成图像与内容图像在内容特征上的差异。平⽅误差函数的两个输⼊均为内容层的输出。\n\n- **样式损失：**样式损失也⼀样通过平⽅误差函数衡量合成图像与样式图像在样式上的差异。我们将样式层的输出（长为h，宽为w，通道数为c），转化成c行hw列的矩阵X，矩阵X可以看作由c个长度为hw的向量$$x_1, . . . , x_c$$组成的。其中向量$$x_i$$代表了通道$$i$$上的样式特征。\n\n  这些向量的**格拉姆矩阵 （Gram matrix）$$XX^T\\in R^{c\\times c}$$中$$i$$⾏$$j$$列的元素$$x_{ij}$$即向量$$x_i$$与$$x_j$$的内积**，它表达了通道$$i$$和通道$$j$$上样式特征的相关性。我们⽤这样的格拉姆矩阵表达样式层输出的样式。\n\n  需要注意的是，当hw的值较⼤时，格拉姆矩阵中的元素容易出现较⼤的值。此外，格拉姆矩阵的⾼和宽皆为通道数c。为了让样式损失不受这些值的⼤小影响，**需要除以矩阵中元素的个数，即chw**\n\n- **总变量损失：**有时候，我们学到的合成图像⾥⾯有⼤量⾼频噪点，即有特别亮或者特别暗的颗粒像素。⼀种常⽤的降噪⽅法是总变差降噪（total variation denoising）。\n\n  假设$$x_{i,j}$$表⽰坐标为$$(i, j)$$的像素值，降低总变差损失：\n  $$\n  \\sum_{i, j}\\left|x_{i, j}-x_{i+1, j}\\right|+\\left|x_{i, j}-x_{i, j+1}\\right|\n  $$\n  能够尽可能使邻近的像素值相似。\n\n\n\n- 样式迁移的损失函数即内容损失、样式损失和总变差损失的**加权和**。通过调节这些**权值超参数**， 我们可以权衡合成图像在保留内容、迁移样式以及降噪三⽅⾯的相对重要性。","source":"_posts/CV基础.md","raw":"---\ntitle: CV基础\nmath: true\ndate: 2021-12-16\n---\n\n\n\n# 1 图像增广\n\n- 图像增⼴（image augmentation）技术通过对训练图像做⼀系列随机改变，来产⽣相似但⼜不同的训练样本，从而**扩大训练数据集的规模**\n- 随机改变训练样本可以**降低模型对某些属性的依赖，从而提高模型的泛化能力**。例如，我们可以对图像进⾏不同 方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性\n\n- 常用的方法有：**翻转**、**裁剪**、**变换颜色**。对于翻转，大部分情况左右翻转比上下翻转更通用一些。对于颜色，我们可以从亮度、对比度、饱和度和⾊调四方面进行改变\n- 实际应⽤中我们会将多个图像增⼴方法叠加使⽤\n\n\n\n\n\n# 2 微调\n\n- **迁移学习（transfer learning）**：**将从源数据集学到的知识迁移到目标数据集上**。例如，虽然ImageNet数据集的图像大多跟椅⼦⽆关，但在该数据集上训练的模型可以抽 取较通⽤的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于 识别椅⼦也可能同样有效。\n- 迁移学习中的一种常用技术“微调（**fine tuning）**”，由一下四步构成：\n\n> 1. 在源数据集（如ImageNet数据集）上预训练⼀个神经⽹络模型，即源模型\n> 2. 创建⼀个新的神经⽹络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设 计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适⽤于目标数据集。我们还假设源模型的输出层与源数据集的标签紧密相关，因此在目标模 型中不予采⽤。\n> 3. 为目标模型添加⼀个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。\n> 4. 在目标数据集（如椅⼦数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123170934001.png\" alt=\"image-20220123170934001\" style=\"zoom: 67%;\" />\n>\n> - 可以选择保留除输出层以外的所有层，也可以保留除临近输出层的几层以外的所有层\n> - 由于预训练模型是比较接近正确结果的，而新添加的层是随机初始化。**所以两者的学习率是不同的，预训练的学习率更小**\n\n\n\n\n\n# 3 目标检测和边界框\n\n- 很多时候图像⾥有多个我们感兴趣的目标，我们 不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉⾥，我们将这类任务 称为目标检测（object detection）或物体检测\n- 在目标检测⾥，我们通常使⽤**边界框（bounding box）**来描述目标位置。边界框是⼀个矩形框， 可以由矩形左上⻆的x和y轴坐标与右下⻆的x和y轴坐标确定\n- 它以每个像素为中⼼⽣ 成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为**锚框（anchor box）**\n\n\n\n### 3.1 锚框的生成\n\n- 假设输⼊图像高为h，宽为w。我们分别以图像的每个像素为中⼼⽣成不同形状的锚框。设大小为s $$\\in$$ (0, 1]且宽高比为r > 0，那么锚框的宽和高将分别为$$ws\\sqrt{r}$$和$$hs/\\sqrt{r}$$。当中⼼位置给定时，已 知宽和高的锚框是确定的。⾯我们分别设定好⼀组大小$$s_1, . . . , s_n$$和⼀组宽高比$$r_1, . . . , r_m$$，s和r两两配对能覆盖所有的真实边界框，但是计算复杂度容易更高，所以我们通常只对包含$$s_1$$或$$r_1$$的大小与宽高比的组合感兴趣，即：\n\n$$\n\\left(s_{1}, r_{1}\\right),\\left(s_{1}, r_{2}\\right), \\ldots,\\left(s_{1}, r_{m}\\right),\\left(s_{2}, r_{1}\\right),\\left(s_{3}, r_{1}\\right), \\ldots,\\left(s_{n}, r_{1}\\right)\n$$\n\n\n\n### 3.2 交并比\n\n- Jaccard系数 （Jaccard index）可以衡量两个集合的相似度。给定集合A和B，它们的Jaccard系数即⼆者交集大小除以⼆者并集大小：\n\n$$\nJ(\\mathcal{A}, \\mathcal{B})=\\frac{|\\mathcal{A} \\cap \\mathcal{B}|}{|\\mathcal{A} \\cup \\mathcal{B}|}\n$$\n\n- 我们通 常将Jaccard系数称为交并比（intersection over union，IoU），即两个边界框相交⾯积与相并⾯积之比\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123173756721.png\" alt=\"image-20220123173756721\" style=\"zoom: 80%;\" />\n\n\n\n### 3.3 标注训练集的锚框\n\n- 在训练集中，我们**将每个锚框视为⼀个训练样本**。为了训练目标检测模型，我们需要为每个锚框标注两类标签：**⼀是锚框所含目标的类别，简称类别；⼆是真实边界框相对锚框的偏移量，简称偏移量（offset)**\n- 在目标检测时，我们⾸先⽣成多个锚框，然后为每个锚框预测类别以及偏移量， 接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框\n\n\n\n- 假设图像中锚框分别为$$A_1, A_2, . . . , A_{n_a}$$，真实边界框分别为$$B_1, B_2, . . . , B_{n_b}$$，且$$n_a$$ ≥ $$n_b$$。定义矩 阵$$X \\in R^{n_a\\times n_b}$$，其中第i⾏第j列的元素$$x_{ij}$$为锚框$$A_i$$与真实边界框$$B_j$$的交并比\n> 1. 找到矩阵中最大元素，并将该值对应的真实边界框赋值给锚框，然后从矩阵中去除该行和该列所有元素。然后继续找最大元素，重复上述操作，直到所有真实边界框都被分配完\n> 2. 这个时候，我们已为$$n_b$$个锚框各分配了⼀个真实边界框。接下来，我们 只遍历剩余的$$n_a − n_b$$个锚框：给定其中的锚框$$A_i$$，根据矩阵$$X$$的第i⾏找到与$$A_i$$交并比最大的真实边界框$$B_j$$，**且只有当该交并比大于预先设定的阈值时，才为锚框$$A_i$$分配真实边界框$$B_j$$**\n\n- **如果⼀个锚框A被分配了真实边界框B，将锚框A的类别设为B的类别，并根据B和A的中心坐标的相对位置以及两个框的相对大小为锚框A标注偏 移量。**\n- **由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要⼀些特殊变换，才能使偏移量的分布更均匀从而更容易拟合**。设锚框A及其被分配的真实边界框B的中 ⼼坐标分别为$$(x_a, y_a)$$和$$(x_b, y_b)$$，A和B的宽分别为$$w_a$$和$$w_b$$，高分别为$$h_a$$和$$h_b$$，⼀个常⽤的技巧是将A的偏移量标注为：\n\n$$\n\\left(\\frac{\\frac{x_{b}-x_{a}}{w_{a}}-\\mu_{x}}{\\sigma_{x}}, \\frac{\\frac{y_{b}-y_{a}}{h_{a}}-\\mu_{y}}{\\sigma_{y}}, \\frac{\\log \\frac{w_{b}}{w_{a}}-\\mu_{w}}{\\sigma_{w}}, \\frac{\\log \\frac{h_{b}}{h_{a}}-\\mu_{h}}{\\sigma_{h}}\\right)\n$$\n\n其中常数的默认值为$$µ_x = µ_y = µ_w = µ_h = 0, σ_x = σ_y = 0.1, σ_w = σ_h = 0.2$$\n\n\n\n- 如果一个锚框没有被分配真实边界框，我们只需将该锚框的类别设为**背景**。类别为背景的锚框通常被称为**负类锚框**，其余则被称为**正类锚框**\n\n- **掩码（mask）：** **形状为(批量大小, 锚框个数的四倍)**。掩码变量中的元素 与每个锚框的4个偏移量⼀⼀对应。由于我们不关⼼对背景的检测，有关负类的偏移量不应影响 目标函数。**通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量**\n\n\n\n### 3.4 非极大值抑制\n\n- 为了使结果更加简洁，我们可以移除相似的预测边界框。常⽤的方法叫作非极大值抑制（non-maximum suppression，NMS)：\n\n> 1. 在同⼀图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序， 得到列表L\n> 2. 从L中选取置信度最高的预测边界框$$B_1$$作为基准，将所有与$$B_1$$的交并比大于某阈值 的非基准预测边界框从L中移除。这⾥的阈值是预先设定的超参数\n> 3. 继续重复上述操作，直到L中所有的预测边界框都曾作为基准\n\n- 实践中，我们可以在执⾏非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。\n\n\n\n### 3.5 多尺度目标检测\n\n- 在不同尺度下，我们可以⽣成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多\n- 因此，当使⽤较小 锚框来检测较小目标时，我们可以采样较多的区域；而当使⽤较大锚框来检测较大目标时，我们可以采样较少的区域。\n- 我们可以通过**控制特征图的大小来控制尺度（特征图每个单元在输入图像上对应的感受野可大可小）**。本质上，我们⽤输⼊图像在某个感 受野区域内的信息来预测输⼊图像上与该区域位置相近的锚框的类别和偏移量\n\n\n\n### 3.6 单发多框检测（SSD）\n\n- 它主要由**⼀个基础⽹络块和若干个多尺度特征块串联而成**。其中**基础⽹络块⽤来从原始图像中抽取特征**，因此⼀般会选择常⽤的深度卷积神经⽹络。单 发多框检测论⽂中选⽤了在分类层之前截断的VGG，现在也常⽤ResNet替代\n- 我们可以设计 基础⽹络，使它输出的高和宽较大。这样⼀来，基于该特征图⽣成的锚框数量较多，可以⽤来检 测尺⼨较小的目标\n- 接下来的每个多尺度特征块将上⼀层提供的特征图的**高和宽缩小（如减半）**， 并使**特征图中每个单元在输⼊图像上的感受野变得更⼴阔**。如此⼀来，下图中**越靠近顶部的多 尺度特征块输出的特征图越小，故而基于特征图⽣成的锚框也越少，加之特征图中每个单元感受 野越大，因此更适合检测尺⼨较大的目标**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123182624112.png\" alt=\"image-20220123182624112\" style=\"zoom:80%;\" />\n\n##### 3.6.1 类别预测层\n\n- 设目标的类别个数为q。每个锚框的类别个数将是q + 1，其中类别0表⽰锚框只包含背景\n- 设特征图的高和宽分别为h和w，如果以其中每个单元为中⼼⽣成a个锚框，那么我们需 要对hwa个锚框进⾏分类。如果使⽤全连接层作为输出，**很容易导致模型参数过多。所以我们可以通过通道输出类别**\n- 类别预测层使⽤⼀个**保持输⼊高和宽的卷积层**。这样⼀来，输出和输⼊在特征图宽和高上的空间坐标⼀⼀对应。考虑输出和输⼊同⼀空间坐标(x, y)：**输出特征图上(x, y)坐标的通道里包含了以输⼊特征图(x, y)坐标为中心生成的所有锚框的类别预测**。因此**输出通道数为a(q + 1)， 其中索引为i(q + 1) + j（0 ≤ j ≤ q）的通道代表了索引为i的锚框有关类别索引为j的预测**\n\n\n\n##### 3.6.2 边界框预测层\n\n- 边界框预测层的设计与类别预测层的设计类似。唯⼀不同的是，这⾥需要为每个锚框预测4个偏移量，而不是q + 1个类别\n\n\n\n##### 3.6.3 连结多尺度的预测\n\n- 每个尺度的输出，除了批量大小一样，其他维度的大小均不一样。我们需要将它们变形成统⼀的格式并将多尺度的预测连结，从而让后续计算更简单\n- 所以我们需要将为(批量大小, 通道数, 高, 宽)格式转换成⼆维的(批量大小, 高×宽×通道数)的格式，以方便之后在维度1上的连结。\n\n\n\n##### 3.6.4 损失函数和评价函数\n\n- 目标检测有两个损失：**⼀是有关锚框类别的损失**，我们可以重⽤之前图像分类问题⾥⼀直使⽤ 的交叉熵损失函数；**⼆是有关正类锚框偏移量的损失**\n- 。预测偏移量是⼀个回归问题，但这⾥不 使⽤前⾯介绍过的平方损失，而使⽤L1范数损失，即预测值与真实值之间差的绝对值（其中 使用掩码变量令负类锚框和填充锚框不参与损失的计算）\n- 最后，我们将有关锚框类别和偏移量的损失相加得到模型的最终损失函数。\n- 可以将$$L_1$$损失换成**平滑的$$L_1$$范数损失**，它在零点附近使⽤平方函数从而更加平滑，这是通过⼀个超参数$$\\sigma$$来控制平滑区域的：\n\n$$\nf(x)=\\left\\{\\begin{array}{ll}\n(\\sigma x)^{2} / 2, & \\text { if }|x|<1 / \\sigma^{2} \\\\\n|x|-0.5 / \\sigma^{2}, & \\text { otherwise }\n\\end{array}\\right.\n$$\n\n当$$\\sigma$$很⼤时该损失类似于$$L_1$$范数损失。当它较小时，损失函数较平滑。\n\n- 还可以将交叉熵损失换成**焦点损失（focal loss）**：\n\n$$\n-\\alpha\\left(1-p_{j}\\right)^{\\gamma} \\log p_{j}\n$$\n\n焦点损失适用于比较密集的目标检测，即要判定的类别比较多的情况。我们将一个锚框标签的类别作为正类，其余都作为负类（包括背景），那么这就转换成了一个二分类问题，回顾二分类的交叉熵损失函数：\n$$\n\\mathrm{L}=-\\mathrm{ylog} y^{\\prime}-(1-y) \\log \\left(1-y^{\\prime}\\right)=\\left\\{\\begin{array}{ll}\n-\\log y^{\\prime} & , \\quad y=1 \\\\\n-\\log \\left(1-y^{\\prime}\\right), & y=0\n\\end{array}\\right.\n$$\n可以看到当标签为负类（y=0），且正类预测概率$$y'$$较大时，会产生较大的损失。而由于正负样本的极其不均衡（比如有1000个类别，正类只有1种，负类则有999种），所以负样本会主导梯度的更新方向，使得整体学习方向跑偏\n\n上述的负类因为正负样本的不均衡，所以负类是是易分类的样本（$$p_j > 0.5$$），而焦点损失中的$$(1-p_j)^{\\gamma}$$就是为了减轻易分类样本的权重，让对象检测器更关注难分类的样本（即正样本）\n\n\n\n\n\n# 4 区域卷积神经网络（R-CNN）\n\n### 4.1 R-CNN\n\n- R-CNN⾸先对图像**选取若干提议区域**（如锚框也是⼀种选取方法）并标注它们的类别和边界框 （如偏移量）。然后，**⽤卷积神经⽹络对每个提议区域做前向计算抽取特征。之后，我们⽤每个提议区域的特征预测类别和边界框**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123184832784.png\" alt=\"image-20220123184832784\" style=\"zoom:80%;\" />\n\n- 具体来说，R-CNN主要由以下4步构成：\n\n  > 1. 对输⼊图像使⽤选择性搜索（selective search）来**选取多个高质量的提议区域**。这些提议区域通常是在**多个尺度下**选取的，并具有不同的形状和大小。**每个提议区域将被标注类 别和真实边界框。**\n  > 2. 选取⼀个预训练的卷积神经⽹络，并将其在输出层之前截断\n  > 3. 将每个提议区域的特征连同其标注的类别作为⼀个样本，训练多个⽀持向量机对目标分类。 其中每个⽀持向量机⽤来判断样本是否属于某⼀个类别\n  > 4. 将每个提议区域的特征连同其标注的边界框作为⼀个样本，训练线性回归模型来预测真实边界框\n\n- 一张图片中可能会有很多个提议区域，每个区域都要进行卷积运算。这个**巨大的计算量令R-CNN难以在实际应⽤中被⼴泛采⽤**\n\n\n\n### 4.2 Fast R-CNN\n\n- R-CNN提议区域通常**有大量重叠， 独⽴的特征抽取会导致大量的重复计算**。Fast R-CNN对R-CNN的⼀个主要改进在于**只对整个图像 做卷积神经⽹络的前向计算。**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123185433036.png\" alt=\"image-20220123185433036\" style=\"zoom:80%;\" />\n\n- 主要步骤：\n\n> 1. 与R-CNN相比，Fast R-CNN⽤来提取特征的卷积神经⽹络的输⼊是整个图像，而不是各个提议区域。而且，**这个⽹络通常会参与训练**，即更新模型参数。设输⼊为⼀张图像，将卷积神经⽹络的输出的形状记为$$1 × c × h_1 × w_1$$\n> 2. 假设选择性搜索⽣成n个提议区域。**这些形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域**。这些兴趣区域需要抽取出**形状相同**的特征（假设高和宽均分别指定为$$h_2$$和$$w_2$$）以便于连结后输出（使用**兴趣区域池化（region of interest pooling，RoI池化）层，将卷积神经⽹络的输出和提议区域作为输⼊，输出连结后的各个提议区域抽取的特征**）\n> 3. 通过全连接层将输出形状变换为n × d，其中超参数d取决于模型设计\n> 4. 预测类别时，将全连接层的输出的形状再变换为n × q并使⽤softmax回归（q为类别个数）。 预测边界框时，将全连接层的输出的形状变换为n × 4。也就是说，我们为每个提议区域预 测类别和边界框。\n\n- 兴趣区域池化层：兴趣区域池化层对每个区域的输 出形状是可以直接指定的如下图：\n\n![image-20220123191333584](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191333584.png)\n\n第一张图的蓝色区域是一个提议区域，将其经过2x2兴趣区域池化层后，划分成了4个区域，分别含有元素0、1、4、5（5最 大），2、6（6最大），8、9（9最大），10。输出每个区域的最大元素\n\n\n\n### 4.3 Faster R-CNN\n\n- Fast R-CNN通常需要在选择性搜索中⽣成较多的提议区域，以获得较精确的目标检测结果。Faster R-CNN提出**将选择性搜索替换成区域提议⽹络（region proposal network）**，从而**减少提议区域 的⽣成数量**，并保证目标检测的精度\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191545146.png\" alt=\"image-20220123191545146\" style=\"zoom:67%;\" />\n\n\n\n- 与Fast R-CNN相比，只有⽣成提议区域的方法从选择性搜索变成了区域提议⽹络，而其他部分均保持不变。具体来说，区域提议⽹络的计算步骤如下：\n\n> 1. 使⽤填充为1的3 × 3卷积层变换卷积神经⽹络的输出，并将输出通道数记为c\n> 2. . 以特征图每个单元为中⼼，⽣成多个不同大小和宽高比的锚框并标注它们\n> 3. . ⽤锚框中⼼单元⻓度为c的特征分别预测该锚框的**二元类别（含目标还是背景，需要reshape）**和边界框\n> 4.  使⽤非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测 边界框即兴趣区域池化层所需要的提议区域。\n\n- 区域提议⽹络作为Faster R-CNN的⼀部分，是和整个模型⼀起训练得到的。也就 是说，**Faster R-CNN的目标函数既包括目标检测中的类别和边界框预测，⼜包括区域提议⽹络中 锚框的⼆元类别和边界框预测**\n\n\n\n### 4.4 Mask R-CNN\n\n- 如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利⽤这些详尽 的标注信息进⼀步提升目标检测的精度。\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123193610616.png\" alt=\"image-20220123193610616\" style=\"zoom: 80%;\" />\n\n- Mask R-CNN将兴趣区域池化层 替换成了兴趣区域对⻬层，即通过**双线性插值（bilinear interpolation）（一种常用的上采样方法，目的是将下一层的特征图的单元于上一层特征图单元对齐）**来保留特征图上的空间信息，从而更适于像素级预测\n\n\n\n\n\n# 5 语义分割\n\n- 语义分割（semantic segmentation）关注如何将图像分割成属于不同语义类别的区域。值得⼀提的是，这些语义区域的标注和预测都是像素级的\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123194304346.png\" alt=\"image-20220123194304346\" style=\"zoom: 67%;\" />\n\n- 计算机视觉领域还有2个与语义分割相似的重要问题，即**图像分割（image segmentation）**和**实例分割（instance segmentation）**\n- **图像分割**将图像分割成若干组成区域。**这类问题的方法通常利⽤图像中像素之间的相关性**。 它在训练时不需要有关图像像素的标签信息，在预测时也⽆法保证分割出的区域具有我们希望得到的语义。如上图图像分割可能将狗分割成两个区域：⼀个覆盖以⿊⾊为主的嘴巴和眼睛，而另⼀个覆盖以⻩⾊为主的其余部分⾝体\n- **实例分割**研究如何 识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割**不仅需要区分语 义，还要区分不同的目标实例**。如果图像中有两只狗，实例分割需要区分像素属于这两只 狗中的哪⼀只。\n\n\n\n- 如果我们通过缩放图像使其符合模型的输⼊形状。然而在语义分割⾥，**需要将预测的像素类别重新映射回原始尺⼨的输⼊图像**。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，**我们将图像裁剪成固定尺⼨而不是缩放**（对于实际尺寸小于规定尺寸的图像，需要移除）\n\n\n\n\n\n# 6 全卷积网络（FCN）\n\n- 全卷积⽹络（fully convolutional network，FCN）采⽤卷积神经⽹络实现了从图像像素到像素类别的变换。\n- 全卷积⽹络通过**转置卷积（transposed convolution）层将中间层特征图的高和宽变换回输⼊图像的尺⼨**\n\n\n\n- 因为卷积运算可以用矩阵乘法来实现，假设input进行卷积运算相当于input矩阵乘一个$$W$$矩阵得到特征图featrue。那么可以通过featrue乘一个$$W^T$$来变回input的形状。**所以可以通过转置卷积层来交换卷积层输入和输出的形状**\n\n\n\n### 6.1 模型构造\n\n- 全卷积⽹络先使⽤卷积神经⽹络抽取图像特征，然后通过1 × 1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的⾼和宽变换为输⼊图像的尺⼨\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124154145643.png\" alt=\"image-20220124154145643\" style=\"zoom: 67%;\" />\n\n\n\n### 6.2 初始化转置卷积层\n\n- **双线性插值：**首先介绍一下单线性插值（一维）：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190819135319360.jpg\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\" />\n\n我们知道$$x_0,x_1,y_0,y_1,x$$的值，要求$$y$$的值：\n$$\ny = y_0 + \\frac{y_1 - y_0}{x_1 - x_0}(x - x_0)\n$$\n而**双线性插值其实就是在不同的维度上单线性插值两次**，已知$$Q_{11}(x_1,y_1),Q_{12}(x_1,y_2),Q_{21}(x_2,y_1),Q_{22}(x_2,y_2)$$，求其中点P(x,y)的函数值：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-ad3d95548a97aa47ca85867cd0f2e161_720w.jpg\" alt=\"img\" style=\"zoom:67%;\" />\n\n首先在x方向单线性插值两次：\n$$\n\\begin{array}{l}\nf\\left(R_{1}\\right)=\\frac{x_{2}-x}{x_{2}-x_{1}} f\\left(Q_{11}\\right)+\\frac{x-x_{1}}{x_{2}-x_{1}} f\\left(Q_{21}\\right) \\\\\nf\\left(R_{2}\\right)=\\frac{x_{2}-x}{x_{2}-x_{1}} f\\left(Q_{12}\\right)+\\frac{x-x_{1}}{x_{2}-x_{1}} f\\left(Q_{22}\\right)\n\\end{array}\n$$\n然后再在y方向单线性插值一次：\n$$\nf(P)=\\frac{y_{2}-y}{y_{2}-y_{1}} f\\left(R_{1}\\right)+\\frac{y-y_{1}}{y_{2}-y_{1}} f\\left(R_{2}\\right)\n$$\n即可得到结果。\n\n\n\n- 在全卷积⽹络中，我们一般**将转置卷积层初始化为双线性插值的上采样**，具体方法是：\n\n> 1. 为了得到输出图像在坐 标(x, y)上的像素，先将该坐标映射到输⼊图像的坐标(x ′ , y′ )，例如，根据输⼊与输出的尺⼨之⽐来映射。\n> 2. 映射后的x ′和y ′通常是实数。然后，在输⼊图像上找到与坐标(x ′ , y′ )最近的4像素。最后， 输出图像在坐标(x, y)上的像素依据输⼊图像上这4像素及其与(x ′ , y′ )的相对距离来计算（用双线性插值）\n\n\n\n- 如果步幅为s、填充为s/2（假设s/2为整数）、卷积核的⾼和宽为2s，转置卷积核将输⼊的⾼和宽分别放⼤s倍\n\n- 转置卷积层的输出形状可能会不一样，这是因为当**输入图像的高宽无法整除步幅时，输出的高宽会有所偏差**。为了解决这个问题，我们可以在图像中**截取多块⾼和宽为步幅的整数倍的矩形区域**，并分别对这些区域中的像素做前向计算。**这些区域的并集需要完整覆盖输⼊图像**。当⼀个像素被多个区域所覆盖时，它在不同区域 前向计算中转置卷积层输出的**平均值可以作为softmax运算的输⼊**，从而预测类别\n\n\n\n\n\n# 7 样式迁移\n\n- 使⽤卷积神经⽹络⾃动将某图像中的样式应⽤在另⼀图像之上，即样式迁移（style transfer）。需要两张输⼊图像，⼀张是**内容图像**，另⼀张是**样式图像**， 我们将使⽤神经⽹络**修改内容图像使其在样式上接近样式图像**\n\n- 主要步骤：\n\n> 1. ⾸先，我们初始化合成图像，例如 将其**初始化成内容图像**。该合成图像是样式迁移过程中**唯⼀需要更新的变量**，\n> 2. 我们选择⼀个预训练的卷积神经⽹络来**抽取图像的特征**，其中的**模型参数在训练中⽆须更新**。深度卷积神经⽹络**凭借多个层逐级抽取图像的特征。我们可以选择其中某些 层的输出作为内容特征或样式特征**，例如下图，这⾥选取的预训练的神经⽹络含有3个卷积 层，其中第⼆层输出图像的内容特征，而第⼀层和第三层的输出被作为图像的样式特征\n> 3. 样式迁移常⽤的损失函数由3部分组成：**内容损失（content loss）**使合成图像与内容图像在内容特征上接近，**样式损失（style loss）**令合成图像与样式图像 在样式特征上接近，而**总变差损失（total variation loss）**则有助于减少合成图像中的噪点\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124175546385.png\" alt=\"image-20220124175546385\" style=\"zoom:50%;\" />\n\n\n\n### 7.1 内容层和样式层的选择\n\n- **⼀般来说，越靠近输⼊层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成 图像过多保留内容图像的细节，我们选择较靠近输出的层来输出图像的内容特征**\n- **我们还可以选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层**\n- 例如VGG-19,使⽤了5个卷积块。我们可以选择第四卷积块的最后⼀个卷积层作为内容层，以及每个卷积块的第⼀个卷积层作为样式层\n\n\n\n### 7.2 损失函数\n\n- **内容损失：**与线性回归中的损失函数类似，内容损失通过平⽅误差函数衡量合成图像与内容图像在内容特征上的差异。平⽅误差函数的两个输⼊均为内容层的输出。\n\n- **样式损失：**样式损失也⼀样通过平⽅误差函数衡量合成图像与样式图像在样式上的差异。我们将样式层的输出（长为h，宽为w，通道数为c），转化成c行hw列的矩阵X，矩阵X可以看作由c个长度为hw的向量$$x_1, . . . , x_c$$组成的。其中向量$$x_i$$代表了通道$$i$$上的样式特征。\n\n  这些向量的**格拉姆矩阵 （Gram matrix）$$XX^T\\in R^{c\\times c}$$中$$i$$⾏$$j$$列的元素$$x_{ij}$$即向量$$x_i$$与$$x_j$$的内积**，它表达了通道$$i$$和通道$$j$$上样式特征的相关性。我们⽤这样的格拉姆矩阵表达样式层输出的样式。\n\n  需要注意的是，当hw的值较⼤时，格拉姆矩阵中的元素容易出现较⼤的值。此外，格拉姆矩阵的⾼和宽皆为通道数c。为了让样式损失不受这些值的⼤小影响，**需要除以矩阵中元素的个数，即chw**\n\n- **总变量损失：**有时候，我们学到的合成图像⾥⾯有⼤量⾼频噪点，即有特别亮或者特别暗的颗粒像素。⼀种常⽤的降噪⽅法是总变差降噪（total variation denoising）。\n\n  假设$$x_{i,j}$$表⽰坐标为$$(i, j)$$的像素值，降低总变差损失：\n  $$\n  \\sum_{i, j}\\left|x_{i, j}-x_{i+1, j}\\right|+\\left|x_{i, j}-x_{i, j+1}\\right|\n  $$\n  能够尽可能使邻近的像素值相似。\n\n\n\n- 样式迁移的损失函数即内容损失、样式损失和总变差损失的**加权和**。通过调节这些**权值超参数**， 我们可以权衡合成图像在保留内容、迁移样式以及降噪三⽅⾯的相对重要性。","slug":"CV基础","published":1,"updated":"2022-12-20T06:16:51.882Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1a00037csz1viq55gs","content":"<h1 id=\"1-图像增广\"><a href=\"#1-图像增广\" class=\"headerlink\" title=\"1 图像增广\"></a>1 图像增广</h1><ul>\n<li>图像增⼴（image augmentation）技术通过对训练图像做⼀系列随机改变，来产⽣相似但⼜不同的训练样本，从而<strong>扩大训练数据集的规模</strong></li>\n<li><p>随机改变训练样本可以<strong>降低模型对某些属性的依赖，从而提高模型的泛化能力</strong>。例如，我们可以对图像进⾏不同 方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性</p>\n</li>\n<li><p>常用的方法有：<strong>翻转</strong>、<strong>裁剪</strong>、<strong>变换颜色</strong>。对于翻转，大部分情况左右翻转比上下翻转更通用一些。对于颜色，我们可以从亮度、对比度、饱和度和⾊调四方面进行改变</p>\n</li>\n<li>实际应⽤中我们会将多个图像增⼴方法叠加使⽤</li>\n</ul>\n<h1 id=\"2-微调\"><a href=\"#2-微调\" class=\"headerlink\" title=\"2 微调\"></a>2 微调</h1><ul>\n<li><strong>迁移学习（transfer learning）</strong>：<strong>将从源数据集学到的知识迁移到目标数据集上</strong>。例如，虽然ImageNet数据集的图像大多跟椅⼦⽆关，但在该数据集上训练的模型可以抽 取较通⽤的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于 识别椅⼦也可能同样有效。</li>\n<li>迁移学习中的一种常用技术“微调（<strong>fine tuning）</strong>”，由一下四步构成：</li>\n</ul>\n<blockquote>\n<ol>\n<li>在源数据集（如ImageNet数据集）上预训练⼀个神经⽹络模型，即源模型</li>\n<li>创建⼀个新的神经⽹络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设 计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适⽤于目标数据集。我们还假设源模型的输出层与源数据集的标签紧密相关，因此在目标模 型中不予采⽤。</li>\n<li>为目标模型添加⼀个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</li>\n<li>在目标数据集（如椅⼦数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123170934001.png\" alt=\"image-20220123170934001\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>可以选择保留除输出层以外的所有层，也可以保留除临近输出层的几层以外的所有层</li>\n<li>由于预训练模型是比较接近正确结果的，而新添加的层是随机初始化。<strong>所以两者的学习率是不同的，预训练的学习率更小</strong></li>\n</ul>\n</blockquote>\n<h1 id=\"3-目标检测和边界框\"><a href=\"#3-目标检测和边界框\" class=\"headerlink\" title=\"3 目标检测和边界框\"></a>3 目标检测和边界框</h1><ul>\n<li>很多时候图像⾥有多个我们感兴趣的目标，我们 不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉⾥，我们将这类任务 称为目标检测（object detection）或物体检测</li>\n<li>在目标检测⾥，我们通常使⽤<strong>边界框（bounding box）</strong>来描述目标位置。边界框是⼀个矩形框， 可以由矩形左上⻆的x和y轴坐标与右下⻆的x和y轴坐标确定</li>\n<li>它以每个像素为中⼼⽣ 成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为<strong>锚框（anchor box）</strong></li>\n</ul>\n<h3 id=\"3-1-锚框的生成\"><a href=\"#3-1-锚框的生成\" class=\"headerlink\" title=\"3.1 锚框的生成\"></a>3.1 锚框的生成</h3><ul>\n<li>假设输⼊图像高为h，宽为w。我们分别以图像的每个像素为中⼼⽣成不同形状的锚框。设大小为s <script type=\"math/tex\">\\in</script> (0, 1]且宽高比为r &gt; 0，那么锚框的宽和高将分别为<script type=\"math/tex\">ws\\sqrt{r}</script>和<script type=\"math/tex\">hs/\\sqrt{r}</script>。当中⼼位置给定时，已 知宽和高的锚框是确定的。⾯我们分别设定好⼀组大小<script type=\"math/tex\">s_1, . . . , s_n</script>和⼀组宽高比<script type=\"math/tex\">r_1, . . . , r_m</script>，s和r两两配对能覆盖所有的真实边界框，但是计算复杂度容易更高，所以我们通常只对包含<script type=\"math/tex\">s_1</script>或<script type=\"math/tex\">r_1</script>的大小与宽高比的组合感兴趣，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\left(s_{1}, r_{1}\\right),\\left(s_{1}, r_{2}\\right), \\ldots,\\left(s_{1}, r_{m}\\right),\\left(s_{2}, r_{1}\\right),\\left(s_{3}, r_{1}\\right), \\ldots,\\left(s_{n}, r_{1}\\right)</script><h3 id=\"3-2-交并比\"><a href=\"#3-2-交并比\" class=\"headerlink\" title=\"3.2 交并比\"></a>3.2 交并比</h3><ul>\n<li>Jaccard系数 （Jaccard index）可以衡量两个集合的相似度。给定集合A和B，它们的Jaccard系数即⼆者交集大小除以⼆者并集大小：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ(\\mathcal{A}, \\mathcal{B})=\\frac{|\\mathcal{A} \\cap \\mathcal{B}|}{|\\mathcal{A} \\cup \\mathcal{B}|}</script><ul>\n<li>我们通 常将Jaccard系数称为交并比（intersection over union，IoU），即两个边界框相交⾯积与相并⾯积之比</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123173756721.png\" alt=\"image-20220123173756721\" style=\"zoom: 80%;\" /></p>\n<h3 id=\"3-3-标注训练集的锚框\"><a href=\"#3-3-标注训练集的锚框\" class=\"headerlink\" title=\"3.3 标注训练集的锚框\"></a>3.3 标注训练集的锚框</h3><ul>\n<li>在训练集中，我们<strong>将每个锚框视为⼀个训练样本</strong>。为了训练目标检测模型，我们需要为每个锚框标注两类标签：<strong>⼀是锚框所含目标的类别，简称类别；⼆是真实边界框相对锚框的偏移量，简称偏移量（offset)</strong></li>\n<li>在目标检测时，我们⾸先⽣成多个锚框，然后为每个锚框预测类别以及偏移量， 接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框</li>\n</ul>\n<ul>\n<li><p>假设图像中锚框分别为<script type=\"math/tex\">A_1, A_2, . . . , A_{n_a}</script>，真实边界框分别为<script type=\"math/tex\">B_1, B_2, . . . , B_{n_b}</script>，且<script type=\"math/tex\">n_a</script> ≥ <script type=\"math/tex\">n_b</script>。定义矩 阵<script type=\"math/tex\">X \\in R^{n_a\\times n_b}</script>，其中第i⾏第j列的元素<script type=\"math/tex\">x_{ij}</script>为锚框<script type=\"math/tex\">A_i</script>与真实边界框<script type=\"math/tex\">B_j</script>的交并比</p>\n<blockquote>\n<ol>\n<li>找到矩阵中最大元素，并将该值对应的真实边界框赋值给锚框，然后从矩阵中去除该行和该列所有元素。然后继续找最大元素，重复上述操作，直到所有真实边界框都被分配完</li>\n<li>这个时候，我们已为<script type=\"math/tex\">n_b</script>个锚框各分配了⼀个真实边界框。接下来，我们 只遍历剩余的<script type=\"math/tex\">n_a − n_b</script>个锚框：给定其中的锚框<script type=\"math/tex\">A_i</script>，根据矩阵<script type=\"math/tex\">X</script>的第i⾏找到与<script type=\"math/tex\">A_i</script>交并比最大的真实边界框<script type=\"math/tex\">B_j</script>，<strong>且只有当该交并比大于预先设定的阈值时，才为锚框<script type=\"math/tex\">A_i</script>分配真实边界框<script type=\"math/tex\">B_j</script></strong></li>\n</ol>\n</blockquote>\n</li>\n<li><p><strong>如果⼀个锚框A被分配了真实边界框B，将锚框A的类别设为B的类别，并根据B和A的中心坐标的相对位置以及两个框的相对大小为锚框A标注偏 移量。</strong></p>\n</li>\n<li><strong>由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要⼀些特殊变换，才能使偏移量的分布更均匀从而更容易拟合</strong>。设锚框A及其被分配的真实边界框B的中 ⼼坐标分别为<script type=\"math/tex\">(x_a, y_a)</script>和<script type=\"math/tex\">(x_b, y_b)</script>，A和B的宽分别为<script type=\"math/tex\">w_a</script>和<script type=\"math/tex\">w_b</script>，高分别为<script type=\"math/tex\">h_a</script>和<script type=\"math/tex\">h_b</script>，⼀个常⽤的技巧是将A的偏移量标注为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\left(\\frac{\\frac{x_{b}-x_{a}}{w_{a}}-\\mu_{x}}{\\sigma_{x}}, \\frac{\\frac{y_{b}-y_{a}}{h_{a}}-\\mu_{y}}{\\sigma_{y}}, \\frac{\\log \\frac{w_{b}}{w_{a}}-\\mu_{w}}{\\sigma_{w}}, \\frac{\\log \\frac{h_{b}}{h_{a}}-\\mu_{h}}{\\sigma_{h}}\\right)</script><p>其中常数的默认值为<script type=\"math/tex\">µ_x = µ_y = µ_w = µ_h = 0, σ_x = σ_y = 0.1, σ_w = σ_h = 0.2</script></p>\n<ul>\n<li><p>如果一个锚框没有被分配真实边界框，我们只需将该锚框的类别设为<strong>背景</strong>。类别为背景的锚框通常被称为<strong>负类锚框</strong>，其余则被称为<strong>正类锚框</strong></p>\n</li>\n<li><p><strong>掩码（mask）：</strong> <strong>形状为(批量大小, 锚框个数的四倍)</strong>。掩码变量中的元素 与每个锚框的4个偏移量⼀⼀对应。由于我们不关⼼对背景的检测，有关负类的偏移量不应影响 目标函数。<strong>通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量</strong></p>\n</li>\n</ul>\n<h3 id=\"3-4-非极大值抑制\"><a href=\"#3-4-非极大值抑制\" class=\"headerlink\" title=\"3.4 非极大值抑制\"></a>3.4 非极大值抑制</h3><ul>\n<li>为了使结果更加简洁，我们可以移除相似的预测边界框。常⽤的方法叫作非极大值抑制（non-maximum suppression，NMS)：</li>\n</ul>\n<blockquote>\n<ol>\n<li>在同⼀图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序， 得到列表L</li>\n<li>从L中选取置信度最高的预测边界框<script type=\"math/tex\">B_1</script>作为基准，将所有与<script type=\"math/tex\">B_1</script>的交并比大于某阈值 的非基准预测边界框从L中移除。这⾥的阈值是预先设定的超参数</li>\n<li>继续重复上述操作，直到L中所有的预测边界框都曾作为基准</li>\n</ol>\n</blockquote>\n<ul>\n<li>实践中，我们可以在执⾏非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。</li>\n</ul>\n<h3 id=\"3-5-多尺度目标检测\"><a href=\"#3-5-多尺度目标检测\" class=\"headerlink\" title=\"3.5 多尺度目标检测\"></a>3.5 多尺度目标检测</h3><ul>\n<li>在不同尺度下，我们可以⽣成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多</li>\n<li>因此，当使⽤较小 锚框来检测较小目标时，我们可以采样较多的区域；而当使⽤较大锚框来检测较大目标时，我们可以采样较少的区域。</li>\n<li>我们可以通过<strong>控制特征图的大小来控制尺度（特征图每个单元在输入图像上对应的感受野可大可小）</strong>。本质上，我们⽤输⼊图像在某个感 受野区域内的信息来预测输⼊图像上与该区域位置相近的锚框的类别和偏移量</li>\n</ul>\n<h3 id=\"3-6-单发多框检测（SSD）\"><a href=\"#3-6-单发多框检测（SSD）\" class=\"headerlink\" title=\"3.6 单发多框检测（SSD）\"></a>3.6 单发多框检测（SSD）</h3><ul>\n<li>它主要由<strong>⼀个基础⽹络块和若干个多尺度特征块串联而成</strong>。其中<strong>基础⽹络块⽤来从原始图像中抽取特征</strong>，因此⼀般会选择常⽤的深度卷积神经⽹络。单 发多框检测论⽂中选⽤了在分类层之前截断的VGG，现在也常⽤ResNet替代</li>\n<li>我们可以设计 基础⽹络，使它输出的高和宽较大。这样⼀来，基于该特征图⽣成的锚框数量较多，可以⽤来检 测尺⼨较小的目标</li>\n<li>接下来的每个多尺度特征块将上⼀层提供的特征图的<strong>高和宽缩小（如减半）</strong>， 并使<strong>特征图中每个单元在输⼊图像上的感受野变得更⼴阔</strong>。如此⼀来，下图中<strong>越靠近顶部的多 尺度特征块输出的特征图越小，故而基于特征图⽣成的锚框也越少，加之特征图中每个单元感受 野越大，因此更适合检测尺⼨较大的目标</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123182624112.png\" alt=\"image-20220123182624112\" style=\"zoom:80%;\" /></p>\n<h5 id=\"3-6-1-类别预测层\"><a href=\"#3-6-1-类别预测层\" class=\"headerlink\" title=\"3.6.1 类别预测层\"></a>3.6.1 类别预测层</h5><ul>\n<li>设目标的类别个数为q。每个锚框的类别个数将是q + 1，其中类别0表⽰锚框只包含背景</li>\n<li>设特征图的高和宽分别为h和w，如果以其中每个单元为中⼼⽣成a个锚框，那么我们需 要对hwa个锚框进⾏分类。如果使⽤全连接层作为输出，<strong>很容易导致模型参数过多。所以我们可以通过通道输出类别</strong></li>\n<li>类别预测层使⽤⼀个<strong>保持输⼊高和宽的卷积层</strong>。这样⼀来，输出和输⼊在特征图宽和高上的空间坐标⼀⼀对应。考虑输出和输⼊同⼀空间坐标(x, y)：<strong>输出特征图上(x, y)坐标的通道里包含了以输⼊特征图(x, y)坐标为中心生成的所有锚框的类别预测</strong>。因此<strong>输出通道数为a(q + 1)， 其中索引为i(q + 1) + j（0 ≤ j ≤ q）的通道代表了索引为i的锚框有关类别索引为j的预测</strong></li>\n</ul>\n<h5 id=\"3-6-2-边界框预测层\"><a href=\"#3-6-2-边界框预测层\" class=\"headerlink\" title=\"3.6.2 边界框预测层\"></a>3.6.2 边界框预测层</h5><ul>\n<li>边界框预测层的设计与类别预测层的设计类似。唯⼀不同的是，这⾥需要为每个锚框预测4个偏移量，而不是q + 1个类别</li>\n</ul>\n<h5 id=\"3-6-3-连结多尺度的预测\"><a href=\"#3-6-3-连结多尺度的预测\" class=\"headerlink\" title=\"3.6.3 连结多尺度的预测\"></a>3.6.3 连结多尺度的预测</h5><ul>\n<li>每个尺度的输出，除了批量大小一样，其他维度的大小均不一样。我们需要将它们变形成统⼀的格式并将多尺度的预测连结，从而让后续计算更简单</li>\n<li>所以我们需要将为(批量大小, 通道数, 高, 宽)格式转换成⼆维的(批量大小, 高×宽×通道数)的格式，以方便之后在维度1上的连结。</li>\n</ul>\n<h5 id=\"3-6-4-损失函数和评价函数\"><a href=\"#3-6-4-损失函数和评价函数\" class=\"headerlink\" title=\"3.6.4 损失函数和评价函数\"></a>3.6.4 损失函数和评价函数</h5><ul>\n<li>目标检测有两个损失：<strong>⼀是有关锚框类别的损失</strong>，我们可以重⽤之前图像分类问题⾥⼀直使⽤ 的交叉熵损失函数；<strong>⼆是有关正类锚框偏移量的损失</strong></li>\n<li>。预测偏移量是⼀个回归问题，但这⾥不 使⽤前⾯介绍过的平方损失，而使⽤L1范数损失，即预测值与真实值之间差的绝对值（其中 使用掩码变量令负类锚框和填充锚框不参与损失的计算）</li>\n<li>最后，我们将有关锚框类别和偏移量的损失相加得到模型的最终损失函数。</li>\n<li>可以将<script type=\"math/tex\">L_1</script>损失换成<strong>平滑的<script type=\"math/tex\">L_1</script>范数损失</strong>，它在零点附近使⽤平方函数从而更加平滑，这是通过⼀个超参数<script type=\"math/tex\">\\sigma</script>来控制平滑区域的：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x)=\\left\\{\\begin{array}{ll}\n(\\sigma x)^{2} / 2, & \\text { if }|x|<1 / \\sigma^{2} \\\\\n|x|-0.5 / \\sigma^{2}, & \\text { otherwise }\n\\end{array}\\right.</script><p>当<script type=\"math/tex\">\\sigma</script>很⼤时该损失类似于<script type=\"math/tex\">L_1</script>范数损失。当它较小时，损失函数较平滑。</p>\n<ul>\n<li>还可以将交叉熵损失换成<strong>焦点损失（focal loss）</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\alpha\\left(1-p_{j}\\right)^{\\gamma} \\log p_{j}</script><p>焦点损失适用于比较密集的目标检测，即要判定的类别比较多的情况。我们将一个锚框标签的类别作为正类，其余都作为负类（包括背景），那么这就转换成了一个二分类问题，回顾二分类的交叉熵损失函数：</p>\n<script type=\"math/tex; mode=display\">\n\\mathrm{L}=-\\mathrm{ylog} y^{\\prime}-(1-y) \\log \\left(1-y^{\\prime}\\right)=\\left\\{\\begin{array}{ll}\n-\\log y^{\\prime} & , \\quad y=1 \\\\\n-\\log \\left(1-y^{\\prime}\\right), & y=0\n\\end{array}\\right.</script><p>可以看到当标签为负类（y=0），且正类预测概率<script type=\"math/tex\">y'</script>较大时，会产生较大的损失。而由于正负样本的极其不均衡（比如有1000个类别，正类只有1种，负类则有999种），所以负样本会主导梯度的更新方向，使得整体学习方向跑偏</p>\n<p>上述的负类因为正负样本的不均衡，所以负类是是易分类的样本（<script type=\"math/tex\">p_j > 0.5</script>），而焦点损失中的<script type=\"math/tex\">(1-p_j)^{\\gamma}</script>就是为了减轻易分类样本的权重，让对象检测器更关注难分类的样本（即正样本）</p>\n<h1 id=\"4-区域卷积神经网络（R-CNN）\"><a href=\"#4-区域卷积神经网络（R-CNN）\" class=\"headerlink\" title=\"4 区域卷积神经网络（R-CNN）\"></a>4 区域卷积神经网络（R-CNN）</h1><h3 id=\"4-1-R-CNN\"><a href=\"#4-1-R-CNN\" class=\"headerlink\" title=\"4.1 R-CNN\"></a>4.1 R-CNN</h3><ul>\n<li>R-CNN⾸先对图像<strong>选取若干提议区域</strong>（如锚框也是⼀种选取方法）并标注它们的类别和边界框 （如偏移量）。然后，<strong>⽤卷积神经⽹络对每个提议区域做前向计算抽取特征。之后，我们⽤每个提议区域的特征预测类别和边界框</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123184832784.png\" alt=\"image-20220123184832784\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><p>具体来说，R-CNN主要由以下4步构成：</p>\n<blockquote>\n<ol>\n<li>对输⼊图像使⽤选择性搜索（selective search）来<strong>选取多个高质量的提议区域</strong>。这些提议区域通常是在<strong>多个尺度下</strong>选取的，并具有不同的形状和大小。<strong>每个提议区域将被标注类 别和真实边界框。</strong></li>\n<li>选取⼀个预训练的卷积神经⽹络，并将其在输出层之前截断</li>\n<li>将每个提议区域的特征连同其标注的类别作为⼀个样本，训练多个⽀持向量机对目标分类。 其中每个⽀持向量机⽤来判断样本是否属于某⼀个类别</li>\n<li>将每个提议区域的特征连同其标注的边界框作为⼀个样本，训练线性回归模型来预测真实边界框</li>\n</ol>\n</blockquote>\n</li>\n<li><p>一张图片中可能会有很多个提议区域，每个区域都要进行卷积运算。这个<strong>巨大的计算量令R-CNN难以在实际应⽤中被⼴泛采⽤</strong></p>\n</li>\n</ul>\n<h3 id=\"4-2-Fast-R-CNN\"><a href=\"#4-2-Fast-R-CNN\" class=\"headerlink\" title=\"4.2 Fast R-CNN\"></a>4.2 Fast R-CNN</h3><ul>\n<li>R-CNN提议区域通常<strong>有大量重叠， 独⽴的特征抽取会导致大量的重复计算</strong>。Fast R-CNN对R-CNN的⼀个主要改进在于<strong>只对整个图像 做卷积神经⽹络的前向计算。</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123185433036.png\" alt=\"image-20220123185433036\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>主要步骤：</li>\n</ul>\n<blockquote>\n<ol>\n<li>与R-CNN相比，Fast R-CNN⽤来提取特征的卷积神经⽹络的输⼊是整个图像，而不是各个提议区域。而且，<strong>这个⽹络通常会参与训练</strong>，即更新模型参数。设输⼊为⼀张图像，将卷积神经⽹络的输出的形状记为<script type=\"math/tex\">1 × c × h_1 × w_1</script></li>\n<li>假设选择性搜索⽣成n个提议区域。<strong>这些形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域</strong>。这些兴趣区域需要抽取出<strong>形状相同</strong>的特征（假设高和宽均分别指定为<script type=\"math/tex\">h_2</script>和<script type=\"math/tex\">w_2</script>）以便于连结后输出（使用<strong>兴趣区域池化（region of interest pooling，RoI池化）层，将卷积神经⽹络的输出和提议区域作为输⼊，输出连结后的各个提议区域抽取的特征</strong>）</li>\n<li>通过全连接层将输出形状变换为n × d，其中超参数d取决于模型设计</li>\n<li>预测类别时，将全连接层的输出的形状再变换为n × q并使⽤softmax回归（q为类别个数）。 预测边界框时，将全连接层的输出的形状变换为n × 4。也就是说，我们为每个提议区域预 测类别和边界框。</li>\n</ol>\n</blockquote>\n<ul>\n<li>兴趣区域池化层：兴趣区域池化层对每个区域的输 出形状是可以直接指定的如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191333584.png\" alt=\"image-20220123191333584\"></p>\n<p>第一张图的蓝色区域是一个提议区域，将其经过2x2兴趣区域池化层后，划分成了4个区域，分别含有元素0、1、4、5（5最 大），2、6（6最大），8、9（9最大），10。输出每个区域的最大元素</p>\n<h3 id=\"4-3-Faster-R-CNN\"><a href=\"#4-3-Faster-R-CNN\" class=\"headerlink\" title=\"4.3 Faster R-CNN\"></a>4.3 Faster R-CNN</h3><ul>\n<li>Fast R-CNN通常需要在选择性搜索中⽣成较多的提议区域，以获得较精确的目标检测结果。Faster R-CNN提出<strong>将选择性搜索替换成区域提议⽹络（region proposal network）</strong>，从而<strong>减少提议区域 的⽣成数量</strong>，并保证目标检测的精度</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191545146.png\" alt=\"image-20220123191545146\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>与Fast R-CNN相比，只有⽣成提议区域的方法从选择性搜索变成了区域提议⽹络，而其他部分均保持不变。具体来说，区域提议⽹络的计算步骤如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li>使⽤填充为1的3 × 3卷积层变换卷积神经⽹络的输出，并将输出通道数记为c</li>\n<li>. 以特征图每个单元为中⼼，⽣成多个不同大小和宽高比的锚框并标注它们</li>\n<li>. ⽤锚框中⼼单元⻓度为c的特征分别预测该锚框的<strong>二元类别（含目标还是背景，需要reshape）</strong>和边界框</li>\n<li>使⽤非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测 边界框即兴趣区域池化层所需要的提议区域。</li>\n</ol>\n</blockquote>\n<ul>\n<li>区域提议⽹络作为Faster R-CNN的⼀部分，是和整个模型⼀起训练得到的。也就 是说，<strong>Faster R-CNN的目标函数既包括目标检测中的类别和边界框预测，⼜包括区域提议⽹络中 锚框的⼆元类别和边界框预测</strong></li>\n</ul>\n<h3 id=\"4-4-Mask-R-CNN\"><a href=\"#4-4-Mask-R-CNN\" class=\"headerlink\" title=\"4.4 Mask R-CNN\"></a>4.4 Mask R-CNN</h3><ul>\n<li>如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利⽤这些详尽 的标注信息进⼀步提升目标检测的精度。</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123193610616.png\" alt=\"image-20220123193610616\" style=\"zoom: 80%;\" /></p>\n<ul>\n<li>Mask R-CNN将兴趣区域池化层 替换成了兴趣区域对⻬层，即通过<strong>双线性插值（bilinear interpolation）（一种常用的上采样方法，目的是将下一层的特征图的单元于上一层特征图单元对齐）</strong>来保留特征图上的空间信息，从而更适于像素级预测</li>\n</ul>\n<h1 id=\"5-语义分割\"><a href=\"#5-语义分割\" class=\"headerlink\" title=\"5 语义分割\"></a>5 语义分割</h1><ul>\n<li>语义分割（semantic segmentation）关注如何将图像分割成属于不同语义类别的区域。值得⼀提的是，这些语义区域的标注和预测都是像素级的</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123194304346.png\" alt=\"image-20220123194304346\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>计算机视觉领域还有2个与语义分割相似的重要问题，即<strong>图像分割（image segmentation）</strong>和<strong>实例分割（instance segmentation）</strong></li>\n<li><strong>图像分割</strong>将图像分割成若干组成区域。<strong>这类问题的方法通常利⽤图像中像素之间的相关性</strong>。 它在训练时不需要有关图像像素的标签信息，在预测时也⽆法保证分割出的区域具有我们希望得到的语义。如上图图像分割可能将狗分割成两个区域：⼀个覆盖以⿊⾊为主的嘴巴和眼睛，而另⼀个覆盖以⻩⾊为主的其余部分⾝体</li>\n<li><strong>实例分割</strong>研究如何 识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割<strong>不仅需要区分语 义，还要区分不同的目标实例</strong>。如果图像中有两只狗，实例分割需要区分像素属于这两只 狗中的哪⼀只。</li>\n</ul>\n<ul>\n<li>如果我们通过缩放图像使其符合模型的输⼊形状。然而在语义分割⾥，<strong>需要将预测的像素类别重新映射回原始尺⼨的输⼊图像</strong>。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，<strong>我们将图像裁剪成固定尺⼨而不是缩放</strong>（对于实际尺寸小于规定尺寸的图像，需要移除）</li>\n</ul>\n<h1 id=\"6-全卷积网络（FCN）\"><a href=\"#6-全卷积网络（FCN）\" class=\"headerlink\" title=\"6 全卷积网络（FCN）\"></a>6 全卷积网络（FCN）</h1><ul>\n<li>全卷积⽹络（fully convolutional network，FCN）采⽤卷积神经⽹络实现了从图像像素到像素类别的变换。</li>\n<li>全卷积⽹络通过<strong>转置卷积（transposed convolution）层将中间层特征图的高和宽变换回输⼊图像的尺⼨</strong></li>\n</ul>\n<ul>\n<li>因为卷积运算可以用矩阵乘法来实现，假设input进行卷积运算相当于input矩阵乘一个<script type=\"math/tex\">W</script>矩阵得到特征图featrue。那么可以通过featrue乘一个<script type=\"math/tex\">W^T</script>来变回input的形状。<strong>所以可以通过转置卷积层来交换卷积层输入和输出的形状</strong></li>\n</ul>\n<h3 id=\"6-1-模型构造\"><a href=\"#6-1-模型构造\" class=\"headerlink\" title=\"6.1 模型构造\"></a>6.1 模型构造</h3><ul>\n<li>全卷积⽹络先使⽤卷积神经⽹络抽取图像特征，然后通过1 × 1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的⾼和宽变换为输⼊图像的尺⼨</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124154145643.png\" alt=\"image-20220124154145643\" style=\"zoom: 67%;\" /></p>\n<h3 id=\"6-2-初始化转置卷积层\"><a href=\"#6-2-初始化转置卷积层\" class=\"headerlink\" title=\"6.2 初始化转置卷积层\"></a>6.2 初始化转置卷积层</h3><ul>\n<li><strong>双线性插值：</strong>首先介绍一下单线性插值（一维）：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190819135319360.jpg\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\" /></p>\n<p>我们知道<script type=\"math/tex\">x_0,x_1,y_0,y_1,x</script>的值，要求<script type=\"math/tex\">y</script>的值：</p>\n<script type=\"math/tex; mode=display\">\ny = y_0 + \\frac{y_1 - y_0}{x_1 - x_0}(x - x_0)</script><p>而<strong>双线性插值其实就是在不同的维度上单线性插值两次</strong>，已知<script type=\"math/tex\">Q_{11}(x_1,y_1),Q_{12}(x_1,y_2),Q_{21}(x_2,y_1),Q_{22}(x_2,y_2)</script>，求其中点P(x,y)的函数值：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-ad3d95548a97aa47ca85867cd0f2e161_720w.jpg\" alt=\"img\" style=\"zoom:67%;\" /></p>\n<p>首先在x方向单线性插值两次：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nf\\left(R_{1}\\right)=\\frac{x_{2}-x}{x_{2}-x_{1}} f\\left(Q_{11}\\right)+\\frac{x-x_{1}}{x_{2}-x_{1}} f\\left(Q_{21}\\right) \\\\\nf\\left(R_{2}\\right)=\\frac{x_{2}-x}{x_{2}-x_{1}} f\\left(Q_{12}\\right)+\\frac{x-x_{1}}{x_{2}-x_{1}} f\\left(Q_{22}\\right)\n\\end{array}</script><p>然后再在y方向单线性插值一次：</p>\n<script type=\"math/tex; mode=display\">\nf(P)=\\frac{y_{2}-y}{y_{2}-y_{1}} f\\left(R_{1}\\right)+\\frac{y-y_{1}}{y_{2}-y_{1}} f\\left(R_{2}\\right)</script><p>即可得到结果。</p>\n<ul>\n<li>在全卷积⽹络中，我们一般<strong>将转置卷积层初始化为双线性插值的上采样</strong>，具体方法是：</li>\n</ul>\n<blockquote>\n<ol>\n<li>为了得到输出图像在坐 标(x, y)上的像素，先将该坐标映射到输⼊图像的坐标(x ′ , y′ )，例如，根据输⼊与输出的尺⼨之⽐来映射。</li>\n<li>映射后的x ′和y ′通常是实数。然后，在输⼊图像上找到与坐标(x ′ , y′ )最近的4像素。最后， 输出图像在坐标(x, y)上的像素依据输⼊图像上这4像素及其与(x ′ , y′ )的相对距离来计算（用双线性插值）</li>\n</ol>\n</blockquote>\n<ul>\n<li><p>如果步幅为s、填充为s/2（假设s/2为整数）、卷积核的⾼和宽为2s，转置卷积核将输⼊的⾼和宽分别放⼤s倍</p>\n</li>\n<li><p>转置卷积层的输出形状可能会不一样，这是因为当<strong>输入图像的高宽无法整除步幅时，输出的高宽会有所偏差</strong>。为了解决这个问题，我们可以在图像中<strong>截取多块⾼和宽为步幅的整数倍的矩形区域</strong>，并分别对这些区域中的像素做前向计算。<strong>这些区域的并集需要完整覆盖输⼊图像</strong>。当⼀个像素被多个区域所覆盖时，它在不同区域 前向计算中转置卷积层输出的<strong>平均值可以作为softmax运算的输⼊</strong>，从而预测类别</p>\n</li>\n</ul>\n<h1 id=\"7-样式迁移\"><a href=\"#7-样式迁移\" class=\"headerlink\" title=\"7 样式迁移\"></a>7 样式迁移</h1><ul>\n<li><p>使⽤卷积神经⽹络⾃动将某图像中的样式应⽤在另⼀图像之上，即样式迁移（style transfer）。需要两张输⼊图像，⼀张是<strong>内容图像</strong>，另⼀张是<strong>样式图像</strong>， 我们将使⽤神经⽹络<strong>修改内容图像使其在样式上接近样式图像</strong></p>\n</li>\n<li><p>主要步骤：</p>\n</li>\n</ul>\n<blockquote>\n<ol>\n<li>⾸先，我们初始化合成图像，例如 将其<strong>初始化成内容图像</strong>。该合成图像是样式迁移过程中<strong>唯⼀需要更新的变量</strong>，</li>\n<li>我们选择⼀个预训练的卷积神经⽹络来<strong>抽取图像的特征</strong>，其中的<strong>模型参数在训练中⽆须更新</strong>。深度卷积神经⽹络<strong>凭借多个层逐级抽取图像的特征。我们可以选择其中某些 层的输出作为内容特征或样式特征</strong>，例如下图，这⾥选取的预训练的神经⽹络含有3个卷积 层，其中第⼆层输出图像的内容特征，而第⼀层和第三层的输出被作为图像的样式特征</li>\n<li>样式迁移常⽤的损失函数由3部分组成：<strong>内容损失（content loss）</strong>使合成图像与内容图像在内容特征上接近，<strong>样式损失（style loss）</strong>令合成图像与样式图像 在样式特征上接近，而<strong>总变差损失（total variation loss）</strong>则有助于减少合成图像中的噪点</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124175546385.png\" alt=\"image-20220124175546385\" style=\"zoom:50%;\" /></p>\n</blockquote>\n<h3 id=\"7-1-内容层和样式层的选择\"><a href=\"#7-1-内容层和样式层的选择\" class=\"headerlink\" title=\"7.1 内容层和样式层的选择\"></a>7.1 内容层和样式层的选择</h3><ul>\n<li><strong>⼀般来说，越靠近输⼊层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成 图像过多保留内容图像的细节，我们选择较靠近输出的层来输出图像的内容特征</strong></li>\n<li><strong>我们还可以选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层</strong></li>\n<li>例如VGG-19,使⽤了5个卷积块。我们可以选择第四卷积块的最后⼀个卷积层作为内容层，以及每个卷积块的第⼀个卷积层作为样式层</li>\n</ul>\n<h3 id=\"7-2-损失函数\"><a href=\"#7-2-损失函数\" class=\"headerlink\" title=\"7.2 损失函数\"></a>7.2 损失函数</h3><ul>\n<li><p><strong>内容损失：</strong>与线性回归中的损失函数类似，内容损失通过平⽅误差函数衡量合成图像与内容图像在内容特征上的差异。平⽅误差函数的两个输⼊均为内容层的输出。</p>\n</li>\n<li><p><strong>样式损失：</strong>样式损失也⼀样通过平⽅误差函数衡量合成图像与样式图像在样式上的差异。我们将样式层的输出（长为h，宽为w，通道数为c），转化成c行hw列的矩阵X，矩阵X可以看作由c个长度为hw的向量<script type=\"math/tex\">x_1, . . . , x_c</script>组成的。其中向量<script type=\"math/tex\">x_i</script>代表了通道<script type=\"math/tex\">i</script>上的样式特征。</p>\n<p>这些向量的<strong>格拉姆矩阵 （Gram matrix）<script type=\"math/tex\">XX^T\\in R^{c\\times c}</script>中<script type=\"math/tex\">i</script>⾏<script type=\"math/tex\">j</script>列的元素<script type=\"math/tex\">x_{ij}</script>即向量<script type=\"math/tex\">x_i</script>与<script type=\"math/tex\">x_j</script>的内积</strong>，它表达了通道<script type=\"math/tex\">i</script>和通道<script type=\"math/tex\">j</script>上样式特征的相关性。我们⽤这样的格拉姆矩阵表达样式层输出的样式。</p>\n<p>需要注意的是，当hw的值较⼤时，格拉姆矩阵中的元素容易出现较⼤的值。此外，格拉姆矩阵的⾼和宽皆为通道数c。为了让样式损失不受这些值的⼤小影响，<strong>需要除以矩阵中元素的个数，即chw</strong></p>\n</li>\n<li><p><strong>总变量损失：</strong>有时候，我们学到的合成图像⾥⾯有⼤量⾼频噪点，即有特别亮或者特别暗的颗粒像素。⼀种常⽤的降噪⽅法是总变差降噪（total variation denoising）。</p>\n<p>假设<script type=\"math/tex\">x_{i,j}</script>表⽰坐标为<script type=\"math/tex\">(i, j)</script>的像素值，降低总变差损失：</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{i, j}\\left|x_{i, j}-x_{i+1, j}\\right|+\\left|x_{i, j}-x_{i, j+1}\\right|</script><p>能够尽可能使邻近的像素值相似。</p>\n</li>\n</ul>\n<ul>\n<li>样式迁移的损失函数即内容损失、样式损失和总变差损失的<strong>加权和</strong>。通过调节这些<strong>权值超参数</strong>， 我们可以权衡合成图像在保留内容、迁移样式以及降噪三⽅⾯的相对重要性。</li>\n</ul>\n","site":{"data":{}},"wordcount":4495,"excerpt":"","more":"<h1 id=\"1-图像增广\"><a href=\"#1-图像增广\" class=\"headerlink\" title=\"1 图像增广\"></a>1 图像增广</h1><ul>\n<li>图像增⼴（image augmentation）技术通过对训练图像做⼀系列随机改变，来产⽣相似但⼜不同的训练样本，从而<strong>扩大训练数据集的规模</strong></li>\n<li><p>随机改变训练样本可以<strong>降低模型对某些属性的依赖，从而提高模型的泛化能力</strong>。例如，我们可以对图像进⾏不同 方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性</p>\n</li>\n<li><p>常用的方法有：<strong>翻转</strong>、<strong>裁剪</strong>、<strong>变换颜色</strong>。对于翻转，大部分情况左右翻转比上下翻转更通用一些。对于颜色，我们可以从亮度、对比度、饱和度和⾊调四方面进行改变</p>\n</li>\n<li>实际应⽤中我们会将多个图像增⼴方法叠加使⽤</li>\n</ul>\n<h1 id=\"2-微调\"><a href=\"#2-微调\" class=\"headerlink\" title=\"2 微调\"></a>2 微调</h1><ul>\n<li><strong>迁移学习（transfer learning）</strong>：<strong>将从源数据集学到的知识迁移到目标数据集上</strong>。例如，虽然ImageNet数据集的图像大多跟椅⼦⽆关，但在该数据集上训练的模型可以抽 取较通⽤的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于 识别椅⼦也可能同样有效。</li>\n<li>迁移学习中的一种常用技术“微调（<strong>fine tuning）</strong>”，由一下四步构成：</li>\n</ul>\n<blockquote>\n<ol>\n<li>在源数据集（如ImageNet数据集）上预训练⼀个神经⽹络模型，即源模型</li>\n<li>创建⼀个新的神经⽹络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设 计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适⽤于目标数据集。我们还假设源模型的输出层与源数据集的标签紧密相关，因此在目标模 型中不予采⽤。</li>\n<li>为目标模型添加⼀个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</li>\n<li>在目标数据集（如椅⼦数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123170934001.png\" alt=\"image-20220123170934001\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>可以选择保留除输出层以外的所有层，也可以保留除临近输出层的几层以外的所有层</li>\n<li>由于预训练模型是比较接近正确结果的，而新添加的层是随机初始化。<strong>所以两者的学习率是不同的，预训练的学习率更小</strong></li>\n</ul>\n</blockquote>\n<h1 id=\"3-目标检测和边界框\"><a href=\"#3-目标检测和边界框\" class=\"headerlink\" title=\"3 目标检测和边界框\"></a>3 目标检测和边界框</h1><ul>\n<li>很多时候图像⾥有多个我们感兴趣的目标，我们 不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉⾥，我们将这类任务 称为目标检测（object detection）或物体检测</li>\n<li>在目标检测⾥，我们通常使⽤<strong>边界框（bounding box）</strong>来描述目标位置。边界框是⼀个矩形框， 可以由矩形左上⻆的x和y轴坐标与右下⻆的x和y轴坐标确定</li>\n<li>它以每个像素为中⼼⽣ 成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为<strong>锚框（anchor box）</strong></li>\n</ul>\n<h3 id=\"3-1-锚框的生成\"><a href=\"#3-1-锚框的生成\" class=\"headerlink\" title=\"3.1 锚框的生成\"></a>3.1 锚框的生成</h3><ul>\n<li>假设输⼊图像高为h，宽为w。我们分别以图像的每个像素为中⼼⽣成不同形状的锚框。设大小为s <script type=\"math/tex\">\\in</script> (0, 1]且宽高比为r &gt; 0，那么锚框的宽和高将分别为<script type=\"math/tex\">ws\\sqrt{r}</script>和<script type=\"math/tex\">hs/\\sqrt{r}</script>。当中⼼位置给定时，已 知宽和高的锚框是确定的。⾯我们分别设定好⼀组大小<script type=\"math/tex\">s_1, . . . , s_n</script>和⼀组宽高比<script type=\"math/tex\">r_1, . . . , r_m</script>，s和r两两配对能覆盖所有的真实边界框，但是计算复杂度容易更高，所以我们通常只对包含<script type=\"math/tex\">s_1</script>或<script type=\"math/tex\">r_1</script>的大小与宽高比的组合感兴趣，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\left(s_{1}, r_{1}\\right),\\left(s_{1}, r_{2}\\right), \\ldots,\\left(s_{1}, r_{m}\\right),\\left(s_{2}, r_{1}\\right),\\left(s_{3}, r_{1}\\right), \\ldots,\\left(s_{n}, r_{1}\\right)</script><h3 id=\"3-2-交并比\"><a href=\"#3-2-交并比\" class=\"headerlink\" title=\"3.2 交并比\"></a>3.2 交并比</h3><ul>\n<li>Jaccard系数 （Jaccard index）可以衡量两个集合的相似度。给定集合A和B，它们的Jaccard系数即⼆者交集大小除以⼆者并集大小：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ(\\mathcal{A}, \\mathcal{B})=\\frac{|\\mathcal{A} \\cap \\mathcal{B}|}{|\\mathcal{A} \\cup \\mathcal{B}|}</script><ul>\n<li>我们通 常将Jaccard系数称为交并比（intersection over union，IoU），即两个边界框相交⾯积与相并⾯积之比</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123173756721.png\" alt=\"image-20220123173756721\" style=\"zoom: 80%;\" /></p>\n<h3 id=\"3-3-标注训练集的锚框\"><a href=\"#3-3-标注训练集的锚框\" class=\"headerlink\" title=\"3.3 标注训练集的锚框\"></a>3.3 标注训练集的锚框</h3><ul>\n<li>在训练集中，我们<strong>将每个锚框视为⼀个训练样本</strong>。为了训练目标检测模型，我们需要为每个锚框标注两类标签：<strong>⼀是锚框所含目标的类别，简称类别；⼆是真实边界框相对锚框的偏移量，简称偏移量（offset)</strong></li>\n<li>在目标检测时，我们⾸先⽣成多个锚框，然后为每个锚框预测类别以及偏移量， 接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框</li>\n</ul>\n<ul>\n<li><p>假设图像中锚框分别为<script type=\"math/tex\">A_1, A_2, . . . , A_{n_a}</script>，真实边界框分别为<script type=\"math/tex\">B_1, B_2, . . . , B_{n_b}</script>，且<script type=\"math/tex\">n_a</script> ≥ <script type=\"math/tex\">n_b</script>。定义矩 阵<script type=\"math/tex\">X \\in R^{n_a\\times n_b}</script>，其中第i⾏第j列的元素<script type=\"math/tex\">x_{ij}</script>为锚框<script type=\"math/tex\">A_i</script>与真实边界框<script type=\"math/tex\">B_j</script>的交并比</p>\n<blockquote>\n<ol>\n<li>找到矩阵中最大元素，并将该值对应的真实边界框赋值给锚框，然后从矩阵中去除该行和该列所有元素。然后继续找最大元素，重复上述操作，直到所有真实边界框都被分配完</li>\n<li>这个时候，我们已为<script type=\"math/tex\">n_b</script>个锚框各分配了⼀个真实边界框。接下来，我们 只遍历剩余的<script type=\"math/tex\">n_a − n_b</script>个锚框：给定其中的锚框<script type=\"math/tex\">A_i</script>，根据矩阵<script type=\"math/tex\">X</script>的第i⾏找到与<script type=\"math/tex\">A_i</script>交并比最大的真实边界框<script type=\"math/tex\">B_j</script>，<strong>且只有当该交并比大于预先设定的阈值时，才为锚框<script type=\"math/tex\">A_i</script>分配真实边界框<script type=\"math/tex\">B_j</script></strong></li>\n</ol>\n</blockquote>\n</li>\n<li><p><strong>如果⼀个锚框A被分配了真实边界框B，将锚框A的类别设为B的类别，并根据B和A的中心坐标的相对位置以及两个框的相对大小为锚框A标注偏 移量。</strong></p>\n</li>\n<li><strong>由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要⼀些特殊变换，才能使偏移量的分布更均匀从而更容易拟合</strong>。设锚框A及其被分配的真实边界框B的中 ⼼坐标分别为<script type=\"math/tex\">(x_a, y_a)</script>和<script type=\"math/tex\">(x_b, y_b)</script>，A和B的宽分别为<script type=\"math/tex\">w_a</script>和<script type=\"math/tex\">w_b</script>，高分别为<script type=\"math/tex\">h_a</script>和<script type=\"math/tex\">h_b</script>，⼀个常⽤的技巧是将A的偏移量标注为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\left(\\frac{\\frac{x_{b}-x_{a}}{w_{a}}-\\mu_{x}}{\\sigma_{x}}, \\frac{\\frac{y_{b}-y_{a}}{h_{a}}-\\mu_{y}}{\\sigma_{y}}, \\frac{\\log \\frac{w_{b}}{w_{a}}-\\mu_{w}}{\\sigma_{w}}, \\frac{\\log \\frac{h_{b}}{h_{a}}-\\mu_{h}}{\\sigma_{h}}\\right)</script><p>其中常数的默认值为<script type=\"math/tex\">µ_x = µ_y = µ_w = µ_h = 0, σ_x = σ_y = 0.1, σ_w = σ_h = 0.2</script></p>\n<ul>\n<li><p>如果一个锚框没有被分配真实边界框，我们只需将该锚框的类别设为<strong>背景</strong>。类别为背景的锚框通常被称为<strong>负类锚框</strong>，其余则被称为<strong>正类锚框</strong></p>\n</li>\n<li><p><strong>掩码（mask）：</strong> <strong>形状为(批量大小, 锚框个数的四倍)</strong>。掩码变量中的元素 与每个锚框的4个偏移量⼀⼀对应。由于我们不关⼼对背景的检测，有关负类的偏移量不应影响 目标函数。<strong>通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量</strong></p>\n</li>\n</ul>\n<h3 id=\"3-4-非极大值抑制\"><a href=\"#3-4-非极大值抑制\" class=\"headerlink\" title=\"3.4 非极大值抑制\"></a>3.4 非极大值抑制</h3><ul>\n<li>为了使结果更加简洁，我们可以移除相似的预测边界框。常⽤的方法叫作非极大值抑制（non-maximum suppression，NMS)：</li>\n</ul>\n<blockquote>\n<ol>\n<li>在同⼀图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序， 得到列表L</li>\n<li>从L中选取置信度最高的预测边界框<script type=\"math/tex\">B_1</script>作为基准，将所有与<script type=\"math/tex\">B_1</script>的交并比大于某阈值 的非基准预测边界框从L中移除。这⾥的阈值是预先设定的超参数</li>\n<li>继续重复上述操作，直到L中所有的预测边界框都曾作为基准</li>\n</ol>\n</blockquote>\n<ul>\n<li>实践中，我们可以在执⾏非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。</li>\n</ul>\n<h3 id=\"3-5-多尺度目标检测\"><a href=\"#3-5-多尺度目标检测\" class=\"headerlink\" title=\"3.5 多尺度目标检测\"></a>3.5 多尺度目标检测</h3><ul>\n<li>在不同尺度下，我们可以⽣成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多</li>\n<li>因此，当使⽤较小 锚框来检测较小目标时，我们可以采样较多的区域；而当使⽤较大锚框来检测较大目标时，我们可以采样较少的区域。</li>\n<li>我们可以通过<strong>控制特征图的大小来控制尺度（特征图每个单元在输入图像上对应的感受野可大可小）</strong>。本质上，我们⽤输⼊图像在某个感 受野区域内的信息来预测输⼊图像上与该区域位置相近的锚框的类别和偏移量</li>\n</ul>\n<h3 id=\"3-6-单发多框检测（SSD）\"><a href=\"#3-6-单发多框检测（SSD）\" class=\"headerlink\" title=\"3.6 单发多框检测（SSD）\"></a>3.6 单发多框检测（SSD）</h3><ul>\n<li>它主要由<strong>⼀个基础⽹络块和若干个多尺度特征块串联而成</strong>。其中<strong>基础⽹络块⽤来从原始图像中抽取特征</strong>，因此⼀般会选择常⽤的深度卷积神经⽹络。单 发多框检测论⽂中选⽤了在分类层之前截断的VGG，现在也常⽤ResNet替代</li>\n<li>我们可以设计 基础⽹络，使它输出的高和宽较大。这样⼀来，基于该特征图⽣成的锚框数量较多，可以⽤来检 测尺⼨较小的目标</li>\n<li>接下来的每个多尺度特征块将上⼀层提供的特征图的<strong>高和宽缩小（如减半）</strong>， 并使<strong>特征图中每个单元在输⼊图像上的感受野变得更⼴阔</strong>。如此⼀来，下图中<strong>越靠近顶部的多 尺度特征块输出的特征图越小，故而基于特征图⽣成的锚框也越少，加之特征图中每个单元感受 野越大，因此更适合检测尺⼨较大的目标</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123182624112.png\" alt=\"image-20220123182624112\" style=\"zoom:80%;\" /></p>\n<h5 id=\"3-6-1-类别预测层\"><a href=\"#3-6-1-类别预测层\" class=\"headerlink\" title=\"3.6.1 类别预测层\"></a>3.6.1 类别预测层</h5><ul>\n<li>设目标的类别个数为q。每个锚框的类别个数将是q + 1，其中类别0表⽰锚框只包含背景</li>\n<li>设特征图的高和宽分别为h和w，如果以其中每个单元为中⼼⽣成a个锚框，那么我们需 要对hwa个锚框进⾏分类。如果使⽤全连接层作为输出，<strong>很容易导致模型参数过多。所以我们可以通过通道输出类别</strong></li>\n<li>类别预测层使⽤⼀个<strong>保持输⼊高和宽的卷积层</strong>。这样⼀来，输出和输⼊在特征图宽和高上的空间坐标⼀⼀对应。考虑输出和输⼊同⼀空间坐标(x, y)：<strong>输出特征图上(x, y)坐标的通道里包含了以输⼊特征图(x, y)坐标为中心生成的所有锚框的类别预测</strong>。因此<strong>输出通道数为a(q + 1)， 其中索引为i(q + 1) + j（0 ≤ j ≤ q）的通道代表了索引为i的锚框有关类别索引为j的预测</strong></li>\n</ul>\n<h5 id=\"3-6-2-边界框预测层\"><a href=\"#3-6-2-边界框预测层\" class=\"headerlink\" title=\"3.6.2 边界框预测层\"></a>3.6.2 边界框预测层</h5><ul>\n<li>边界框预测层的设计与类别预测层的设计类似。唯⼀不同的是，这⾥需要为每个锚框预测4个偏移量，而不是q + 1个类别</li>\n</ul>\n<h5 id=\"3-6-3-连结多尺度的预测\"><a href=\"#3-6-3-连结多尺度的预测\" class=\"headerlink\" title=\"3.6.3 连结多尺度的预测\"></a>3.6.3 连结多尺度的预测</h5><ul>\n<li>每个尺度的输出，除了批量大小一样，其他维度的大小均不一样。我们需要将它们变形成统⼀的格式并将多尺度的预测连结，从而让后续计算更简单</li>\n<li>所以我们需要将为(批量大小, 通道数, 高, 宽)格式转换成⼆维的(批量大小, 高×宽×通道数)的格式，以方便之后在维度1上的连结。</li>\n</ul>\n<h5 id=\"3-6-4-损失函数和评价函数\"><a href=\"#3-6-4-损失函数和评价函数\" class=\"headerlink\" title=\"3.6.4 损失函数和评价函数\"></a>3.6.4 损失函数和评价函数</h5><ul>\n<li>目标检测有两个损失：<strong>⼀是有关锚框类别的损失</strong>，我们可以重⽤之前图像分类问题⾥⼀直使⽤ 的交叉熵损失函数；<strong>⼆是有关正类锚框偏移量的损失</strong></li>\n<li>。预测偏移量是⼀个回归问题，但这⾥不 使⽤前⾯介绍过的平方损失，而使⽤L1范数损失，即预测值与真实值之间差的绝对值（其中 使用掩码变量令负类锚框和填充锚框不参与损失的计算）</li>\n<li>最后，我们将有关锚框类别和偏移量的损失相加得到模型的最终损失函数。</li>\n<li>可以将<script type=\"math/tex\">L_1</script>损失换成<strong>平滑的<script type=\"math/tex\">L_1</script>范数损失</strong>，它在零点附近使⽤平方函数从而更加平滑，这是通过⼀个超参数<script type=\"math/tex\">\\sigma</script>来控制平滑区域的：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x)=\\left\\{\\begin{array}{ll}\n(\\sigma x)^{2} / 2, & \\text { if }|x|<1 / \\sigma^{2} \\\\\n|x|-0.5 / \\sigma^{2}, & \\text { otherwise }\n\\end{array}\\right.</script><p>当<script type=\"math/tex\">\\sigma</script>很⼤时该损失类似于<script type=\"math/tex\">L_1</script>范数损失。当它较小时，损失函数较平滑。</p>\n<ul>\n<li>还可以将交叉熵损失换成<strong>焦点损失（focal loss）</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\alpha\\left(1-p_{j}\\right)^{\\gamma} \\log p_{j}</script><p>焦点损失适用于比较密集的目标检测，即要判定的类别比较多的情况。我们将一个锚框标签的类别作为正类，其余都作为负类（包括背景），那么这就转换成了一个二分类问题，回顾二分类的交叉熵损失函数：</p>\n<script type=\"math/tex; mode=display\">\n\\mathrm{L}=-\\mathrm{ylog} y^{\\prime}-(1-y) \\log \\left(1-y^{\\prime}\\right)=\\left\\{\\begin{array}{ll}\n-\\log y^{\\prime} & , \\quad y=1 \\\\\n-\\log \\left(1-y^{\\prime}\\right), & y=0\n\\end{array}\\right.</script><p>可以看到当标签为负类（y=0），且正类预测概率<script type=\"math/tex\">y'</script>较大时，会产生较大的损失。而由于正负样本的极其不均衡（比如有1000个类别，正类只有1种，负类则有999种），所以负样本会主导梯度的更新方向，使得整体学习方向跑偏</p>\n<p>上述的负类因为正负样本的不均衡，所以负类是是易分类的样本（<script type=\"math/tex\">p_j > 0.5</script>），而焦点损失中的<script type=\"math/tex\">(1-p_j)^{\\gamma}</script>就是为了减轻易分类样本的权重，让对象检测器更关注难分类的样本（即正样本）</p>\n<h1 id=\"4-区域卷积神经网络（R-CNN）\"><a href=\"#4-区域卷积神经网络（R-CNN）\" class=\"headerlink\" title=\"4 区域卷积神经网络（R-CNN）\"></a>4 区域卷积神经网络（R-CNN）</h1><h3 id=\"4-1-R-CNN\"><a href=\"#4-1-R-CNN\" class=\"headerlink\" title=\"4.1 R-CNN\"></a>4.1 R-CNN</h3><ul>\n<li>R-CNN⾸先对图像<strong>选取若干提议区域</strong>（如锚框也是⼀种选取方法）并标注它们的类别和边界框 （如偏移量）。然后，<strong>⽤卷积神经⽹络对每个提议区域做前向计算抽取特征。之后，我们⽤每个提议区域的特征预测类别和边界框</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123184832784.png\" alt=\"image-20220123184832784\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><p>具体来说，R-CNN主要由以下4步构成：</p>\n<blockquote>\n<ol>\n<li>对输⼊图像使⽤选择性搜索（selective search）来<strong>选取多个高质量的提议区域</strong>。这些提议区域通常是在<strong>多个尺度下</strong>选取的，并具有不同的形状和大小。<strong>每个提议区域将被标注类 别和真实边界框。</strong></li>\n<li>选取⼀个预训练的卷积神经⽹络，并将其在输出层之前截断</li>\n<li>将每个提议区域的特征连同其标注的类别作为⼀个样本，训练多个⽀持向量机对目标分类。 其中每个⽀持向量机⽤来判断样本是否属于某⼀个类别</li>\n<li>将每个提议区域的特征连同其标注的边界框作为⼀个样本，训练线性回归模型来预测真实边界框</li>\n</ol>\n</blockquote>\n</li>\n<li><p>一张图片中可能会有很多个提议区域，每个区域都要进行卷积运算。这个<strong>巨大的计算量令R-CNN难以在实际应⽤中被⼴泛采⽤</strong></p>\n</li>\n</ul>\n<h3 id=\"4-2-Fast-R-CNN\"><a href=\"#4-2-Fast-R-CNN\" class=\"headerlink\" title=\"4.2 Fast R-CNN\"></a>4.2 Fast R-CNN</h3><ul>\n<li>R-CNN提议区域通常<strong>有大量重叠， 独⽴的特征抽取会导致大量的重复计算</strong>。Fast R-CNN对R-CNN的⼀个主要改进在于<strong>只对整个图像 做卷积神经⽹络的前向计算。</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123185433036.png\" alt=\"image-20220123185433036\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>主要步骤：</li>\n</ul>\n<blockquote>\n<ol>\n<li>与R-CNN相比，Fast R-CNN⽤来提取特征的卷积神经⽹络的输⼊是整个图像，而不是各个提议区域。而且，<strong>这个⽹络通常会参与训练</strong>，即更新模型参数。设输⼊为⼀张图像，将卷积神经⽹络的输出的形状记为<script type=\"math/tex\">1 × c × h_1 × w_1</script></li>\n<li>假设选择性搜索⽣成n个提议区域。<strong>这些形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域</strong>。这些兴趣区域需要抽取出<strong>形状相同</strong>的特征（假设高和宽均分别指定为<script type=\"math/tex\">h_2</script>和<script type=\"math/tex\">w_2</script>）以便于连结后输出（使用<strong>兴趣区域池化（region of interest pooling，RoI池化）层，将卷积神经⽹络的输出和提议区域作为输⼊，输出连结后的各个提议区域抽取的特征</strong>）</li>\n<li>通过全连接层将输出形状变换为n × d，其中超参数d取决于模型设计</li>\n<li>预测类别时，将全连接层的输出的形状再变换为n × q并使⽤softmax回归（q为类别个数）。 预测边界框时，将全连接层的输出的形状变换为n × 4。也就是说，我们为每个提议区域预 测类别和边界框。</li>\n</ol>\n</blockquote>\n<ul>\n<li>兴趣区域池化层：兴趣区域池化层对每个区域的输 出形状是可以直接指定的如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191333584.png\" alt=\"image-20220123191333584\"></p>\n<p>第一张图的蓝色区域是一个提议区域，将其经过2x2兴趣区域池化层后，划分成了4个区域，分别含有元素0、1、4、5（5最 大），2、6（6最大），8、9（9最大），10。输出每个区域的最大元素</p>\n<h3 id=\"4-3-Faster-R-CNN\"><a href=\"#4-3-Faster-R-CNN\" class=\"headerlink\" title=\"4.3 Faster R-CNN\"></a>4.3 Faster R-CNN</h3><ul>\n<li>Fast R-CNN通常需要在选择性搜索中⽣成较多的提议区域，以获得较精确的目标检测结果。Faster R-CNN提出<strong>将选择性搜索替换成区域提议⽹络（region proposal network）</strong>，从而<strong>减少提议区域 的⽣成数量</strong>，并保证目标检测的精度</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123191545146.png\" alt=\"image-20220123191545146\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>与Fast R-CNN相比，只有⽣成提议区域的方法从选择性搜索变成了区域提议⽹络，而其他部分均保持不变。具体来说，区域提议⽹络的计算步骤如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li>使⽤填充为1的3 × 3卷积层变换卷积神经⽹络的输出，并将输出通道数记为c</li>\n<li>. 以特征图每个单元为中⼼，⽣成多个不同大小和宽高比的锚框并标注它们</li>\n<li>. ⽤锚框中⼼单元⻓度为c的特征分别预测该锚框的<strong>二元类别（含目标还是背景，需要reshape）</strong>和边界框</li>\n<li>使⽤非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测 边界框即兴趣区域池化层所需要的提议区域。</li>\n</ol>\n</blockquote>\n<ul>\n<li>区域提议⽹络作为Faster R-CNN的⼀部分，是和整个模型⼀起训练得到的。也就 是说，<strong>Faster R-CNN的目标函数既包括目标检测中的类别和边界框预测，⼜包括区域提议⽹络中 锚框的⼆元类别和边界框预测</strong></li>\n</ul>\n<h3 id=\"4-4-Mask-R-CNN\"><a href=\"#4-4-Mask-R-CNN\" class=\"headerlink\" title=\"4.4 Mask R-CNN\"></a>4.4 Mask R-CNN</h3><ul>\n<li>如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利⽤这些详尽 的标注信息进⼀步提升目标检测的精度。</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123193610616.png\" alt=\"image-20220123193610616\" style=\"zoom: 80%;\" /></p>\n<ul>\n<li>Mask R-CNN将兴趣区域池化层 替换成了兴趣区域对⻬层，即通过<strong>双线性插值（bilinear interpolation）（一种常用的上采样方法，目的是将下一层的特征图的单元于上一层特征图单元对齐）</strong>来保留特征图上的空间信息，从而更适于像素级预测</li>\n</ul>\n<h1 id=\"5-语义分割\"><a href=\"#5-语义分割\" class=\"headerlink\" title=\"5 语义分割\"></a>5 语义分割</h1><ul>\n<li>语义分割（semantic segmentation）关注如何将图像分割成属于不同语义类别的区域。值得⼀提的是，这些语义区域的标注和预测都是像素级的</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220123194304346.png\" alt=\"image-20220123194304346\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>计算机视觉领域还有2个与语义分割相似的重要问题，即<strong>图像分割（image segmentation）</strong>和<strong>实例分割（instance segmentation）</strong></li>\n<li><strong>图像分割</strong>将图像分割成若干组成区域。<strong>这类问题的方法通常利⽤图像中像素之间的相关性</strong>。 它在训练时不需要有关图像像素的标签信息，在预测时也⽆法保证分割出的区域具有我们希望得到的语义。如上图图像分割可能将狗分割成两个区域：⼀个覆盖以⿊⾊为主的嘴巴和眼睛，而另⼀个覆盖以⻩⾊为主的其余部分⾝体</li>\n<li><strong>实例分割</strong>研究如何 识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割<strong>不仅需要区分语 义，还要区分不同的目标实例</strong>。如果图像中有两只狗，实例分割需要区分像素属于这两只 狗中的哪⼀只。</li>\n</ul>\n<ul>\n<li>如果我们通过缩放图像使其符合模型的输⼊形状。然而在语义分割⾥，<strong>需要将预测的像素类别重新映射回原始尺⼨的输⼊图像</strong>。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，<strong>我们将图像裁剪成固定尺⼨而不是缩放</strong>（对于实际尺寸小于规定尺寸的图像，需要移除）</li>\n</ul>\n<h1 id=\"6-全卷积网络（FCN）\"><a href=\"#6-全卷积网络（FCN）\" class=\"headerlink\" title=\"6 全卷积网络（FCN）\"></a>6 全卷积网络（FCN）</h1><ul>\n<li>全卷积⽹络（fully convolutional network，FCN）采⽤卷积神经⽹络实现了从图像像素到像素类别的变换。</li>\n<li>全卷积⽹络通过<strong>转置卷积（transposed convolution）层将中间层特征图的高和宽变换回输⼊图像的尺⼨</strong></li>\n</ul>\n<ul>\n<li>因为卷积运算可以用矩阵乘法来实现，假设input进行卷积运算相当于input矩阵乘一个<script type=\"math/tex\">W</script>矩阵得到特征图featrue。那么可以通过featrue乘一个<script type=\"math/tex\">W^T</script>来变回input的形状。<strong>所以可以通过转置卷积层来交换卷积层输入和输出的形状</strong></li>\n</ul>\n<h3 id=\"6-1-模型构造\"><a href=\"#6-1-模型构造\" class=\"headerlink\" title=\"6.1 模型构造\"></a>6.1 模型构造</h3><ul>\n<li>全卷积⽹络先使⽤卷积神经⽹络抽取图像特征，然后通过1 × 1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的⾼和宽变换为输⼊图像的尺⼨</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124154145643.png\" alt=\"image-20220124154145643\" style=\"zoom: 67%;\" /></p>\n<h3 id=\"6-2-初始化转置卷积层\"><a href=\"#6-2-初始化转置卷积层\" class=\"headerlink\" title=\"6.2 初始化转置卷积层\"></a>6.2 初始化转置卷积层</h3><ul>\n<li><strong>双线性插值：</strong>首先介绍一下单线性插值（一维）：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190819135319360.jpg\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\" /></p>\n<p>我们知道<script type=\"math/tex\">x_0,x_1,y_0,y_1,x</script>的值，要求<script type=\"math/tex\">y</script>的值：</p>\n<script type=\"math/tex; mode=display\">\ny = y_0 + \\frac{y_1 - y_0}{x_1 - x_0}(x - x_0)</script><p>而<strong>双线性插值其实就是在不同的维度上单线性插值两次</strong>，已知<script type=\"math/tex\">Q_{11}(x_1,y_1),Q_{12}(x_1,y_2),Q_{21}(x_2,y_1),Q_{22}(x_2,y_2)</script>，求其中点P(x,y)的函数值：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-ad3d95548a97aa47ca85867cd0f2e161_720w.jpg\" alt=\"img\" style=\"zoom:67%;\" /></p>\n<p>首先在x方向单线性插值两次：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nf\\left(R_{1}\\right)=\\frac{x_{2}-x}{x_{2}-x_{1}} f\\left(Q_{11}\\right)+\\frac{x-x_{1}}{x_{2}-x_{1}} f\\left(Q_{21}\\right) \\\\\nf\\left(R_{2}\\right)=\\frac{x_{2}-x}{x_{2}-x_{1}} f\\left(Q_{12}\\right)+\\frac{x-x_{1}}{x_{2}-x_{1}} f\\left(Q_{22}\\right)\n\\end{array}</script><p>然后再在y方向单线性插值一次：</p>\n<script type=\"math/tex; mode=display\">\nf(P)=\\frac{y_{2}-y}{y_{2}-y_{1}} f\\left(R_{1}\\right)+\\frac{y-y_{1}}{y_{2}-y_{1}} f\\left(R_{2}\\right)</script><p>即可得到结果。</p>\n<ul>\n<li>在全卷积⽹络中，我们一般<strong>将转置卷积层初始化为双线性插值的上采样</strong>，具体方法是：</li>\n</ul>\n<blockquote>\n<ol>\n<li>为了得到输出图像在坐 标(x, y)上的像素，先将该坐标映射到输⼊图像的坐标(x ′ , y′ )，例如，根据输⼊与输出的尺⼨之⽐来映射。</li>\n<li>映射后的x ′和y ′通常是实数。然后，在输⼊图像上找到与坐标(x ′ , y′ )最近的4像素。最后， 输出图像在坐标(x, y)上的像素依据输⼊图像上这4像素及其与(x ′ , y′ )的相对距离来计算（用双线性插值）</li>\n</ol>\n</blockquote>\n<ul>\n<li><p>如果步幅为s、填充为s/2（假设s/2为整数）、卷积核的⾼和宽为2s，转置卷积核将输⼊的⾼和宽分别放⼤s倍</p>\n</li>\n<li><p>转置卷积层的输出形状可能会不一样，这是因为当<strong>输入图像的高宽无法整除步幅时，输出的高宽会有所偏差</strong>。为了解决这个问题，我们可以在图像中<strong>截取多块⾼和宽为步幅的整数倍的矩形区域</strong>，并分别对这些区域中的像素做前向计算。<strong>这些区域的并集需要完整覆盖输⼊图像</strong>。当⼀个像素被多个区域所覆盖时，它在不同区域 前向计算中转置卷积层输出的<strong>平均值可以作为softmax运算的输⼊</strong>，从而预测类别</p>\n</li>\n</ul>\n<h1 id=\"7-样式迁移\"><a href=\"#7-样式迁移\" class=\"headerlink\" title=\"7 样式迁移\"></a>7 样式迁移</h1><ul>\n<li><p>使⽤卷积神经⽹络⾃动将某图像中的样式应⽤在另⼀图像之上，即样式迁移（style transfer）。需要两张输⼊图像，⼀张是<strong>内容图像</strong>，另⼀张是<strong>样式图像</strong>， 我们将使⽤神经⽹络<strong>修改内容图像使其在样式上接近样式图像</strong></p>\n</li>\n<li><p>主要步骤：</p>\n</li>\n</ul>\n<blockquote>\n<ol>\n<li>⾸先，我们初始化合成图像，例如 将其<strong>初始化成内容图像</strong>。该合成图像是样式迁移过程中<strong>唯⼀需要更新的变量</strong>，</li>\n<li>我们选择⼀个预训练的卷积神经⽹络来<strong>抽取图像的特征</strong>，其中的<strong>模型参数在训练中⽆须更新</strong>。深度卷积神经⽹络<strong>凭借多个层逐级抽取图像的特征。我们可以选择其中某些 层的输出作为内容特征或样式特征</strong>，例如下图，这⾥选取的预训练的神经⽹络含有3个卷积 层，其中第⼆层输出图像的内容特征，而第⼀层和第三层的输出被作为图像的样式特征</li>\n<li>样式迁移常⽤的损失函数由3部分组成：<strong>内容损失（content loss）</strong>使合成图像与内容图像在内容特征上接近，<strong>样式损失（style loss）</strong>令合成图像与样式图像 在样式特征上接近，而<strong>总变差损失（total variation loss）</strong>则有助于减少合成图像中的噪点</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220124175546385.png\" alt=\"image-20220124175546385\" style=\"zoom:50%;\" /></p>\n</blockquote>\n<h3 id=\"7-1-内容层和样式层的选择\"><a href=\"#7-1-内容层和样式层的选择\" class=\"headerlink\" title=\"7.1 内容层和样式层的选择\"></a>7.1 内容层和样式层的选择</h3><ul>\n<li><strong>⼀般来说，越靠近输⼊层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成 图像过多保留内容图像的细节，我们选择较靠近输出的层来输出图像的内容特征</strong></li>\n<li><strong>我们还可以选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层</strong></li>\n<li>例如VGG-19,使⽤了5个卷积块。我们可以选择第四卷积块的最后⼀个卷积层作为内容层，以及每个卷积块的第⼀个卷积层作为样式层</li>\n</ul>\n<h3 id=\"7-2-损失函数\"><a href=\"#7-2-损失函数\" class=\"headerlink\" title=\"7.2 损失函数\"></a>7.2 损失函数</h3><ul>\n<li><p><strong>内容损失：</strong>与线性回归中的损失函数类似，内容损失通过平⽅误差函数衡量合成图像与内容图像在内容特征上的差异。平⽅误差函数的两个输⼊均为内容层的输出。</p>\n</li>\n<li><p><strong>样式损失：</strong>样式损失也⼀样通过平⽅误差函数衡量合成图像与样式图像在样式上的差异。我们将样式层的输出（长为h，宽为w，通道数为c），转化成c行hw列的矩阵X，矩阵X可以看作由c个长度为hw的向量<script type=\"math/tex\">x_1, . . . , x_c</script>组成的。其中向量<script type=\"math/tex\">x_i</script>代表了通道<script type=\"math/tex\">i</script>上的样式特征。</p>\n<p>这些向量的<strong>格拉姆矩阵 （Gram matrix）<script type=\"math/tex\">XX^T\\in R^{c\\times c}</script>中<script type=\"math/tex\">i</script>⾏<script type=\"math/tex\">j</script>列的元素<script type=\"math/tex\">x_{ij}</script>即向量<script type=\"math/tex\">x_i</script>与<script type=\"math/tex\">x_j</script>的内积</strong>，它表达了通道<script type=\"math/tex\">i</script>和通道<script type=\"math/tex\">j</script>上样式特征的相关性。我们⽤这样的格拉姆矩阵表达样式层输出的样式。</p>\n<p>需要注意的是，当hw的值较⼤时，格拉姆矩阵中的元素容易出现较⼤的值。此外，格拉姆矩阵的⾼和宽皆为通道数c。为了让样式损失不受这些值的⼤小影响，<strong>需要除以矩阵中元素的个数，即chw</strong></p>\n</li>\n<li><p><strong>总变量损失：</strong>有时候，我们学到的合成图像⾥⾯有⼤量⾼频噪点，即有特别亮或者特别暗的颗粒像素。⼀种常⽤的降噪⽅法是总变差降噪（total variation denoising）。</p>\n<p>假设<script type=\"math/tex\">x_{i,j}</script>表⽰坐标为<script type=\"math/tex\">(i, j)</script>的像素值，降低总变差损失：</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{i, j}\\left|x_{i, j}-x_{i+1, j}\\right|+\\left|x_{i, j}-x_{i, j+1}\\right|</script><p>能够尽可能使邻近的像素值相似。</p>\n</li>\n</ul>\n<ul>\n<li>样式迁移的损失函数即内容损失、样式损失和总变差损失的<strong>加权和</strong>。通过调节这些<strong>权值超参数</strong>， 我们可以权衡合成图像在保留内容、迁移样式以及降噪三⽅⾯的相对重要性。</li>\n</ul>\n"},{"title":"EDA和AEDA","math":true,"date":"2022-09-23T16:00:00.000Z","_content":"\n\n\n- 本节主要介绍NLP领域的两种简单数据扩充方法：**EDA和AEDA**\n- 还有许多其他的数据扩充方法， 例如将文本进行back-translation，即将文本翻译一次又翻译回去，从而扩充文本，还可以通过各种深度学习模型进行扩充。**但是这些方法都太过\"expensive\"，而EDA和AEDA就相比之下比较简单，只需要在输入文本之前一定的预处理即可。**\n\n\n\n# 1 EDA\n\n### 1.1 EDA的基本方法\n\n- **EDA的基本方法包括四种：**\n> 1. **Synonym Replacement (SR，同义词替换)：**随机挑选n个词**（不能是停用词）**，然后将每个词随机替换成同义词\n> 2. **Random Insertion (RI，随机插入)：**挑选随机词**（不能是停用词）**的随机同义词，插入随机位置，进行n次\n> 3. **Random Swap (RS，随机交换)：**随机挑选两个词，交换位置，进行n次\n> 4. **Random Deletion (RD，随机删除)：**使用概率p随机删除每个词\n\n- EDA的做法是，**对输入的句子进行改变，但是尽量不改变其句意，也就是使句意和true label尽量对应**，所以使用同义词替换等方法来增加噪音，但不能增加过多。其中，对于长句子，相比于短句子，能吸收更多的噪音，更能保持true label\n- 进行SR和RI时，不是选择随机词进行操作，而是使用同义词，**目的就是为了尽量不改变原始句意**\n- **超参的选择：**\n\n> 假设句子长度为$$l$$，则$$n=\\alpha l$$，$$\\alpha$$表明了多少比例的词语会被改变。并且对于RD，我们使用$$p=\\alpha$$。对于每个句子，我们创造$$n_{aug}$$个扩充句子\n\n\n\n### 1.2 EDA不同模型上的表现\n\n- 可以看到，EDA在RNN和CNN上实现了准确率的提升，并且对于小数据集，提升更为明显\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808144843149.png\" alt=\"image-20220808144843149\" style=\"zoom: 80%;\" />\n\n\n\n### 1.3 不同数据集大小对EDA的影响\n\n- 作者对多个数据集进行了测试，并且在最后（图f）给出了在所有数据集上的平均结果，以探究不同大小的数据集对EDA效果的影响：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808145353524.png\" alt=\"image-20220808145353524\" style=\"zoom:90%;\" />\n\n- 在图f中，不使用EDA的最高准确率是88.3%，是在使用所有数据集时实现的。但是使用EDA时最高准确率为88.6%，**甚至只是用了一半的源数据**\n- **总的来说，EDA对于小数据集的影响更大**\n\n\n\n### 1.4 EDA是否会影响True Label\n\n- **作者的实验步骤是：**对于一个pro-con分类任务（PC），先不应用EDA进行训练，然后在测试集上，进行数据扩充（每个源数据扩充九个数据），将源数据和扩充数据一起输入模型测试，将最后一个dense层得到的向量使用t-SNE表示，然后得到如下结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808150515348.png\" alt=\"image-20220808150515348\" style=\"zoom:80%;\" />\n\n- 可以看到**扩充数据的潜在语义空间是接近源数据的**，所以对于多数情况，EDA是不会改变true label的\n\n\n\n### 1.5 消融实验\n\n- EDA是四种扩充方法的结合，而对于这四种方法，作者通过每次分别只使用一次方法，来探究四种方法各自的贡献和效果。并且对不同的$$\\alpha$$取值进行选取：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808151914033.png\" alt=\"image-20220808151914033\" style=\"zoom:90%;\" />\n\n- 四种操作都获得了一定的提升，其中：\n> - **对于SR：**使用小的$$\\alpha$$获得了提升，但是过大的$$\\alpha$$反而降低了表现，推测原因为：过多的替换改变了原本的句意\n> - **对于RI：**提升对于$$\\alpha$$的改变不是特别敏感，更为稳定，推测原因为：原本的词和相对位置保留了下来\n> - **对于RS：**在$$\\alpha \\le 0.2$$时获得较大提升，但在$$\\alpha \\ge 0.3$$时出现了下降，推测原因为：交换过多的词其实就等同于将整个句子词语的顺序重新排列一遍\n> - **对于RD：**小$$\\alpha$$有很大的提升，但是大的$$\\alpha$$十分影响表现，推测原因为：删除过多的词使句子变得无法理解\n\n- **通过实验，作者推荐通常取$$\\alpha=0.1$$**\n\n\n\n### 1.6 扩充几句最为合适\n\n- 其实就是对超参$$n_{aug}$$的选择，实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808154726577.png\" alt=\"image-20220808154726577\" style=\"zoom:80%;\" />\n\n- 可以看到，对于小数据集，$$n_{aug}$$最好大一些，而大数据集则不需要那么多扩充数据\n- 作者还给出了**推荐的超参：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808155215524.png\" alt=\"image-20220808155215524\" style=\"zoom:90%;\" />\n\n\n\n### 1.7 结论\n\n- 尽管EDA实现了一定的提升，尤其是在小数据集上，但是仍有一定的**限制**：\n\n> 1. 通过实验可以发现，**EDA在数据充足时，提升的效果是十分有限的**，基本都是1%不到\n> 2. 并且就算是使用小数据集，**在使用pre-trained model时，如BERT等，得到的提升也是十分微小的**\n\n- EDA的**本质作用**可以总结为以下两点：\n\n> 1. 产生了一定程度的噪音，来**阻止模型过拟合**\n> 2. 通过SR和RI操作，可以产生新的词典，使模型可以**泛化在测试集中而不在训练集中的词**\n\n\n\n\n\n# 2 AEDA\n\n### 2.1 AEDA的基本方法\n\n- 其实就是随机位置插入随机标点，**插入次数选择$$1 \\sim \\frac{1}{3}sentence\\_length$$的随机数**，插入的标点符号为：**{\".\", \";\", \"?\", \":\", \"!\", \",\"}**，举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125655875.png\" alt=\"image-20220809125655875\" style=\"zoom:67%;\" />\n\n- 对比一下EDA，**EDA的交换操作改变了文本顺序，并且删除操作会造成信息的损失，从而造成对模型的\"misleading\"**。而AEDA则会保留文本的顺序和词语。作者还做了详细的实验进行验证和对比\n\n\n\n### 2.2 EDA和AEDA的对比\n\n- 作者分别在CNN和RNN上进行了实验，进行数据扩充时，每个源数据扩充了16个数据，实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808175040407.png\" alt=\"image-20220808175040407\" style=\"zoom:75%;\" />\n\n- 可以看到EDA尽在小数据集上有所提升，但是在大数据集上表现更差了。但是AEDA在所有数据集上都有提升，尤其是在小数据集上更为明显。\n- 作者认为造成这种结果的原因是：EDA的替换和删除操作给模型增加了许多\"misleading\"的信息\n\n> The reason why EDA does not perform well can be attributed to the operations such as deletion and substitution which insert more misleading information to the network as the number of augmentations grows. In contrast, AEDA keeps the original information in all augmentations  \n\n- 此外，作者还通过不同的数据集，针对数据集大小展开了研究，结果如下：\n\n![image-20220809125001403](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125001403.png)\n\n\n\n### 2.3 扩充几句最为合适\n\n- 作者还探究了每个源数据扩充几句数据最为合适，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125426286.png\" alt=\"image-20220809125426286\" style=\"zoom:70%;\" />\n\n- 作者并没有在论文中指出最合适的超参，但是个人觉得大多数时候**扩充一到两句**就够了\n\n\n\n### 2.4 对于BERT的提升\n\n- 作者对于BERT模型，进行了加EDA和AEDA的对比，每个源数据只扩充了一句，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809132749401.png\" alt=\"image-20220809132749401\" style=\"zoom:80%;\" />\n\n- EDA反倒下降了表现（有可能是$$n_{aug}$$只有1），而AEDA实现了细微的提升（还是十分有限。。。）","source":"_posts/EDA和AEDA.md","raw":"---\ntitle: EDA和AEDA\nmath: true\ndate: 2022-9-24\n---\n\n\n\n- 本节主要介绍NLP领域的两种简单数据扩充方法：**EDA和AEDA**\n- 还有许多其他的数据扩充方法， 例如将文本进行back-translation，即将文本翻译一次又翻译回去，从而扩充文本，还可以通过各种深度学习模型进行扩充。**但是这些方法都太过\"expensive\"，而EDA和AEDA就相比之下比较简单，只需要在输入文本之前一定的预处理即可。**\n\n\n\n# 1 EDA\n\n### 1.1 EDA的基本方法\n\n- **EDA的基本方法包括四种：**\n> 1. **Synonym Replacement (SR，同义词替换)：**随机挑选n个词**（不能是停用词）**，然后将每个词随机替换成同义词\n> 2. **Random Insertion (RI，随机插入)：**挑选随机词**（不能是停用词）**的随机同义词，插入随机位置，进行n次\n> 3. **Random Swap (RS，随机交换)：**随机挑选两个词，交换位置，进行n次\n> 4. **Random Deletion (RD，随机删除)：**使用概率p随机删除每个词\n\n- EDA的做法是，**对输入的句子进行改变，但是尽量不改变其句意，也就是使句意和true label尽量对应**，所以使用同义词替换等方法来增加噪音，但不能增加过多。其中，对于长句子，相比于短句子，能吸收更多的噪音，更能保持true label\n- 进行SR和RI时，不是选择随机词进行操作，而是使用同义词，**目的就是为了尽量不改变原始句意**\n- **超参的选择：**\n\n> 假设句子长度为$$l$$，则$$n=\\alpha l$$，$$\\alpha$$表明了多少比例的词语会被改变。并且对于RD，我们使用$$p=\\alpha$$。对于每个句子，我们创造$$n_{aug}$$个扩充句子\n\n\n\n### 1.2 EDA不同模型上的表现\n\n- 可以看到，EDA在RNN和CNN上实现了准确率的提升，并且对于小数据集，提升更为明显\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808144843149.png\" alt=\"image-20220808144843149\" style=\"zoom: 80%;\" />\n\n\n\n### 1.3 不同数据集大小对EDA的影响\n\n- 作者对多个数据集进行了测试，并且在最后（图f）给出了在所有数据集上的平均结果，以探究不同大小的数据集对EDA效果的影响：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808145353524.png\" alt=\"image-20220808145353524\" style=\"zoom:90%;\" />\n\n- 在图f中，不使用EDA的最高准确率是88.3%，是在使用所有数据集时实现的。但是使用EDA时最高准确率为88.6%，**甚至只是用了一半的源数据**\n- **总的来说，EDA对于小数据集的影响更大**\n\n\n\n### 1.4 EDA是否会影响True Label\n\n- **作者的实验步骤是：**对于一个pro-con分类任务（PC），先不应用EDA进行训练，然后在测试集上，进行数据扩充（每个源数据扩充九个数据），将源数据和扩充数据一起输入模型测试，将最后一个dense层得到的向量使用t-SNE表示，然后得到如下结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808150515348.png\" alt=\"image-20220808150515348\" style=\"zoom:80%;\" />\n\n- 可以看到**扩充数据的潜在语义空间是接近源数据的**，所以对于多数情况，EDA是不会改变true label的\n\n\n\n### 1.5 消融实验\n\n- EDA是四种扩充方法的结合，而对于这四种方法，作者通过每次分别只使用一次方法，来探究四种方法各自的贡献和效果。并且对不同的$$\\alpha$$取值进行选取：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808151914033.png\" alt=\"image-20220808151914033\" style=\"zoom:90%;\" />\n\n- 四种操作都获得了一定的提升，其中：\n> - **对于SR：**使用小的$$\\alpha$$获得了提升，但是过大的$$\\alpha$$反而降低了表现，推测原因为：过多的替换改变了原本的句意\n> - **对于RI：**提升对于$$\\alpha$$的改变不是特别敏感，更为稳定，推测原因为：原本的词和相对位置保留了下来\n> - **对于RS：**在$$\\alpha \\le 0.2$$时获得较大提升，但在$$\\alpha \\ge 0.3$$时出现了下降，推测原因为：交换过多的词其实就等同于将整个句子词语的顺序重新排列一遍\n> - **对于RD：**小$$\\alpha$$有很大的提升，但是大的$$\\alpha$$十分影响表现，推测原因为：删除过多的词使句子变得无法理解\n\n- **通过实验，作者推荐通常取$$\\alpha=0.1$$**\n\n\n\n### 1.6 扩充几句最为合适\n\n- 其实就是对超参$$n_{aug}$$的选择，实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808154726577.png\" alt=\"image-20220808154726577\" style=\"zoom:80%;\" />\n\n- 可以看到，对于小数据集，$$n_{aug}$$最好大一些，而大数据集则不需要那么多扩充数据\n- 作者还给出了**推荐的超参：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808155215524.png\" alt=\"image-20220808155215524\" style=\"zoom:90%;\" />\n\n\n\n### 1.7 结论\n\n- 尽管EDA实现了一定的提升，尤其是在小数据集上，但是仍有一定的**限制**：\n\n> 1. 通过实验可以发现，**EDA在数据充足时，提升的效果是十分有限的**，基本都是1%不到\n> 2. 并且就算是使用小数据集，**在使用pre-trained model时，如BERT等，得到的提升也是十分微小的**\n\n- EDA的**本质作用**可以总结为以下两点：\n\n> 1. 产生了一定程度的噪音，来**阻止模型过拟合**\n> 2. 通过SR和RI操作，可以产生新的词典，使模型可以**泛化在测试集中而不在训练集中的词**\n\n\n\n\n\n# 2 AEDA\n\n### 2.1 AEDA的基本方法\n\n- 其实就是随机位置插入随机标点，**插入次数选择$$1 \\sim \\frac{1}{3}sentence\\_length$$的随机数**，插入的标点符号为：**{\".\", \";\", \"?\", \":\", \"!\", \",\"}**，举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125655875.png\" alt=\"image-20220809125655875\" style=\"zoom:67%;\" />\n\n- 对比一下EDA，**EDA的交换操作改变了文本顺序，并且删除操作会造成信息的损失，从而造成对模型的\"misleading\"**。而AEDA则会保留文本的顺序和词语。作者还做了详细的实验进行验证和对比\n\n\n\n### 2.2 EDA和AEDA的对比\n\n- 作者分别在CNN和RNN上进行了实验，进行数据扩充时，每个源数据扩充了16个数据，实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808175040407.png\" alt=\"image-20220808175040407\" style=\"zoom:75%;\" />\n\n- 可以看到EDA尽在小数据集上有所提升，但是在大数据集上表现更差了。但是AEDA在所有数据集上都有提升，尤其是在小数据集上更为明显。\n- 作者认为造成这种结果的原因是：EDA的替换和删除操作给模型增加了许多\"misleading\"的信息\n\n> The reason why EDA does not perform well can be attributed to the operations such as deletion and substitution which insert more misleading information to the network as the number of augmentations grows. In contrast, AEDA keeps the original information in all augmentations  \n\n- 此外，作者还通过不同的数据集，针对数据集大小展开了研究，结果如下：\n\n![image-20220809125001403](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125001403.png)\n\n\n\n### 2.3 扩充几句最为合适\n\n- 作者还探究了每个源数据扩充几句数据最为合适，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125426286.png\" alt=\"image-20220809125426286\" style=\"zoom:70%;\" />\n\n- 作者并没有在论文中指出最合适的超参，但是个人觉得大多数时候**扩充一到两句**就够了\n\n\n\n### 2.4 对于BERT的提升\n\n- 作者对于BERT模型，进行了加EDA和AEDA的对比，每个源数据只扩充了一句，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809132749401.png\" alt=\"image-20220809132749401\" style=\"zoom:80%;\" />\n\n- EDA反倒下降了表现（有可能是$$n_{aug}$$只有1），而AEDA实现了细微的提升（还是十分有限。。。）","slug":"EDA和AEDA","published":1,"updated":"2022-12-20T06:24:50.218Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1a00047cszckot1le4","content":"<ul>\n<li>本节主要介绍NLP领域的两种简单数据扩充方法：<strong>EDA和AEDA</strong></li>\n<li>还有许多其他的数据扩充方法， 例如将文本进行back-translation，即将文本翻译一次又翻译回去，从而扩充文本，还可以通过各种深度学习模型进行扩充。<strong>但是这些方法都太过”expensive”，而EDA和AEDA就相比之下比较简单，只需要在输入文本之前一定的预处理即可。</strong></li>\n</ul>\n<h1 id=\"1-EDA\"><a href=\"#1-EDA\" class=\"headerlink\" title=\"1 EDA\"></a>1 EDA</h1><h3 id=\"1-1-EDA的基本方法\"><a href=\"#1-1-EDA的基本方法\" class=\"headerlink\" title=\"1.1 EDA的基本方法\"></a>1.1 EDA的基本方法</h3><ul>\n<li><p><strong>EDA的基本方法包括四种：</strong></p>\n<blockquote>\n<ol>\n<li><strong>Synonym Replacement (SR，同义词替换)：</strong>随机挑选n个词<strong>（不能是停用词）</strong>，然后将每个词随机替换成同义词</li>\n<li><strong>Random Insertion (RI，随机插入)：</strong>挑选随机词<strong>（不能是停用词）</strong>的随机同义词，插入随机位置，进行n次</li>\n<li><strong>Random Swap (RS，随机交换)：</strong>随机挑选两个词，交换位置，进行n次</li>\n<li><strong>Random Deletion (RD，随机删除)：</strong>使用概率p随机删除每个词</li>\n</ol>\n</blockquote>\n</li>\n<li><p>EDA的做法是，<strong>对输入的句子进行改变，但是尽量不改变其句意，也就是使句意和true label尽量对应</strong>，所以使用同义词替换等方法来增加噪音，但不能增加过多。其中，对于长句子，相比于短句子，能吸收更多的噪音，更能保持true label</p>\n</li>\n<li>进行SR和RI时，不是选择随机词进行操作，而是使用同义词，<strong>目的就是为了尽量不改变原始句意</strong></li>\n<li><strong>超参的选择：</strong></li>\n</ul>\n<blockquote>\n<p>假设句子长度为<script type=\"math/tex\">l</script>，则<script type=\"math/tex\">n=\\alpha l</script>，<script type=\"math/tex\">\\alpha</script>表明了多少比例的词语会被改变。并且对于RD，我们使用<script type=\"math/tex\">p=\\alpha</script>。对于每个句子，我们创造<script type=\"math/tex\">n_{aug}</script>个扩充句子</p>\n</blockquote>\n<h3 id=\"1-2-EDA不同模型上的表现\"><a href=\"#1-2-EDA不同模型上的表现\" class=\"headerlink\" title=\"1.2 EDA不同模型上的表现\"></a>1.2 EDA不同模型上的表现</h3><ul>\n<li>可以看到，EDA在RNN和CNN上实现了准确率的提升，并且对于小数据集，提升更为明显</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808144843149.png\" alt=\"image-20220808144843149\" style=\"zoom: 80%;\" /></p>\n<h3 id=\"1-3-不同数据集大小对EDA的影响\"><a href=\"#1-3-不同数据集大小对EDA的影响\" class=\"headerlink\" title=\"1.3 不同数据集大小对EDA的影响\"></a>1.3 不同数据集大小对EDA的影响</h3><ul>\n<li>作者对多个数据集进行了测试，并且在最后（图f）给出了在所有数据集上的平均结果，以探究不同大小的数据集对EDA效果的影响：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808145353524.png\" alt=\"image-20220808145353524\" style=\"zoom:90%;\" /></p>\n<ul>\n<li>在图f中，不使用EDA的最高准确率是88.3%，是在使用所有数据集时实现的。但是使用EDA时最高准确率为88.6%，<strong>甚至只是用了一半的源数据</strong></li>\n<li><strong>总的来说，EDA对于小数据集的影响更大</strong></li>\n</ul>\n<h3 id=\"1-4-EDA是否会影响True-Label\"><a href=\"#1-4-EDA是否会影响True-Label\" class=\"headerlink\" title=\"1.4 EDA是否会影响True Label\"></a>1.4 EDA是否会影响True Label</h3><ul>\n<li><strong>作者的实验步骤是：</strong>对于一个pro-con分类任务（PC），先不应用EDA进行训练，然后在测试集上，进行数据扩充（每个源数据扩充九个数据），将源数据和扩充数据一起输入模型测试，将最后一个dense层得到的向量使用t-SNE表示，然后得到如下结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808150515348.png\" alt=\"image-20220808150515348\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>可以看到<strong>扩充数据的潜在语义空间是接近源数据的</strong>，所以对于多数情况，EDA是不会改变true label的</li>\n</ul>\n<h3 id=\"1-5-消融实验\"><a href=\"#1-5-消融实验\" class=\"headerlink\" title=\"1.5 消融实验\"></a>1.5 消融实验</h3><ul>\n<li>EDA是四种扩充方法的结合，而对于这四种方法，作者通过每次分别只使用一次方法，来探究四种方法各自的贡献和效果。并且对不同的<script type=\"math/tex\">\\alpha</script>取值进行选取：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808151914033.png\" alt=\"image-20220808151914033\" style=\"zoom:90%;\" /></p>\n<ul>\n<li><p>四种操作都获得了一定的提升，其中：</p>\n<blockquote>\n<ul>\n<li><strong>对于SR：</strong>使用小的<script type=\"math/tex\">\\alpha</script>获得了提升，但是过大的<script type=\"math/tex\">\\alpha</script>反而降低了表现，推测原因为：过多的替换改变了原本的句意</li>\n<li><strong>对于RI：</strong>提升对于<script type=\"math/tex\">\\alpha</script>的改变不是特别敏感，更为稳定，推测原因为：原本的词和相对位置保留了下来</li>\n<li><strong>对于RS：</strong>在<script type=\"math/tex\">\\alpha \\le 0.2</script>时获得较大提升，但在<script type=\"math/tex\">\\alpha \\ge 0.3</script>时出现了下降，推测原因为：交换过多的词其实就等同于将整个句子词语的顺序重新排列一遍</li>\n<li><strong>对于RD：</strong>小<script type=\"math/tex\">\\alpha</script>有很大的提升，但是大的<script type=\"math/tex\">\\alpha</script>十分影响表现，推测原因为：删除过多的词使句子变得无法理解</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>通过实验，作者推荐通常取<script type=\"math/tex\">\\alpha=0.1</script></strong></p>\n</li>\n</ul>\n<h3 id=\"1-6-扩充几句最为合适\"><a href=\"#1-6-扩充几句最为合适\" class=\"headerlink\" title=\"1.6 扩充几句最为合适\"></a>1.6 扩充几句最为合适</h3><ul>\n<li>其实就是对超参<script type=\"math/tex\">n_{aug}</script>的选择，实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808154726577.png\" alt=\"image-20220808154726577\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>可以看到，对于小数据集，<script type=\"math/tex\">n_{aug}</script>最好大一些，而大数据集则不需要那么多扩充数据</li>\n<li>作者还给出了<strong>推荐的超参：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808155215524.png\" alt=\"image-20220808155215524\" style=\"zoom:90%;\" /></p>\n<h3 id=\"1-7-结论\"><a href=\"#1-7-结论\" class=\"headerlink\" title=\"1.7 结论\"></a>1.7 结论</h3><ul>\n<li>尽管EDA实现了一定的提升，尤其是在小数据集上，但是仍有一定的<strong>限制</strong>：</li>\n</ul>\n<blockquote>\n<ol>\n<li>通过实验可以发现，<strong>EDA在数据充足时，提升的效果是十分有限的</strong>，基本都是1%不到</li>\n<li>并且就算是使用小数据集，<strong>在使用pre-trained model时，如BERT等，得到的提升也是十分微小的</strong></li>\n</ol>\n</blockquote>\n<ul>\n<li>EDA的<strong>本质作用</strong>可以总结为以下两点：</li>\n</ul>\n<blockquote>\n<ol>\n<li>产生了一定程度的噪音，来<strong>阻止模型过拟合</strong></li>\n<li>通过SR和RI操作，可以产生新的词典，使模型可以<strong>泛化在测试集中而不在训练集中的词</strong></li>\n</ol>\n</blockquote>\n<h1 id=\"2-AEDA\"><a href=\"#2-AEDA\" class=\"headerlink\" title=\"2 AEDA\"></a>2 AEDA</h1><h3 id=\"2-1-AEDA的基本方法\"><a href=\"#2-1-AEDA的基本方法\" class=\"headerlink\" title=\"2.1 AEDA的基本方法\"></a>2.1 AEDA的基本方法</h3><ul>\n<li>其实就是随机位置插入随机标点，<strong>插入次数选择<script type=\"math/tex\">1 \\sim \\frac{1}{3}sentence\\_length</script>的随机数</strong>，插入的标点符号为：<strong>{“.”, “;”, “?”, “:”, “!”, “,”}</strong>，举个栗子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125655875.png\" alt=\"image-20220809125655875\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>对比一下EDA，<strong>EDA的交换操作改变了文本顺序，并且删除操作会造成信息的损失，从而造成对模型的”misleading”</strong>。而AEDA则会保留文本的顺序和词语。作者还做了详细的实验进行验证和对比</li>\n</ul>\n<h3 id=\"2-2-EDA和AEDA的对比\"><a href=\"#2-2-EDA和AEDA的对比\" class=\"headerlink\" title=\"2.2 EDA和AEDA的对比\"></a>2.2 EDA和AEDA的对比</h3><ul>\n<li>作者分别在CNN和RNN上进行了实验，进行数据扩充时，每个源数据扩充了16个数据，实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808175040407.png\" alt=\"image-20220808175040407\" style=\"zoom:75%;\" /></p>\n<ul>\n<li>可以看到EDA尽在小数据集上有所提升，但是在大数据集上表现更差了。但是AEDA在所有数据集上都有提升，尤其是在小数据集上更为明显。</li>\n<li>作者认为造成这种结果的原因是：EDA的替换和删除操作给模型增加了许多”misleading”的信息</li>\n</ul>\n<blockquote>\n<p>The reason why EDA does not perform well can be attributed to the operations such as deletion and substitution which insert more misleading information to the network as the number of augmentations grows. In contrast, AEDA keeps the original information in all augmentations  </p>\n</blockquote>\n<ul>\n<li>此外，作者还通过不同的数据集，针对数据集大小展开了研究，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125001403.png\" alt=\"image-20220809125001403\"></p>\n<h3 id=\"2-3-扩充几句最为合适\"><a href=\"#2-3-扩充几句最为合适\" class=\"headerlink\" title=\"2.3 扩充几句最为合适\"></a>2.3 扩充几句最为合适</h3><ul>\n<li>作者还探究了每个源数据扩充几句数据最为合适，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125426286.png\" alt=\"image-20220809125426286\" style=\"zoom:70%;\" /></p>\n<ul>\n<li>作者并没有在论文中指出最合适的超参，但是个人觉得大多数时候<strong>扩充一到两句</strong>就够了</li>\n</ul>\n<h3 id=\"2-4-对于BERT的提升\"><a href=\"#2-4-对于BERT的提升\" class=\"headerlink\" title=\"2.4 对于BERT的提升\"></a>2.4 对于BERT的提升</h3><ul>\n<li>作者对于BERT模型，进行了加EDA和AEDA的对比，每个源数据只扩充了一句，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809132749401.png\" alt=\"image-20220809132749401\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>EDA反倒下降了表现（有可能是<script type=\"math/tex\">n_{aug}</script>只有1），而AEDA实现了细微的提升（还是十分有限。。。）</li>\n</ul>\n","site":{"data":{}},"wordcount":2543,"excerpt":"","more":"<ul>\n<li>本节主要介绍NLP领域的两种简单数据扩充方法：<strong>EDA和AEDA</strong></li>\n<li>还有许多其他的数据扩充方法， 例如将文本进行back-translation，即将文本翻译一次又翻译回去，从而扩充文本，还可以通过各种深度学习模型进行扩充。<strong>但是这些方法都太过”expensive”，而EDA和AEDA就相比之下比较简单，只需要在输入文本之前一定的预处理即可。</strong></li>\n</ul>\n<h1 id=\"1-EDA\"><a href=\"#1-EDA\" class=\"headerlink\" title=\"1 EDA\"></a>1 EDA</h1><h3 id=\"1-1-EDA的基本方法\"><a href=\"#1-1-EDA的基本方法\" class=\"headerlink\" title=\"1.1 EDA的基本方法\"></a>1.1 EDA的基本方法</h3><ul>\n<li><p><strong>EDA的基本方法包括四种：</strong></p>\n<blockquote>\n<ol>\n<li><strong>Synonym Replacement (SR，同义词替换)：</strong>随机挑选n个词<strong>（不能是停用词）</strong>，然后将每个词随机替换成同义词</li>\n<li><strong>Random Insertion (RI，随机插入)：</strong>挑选随机词<strong>（不能是停用词）</strong>的随机同义词，插入随机位置，进行n次</li>\n<li><strong>Random Swap (RS，随机交换)：</strong>随机挑选两个词，交换位置，进行n次</li>\n<li><strong>Random Deletion (RD，随机删除)：</strong>使用概率p随机删除每个词</li>\n</ol>\n</blockquote>\n</li>\n<li><p>EDA的做法是，<strong>对输入的句子进行改变，但是尽量不改变其句意，也就是使句意和true label尽量对应</strong>，所以使用同义词替换等方法来增加噪音，但不能增加过多。其中，对于长句子，相比于短句子，能吸收更多的噪音，更能保持true label</p>\n</li>\n<li>进行SR和RI时，不是选择随机词进行操作，而是使用同义词，<strong>目的就是为了尽量不改变原始句意</strong></li>\n<li><strong>超参的选择：</strong></li>\n</ul>\n<blockquote>\n<p>假设句子长度为<script type=\"math/tex\">l</script>，则<script type=\"math/tex\">n=\\alpha l</script>，<script type=\"math/tex\">\\alpha</script>表明了多少比例的词语会被改变。并且对于RD，我们使用<script type=\"math/tex\">p=\\alpha</script>。对于每个句子，我们创造<script type=\"math/tex\">n_{aug}</script>个扩充句子</p>\n</blockquote>\n<h3 id=\"1-2-EDA不同模型上的表现\"><a href=\"#1-2-EDA不同模型上的表现\" class=\"headerlink\" title=\"1.2 EDA不同模型上的表现\"></a>1.2 EDA不同模型上的表现</h3><ul>\n<li>可以看到，EDA在RNN和CNN上实现了准确率的提升，并且对于小数据集，提升更为明显</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808144843149.png\" alt=\"image-20220808144843149\" style=\"zoom: 80%;\" /></p>\n<h3 id=\"1-3-不同数据集大小对EDA的影响\"><a href=\"#1-3-不同数据集大小对EDA的影响\" class=\"headerlink\" title=\"1.3 不同数据集大小对EDA的影响\"></a>1.3 不同数据集大小对EDA的影响</h3><ul>\n<li>作者对多个数据集进行了测试，并且在最后（图f）给出了在所有数据集上的平均结果，以探究不同大小的数据集对EDA效果的影响：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808145353524.png\" alt=\"image-20220808145353524\" style=\"zoom:90%;\" /></p>\n<ul>\n<li>在图f中，不使用EDA的最高准确率是88.3%，是在使用所有数据集时实现的。但是使用EDA时最高准确率为88.6%，<strong>甚至只是用了一半的源数据</strong></li>\n<li><strong>总的来说，EDA对于小数据集的影响更大</strong></li>\n</ul>\n<h3 id=\"1-4-EDA是否会影响True-Label\"><a href=\"#1-4-EDA是否会影响True-Label\" class=\"headerlink\" title=\"1.4 EDA是否会影响True Label\"></a>1.4 EDA是否会影响True Label</h3><ul>\n<li><strong>作者的实验步骤是：</strong>对于一个pro-con分类任务（PC），先不应用EDA进行训练，然后在测试集上，进行数据扩充（每个源数据扩充九个数据），将源数据和扩充数据一起输入模型测试，将最后一个dense层得到的向量使用t-SNE表示，然后得到如下结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808150515348.png\" alt=\"image-20220808150515348\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>可以看到<strong>扩充数据的潜在语义空间是接近源数据的</strong>，所以对于多数情况，EDA是不会改变true label的</li>\n</ul>\n<h3 id=\"1-5-消融实验\"><a href=\"#1-5-消融实验\" class=\"headerlink\" title=\"1.5 消融实验\"></a>1.5 消融实验</h3><ul>\n<li>EDA是四种扩充方法的结合，而对于这四种方法，作者通过每次分别只使用一次方法，来探究四种方法各自的贡献和效果。并且对不同的<script type=\"math/tex\">\\alpha</script>取值进行选取：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808151914033.png\" alt=\"image-20220808151914033\" style=\"zoom:90%;\" /></p>\n<ul>\n<li><p>四种操作都获得了一定的提升，其中：</p>\n<blockquote>\n<ul>\n<li><strong>对于SR：</strong>使用小的<script type=\"math/tex\">\\alpha</script>获得了提升，但是过大的<script type=\"math/tex\">\\alpha</script>反而降低了表现，推测原因为：过多的替换改变了原本的句意</li>\n<li><strong>对于RI：</strong>提升对于<script type=\"math/tex\">\\alpha</script>的改变不是特别敏感，更为稳定，推测原因为：原本的词和相对位置保留了下来</li>\n<li><strong>对于RS：</strong>在<script type=\"math/tex\">\\alpha \\le 0.2</script>时获得较大提升，但在<script type=\"math/tex\">\\alpha \\ge 0.3</script>时出现了下降，推测原因为：交换过多的词其实就等同于将整个句子词语的顺序重新排列一遍</li>\n<li><strong>对于RD：</strong>小<script type=\"math/tex\">\\alpha</script>有很大的提升，但是大的<script type=\"math/tex\">\\alpha</script>十分影响表现，推测原因为：删除过多的词使句子变得无法理解</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>通过实验，作者推荐通常取<script type=\"math/tex\">\\alpha=0.1</script></strong></p>\n</li>\n</ul>\n<h3 id=\"1-6-扩充几句最为合适\"><a href=\"#1-6-扩充几句最为合适\" class=\"headerlink\" title=\"1.6 扩充几句最为合适\"></a>1.6 扩充几句最为合适</h3><ul>\n<li>其实就是对超参<script type=\"math/tex\">n_{aug}</script>的选择，实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808154726577.png\" alt=\"image-20220808154726577\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>可以看到，对于小数据集，<script type=\"math/tex\">n_{aug}</script>最好大一些，而大数据集则不需要那么多扩充数据</li>\n<li>作者还给出了<strong>推荐的超参：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808155215524.png\" alt=\"image-20220808155215524\" style=\"zoom:90%;\" /></p>\n<h3 id=\"1-7-结论\"><a href=\"#1-7-结论\" class=\"headerlink\" title=\"1.7 结论\"></a>1.7 结论</h3><ul>\n<li>尽管EDA实现了一定的提升，尤其是在小数据集上，但是仍有一定的<strong>限制</strong>：</li>\n</ul>\n<blockquote>\n<ol>\n<li>通过实验可以发现，<strong>EDA在数据充足时，提升的效果是十分有限的</strong>，基本都是1%不到</li>\n<li>并且就算是使用小数据集，<strong>在使用pre-trained model时，如BERT等，得到的提升也是十分微小的</strong></li>\n</ol>\n</blockquote>\n<ul>\n<li>EDA的<strong>本质作用</strong>可以总结为以下两点：</li>\n</ul>\n<blockquote>\n<ol>\n<li>产生了一定程度的噪音，来<strong>阻止模型过拟合</strong></li>\n<li>通过SR和RI操作，可以产生新的词典，使模型可以<strong>泛化在测试集中而不在训练集中的词</strong></li>\n</ol>\n</blockquote>\n<h1 id=\"2-AEDA\"><a href=\"#2-AEDA\" class=\"headerlink\" title=\"2 AEDA\"></a>2 AEDA</h1><h3 id=\"2-1-AEDA的基本方法\"><a href=\"#2-1-AEDA的基本方法\" class=\"headerlink\" title=\"2.1 AEDA的基本方法\"></a>2.1 AEDA的基本方法</h3><ul>\n<li>其实就是随机位置插入随机标点，<strong>插入次数选择<script type=\"math/tex\">1 \\sim \\frac{1}{3}sentence\\_length</script>的随机数</strong>，插入的标点符号为：<strong>{“.”, “;”, “?”, “:”, “!”, “,”}</strong>，举个栗子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125655875.png\" alt=\"image-20220809125655875\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>对比一下EDA，<strong>EDA的交换操作改变了文本顺序，并且删除操作会造成信息的损失，从而造成对模型的”misleading”</strong>。而AEDA则会保留文本的顺序和词语。作者还做了详细的实验进行验证和对比</li>\n</ul>\n<h3 id=\"2-2-EDA和AEDA的对比\"><a href=\"#2-2-EDA和AEDA的对比\" class=\"headerlink\" title=\"2.2 EDA和AEDA的对比\"></a>2.2 EDA和AEDA的对比</h3><ul>\n<li>作者分别在CNN和RNN上进行了实验，进行数据扩充时，每个源数据扩充了16个数据，实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220808175040407.png\" alt=\"image-20220808175040407\" style=\"zoom:75%;\" /></p>\n<ul>\n<li>可以看到EDA尽在小数据集上有所提升，但是在大数据集上表现更差了。但是AEDA在所有数据集上都有提升，尤其是在小数据集上更为明显。</li>\n<li>作者认为造成这种结果的原因是：EDA的替换和删除操作给模型增加了许多”misleading”的信息</li>\n</ul>\n<blockquote>\n<p>The reason why EDA does not perform well can be attributed to the operations such as deletion and substitution which insert more misleading information to the network as the number of augmentations grows. In contrast, AEDA keeps the original information in all augmentations  </p>\n</blockquote>\n<ul>\n<li>此外，作者还通过不同的数据集，针对数据集大小展开了研究，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125001403.png\" alt=\"image-20220809125001403\"></p>\n<h3 id=\"2-3-扩充几句最为合适\"><a href=\"#2-3-扩充几句最为合适\" class=\"headerlink\" title=\"2.3 扩充几句最为合适\"></a>2.3 扩充几句最为合适</h3><ul>\n<li>作者还探究了每个源数据扩充几句数据最为合适，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809125426286.png\" alt=\"image-20220809125426286\" style=\"zoom:70%;\" /></p>\n<ul>\n<li>作者并没有在论文中指出最合适的超参，但是个人觉得大多数时候<strong>扩充一到两句</strong>就够了</li>\n</ul>\n<h3 id=\"2-4-对于BERT的提升\"><a href=\"#2-4-对于BERT的提升\" class=\"headerlink\" title=\"2.4 对于BERT的提升\"></a>2.4 对于BERT的提升</h3><ul>\n<li>作者对于BERT模型，进行了加EDA和AEDA的对比，每个源数据只扩充了一句，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809132749401.png\" alt=\"image-20220809132749401\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>EDA反倒下降了表现（有可能是<script type=\"math/tex\">n_{aug}</script>只有1），而AEDA实现了细微的提升（还是十分有限。。。）</li>\n</ul>\n"},{"title":"GPT","math":"ture","date":"2023-03-09T16:00:00.000Z","_content":"\n\n\n# 1 GPT v1\n\n- GPT采用无监督预训练+下游任务微调的方法\n\n### 1.1 模型结构\n- 采用12个堆叠的Transformer的Decoder块（去除了和encoder连接的那个Multi-Head）：\n\n  <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_1.png\" alt=\"IMG_1\" style=\"zoom:40%;\" />\n\n\n\n### 1.2 模型训练目标\n\n##### 1.2.1 无监督预训练目标\n- 无监督预训练采用的是LM（语言模型）的训练方法，采用n元语法：\n$$\nL_{1}(\\mathcal{U})=\\sum_{i} \\log P\\left(u_{i} \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right)\n$$\n其中k即n元语法中的n，具**体实现中k是取最大，即表示使用前面的所有词（个人觉得他这里说的有点歧义）**，$$\\Theta$$是模型参数\n- 具体到模型实现上， 类似于word2vec的实现方法，当要预测当前时间步的词u时，采用前面所有的词$$U = (u_{-k}, ..., u_{-1})$$来进行预测：\n$$\n\\begin{aligned}\nh_{0} & =U W_{e}+W_{p} \\\\\nh_{l} & =\\text { transformer_block }\\left(h_{l-1}\\right) \\forall i \\in[1, n] \\\\\nP(u) & =\\operatorname{softmax}\\left(h_{n} W_{e}^{T}\\right)\n\\end{aligned}\n$$\n其中$$W_e \\in (vocab\\_size, embedding\\_dim)$$是embedding矩阵，$$W_p \\in (seq\\_len, embedding\\_dim)$$是**学习到的**位置编码，n表示Transformer层数。**注意最后还是乘的$$W_e$$表示使用了Weight Tying**。具体实现是和Transformer一样的\n\n##### 1.2.2 有监督微调\n- 有监督任务一般都是在最后接一个全连接，训练目标是：\n$$\nL_{2}(\\mathcal{C})=\\sum_{(x, y)} \\log P\\left(y \\mid x^{1}, \\ldots, x^{m}\\right)\n$$\n其中x是输入，y是label\n- 在微调的时候，作者还加入了LM无监督任务作为额外目标，那么微调时的训练目标变为：\n$$\nL_{3}(\\mathcal{C})=L_{2}(\\mathcal{C})+\\lambda * L_{1}(\\mathcal{C})\n$$\n其中$$\\lambda$$表示权重\n- **这么做的优点是：**可以增加模型泛化能力和收敛速度，后面作者还对此做了消融实验\n\n\n\n### 1.3 微调具体实现方法\n\n- GPT针对不同类型的下游任务，其做法是不同的。尤其是**由于在预训练时，是在连续通顺文本上训练的，所以在下游任务上有多个输入时，句子之间的相对顺序尤为重要**\n- 最初的输入还要加三个特殊token：起始token（\\<s\\>）、分隔token（$）、结束token（\\<e\\>）\n- **方法汇总：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_2.png\" alt=\"IMG_2\" style=\"zoom: 67%;\" />\n\n- 在做Textual entailment任务时，由于前提p和假设h是有前后文关系的，所以直接p在前h在后即可，中间用$做分隔\n- 在做Similarity任务时，因为没有明确前后文关系，所以将两种排列顺序分别通过模型，最后将输出结果按元素相加，再喂入mlp\n- 在做QA或尝试推理这类多选择任务时，上下文在前，选择在后，如给定背景上下文z、问题q、回答集$$\\{a_k\\}$$，那么分别构造$$[z;q;\\$;a_k]$$作为输入。最后将结果通过softmax映射为概率\n\n\n\n### 1.4 模型训练\n\n##### 1.4.1 无监督预训练\n- 采用Adam算法，并且加了warm up，最大学习率为2.5e-4\n- epoch = 100，batch size = 64\n- 采用$$N(0, 0.02)$$进行参数初始化，由于含有Layer Norm，所以初始化不需要太关注\n- 激活函数采用GELU\n\n##### 1.4.2 有监督微调\n- 在mlp中也加入了dropout\n- learning rate = 6.25e-5，batch size = 32, epochs = 3\n- 采用线性学习率衰减，在0.2%的训练中使用了warm up，超参$$\\lambda = 0.5$$\n\n\n\n### 1.5 下游任务表现\n- **NLI任务：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_3.png\" alt=\"IMG_3\" style=\"zoom: 50%;\" />\n\n- **QA && 常识推理：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_4.png\" alt=\"IMG_4\" style=\"zoom: 50%;\" />\n\n- **语义相似 && 分类任务：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_5.png\" alt=\"IMG_5\" style=\"zoom:50%;\" />\n\n\n\n### 1.6 消融实验\n\n##### 1.6.1 迁移的decoder个数的影响\n- 将预训练之后的模型的一部分decoder用于下游任务，得到结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_6.png\" alt=\"IMG_6\" style=\"zoom: 60%;\" />\n\n由上图可知模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间\n\n- **结论：**预训练得到的每个decoder都是对下游任务有作用的（个人觉得就是模型表达能力更加强大，并且不同的decoder所包含的知识是不同的）\n\n##### 1.6.2 预训练的作用\n- 作者去除了微调，以验证模型的zero-shot能力（没有进行过下游任务训练，而在下游的表现），并且和LSTM进行了比较（同样没有进行下游任务）：  \n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_7.png\" alt=\"IMG_7\" style=\"zoom:60%;\" />\n\n- **结论：**生成式预训练任务是提升其语言建模能力，可以支持各种各样的下游相关任务。并且与 LSTM 相比，Transformer 的结构化注意力记忆有助于迁移\n\n##### 1.6.3 其他实验\n- 作者还探究了**微调时将LM作为额外目标的作用、将模型换为LSTM的对比、pre-training的作用**：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_8.png\" alt=\"IMG_8\" style=\"zoom:50%;\" />\n\n- **结论：**\n1. LM额外目标在大数据集上有提升，但是小数据集上没有\n2. pre-training不可缺少\n\n\n\n\n\n# 2 GPT v2\n\n### 2.1 主要思想\n- GPT2主要着眼于**只使用无监督的LM训练任务，来使模型具有zero-shot能力，不使用有监督数据微调，直接应用于下游任务**\n- 本篇文章的核心观点就是：**只要无监督数据量足够大且足够多样，那么有监督任务就是无监督任务的子集。从一个尽可能大且多样化的数据集中一定能收集到不同领域不同任务相关的自然语言描述示例**\n> Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible.\n- 举个例子：\n> 1. 比如我在训练语言模型时，有一句话“The translation of word Machine Learning in chinese is 机器学习”，那在训练完这句话时，语言模型就自然地将翻译任务和任务的输入输出都学到了\n> 2. 再比如，又碰到一句话“美国的总统是特朗普”，这一句话训练完，也就是一个小的问答了\n> 3. 文章也给了用于训练的WebText Dataset中的英法互译真实实例：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_9.png\" alt=\"IMG_9\" style=\"zoom:50%;\" />\n- 还有一个需要注意的点是，在下游任务时，**由于预训练的预料都会自然、通顺的语言形式，所以下游任务的输入文本也需要重新构造为自然、通顺的形式**，如：\n> 机器翻译任务：translate to french, { english text }, { french text }\n> 阅读理解任务：answer the question, { document }, { question }, { answer }\n\n\n\n### 2.2 训练目标\n\n- GPT2的训练目标仍是LM，但是下游任务的建模发生了一些转变\n- 一般的有监督任务是在估计分布：\n$$\nP(output|input)\n$$\n- 然而GPT2由于是要用同一个模型进行多任务，所以建模变为：\n$$\nP(output|input, task)\n$$\n对于output的估计还要基于具体是什么任务，相同的输入，不同的任务，所产生的output可能是不同的\n- 针对不同任务，具体做法的话，就是上文提到的，将有监督数据构造为自然语言形式\n\n\n\n### 2.3 模型结构\n\n- 大体结构还是和GPT1一样，但是做了如下改动：\n1. Layer Norm由每个sub-block之后，移到了之前：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_11.png\" alt=\"IMG_11\" style=\"zoom:50%;\" />\n\n2. 在模型最后一个自注意力层之后，额外增加一个Layer Norm\n3. 根据残差块的数量，减少了residual path所对应的权重，具体来说，模型一共有N个残差块，那么residual path的权重就都要乘$$1 / \\sqrt{N}$$\n4. 词汇量增加到50257，上下文大小从512增加到1024，batch size增加到512\n- 模型结构大致如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_10.png\" alt=\"IMG_10\" style=\"zoom:40%;\" />\n\n\n\n### 2.4 实验结果\n\n- 在实验效果上，由于 GPT-2 主要是做 zero-shot，所以在实验部分，很多的实验对比都是在无监督的设定下进行的，也就是说他对比的都是无监督的算法\n\n![image-20230329234220217](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230329234220217.png)\n\n- GPT-2 在较多任务上对比无监督算法取得了一定的提升，证明了 zero-shot 的能力。但是，在很多任务上与有监督微调的方法相比还是有一些差距的\n\n\n\n# 3 GPT v3\n\n- GPT3不再像GPT2一样完全主推zero-shot，**而是采用few-shot，采用少量的有监督样本（一般10～100）来辅助模型进行推理**。但是，**GPT3采用有监督样本仅用于推理预测的时候，而不会进行微调的参数更新**\n\n\n\n### 3.1 模型结构\n\n- GPT3采用和GPT2一样的结构，**但是将其中的注意力机制变为了Sparse Attention**\n- 传统的Attention是每个token之间两两计算attentino，复杂度为$$O(n^2)$$\n- 而Sparse Attention除了相对距离不超过 k 以及相对距离为 k，2k，3k，... 的 token，其他所有 token 的注意力都设为 0，如下图所示：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_14.png\" alt=\"IMG_14\" style=\"zoom: 80%;\" />\n\n其计算复杂度为$$O(n * \\log n)$$\n- Sparse Attention的好处：\n> 1. **减少注意力层的计算复杂度**，节约显存和耗时，从而能够处理更长的输入序列\n> 2. **具有“局部紧密相关和远程稀疏相关”的特性**，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少\n- 最后实验了不同规模的模型：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_15.png\" alt=\"IMG_15\" style=\"zoom: 40%;\" />\n\n\n\n### 3.2 下游评估方法\n- 具体到下游任务是，采用了三种不同的方法**（注意这三种方法都只用于推理预测，不会进行参数更新）**：\n> 1. **Zero-shot：**仅使用当前任务的自然语言描述，不进行任何梯度更新\n> 2. **One-shot：**当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新\n> 3. **Few-shot：**当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新，也被称为**in-context learning（上下文学习）**\n- **和fine-tune的对比：** \n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_16.png\" alt=\"IMG_16\" style=\"zoom: 40%;\" />\n\nFew-shot虽然和fine-tune一样都用到多个有监督数据，但是其数据量的需要较少（一般10～100个数据），摒弃不进行参数更新\n\n- **三种方法对比的实验效果：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_17.png\" alt=\"IMG_17\" style=\"zoom: 40%;\" />\n\n\n\n### 3.3 训练数据\n\n- GPT-3 使用了多个数据集，其中最大的是 Common Crawl，原始未处理的数据达到了 45TB，其实在 GPT-2 的时候他们就有考虑使用这个数据集，但是后来还是觉得这个**数据集太脏了**所以没用，但是现在 GPT-3 的模型规模太大了，使得训练对数据量的需求也增加了很多，他们不得不重新考虑这个数据集。因此，他们必须在这个数据集上做一些额外的数据清洗工作来尽量保证数据的质量\n- **数据处理包括：**\n1. 采用GPT2中的WebText、Wikiedia等高质量文本作为正样本，用Common Crawl中的样本作为负样本，训练一个LR二分类器，然后采用这个分类器对Common Crawl采样，只保留其中的正样本\n2. 采用MinHashLSH算法，进行相似文本的去重，减少了大约10%的样本\n3. 加入其他的高质量数据集，不同数据集是通过不同的权重进行采样：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_18.png\" alt=\"IMG_18\" style=\"zoom:50%;\" />\n\n\n\n### 3.4 GPT3的局限性\n1. 当生成文本长度较长时，GPT-3 还是会出现各种问题，比如重复生成一段话，前后矛盾，逻辑衔接不好等等；\n2. 模型和结构的局限性，对于某一些任务，比如填空类型的文本任务，使用单向的自回归语言模型确实存在一定的局限性，这时候如果同时考虑上文和下文的话，效果很可能会更好一些；\n3. 预训练语言模型的通病，在训练时，语料中所有的词都被同等看待，对于一些虚词或无意义的词同样需要花费很多计算量去学习，无法区分学习重点；\n4. 样本有效性或者利用率过低，训一个模型几乎要把整个互联网上的文本数据全都用起来，这与我们人类学习时所需要的成本存在非常大的差异，这方面也是未来人工智能研究的重点；\n5. 有一个不太确定的点是，模型到底是在“学习”还是在“记忆”？我们当然希望它能够学习，但是在使用数据量如此大的情况下，很难去判断它到底是什么样的；\n6. 众所周知，GPT-3 的训练和使用成本都太大了；\n7. GPT-3 跟很多深度学习模型一样，都是不可解释的，没办法知道模型内部到底是如何作出一系列决策的；\n8. 训练数据中可能存在种族、性别等偏见，导致模型也会有这种偏见\n\n\n\n\n\n# 4 InstructGPT\n\n### 4.1 GPT存在的问题\n- GPT的训练方式是采用LM的方法，是估计下一个时间步的词的概率分布：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_19.png\" alt=\"IMG_19\" style=\"zoom:67%;\" />\n\n- 但是由于这是一个概率分布，所以模型的一些输入可能并不符合人类的预期，比如：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_20.png\" alt=\"IMG_20\" style=\"zoom:67%;\" />\n\n- **对于上述问题的解决方案有两种：**\n> 1. 在训练数据上，构造更加好的问答数据集，但是所花费的人工成本极大，因为训练数据集很大\n> 2. 引入一个“老师”，让老师对GPT生成的回答进行打分排序，告诉模型人类更期望哪种结果**（这里的老师既可以是真人，也就是使用在线学习；也可以训练一个Reward Model来对模型结果自动打分排序）**\n\n\n\n### 4.2 实现方案\n\n- 模型通过**三个不同的数据集**，完成了三个子任务：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_21.png\" alt=\"IMG_21\" style=\"zoom:55%;\" />\n\n1. **有监督微调（SFT）：**通过常见的prompt和labeler编写的response，来对GPT-3进行LM任务微调\n2. **训练奖励模型（RM）：**通过常见的prompt，让SFT微调后的GPT模型生成多个response，labeler对这些response进行排序。再使用这些prompt+ response对，输入GPT进行打分\n3. **强化学习（RL）：**只需要prompt，不需要有监督，采用PPO算法，再次微调SFT微调后的模型\n\n\n\n### 4.3 训练数据\n\n- 训练数据所用到的prompt来自两部分：\n> 1. labeler先构造了一批prompt和对应的response，对GPT-3进行微调，然后上线内测\n> 2. 将内测用户的prompt又收集起来，由labeler撰写response\n- 然后将两部分数据分为三个子任务的数据集： \n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_22.png\" alt=\"IMG_22\" style=\"zoom: 67%;\" />\n\n注意，最后RL的数据集是只有prompt，且只包含收集的用户的prompt，而SFT和RM是两者都有\n\n\n\n### 4.4 实现细节\n\n##### 4.4.1 SFT\n- 方法很简单，就不多赘述\n- 值得注意的是，作者在SFT中一共训练了16个epoch，但是发现在第一个epoch后就过拟合了（这么大的模型用这么小的数据肯定过拟合）。**但是由于这个模型并不是微调完就直接拿来用，所以过拟合也没关系。甚至更多的epoch甚至能产生更高的RM分数的输出**\n\n\n\n##### 4.4.2 RM\n\n- 先采用SFT后的模型，对一个prompt生成多个response，并对每一对prompt+response，让labeler进行排序：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_23.png\" alt=\"IMG_23\" style=\"zoom: 25%;\" />\n\n- 然后将SFT后的模型最后的输出层去掉，转而变为一个只有一个神经元的线性层\n- 将每一对prompt+response连结起来，输入该模型，最后输出相当于两者契合的logit分数。然后采用以下损失函数进行优化：\n$$\n\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c}\nK \\\\\n2\n\\end{array}\\right)} E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_{\\theta}\\left(x, y_{w}\\right)-r_{\\theta}\\left(x, y_{l}\\right)\\right)\\right)\\right]\n$$\n其中，K是每个prompt生成的response数量，$$y_w, y_l$$分别是prompt输入x的输出response，且$$rank(y_w) \\ge rank(y_l)$$，外层函数就相当于一个Logestic Regression\n- RM是采用的6B的模型，因为作者发现**大模型（比如175B）训练后期loss不稳定**\n- 此外，作者还提出了另一种方法：采用交叉熵，将排名第一的输出当作正样本，其他输出当作负样本，但是**非常容易过拟合**\n\n\n\n##### 4.4.3 RL\n\n- RL涉及三个模型：RM模型$$r_{\\theta}$$、SFT模型$$\\pi^{SFT}$$和我们最终想要得到的RL模型$$\\pi^{RL}$$；以及两个数据集RL自身的数据集$$D_{RL}$$和预训练时的一部分数据集$$D_{pretrain}$$\n- 优化目标如下：\n$$\n\\begin{aligned}\n\\operatorname{objective}(\\phi)= & E_{(x, y) \\sim D_{\\pi_{\\phi}^{\\mathrm{RL}}}}\\left[r_{\\theta}(x, y)-\\beta \\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\\n& \\gamma E_{x \\sim D_{\\text {prerrain }}}\\left[\\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(x)\\right)\\right]\n\\end{aligned}\n$$\n- 一开始$$\\pi^{RL}$$是由$$\\pi^{SFT}$$初始化得来的\n- 对于第一项，是想让$$\\pi^{RL}$$的输出得到的RM分数尽可能高，并且在这个微调过程中，$$\\pi^{RL}$$和$$\\pi^{SFT}$$的差距不能过大，所以减去两者的KL散度来保证这个差距\n- 如果只使用第一项，方法就称作PPO。但是为了防止模型遗忘预训练时的知识，引入第二项，也就是预训练任务的优化目标，加入第二项后则称为PPO-ptx\n\n\n\n### 4.5 实验结果\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_24.png\" alt=\"IMG_24\" style=\"zoom: 80%;\" />\n\n\n\n\n# 5 GPT vs BERT\n- 编码器和解码器的选取倒不是 GPT 和 BERT 的区别，它们的区别主要是预训练目标函数的选取，有人认为 GPT 选择的是一个更难的训练目标，它是根据前面的信息去预测下文，预测未来肯定是比完形填空难度要更大的。这也能从某种程度上解释了为什么相同规模的 GPT 和 BERT 模型，GPT 的效果要比 BERT 差。\n- 但是从另一个角度去想，如果能够把预测未来这个事情做好的话，它最终所能达到的效果的天花板一定是更高的，这可能也是 OpenAI 从一开始到现在一直坚持使用标准语言模型目标函数来做预训练模型的其中一个原因吧，当然这只是一种猜想。事实证明，从 GPT-3 开始，到最近的 ChatGPT，OpenAI 所取得的令人惊艳的效果也一定程度上证明了他们的选择的正确性。\n\n\n","source":"_posts/GPT.md","raw":"---\ntitle: GPT\nmath: ture\ndate: 2023-3-10\n---\n\n\n\n# 1 GPT v1\n\n- GPT采用无监督预训练+下游任务微调的方法\n\n### 1.1 模型结构\n- 采用12个堆叠的Transformer的Decoder块（去除了和encoder连接的那个Multi-Head）：\n\n  <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_1.png\" alt=\"IMG_1\" style=\"zoom:40%;\" />\n\n\n\n### 1.2 模型训练目标\n\n##### 1.2.1 无监督预训练目标\n- 无监督预训练采用的是LM（语言模型）的训练方法，采用n元语法：\n$$\nL_{1}(\\mathcal{U})=\\sum_{i} \\log P\\left(u_{i} \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right)\n$$\n其中k即n元语法中的n，具**体实现中k是取最大，即表示使用前面的所有词（个人觉得他这里说的有点歧义）**，$$\\Theta$$是模型参数\n- 具体到模型实现上， 类似于word2vec的实现方法，当要预测当前时间步的词u时，采用前面所有的词$$U = (u_{-k}, ..., u_{-1})$$来进行预测：\n$$\n\\begin{aligned}\nh_{0} & =U W_{e}+W_{p} \\\\\nh_{l} & =\\text { transformer_block }\\left(h_{l-1}\\right) \\forall i \\in[1, n] \\\\\nP(u) & =\\operatorname{softmax}\\left(h_{n} W_{e}^{T}\\right)\n\\end{aligned}\n$$\n其中$$W_e \\in (vocab\\_size, embedding\\_dim)$$是embedding矩阵，$$W_p \\in (seq\\_len, embedding\\_dim)$$是**学习到的**位置编码，n表示Transformer层数。**注意最后还是乘的$$W_e$$表示使用了Weight Tying**。具体实现是和Transformer一样的\n\n##### 1.2.2 有监督微调\n- 有监督任务一般都是在最后接一个全连接，训练目标是：\n$$\nL_{2}(\\mathcal{C})=\\sum_{(x, y)} \\log P\\left(y \\mid x^{1}, \\ldots, x^{m}\\right)\n$$\n其中x是输入，y是label\n- 在微调的时候，作者还加入了LM无监督任务作为额外目标，那么微调时的训练目标变为：\n$$\nL_{3}(\\mathcal{C})=L_{2}(\\mathcal{C})+\\lambda * L_{1}(\\mathcal{C})\n$$\n其中$$\\lambda$$表示权重\n- **这么做的优点是：**可以增加模型泛化能力和收敛速度，后面作者还对此做了消融实验\n\n\n\n### 1.3 微调具体实现方法\n\n- GPT针对不同类型的下游任务，其做法是不同的。尤其是**由于在预训练时，是在连续通顺文本上训练的，所以在下游任务上有多个输入时，句子之间的相对顺序尤为重要**\n- 最初的输入还要加三个特殊token：起始token（\\<s\\>）、分隔token（$）、结束token（\\<e\\>）\n- **方法汇总：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_2.png\" alt=\"IMG_2\" style=\"zoom: 67%;\" />\n\n- 在做Textual entailment任务时，由于前提p和假设h是有前后文关系的，所以直接p在前h在后即可，中间用$做分隔\n- 在做Similarity任务时，因为没有明确前后文关系，所以将两种排列顺序分别通过模型，最后将输出结果按元素相加，再喂入mlp\n- 在做QA或尝试推理这类多选择任务时，上下文在前，选择在后，如给定背景上下文z、问题q、回答集$$\\{a_k\\}$$，那么分别构造$$[z;q;\\$;a_k]$$作为输入。最后将结果通过softmax映射为概率\n\n\n\n### 1.4 模型训练\n\n##### 1.4.1 无监督预训练\n- 采用Adam算法，并且加了warm up，最大学习率为2.5e-4\n- epoch = 100，batch size = 64\n- 采用$$N(0, 0.02)$$进行参数初始化，由于含有Layer Norm，所以初始化不需要太关注\n- 激活函数采用GELU\n\n##### 1.4.2 有监督微调\n- 在mlp中也加入了dropout\n- learning rate = 6.25e-5，batch size = 32, epochs = 3\n- 采用线性学习率衰减，在0.2%的训练中使用了warm up，超参$$\\lambda = 0.5$$\n\n\n\n### 1.5 下游任务表现\n- **NLI任务：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_3.png\" alt=\"IMG_3\" style=\"zoom: 50%;\" />\n\n- **QA && 常识推理：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_4.png\" alt=\"IMG_4\" style=\"zoom: 50%;\" />\n\n- **语义相似 && 分类任务：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_5.png\" alt=\"IMG_5\" style=\"zoom:50%;\" />\n\n\n\n### 1.6 消融实验\n\n##### 1.6.1 迁移的decoder个数的影响\n- 将预训练之后的模型的一部分decoder用于下游任务，得到结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_6.png\" alt=\"IMG_6\" style=\"zoom: 60%;\" />\n\n由上图可知模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间\n\n- **结论：**预训练得到的每个decoder都是对下游任务有作用的（个人觉得就是模型表达能力更加强大，并且不同的decoder所包含的知识是不同的）\n\n##### 1.6.2 预训练的作用\n- 作者去除了微调，以验证模型的zero-shot能力（没有进行过下游任务训练，而在下游的表现），并且和LSTM进行了比较（同样没有进行下游任务）：  \n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_7.png\" alt=\"IMG_7\" style=\"zoom:60%;\" />\n\n- **结论：**生成式预训练任务是提升其语言建模能力，可以支持各种各样的下游相关任务。并且与 LSTM 相比，Transformer 的结构化注意力记忆有助于迁移\n\n##### 1.6.3 其他实验\n- 作者还探究了**微调时将LM作为额外目标的作用、将模型换为LSTM的对比、pre-training的作用**：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_8.png\" alt=\"IMG_8\" style=\"zoom:50%;\" />\n\n- **结论：**\n1. LM额外目标在大数据集上有提升，但是小数据集上没有\n2. pre-training不可缺少\n\n\n\n\n\n# 2 GPT v2\n\n### 2.1 主要思想\n- GPT2主要着眼于**只使用无监督的LM训练任务，来使模型具有zero-shot能力，不使用有监督数据微调，直接应用于下游任务**\n- 本篇文章的核心观点就是：**只要无监督数据量足够大且足够多样，那么有监督任务就是无监督任务的子集。从一个尽可能大且多样化的数据集中一定能收集到不同领域不同任务相关的自然语言描述示例**\n> Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible.\n- 举个例子：\n> 1. 比如我在训练语言模型时，有一句话“The translation of word Machine Learning in chinese is 机器学习”，那在训练完这句话时，语言模型就自然地将翻译任务和任务的输入输出都学到了\n> 2. 再比如，又碰到一句话“美国的总统是特朗普”，这一句话训练完，也就是一个小的问答了\n> 3. 文章也给了用于训练的WebText Dataset中的英法互译真实实例：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_9.png\" alt=\"IMG_9\" style=\"zoom:50%;\" />\n- 还有一个需要注意的点是，在下游任务时，**由于预训练的预料都会自然、通顺的语言形式，所以下游任务的输入文本也需要重新构造为自然、通顺的形式**，如：\n> 机器翻译任务：translate to french, { english text }, { french text }\n> 阅读理解任务：answer the question, { document }, { question }, { answer }\n\n\n\n### 2.2 训练目标\n\n- GPT2的训练目标仍是LM，但是下游任务的建模发生了一些转变\n- 一般的有监督任务是在估计分布：\n$$\nP(output|input)\n$$\n- 然而GPT2由于是要用同一个模型进行多任务，所以建模变为：\n$$\nP(output|input, task)\n$$\n对于output的估计还要基于具体是什么任务，相同的输入，不同的任务，所产生的output可能是不同的\n- 针对不同任务，具体做法的话，就是上文提到的，将有监督数据构造为自然语言形式\n\n\n\n### 2.3 模型结构\n\n- 大体结构还是和GPT1一样，但是做了如下改动：\n1. Layer Norm由每个sub-block之后，移到了之前：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_11.png\" alt=\"IMG_11\" style=\"zoom:50%;\" />\n\n2. 在模型最后一个自注意力层之后，额外增加一个Layer Norm\n3. 根据残差块的数量，减少了residual path所对应的权重，具体来说，模型一共有N个残差块，那么residual path的权重就都要乘$$1 / \\sqrt{N}$$\n4. 词汇量增加到50257，上下文大小从512增加到1024，batch size增加到512\n- 模型结构大致如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_10.png\" alt=\"IMG_10\" style=\"zoom:40%;\" />\n\n\n\n### 2.4 实验结果\n\n- 在实验效果上，由于 GPT-2 主要是做 zero-shot，所以在实验部分，很多的实验对比都是在无监督的设定下进行的，也就是说他对比的都是无监督的算法\n\n![image-20230329234220217](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230329234220217.png)\n\n- GPT-2 在较多任务上对比无监督算法取得了一定的提升，证明了 zero-shot 的能力。但是，在很多任务上与有监督微调的方法相比还是有一些差距的\n\n\n\n# 3 GPT v3\n\n- GPT3不再像GPT2一样完全主推zero-shot，**而是采用few-shot，采用少量的有监督样本（一般10～100）来辅助模型进行推理**。但是，**GPT3采用有监督样本仅用于推理预测的时候，而不会进行微调的参数更新**\n\n\n\n### 3.1 模型结构\n\n- GPT3采用和GPT2一样的结构，**但是将其中的注意力机制变为了Sparse Attention**\n- 传统的Attention是每个token之间两两计算attentino，复杂度为$$O(n^2)$$\n- 而Sparse Attention除了相对距离不超过 k 以及相对距离为 k，2k，3k，... 的 token，其他所有 token 的注意力都设为 0，如下图所示：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_14.png\" alt=\"IMG_14\" style=\"zoom: 80%;\" />\n\n其计算复杂度为$$O(n * \\log n)$$\n- Sparse Attention的好处：\n> 1. **减少注意力层的计算复杂度**，节约显存和耗时，从而能够处理更长的输入序列\n> 2. **具有“局部紧密相关和远程稀疏相关”的特性**，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少\n- 最后实验了不同规模的模型：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_15.png\" alt=\"IMG_15\" style=\"zoom: 40%;\" />\n\n\n\n### 3.2 下游评估方法\n- 具体到下游任务是，采用了三种不同的方法**（注意这三种方法都只用于推理预测，不会进行参数更新）**：\n> 1. **Zero-shot：**仅使用当前任务的自然语言描述，不进行任何梯度更新\n> 2. **One-shot：**当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新\n> 3. **Few-shot：**当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新，也被称为**in-context learning（上下文学习）**\n- **和fine-tune的对比：** \n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_16.png\" alt=\"IMG_16\" style=\"zoom: 40%;\" />\n\nFew-shot虽然和fine-tune一样都用到多个有监督数据，但是其数据量的需要较少（一般10～100个数据），摒弃不进行参数更新\n\n- **三种方法对比的实验效果：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_17.png\" alt=\"IMG_17\" style=\"zoom: 40%;\" />\n\n\n\n### 3.3 训练数据\n\n- GPT-3 使用了多个数据集，其中最大的是 Common Crawl，原始未处理的数据达到了 45TB，其实在 GPT-2 的时候他们就有考虑使用这个数据集，但是后来还是觉得这个**数据集太脏了**所以没用，但是现在 GPT-3 的模型规模太大了，使得训练对数据量的需求也增加了很多，他们不得不重新考虑这个数据集。因此，他们必须在这个数据集上做一些额外的数据清洗工作来尽量保证数据的质量\n- **数据处理包括：**\n1. 采用GPT2中的WebText、Wikiedia等高质量文本作为正样本，用Common Crawl中的样本作为负样本，训练一个LR二分类器，然后采用这个分类器对Common Crawl采样，只保留其中的正样本\n2. 采用MinHashLSH算法，进行相似文本的去重，减少了大约10%的样本\n3. 加入其他的高质量数据集，不同数据集是通过不同的权重进行采样：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_18.png\" alt=\"IMG_18\" style=\"zoom:50%;\" />\n\n\n\n### 3.4 GPT3的局限性\n1. 当生成文本长度较长时，GPT-3 还是会出现各种问题，比如重复生成一段话，前后矛盾，逻辑衔接不好等等；\n2. 模型和结构的局限性，对于某一些任务，比如填空类型的文本任务，使用单向的自回归语言模型确实存在一定的局限性，这时候如果同时考虑上文和下文的话，效果很可能会更好一些；\n3. 预训练语言模型的通病，在训练时，语料中所有的词都被同等看待，对于一些虚词或无意义的词同样需要花费很多计算量去学习，无法区分学习重点；\n4. 样本有效性或者利用率过低，训一个模型几乎要把整个互联网上的文本数据全都用起来，这与我们人类学习时所需要的成本存在非常大的差异，这方面也是未来人工智能研究的重点；\n5. 有一个不太确定的点是，模型到底是在“学习”还是在“记忆”？我们当然希望它能够学习，但是在使用数据量如此大的情况下，很难去判断它到底是什么样的；\n6. 众所周知，GPT-3 的训练和使用成本都太大了；\n7. GPT-3 跟很多深度学习模型一样，都是不可解释的，没办法知道模型内部到底是如何作出一系列决策的；\n8. 训练数据中可能存在种族、性别等偏见，导致模型也会有这种偏见\n\n\n\n\n\n# 4 InstructGPT\n\n### 4.1 GPT存在的问题\n- GPT的训练方式是采用LM的方法，是估计下一个时间步的词的概率分布：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_19.png\" alt=\"IMG_19\" style=\"zoom:67%;\" />\n\n- 但是由于这是一个概率分布，所以模型的一些输入可能并不符合人类的预期，比如：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_20.png\" alt=\"IMG_20\" style=\"zoom:67%;\" />\n\n- **对于上述问题的解决方案有两种：**\n> 1. 在训练数据上，构造更加好的问答数据集，但是所花费的人工成本极大，因为训练数据集很大\n> 2. 引入一个“老师”，让老师对GPT生成的回答进行打分排序，告诉模型人类更期望哪种结果**（这里的老师既可以是真人，也就是使用在线学习；也可以训练一个Reward Model来对模型结果自动打分排序）**\n\n\n\n### 4.2 实现方案\n\n- 模型通过**三个不同的数据集**，完成了三个子任务：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_21.png\" alt=\"IMG_21\" style=\"zoom:55%;\" />\n\n1. **有监督微调（SFT）：**通过常见的prompt和labeler编写的response，来对GPT-3进行LM任务微调\n2. **训练奖励模型（RM）：**通过常见的prompt，让SFT微调后的GPT模型生成多个response，labeler对这些response进行排序。再使用这些prompt+ response对，输入GPT进行打分\n3. **强化学习（RL）：**只需要prompt，不需要有监督，采用PPO算法，再次微调SFT微调后的模型\n\n\n\n### 4.3 训练数据\n\n- 训练数据所用到的prompt来自两部分：\n> 1. labeler先构造了一批prompt和对应的response，对GPT-3进行微调，然后上线内测\n> 2. 将内测用户的prompt又收集起来，由labeler撰写response\n- 然后将两部分数据分为三个子任务的数据集： \n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_22.png\" alt=\"IMG_22\" style=\"zoom: 67%;\" />\n\n注意，最后RL的数据集是只有prompt，且只包含收集的用户的prompt，而SFT和RM是两者都有\n\n\n\n### 4.4 实现细节\n\n##### 4.4.1 SFT\n- 方法很简单，就不多赘述\n- 值得注意的是，作者在SFT中一共训练了16个epoch，但是发现在第一个epoch后就过拟合了（这么大的模型用这么小的数据肯定过拟合）。**但是由于这个模型并不是微调完就直接拿来用，所以过拟合也没关系。甚至更多的epoch甚至能产生更高的RM分数的输出**\n\n\n\n##### 4.4.2 RM\n\n- 先采用SFT后的模型，对一个prompt生成多个response，并对每一对prompt+response，让labeler进行排序：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_23.png\" alt=\"IMG_23\" style=\"zoom: 25%;\" />\n\n- 然后将SFT后的模型最后的输出层去掉，转而变为一个只有一个神经元的线性层\n- 将每一对prompt+response连结起来，输入该模型，最后输出相当于两者契合的logit分数。然后采用以下损失函数进行优化：\n$$\n\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c}\nK \\\\\n2\n\\end{array}\\right)} E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_{\\theta}\\left(x, y_{w}\\right)-r_{\\theta}\\left(x, y_{l}\\right)\\right)\\right)\\right]\n$$\n其中，K是每个prompt生成的response数量，$$y_w, y_l$$分别是prompt输入x的输出response，且$$rank(y_w) \\ge rank(y_l)$$，外层函数就相当于一个Logestic Regression\n- RM是采用的6B的模型，因为作者发现**大模型（比如175B）训练后期loss不稳定**\n- 此外，作者还提出了另一种方法：采用交叉熵，将排名第一的输出当作正样本，其他输出当作负样本，但是**非常容易过拟合**\n\n\n\n##### 4.4.3 RL\n\n- RL涉及三个模型：RM模型$$r_{\\theta}$$、SFT模型$$\\pi^{SFT}$$和我们最终想要得到的RL模型$$\\pi^{RL}$$；以及两个数据集RL自身的数据集$$D_{RL}$$和预训练时的一部分数据集$$D_{pretrain}$$\n- 优化目标如下：\n$$\n\\begin{aligned}\n\\operatorname{objective}(\\phi)= & E_{(x, y) \\sim D_{\\pi_{\\phi}^{\\mathrm{RL}}}}\\left[r_{\\theta}(x, y)-\\beta \\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\\n& \\gamma E_{x \\sim D_{\\text {prerrain }}}\\left[\\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(x)\\right)\\right]\n\\end{aligned}\n$$\n- 一开始$$\\pi^{RL}$$是由$$\\pi^{SFT}$$初始化得来的\n- 对于第一项，是想让$$\\pi^{RL}$$的输出得到的RM分数尽可能高，并且在这个微调过程中，$$\\pi^{RL}$$和$$\\pi^{SFT}$$的差距不能过大，所以减去两者的KL散度来保证这个差距\n- 如果只使用第一项，方法就称作PPO。但是为了防止模型遗忘预训练时的知识，引入第二项，也就是预训练任务的优化目标，加入第二项后则称为PPO-ptx\n\n\n\n### 4.5 实验结果\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_24.png\" alt=\"IMG_24\" style=\"zoom: 80%;\" />\n\n\n\n\n# 5 GPT vs BERT\n- 编码器和解码器的选取倒不是 GPT 和 BERT 的区别，它们的区别主要是预训练目标函数的选取，有人认为 GPT 选择的是一个更难的训练目标，它是根据前面的信息去预测下文，预测未来肯定是比完形填空难度要更大的。这也能从某种程度上解释了为什么相同规模的 GPT 和 BERT 模型，GPT 的效果要比 BERT 差。\n- 但是从另一个角度去想，如果能够把预测未来这个事情做好的话，它最终所能达到的效果的天花板一定是更高的，这可能也是 OpenAI 从一开始到现在一直坚持使用标准语言模型目标函数来做预训练模型的其中一个原因吧，当然这只是一种猜想。事实证明，从 GPT-3 开始，到最近的 ChatGPT，OpenAI 所取得的令人惊艳的效果也一定程度上证明了他们的选择的正确性。\n\n\n","slug":"GPT","published":1,"updated":"2023-03-29T16:10:31.623Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1b00057csz25cz21l8","content":"<h1 id=\"1-GPT-v1\"><a href=\"#1-GPT-v1\" class=\"headerlink\" title=\"1 GPT v1\"></a>1 GPT v1</h1><ul>\n<li>GPT采用无监督预训练+下游任务微调的方法</li>\n</ul>\n<h3 id=\"1-1-模型结构\"><a href=\"#1-1-模型结构\" class=\"headerlink\" title=\"1.1 模型结构\"></a>1.1 模型结构</h3><ul>\n<li><p>采用12个堆叠的Transformer的Decoder块（去除了和encoder连接的那个Multi-Head）：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_1.png\" alt=\"IMG_1\" style=\"zoom:40%;\" /></p>\n</li>\n</ul>\n<h3 id=\"1-2-模型训练目标\"><a href=\"#1-2-模型训练目标\" class=\"headerlink\" title=\"1.2 模型训练目标\"></a>1.2 模型训练目标</h3><h5 id=\"1-2-1-无监督预训练目标\"><a href=\"#1-2-1-无监督预训练目标\" class=\"headerlink\" title=\"1.2.1 无监督预训练目标\"></a>1.2.1 无监督预训练目标</h5><ul>\n<li>无监督预训练采用的是LM（语言模型）的训练方法，采用n元语法：<script type=\"math/tex; mode=display\">\nL_{1}(\\mathcal{U})=\\sum_{i} \\log P\\left(u_{i} \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right)</script>其中k即n元语法中的n，具<strong>体实现中k是取最大，即表示使用前面的所有词（个人觉得他这里说的有点歧义）</strong>，<script type=\"math/tex\">\\Theta</script>是模型参数</li>\n<li>具体到模型实现上， 类似于word2vec的实现方法，当要预测当前时间步的词u时，采用前面所有的词<script type=\"math/tex\">U = (u_{-k}, ..., u_{-1})</script>来进行预测：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nh_{0} & =U W_{e}+W_{p} \\\\\nh_{l} & =\\text { transformer_block }\\left(h_{l-1}\\right) \\forall i \\in[1, n] \\\\\nP(u) & =\\operatorname{softmax}\\left(h_{n} W_{e}^{T}\\right)\n\\end{aligned}</script>其中<script type=\"math/tex\">W_e \\in (vocab\\_size, embedding\\_dim)</script>是embedding矩阵，<script type=\"math/tex\">W_p \\in (seq\\_len, embedding\\_dim)</script>是<strong>学习到的</strong>位置编码，n表示Transformer层数。<strong>注意最后还是乘的<script type=\"math/tex\">W_e</script>表示使用了Weight Tying</strong>。具体实现是和Transformer一样的</li>\n</ul>\n<h5 id=\"1-2-2-有监督微调\"><a href=\"#1-2-2-有监督微调\" class=\"headerlink\" title=\"1.2.2 有监督微调\"></a>1.2.2 有监督微调</h5><ul>\n<li>有监督任务一般都是在最后接一个全连接，训练目标是：<script type=\"math/tex; mode=display\">\nL_{2}(\\mathcal{C})=\\sum_{(x, y)} \\log P\\left(y \\mid x^{1}, \\ldots, x^{m}\\right)</script>其中x是输入，y是label</li>\n<li>在微调的时候，作者还加入了LM无监督任务作为额外目标，那么微调时的训练目标变为：<script type=\"math/tex; mode=display\">\nL_{3}(\\mathcal{C})=L_{2}(\\mathcal{C})+\\lambda * L_{1}(\\mathcal{C})</script>其中<script type=\"math/tex\">\\lambda</script>表示权重</li>\n<li><strong>这么做的优点是：</strong>可以增加模型泛化能力和收敛速度，后面作者还对此做了消融实验</li>\n</ul>\n<h3 id=\"1-3-微调具体实现方法\"><a href=\"#1-3-微调具体实现方法\" class=\"headerlink\" title=\"1.3 微调具体实现方法\"></a>1.3 微调具体实现方法</h3><ul>\n<li>GPT针对不同类型的下游任务，其做法是不同的。尤其是<strong>由于在预训练时，是在连续通顺文本上训练的，所以在下游任务上有多个输入时，句子之间的相对顺序尤为重要</strong></li>\n<li>最初的输入还要加三个特殊token：起始token（\\<s\\>）、分隔token（$）、结束token（\\<e\\>）</li>\n<li><strong>方法汇总：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_2.png\" alt=\"IMG_2\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>在做Textual entailment任务时，由于前提p和假设h是有前后文关系的，所以直接p在前h在后即可，中间用$做分隔</li>\n<li>在做Similarity任务时，因为没有明确前后文关系，所以将两种排列顺序分别通过模型，最后将输出结果按元素相加，再喂入mlp</li>\n<li>在做QA或尝试推理这类多选择任务时，上下文在前，选择在后，如给定背景上下文z、问题q、回答集<script type=\"math/tex\">\\{a_k\\}</script>，那么分别构造<script type=\"math/tex\">[z;q;\\$;a_k]</script>作为输入。最后将结果通过softmax映射为概率</li>\n</ul>\n<h3 id=\"1-4-模型训练\"><a href=\"#1-4-模型训练\" class=\"headerlink\" title=\"1.4 模型训练\"></a>1.4 模型训练</h3><h5 id=\"1-4-1-无监督预训练\"><a href=\"#1-4-1-无监督预训练\" class=\"headerlink\" title=\"1.4.1 无监督预训练\"></a>1.4.1 无监督预训练</h5><ul>\n<li>采用Adam算法，并且加了warm up，最大学习率为2.5e-4</li>\n<li>epoch = 100，batch size = 64</li>\n<li>采用<script type=\"math/tex\">N(0, 0.02)</script>进行参数初始化，由于含有Layer Norm，所以初始化不需要太关注</li>\n<li>激活函数采用GELU</li>\n</ul>\n<h5 id=\"1-4-2-有监督微调\"><a href=\"#1-4-2-有监督微调\" class=\"headerlink\" title=\"1.4.2 有监督微调\"></a>1.4.2 有监督微调</h5><ul>\n<li>在mlp中也加入了dropout</li>\n<li>learning rate = 6.25e-5，batch size = 32, epochs = 3</li>\n<li>采用线性学习率衰减，在0.2%的训练中使用了warm up，超参<script type=\"math/tex\">\\lambda = 0.5</script></li>\n</ul>\n<h3 id=\"1-5-下游任务表现\"><a href=\"#1-5-下游任务表现\" class=\"headerlink\" title=\"1.5 下游任务表现\"></a>1.5 下游任务表现</h3><ul>\n<li><strong>NLI任务：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_3.png\" alt=\"IMG_3\" style=\"zoom: 50%;\" /></p>\n<ul>\n<li><strong>QA &amp;&amp; 常识推理：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_4.png\" alt=\"IMG_4\" style=\"zoom: 50%;\" /></p>\n<ul>\n<li><strong>语义相似 &amp;&amp; 分类任务：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_5.png\" alt=\"IMG_5\" style=\"zoom:50%;\" /></p>\n<h3 id=\"1-6-消融实验\"><a href=\"#1-6-消融实验\" class=\"headerlink\" title=\"1.6 消融实验\"></a>1.6 消融实验</h3><h5 id=\"1-6-1-迁移的decoder个数的影响\"><a href=\"#1-6-1-迁移的decoder个数的影响\" class=\"headerlink\" title=\"1.6.1 迁移的decoder个数的影响\"></a>1.6.1 迁移的decoder个数的影响</h5><ul>\n<li>将预训练之后的模型的一部分decoder用于下游任务，得到结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_6.png\" alt=\"IMG_6\" style=\"zoom: 60%;\" /></p>\n<p>由上图可知模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间</p>\n<ul>\n<li><strong>结论：</strong>预训练得到的每个decoder都是对下游任务有作用的（个人觉得就是模型表达能力更加强大，并且不同的decoder所包含的知识是不同的）</li>\n</ul>\n<h5 id=\"1-6-2-预训练的作用\"><a href=\"#1-6-2-预训练的作用\" class=\"headerlink\" title=\"1.6.2 预训练的作用\"></a>1.6.2 预训练的作用</h5><ul>\n<li>作者去除了微调，以验证模型的zero-shot能力（没有进行过下游任务训练，而在下游的表现），并且和LSTM进行了比较（同样没有进行下游任务）：  </li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_7.png\" alt=\"IMG_7\" style=\"zoom:60%;\" /></p>\n<ul>\n<li><strong>结论：</strong>生成式预训练任务是提升其语言建模能力，可以支持各种各样的下游相关任务。并且与 LSTM 相比，Transformer 的结构化注意力记忆有助于迁移</li>\n</ul>\n<h5 id=\"1-6-3-其他实验\"><a href=\"#1-6-3-其他实验\" class=\"headerlink\" title=\"1.6.3 其他实验\"></a>1.6.3 其他实验</h5><ul>\n<li>作者还探究了<strong>微调时将LM作为额外目标的作用、将模型换为LSTM的对比、pre-training的作用</strong>：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_8.png\" alt=\"IMG_8\" style=\"zoom:50%;\" /></p>\n<ul>\n<li><strong>结论：</strong></li>\n</ul>\n<ol>\n<li>LM额外目标在大数据集上有提升，但是小数据集上没有</li>\n<li>pre-training不可缺少</li>\n</ol>\n<h1 id=\"2-GPT-v2\"><a href=\"#2-GPT-v2\" class=\"headerlink\" title=\"2 GPT v2\"></a>2 GPT v2</h1><h3 id=\"2-1-主要思想\"><a href=\"#2-1-主要思想\" class=\"headerlink\" title=\"2.1 主要思想\"></a>2.1 主要思想</h3><ul>\n<li>GPT2主要着眼于<strong>只使用无监督的LM训练任务，来使模型具有zero-shot能力，不使用有监督数据微调，直接应用于下游任务</strong></li>\n<li>本篇文章的核心观点就是：<strong>只要无监督数据量足够大且足够多样，那么有监督任务就是无监督任务的子集。从一个尽可能大且多样化的数据集中一定能收集到不同领域不同任务相关的自然语言描述示例</strong><blockquote>\n<p>Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible.</p>\n</blockquote>\n</li>\n<li>举个例子：<blockquote>\n<ol>\n<li>比如我在训练语言模型时，有一句话“The translation of word Machine Learning in chinese is 机器学习”，那在训练完这句话时，语言模型就自然地将翻译任务和任务的输入输出都学到了</li>\n<li>再比如，又碰到一句话“美国的总统是特朗普”，这一句话训练完，也就是一个小的问答了</li>\n<li>文章也给了用于训练的WebText Dataset中的英法互译真实实例：</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_9.png\" alt=\"IMG_9\" style=\"zoom:50%;\" /></p>\n</blockquote>\n</li>\n<li>还有一个需要注意的点是，在下游任务时，<strong>由于预训练的预料都会自然、通顺的语言形式，所以下游任务的输入文本也需要重新构造为自然、通顺的形式</strong>，如：<blockquote>\n<p>机器翻译任务：translate to french, { english text }, { french text }<br>阅读理解任务：answer the question, { document }, { question }, { answer }</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"2-2-训练目标\"><a href=\"#2-2-训练目标\" class=\"headerlink\" title=\"2.2 训练目标\"></a>2.2 训练目标</h3><ul>\n<li>GPT2的训练目标仍是LM，但是下游任务的建模发生了一些转变</li>\n<li>一般的有监督任务是在估计分布：<script type=\"math/tex; mode=display\">\nP(output|input)</script></li>\n<li>然而GPT2由于是要用同一个模型进行多任务，所以建模变为：<script type=\"math/tex; mode=display\">\nP(output|input, task)</script>对于output的估计还要基于具体是什么任务，相同的输入，不同的任务，所产生的output可能是不同的</li>\n<li>针对不同任务，具体做法的话，就是上文提到的，将有监督数据构造为自然语言形式</li>\n</ul>\n<h3 id=\"2-3-模型结构\"><a href=\"#2-3-模型结构\" class=\"headerlink\" title=\"2.3 模型结构\"></a>2.3 模型结构</h3><ul>\n<li>大体结构还是和GPT1一样，但是做了如下改动：</li>\n</ul>\n<ol>\n<li>Layer Norm由每个sub-block之后，移到了之前：</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_11.png\" alt=\"IMG_11\" style=\"zoom:50%;\" /></p>\n<ol>\n<li>在模型最后一个自注意力层之后，额外增加一个Layer Norm</li>\n<li>根据残差块的数量，减少了residual path所对应的权重，具体来说，模型一共有N个残差块，那么residual path的权重就都要乘<script type=\"math/tex\">1 / \\sqrt{N}</script></li>\n<li>词汇量增加到50257，上下文大小从512增加到1024，batch size增加到512</li>\n</ol>\n<ul>\n<li>模型结构大致如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_10.png\" alt=\"IMG_10\" style=\"zoom:40%;\" /></p>\n<h3 id=\"2-4-实验结果\"><a href=\"#2-4-实验结果\" class=\"headerlink\" title=\"2.4 实验结果\"></a>2.4 实验结果</h3><ul>\n<li>在实验效果上，由于 GPT-2 主要是做 zero-shot，所以在实验部分，很多的实验对比都是在无监督的设定下进行的，也就是说他对比的都是无监督的算法</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230329234220217.png\" alt=\"image-20230329234220217\"></p>\n<ul>\n<li>GPT-2 在较多任务上对比无监督算法取得了一定的提升，证明了 zero-shot 的能力。但是，在很多任务上与有监督微调的方法相比还是有一些差距的</li>\n</ul>\n<h1 id=\"3-GPT-v3\"><a href=\"#3-GPT-v3\" class=\"headerlink\" title=\"3 GPT v3\"></a>3 GPT v3</h1><ul>\n<li>GPT3不再像GPT2一样完全主推zero-shot，<strong>而是采用few-shot，采用少量的有监督样本（一般10～100）来辅助模型进行推理</strong>。但是，<strong>GPT3采用有监督样本仅用于推理预测的时候，而不会进行微调的参数更新</strong></li>\n</ul>\n<h3 id=\"3-1-模型结构\"><a href=\"#3-1-模型结构\" class=\"headerlink\" title=\"3.1 模型结构\"></a>3.1 模型结构</h3><ul>\n<li>GPT3采用和GPT2一样的结构，<strong>但是将其中的注意力机制变为了Sparse Attention</strong></li>\n<li>传统的Attention是每个token之间两两计算attentino，复杂度为<script type=\"math/tex\">O(n^2)</script></li>\n<li>而Sparse Attention除了相对距离不超过 k 以及相对距离为 k，2k，3k，… 的 token，其他所有 token 的注意力都设为 0，如下图所示：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_14.png\" alt=\"IMG_14\" style=\"zoom: 80%;\" /></p>\n<p>其计算复杂度为<script type=\"math/tex\">O(n * \\log n)</script></p>\n<ul>\n<li>Sparse Attention的好处：<blockquote>\n<ol>\n<li><strong>减少注意力层的计算复杂度</strong>，节约显存和耗时，从而能够处理更长的输入序列</li>\n<li><strong>具有“局部紧密相关和远程稀疏相关”的特性</strong>，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少</li>\n</ol>\n</blockquote>\n</li>\n<li>最后实验了不同规模的模型：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_15.png\" alt=\"IMG_15\" style=\"zoom: 40%;\" /></p>\n<h3 id=\"3-2-下游评估方法\"><a href=\"#3-2-下游评估方法\" class=\"headerlink\" title=\"3.2 下游评估方法\"></a>3.2 下游评估方法</h3><ul>\n<li>具体到下游任务是，采用了三种不同的方法<strong>（注意这三种方法都只用于推理预测，不会进行参数更新）</strong>：<blockquote>\n<ol>\n<li><strong>Zero-shot：</strong>仅使用当前任务的自然语言描述，不进行任何梯度更新</li>\n<li><strong>One-shot：</strong>当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新</li>\n<li><strong>Few-shot：</strong>当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新，也被称为<strong>in-context learning（上下文学习）</strong></li>\n</ol>\n</blockquote>\n</li>\n<li><strong>和fine-tune的对比：</strong> </li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_16.png\" alt=\"IMG_16\" style=\"zoom: 40%;\" /></p>\n<p>Few-shot虽然和fine-tune一样都用到多个有监督数据，但是其数据量的需要较少（一般10～100个数据），摒弃不进行参数更新</p>\n<ul>\n<li><strong>三种方法对比的实验效果：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_17.png\" alt=\"IMG_17\" style=\"zoom: 40%;\" /></p>\n<h3 id=\"3-3-训练数据\"><a href=\"#3-3-训练数据\" class=\"headerlink\" title=\"3.3 训练数据\"></a>3.3 训练数据</h3><ul>\n<li>GPT-3 使用了多个数据集，其中最大的是 Common Crawl，原始未处理的数据达到了 45TB，其实在 GPT-2 的时候他们就有考虑使用这个数据集，但是后来还是觉得这个<strong>数据集太脏了</strong>所以没用，但是现在 GPT-3 的模型规模太大了，使得训练对数据量的需求也增加了很多，他们不得不重新考虑这个数据集。因此，他们必须在这个数据集上做一些额外的数据清洗工作来尽量保证数据的质量</li>\n<li><strong>数据处理包括：</strong></li>\n</ul>\n<ol>\n<li>采用GPT2中的WebText、Wikiedia等高质量文本作为正样本，用Common Crawl中的样本作为负样本，训练一个LR二分类器，然后采用这个分类器对Common Crawl采样，只保留其中的正样本</li>\n<li>采用MinHashLSH算法，进行相似文本的去重，减少了大约10%的样本</li>\n<li>加入其他的高质量数据集，不同数据集是通过不同的权重进行采样：</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_18.png\" alt=\"IMG_18\" style=\"zoom:50%;\" /></p>\n<h3 id=\"3-4-GPT3的局限性\"><a href=\"#3-4-GPT3的局限性\" class=\"headerlink\" title=\"3.4 GPT3的局限性\"></a>3.4 GPT3的局限性</h3><ol>\n<li>当生成文本长度较长时，GPT-3 还是会出现各种问题，比如重复生成一段话，前后矛盾，逻辑衔接不好等等；</li>\n<li>模型和结构的局限性，对于某一些任务，比如填空类型的文本任务，使用单向的自回归语言模型确实存在一定的局限性，这时候如果同时考虑上文和下文的话，效果很可能会更好一些；</li>\n<li>预训练语言模型的通病，在训练时，语料中所有的词都被同等看待，对于一些虚词或无意义的词同样需要花费很多计算量去学习，无法区分学习重点；</li>\n<li>样本有效性或者利用率过低，训一个模型几乎要把整个互联网上的文本数据全都用起来，这与我们人类学习时所需要的成本存在非常大的差异，这方面也是未来人工智能研究的重点；</li>\n<li>有一个不太确定的点是，模型到底是在“学习”还是在“记忆”？我们当然希望它能够学习，但是在使用数据量如此大的情况下，很难去判断它到底是什么样的；</li>\n<li>众所周知，GPT-3 的训练和使用成本都太大了；</li>\n<li>GPT-3 跟很多深度学习模型一样，都是不可解释的，没办法知道模型内部到底是如何作出一系列决策的；</li>\n<li>训练数据中可能存在种族、性别等偏见，导致模型也会有这种偏见</li>\n</ol>\n<h1 id=\"4-InstructGPT\"><a href=\"#4-InstructGPT\" class=\"headerlink\" title=\"4 InstructGPT\"></a>4 InstructGPT</h1><h3 id=\"4-1-GPT存在的问题\"><a href=\"#4-1-GPT存在的问题\" class=\"headerlink\" title=\"4.1 GPT存在的问题\"></a>4.1 GPT存在的问题</h3><ul>\n<li>GPT的训练方式是采用LM的方法，是估计下一个时间步的词的概率分布：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_19.png\" alt=\"IMG_19\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>但是由于这是一个概率分布，所以模型的一些输入可能并不符合人类的预期，比如：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_20.png\" alt=\"IMG_20\" style=\"zoom:67%;\" /></p>\n<ul>\n<li><strong>对于上述问题的解决方案有两种：</strong><blockquote>\n<ol>\n<li>在训练数据上，构造更加好的问答数据集，但是所花费的人工成本极大，因为训练数据集很大</li>\n<li>引入一个“老师”，让老师对GPT生成的回答进行打分排序，告诉模型人类更期望哪种结果<strong>（这里的老师既可以是真人，也就是使用在线学习；也可以训练一个Reward Model来对模型结果自动打分排序）</strong></li>\n</ol>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"4-2-实现方案\"><a href=\"#4-2-实现方案\" class=\"headerlink\" title=\"4.2 实现方案\"></a>4.2 实现方案</h3><ul>\n<li>模型通过<strong>三个不同的数据集</strong>，完成了三个子任务：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_21.png\" alt=\"IMG_21\" style=\"zoom:55%;\" /></p>\n<ol>\n<li><strong>有监督微调（SFT）：</strong>通过常见的prompt和labeler编写的response，来对GPT-3进行LM任务微调</li>\n<li><strong>训练奖励模型（RM）：</strong>通过常见的prompt，让SFT微调后的GPT模型生成多个response，labeler对这些response进行排序。再使用这些prompt+ response对，输入GPT进行打分</li>\n<li><strong>强化学习（RL）：</strong>只需要prompt，不需要有监督，采用PPO算法，再次微调SFT微调后的模型</li>\n</ol>\n<h3 id=\"4-3-训练数据\"><a href=\"#4-3-训练数据\" class=\"headerlink\" title=\"4.3 训练数据\"></a>4.3 训练数据</h3><ul>\n<li>训练数据所用到的prompt来自两部分：<blockquote>\n<ol>\n<li>labeler先构造了一批prompt和对应的response，对GPT-3进行微调，然后上线内测</li>\n<li>将内测用户的prompt又收集起来，由labeler撰写response</li>\n</ol>\n</blockquote>\n</li>\n<li>然后将两部分数据分为三个子任务的数据集： </li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_22.png\" alt=\"IMG_22\" style=\"zoom: 67%;\" /></p>\n<p>注意，最后RL的数据集是只有prompt，且只包含收集的用户的prompt，而SFT和RM是两者都有</p>\n<h3 id=\"4-4-实现细节\"><a href=\"#4-4-实现细节\" class=\"headerlink\" title=\"4.4 实现细节\"></a>4.4 实现细节</h3><h5 id=\"4-4-1-SFT\"><a href=\"#4-4-1-SFT\" class=\"headerlink\" title=\"4.4.1 SFT\"></a>4.4.1 SFT</h5><ul>\n<li>方法很简单，就不多赘述</li>\n<li>值得注意的是，作者在SFT中一共训练了16个epoch，但是发现在第一个epoch后就过拟合了（这么大的模型用这么小的数据肯定过拟合）。<strong>但是由于这个模型并不是微调完就直接拿来用，所以过拟合也没关系。甚至更多的epoch甚至能产生更高的RM分数的输出</strong></li>\n</ul>\n<h5 id=\"4-4-2-RM\"><a href=\"#4-4-2-RM\" class=\"headerlink\" title=\"4.4.2 RM\"></a>4.4.2 RM</h5><ul>\n<li>先采用SFT后的模型，对一个prompt生成多个response，并对每一对prompt+response，让labeler进行排序：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_23.png\" alt=\"IMG_23\" style=\"zoom: 25%;\" /></p>\n<ul>\n<li>然后将SFT后的模型最后的输出层去掉，转而变为一个只有一个神经元的线性层</li>\n<li>将每一对prompt+response连结起来，输入该模型，最后输出相当于两者契合的logit分数。然后采用以下损失函数进行优化：<script type=\"math/tex; mode=display\">\n\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c}\nK \\\\\n2\n\\end{array}\\right)} E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_{\\theta}\\left(x, y_{w}\\right)-r_{\\theta}\\left(x, y_{l}\\right)\\right)\\right)\\right]</script>其中，K是每个prompt生成的response数量，<script type=\"math/tex\">y_w, y_l</script>分别是prompt输入x的输出response，且<script type=\"math/tex\">rank(y_w) \\ge rank(y_l)</script>，外层函数就相当于一个Logestic Regression</li>\n<li>RM是采用的6B的模型，因为作者发现<strong>大模型（比如175B）训练后期loss不稳定</strong></li>\n<li>此外，作者还提出了另一种方法：采用交叉熵，将排名第一的输出当作正样本，其他输出当作负样本，但是<strong>非常容易过拟合</strong></li>\n</ul>\n<h5 id=\"4-4-3-RL\"><a href=\"#4-4-3-RL\" class=\"headerlink\" title=\"4.4.3 RL\"></a>4.4.3 RL</h5><ul>\n<li>RL涉及三个模型：RM模型<script type=\"math/tex\">r_{\\theta}</script>、SFT模型<script type=\"math/tex\">\\pi^{SFT}</script>和我们最终想要得到的RL模型<script type=\"math/tex\">\\pi^{RL}</script>；以及两个数据集RL自身的数据集<script type=\"math/tex\">D_{RL}</script>和预训练时的一部分数据集<script type=\"math/tex\">D_{pretrain}</script></li>\n<li>优化目标如下：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{objective}(\\phi)= & E_{(x, y) \\sim D_{\\pi_{\\phi}^{\\mathrm{RL}}}}\\left[r_{\\theta}(x, y)-\\beta \\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\\n& \\gamma E_{x \\sim D_{\\text {prerrain }}}\\left[\\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(x)\\right)\\right]\n\\end{aligned}</script></li>\n<li>一开始<script type=\"math/tex\">\\pi^{RL}</script>是由<script type=\"math/tex\">\\pi^{SFT}</script>初始化得来的</li>\n<li>对于第一项，是想让<script type=\"math/tex\">\\pi^{RL}</script>的输出得到的RM分数尽可能高，并且在这个微调过程中，<script type=\"math/tex\">\\pi^{RL}</script>和<script type=\"math/tex\">\\pi^{SFT}</script>的差距不能过大，所以减去两者的KL散度来保证这个差距</li>\n<li>如果只使用第一项，方法就称作PPO。但是为了防止模型遗忘预训练时的知识，引入第二项，也就是预训练任务的优化目标，加入第二项后则称为PPO-ptx</li>\n</ul>\n<h3 id=\"4-5-实验结果\"><a href=\"#4-5-实验结果\" class=\"headerlink\" title=\"4.5 实验结果\"></a>4.5 实验结果</h3><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_24.png\" alt=\"IMG_24\" style=\"zoom: 80%;\" /></p>\n<h1 id=\"5-GPT-vs-BERT\"><a href=\"#5-GPT-vs-BERT\" class=\"headerlink\" title=\"5 GPT vs BERT\"></a>5 GPT vs BERT</h1><ul>\n<li>编码器和解码器的选取倒不是 GPT 和 BERT 的区别，它们的区别主要是预训练目标函数的选取，有人认为 GPT 选择的是一个更难的训练目标，它是根据前面的信息去预测下文，预测未来肯定是比完形填空难度要更大的。这也能从某种程度上解释了为什么相同规模的 GPT 和 BERT 模型，GPT 的效果要比 BERT 差。</li>\n<li>但是从另一个角度去想，如果能够把预测未来这个事情做好的话，它最终所能达到的效果的天花板一定是更高的，这可能也是 OpenAI 从一开始到现在一直坚持使用标准语言模型目标函数来做预训练模型的其中一个原因吧，当然这只是一种猜想。事实证明，从 GPT-3 开始，到最近的 ChatGPT，OpenAI 所取得的令人惊艳的效果也一定程度上证明了他们的选择的正确性。</li>\n</ul>\n","site":{"data":{}},"wordcount":7203,"excerpt":"","more":"<h1 id=\"1-GPT-v1\"><a href=\"#1-GPT-v1\" class=\"headerlink\" title=\"1 GPT v1\"></a>1 GPT v1</h1><ul>\n<li>GPT采用无监督预训练+下游任务微调的方法</li>\n</ul>\n<h3 id=\"1-1-模型结构\"><a href=\"#1-1-模型结构\" class=\"headerlink\" title=\"1.1 模型结构\"></a>1.1 模型结构</h3><ul>\n<li><p>采用12个堆叠的Transformer的Decoder块（去除了和encoder连接的那个Multi-Head）：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_1.png\" alt=\"IMG_1\" style=\"zoom:40%;\" /></p>\n</li>\n</ul>\n<h3 id=\"1-2-模型训练目标\"><a href=\"#1-2-模型训练目标\" class=\"headerlink\" title=\"1.2 模型训练目标\"></a>1.2 模型训练目标</h3><h5 id=\"1-2-1-无监督预训练目标\"><a href=\"#1-2-1-无监督预训练目标\" class=\"headerlink\" title=\"1.2.1 无监督预训练目标\"></a>1.2.1 无监督预训练目标</h5><ul>\n<li>无监督预训练采用的是LM（语言模型）的训练方法，采用n元语法：<script type=\"math/tex; mode=display\">\nL_{1}(\\mathcal{U})=\\sum_{i} \\log P\\left(u_{i} \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right)</script>其中k即n元语法中的n，具<strong>体实现中k是取最大，即表示使用前面的所有词（个人觉得他这里说的有点歧义）</strong>，<script type=\"math/tex\">\\Theta</script>是模型参数</li>\n<li>具体到模型实现上， 类似于word2vec的实现方法，当要预测当前时间步的词u时，采用前面所有的词<script type=\"math/tex\">U = (u_{-k}, ..., u_{-1})</script>来进行预测：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nh_{0} & =U W_{e}+W_{p} \\\\\nh_{l} & =\\text { transformer_block }\\left(h_{l-1}\\right) \\forall i \\in[1, n] \\\\\nP(u) & =\\operatorname{softmax}\\left(h_{n} W_{e}^{T}\\right)\n\\end{aligned}</script>其中<script type=\"math/tex\">W_e \\in (vocab\\_size, embedding\\_dim)</script>是embedding矩阵，<script type=\"math/tex\">W_p \\in (seq\\_len, embedding\\_dim)</script>是<strong>学习到的</strong>位置编码，n表示Transformer层数。<strong>注意最后还是乘的<script type=\"math/tex\">W_e</script>表示使用了Weight Tying</strong>。具体实现是和Transformer一样的</li>\n</ul>\n<h5 id=\"1-2-2-有监督微调\"><a href=\"#1-2-2-有监督微调\" class=\"headerlink\" title=\"1.2.2 有监督微调\"></a>1.2.2 有监督微调</h5><ul>\n<li>有监督任务一般都是在最后接一个全连接，训练目标是：<script type=\"math/tex; mode=display\">\nL_{2}(\\mathcal{C})=\\sum_{(x, y)} \\log P\\left(y \\mid x^{1}, \\ldots, x^{m}\\right)</script>其中x是输入，y是label</li>\n<li>在微调的时候，作者还加入了LM无监督任务作为额外目标，那么微调时的训练目标变为：<script type=\"math/tex; mode=display\">\nL_{3}(\\mathcal{C})=L_{2}(\\mathcal{C})+\\lambda * L_{1}(\\mathcal{C})</script>其中<script type=\"math/tex\">\\lambda</script>表示权重</li>\n<li><strong>这么做的优点是：</strong>可以增加模型泛化能力和收敛速度，后面作者还对此做了消融实验</li>\n</ul>\n<h3 id=\"1-3-微调具体实现方法\"><a href=\"#1-3-微调具体实现方法\" class=\"headerlink\" title=\"1.3 微调具体实现方法\"></a>1.3 微调具体实现方法</h3><ul>\n<li>GPT针对不同类型的下游任务，其做法是不同的。尤其是<strong>由于在预训练时，是在连续通顺文本上训练的，所以在下游任务上有多个输入时，句子之间的相对顺序尤为重要</strong></li>\n<li>最初的输入还要加三个特殊token：起始token（\\<s\\>）、分隔token（$）、结束token（\\<e\\>）</li>\n<li><strong>方法汇总：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_2.png\" alt=\"IMG_2\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>在做Textual entailment任务时，由于前提p和假设h是有前后文关系的，所以直接p在前h在后即可，中间用$做分隔</li>\n<li>在做Similarity任务时，因为没有明确前后文关系，所以将两种排列顺序分别通过模型，最后将输出结果按元素相加，再喂入mlp</li>\n<li>在做QA或尝试推理这类多选择任务时，上下文在前，选择在后，如给定背景上下文z、问题q、回答集<script type=\"math/tex\">\\{a_k\\}</script>，那么分别构造<script type=\"math/tex\">[z;q;\\$;a_k]</script>作为输入。最后将结果通过softmax映射为概率</li>\n</ul>\n<h3 id=\"1-4-模型训练\"><a href=\"#1-4-模型训练\" class=\"headerlink\" title=\"1.4 模型训练\"></a>1.4 模型训练</h3><h5 id=\"1-4-1-无监督预训练\"><a href=\"#1-4-1-无监督预训练\" class=\"headerlink\" title=\"1.4.1 无监督预训练\"></a>1.4.1 无监督预训练</h5><ul>\n<li>采用Adam算法，并且加了warm up，最大学习率为2.5e-4</li>\n<li>epoch = 100，batch size = 64</li>\n<li>采用<script type=\"math/tex\">N(0, 0.02)</script>进行参数初始化，由于含有Layer Norm，所以初始化不需要太关注</li>\n<li>激活函数采用GELU</li>\n</ul>\n<h5 id=\"1-4-2-有监督微调\"><a href=\"#1-4-2-有监督微调\" class=\"headerlink\" title=\"1.4.2 有监督微调\"></a>1.4.2 有监督微调</h5><ul>\n<li>在mlp中也加入了dropout</li>\n<li>learning rate = 6.25e-5，batch size = 32, epochs = 3</li>\n<li>采用线性学习率衰减，在0.2%的训练中使用了warm up，超参<script type=\"math/tex\">\\lambda = 0.5</script></li>\n</ul>\n<h3 id=\"1-5-下游任务表现\"><a href=\"#1-5-下游任务表现\" class=\"headerlink\" title=\"1.5 下游任务表现\"></a>1.5 下游任务表现</h3><ul>\n<li><strong>NLI任务：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_3.png\" alt=\"IMG_3\" style=\"zoom: 50%;\" /></p>\n<ul>\n<li><strong>QA &amp;&amp; 常识推理：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_4.png\" alt=\"IMG_4\" style=\"zoom: 50%;\" /></p>\n<ul>\n<li><strong>语义相似 &amp;&amp; 分类任务：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_5.png\" alt=\"IMG_5\" style=\"zoom:50%;\" /></p>\n<h3 id=\"1-6-消融实验\"><a href=\"#1-6-消融实验\" class=\"headerlink\" title=\"1.6 消融实验\"></a>1.6 消融实验</h3><h5 id=\"1-6-1-迁移的decoder个数的影响\"><a href=\"#1-6-1-迁移的decoder个数的影响\" class=\"headerlink\" title=\"1.6.1 迁移的decoder个数的影响\"></a>1.6.1 迁移的decoder个数的影响</h5><ul>\n<li>将预训练之后的模型的一部分decoder用于下游任务，得到结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_6.png\" alt=\"IMG_6\" style=\"zoom: 60%;\" /></p>\n<p>由上图可知模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间</p>\n<ul>\n<li><strong>结论：</strong>预训练得到的每个decoder都是对下游任务有作用的（个人觉得就是模型表达能力更加强大，并且不同的decoder所包含的知识是不同的）</li>\n</ul>\n<h5 id=\"1-6-2-预训练的作用\"><a href=\"#1-6-2-预训练的作用\" class=\"headerlink\" title=\"1.6.2 预训练的作用\"></a>1.6.2 预训练的作用</h5><ul>\n<li>作者去除了微调，以验证模型的zero-shot能力（没有进行过下游任务训练，而在下游的表现），并且和LSTM进行了比较（同样没有进行下游任务）：  </li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_7.png\" alt=\"IMG_7\" style=\"zoom:60%;\" /></p>\n<ul>\n<li><strong>结论：</strong>生成式预训练任务是提升其语言建模能力，可以支持各种各样的下游相关任务。并且与 LSTM 相比，Transformer 的结构化注意力记忆有助于迁移</li>\n</ul>\n<h5 id=\"1-6-3-其他实验\"><a href=\"#1-6-3-其他实验\" class=\"headerlink\" title=\"1.6.3 其他实验\"></a>1.6.3 其他实验</h5><ul>\n<li>作者还探究了<strong>微调时将LM作为额外目标的作用、将模型换为LSTM的对比、pre-training的作用</strong>：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_8.png\" alt=\"IMG_8\" style=\"zoom:50%;\" /></p>\n<ul>\n<li><strong>结论：</strong></li>\n</ul>\n<ol>\n<li>LM额外目标在大数据集上有提升，但是小数据集上没有</li>\n<li>pre-training不可缺少</li>\n</ol>\n<h1 id=\"2-GPT-v2\"><a href=\"#2-GPT-v2\" class=\"headerlink\" title=\"2 GPT v2\"></a>2 GPT v2</h1><h3 id=\"2-1-主要思想\"><a href=\"#2-1-主要思想\" class=\"headerlink\" title=\"2.1 主要思想\"></a>2.1 主要思想</h3><ul>\n<li>GPT2主要着眼于<strong>只使用无监督的LM训练任务，来使模型具有zero-shot能力，不使用有监督数据微调，直接应用于下游任务</strong></li>\n<li>本篇文章的核心观点就是：<strong>只要无监督数据量足够大且足够多样，那么有监督任务就是无监督任务的子集。从一个尽可能大且多样化的数据集中一定能收集到不同领域不同任务相关的自然语言描述示例</strong><blockquote>\n<p>Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible.</p>\n</blockquote>\n</li>\n<li>举个例子：<blockquote>\n<ol>\n<li>比如我在训练语言模型时，有一句话“The translation of word Machine Learning in chinese is 机器学习”，那在训练完这句话时，语言模型就自然地将翻译任务和任务的输入输出都学到了</li>\n<li>再比如，又碰到一句话“美国的总统是特朗普”，这一句话训练完，也就是一个小的问答了</li>\n<li>文章也给了用于训练的WebText Dataset中的英法互译真实实例：</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_9.png\" alt=\"IMG_9\" style=\"zoom:50%;\" /></p>\n</blockquote>\n</li>\n<li>还有一个需要注意的点是，在下游任务时，<strong>由于预训练的预料都会自然、通顺的语言形式，所以下游任务的输入文本也需要重新构造为自然、通顺的形式</strong>，如：<blockquote>\n<p>机器翻译任务：translate to french, { english text }, { french text }<br>阅读理解任务：answer the question, { document }, { question }, { answer }</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"2-2-训练目标\"><a href=\"#2-2-训练目标\" class=\"headerlink\" title=\"2.2 训练目标\"></a>2.2 训练目标</h3><ul>\n<li>GPT2的训练目标仍是LM，但是下游任务的建模发生了一些转变</li>\n<li>一般的有监督任务是在估计分布：<script type=\"math/tex; mode=display\">\nP(output|input)</script></li>\n<li>然而GPT2由于是要用同一个模型进行多任务，所以建模变为：<script type=\"math/tex; mode=display\">\nP(output|input, task)</script>对于output的估计还要基于具体是什么任务，相同的输入，不同的任务，所产生的output可能是不同的</li>\n<li>针对不同任务，具体做法的话，就是上文提到的，将有监督数据构造为自然语言形式</li>\n</ul>\n<h3 id=\"2-3-模型结构\"><a href=\"#2-3-模型结构\" class=\"headerlink\" title=\"2.3 模型结构\"></a>2.3 模型结构</h3><ul>\n<li>大体结构还是和GPT1一样，但是做了如下改动：</li>\n</ul>\n<ol>\n<li>Layer Norm由每个sub-block之后，移到了之前：</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_11.png\" alt=\"IMG_11\" style=\"zoom:50%;\" /></p>\n<ol>\n<li>在模型最后一个自注意力层之后，额外增加一个Layer Norm</li>\n<li>根据残差块的数量，减少了residual path所对应的权重，具体来说，模型一共有N个残差块，那么residual path的权重就都要乘<script type=\"math/tex\">1 / \\sqrt{N}</script></li>\n<li>词汇量增加到50257，上下文大小从512增加到1024，batch size增加到512</li>\n</ol>\n<ul>\n<li>模型结构大致如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_10.png\" alt=\"IMG_10\" style=\"zoom:40%;\" /></p>\n<h3 id=\"2-4-实验结果\"><a href=\"#2-4-实验结果\" class=\"headerlink\" title=\"2.4 实验结果\"></a>2.4 实验结果</h3><ul>\n<li>在实验效果上，由于 GPT-2 主要是做 zero-shot，所以在实验部分，很多的实验对比都是在无监督的设定下进行的，也就是说他对比的都是无监督的算法</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230329234220217.png\" alt=\"image-20230329234220217\"></p>\n<ul>\n<li>GPT-2 在较多任务上对比无监督算法取得了一定的提升，证明了 zero-shot 的能力。但是，在很多任务上与有监督微调的方法相比还是有一些差距的</li>\n</ul>\n<h1 id=\"3-GPT-v3\"><a href=\"#3-GPT-v3\" class=\"headerlink\" title=\"3 GPT v3\"></a>3 GPT v3</h1><ul>\n<li>GPT3不再像GPT2一样完全主推zero-shot，<strong>而是采用few-shot，采用少量的有监督样本（一般10～100）来辅助模型进行推理</strong>。但是，<strong>GPT3采用有监督样本仅用于推理预测的时候，而不会进行微调的参数更新</strong></li>\n</ul>\n<h3 id=\"3-1-模型结构\"><a href=\"#3-1-模型结构\" class=\"headerlink\" title=\"3.1 模型结构\"></a>3.1 模型结构</h3><ul>\n<li>GPT3采用和GPT2一样的结构，<strong>但是将其中的注意力机制变为了Sparse Attention</strong></li>\n<li>传统的Attention是每个token之间两两计算attentino，复杂度为<script type=\"math/tex\">O(n^2)</script></li>\n<li>而Sparse Attention除了相对距离不超过 k 以及相对距离为 k，2k，3k，… 的 token，其他所有 token 的注意力都设为 0，如下图所示：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_14.png\" alt=\"IMG_14\" style=\"zoom: 80%;\" /></p>\n<p>其计算复杂度为<script type=\"math/tex\">O(n * \\log n)</script></p>\n<ul>\n<li>Sparse Attention的好处：<blockquote>\n<ol>\n<li><strong>减少注意力层的计算复杂度</strong>，节约显存和耗时，从而能够处理更长的输入序列</li>\n<li><strong>具有“局部紧密相关和远程稀疏相关”的特性</strong>，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少</li>\n</ol>\n</blockquote>\n</li>\n<li>最后实验了不同规模的模型：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_15.png\" alt=\"IMG_15\" style=\"zoom: 40%;\" /></p>\n<h3 id=\"3-2-下游评估方法\"><a href=\"#3-2-下游评估方法\" class=\"headerlink\" title=\"3.2 下游评估方法\"></a>3.2 下游评估方法</h3><ul>\n<li>具体到下游任务是，采用了三种不同的方法<strong>（注意这三种方法都只用于推理预测，不会进行参数更新）</strong>：<blockquote>\n<ol>\n<li><strong>Zero-shot：</strong>仅使用当前任务的自然语言描述，不进行任何梯度更新</li>\n<li><strong>One-shot：</strong>当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新</li>\n<li><strong>Few-shot：</strong>当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新，也被称为<strong>in-context learning（上下文学习）</strong></li>\n</ol>\n</blockquote>\n</li>\n<li><strong>和fine-tune的对比：</strong> </li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_16.png\" alt=\"IMG_16\" style=\"zoom: 40%;\" /></p>\n<p>Few-shot虽然和fine-tune一样都用到多个有监督数据，但是其数据量的需要较少（一般10～100个数据），摒弃不进行参数更新</p>\n<ul>\n<li><strong>三种方法对比的实验效果：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_17.png\" alt=\"IMG_17\" style=\"zoom: 40%;\" /></p>\n<h3 id=\"3-3-训练数据\"><a href=\"#3-3-训练数据\" class=\"headerlink\" title=\"3.3 训练数据\"></a>3.3 训练数据</h3><ul>\n<li>GPT-3 使用了多个数据集，其中最大的是 Common Crawl，原始未处理的数据达到了 45TB，其实在 GPT-2 的时候他们就有考虑使用这个数据集，但是后来还是觉得这个<strong>数据集太脏了</strong>所以没用，但是现在 GPT-3 的模型规模太大了，使得训练对数据量的需求也增加了很多，他们不得不重新考虑这个数据集。因此，他们必须在这个数据集上做一些额外的数据清洗工作来尽量保证数据的质量</li>\n<li><strong>数据处理包括：</strong></li>\n</ul>\n<ol>\n<li>采用GPT2中的WebText、Wikiedia等高质量文本作为正样本，用Common Crawl中的样本作为负样本，训练一个LR二分类器，然后采用这个分类器对Common Crawl采样，只保留其中的正样本</li>\n<li>采用MinHashLSH算法，进行相似文本的去重，减少了大约10%的样本</li>\n<li>加入其他的高质量数据集，不同数据集是通过不同的权重进行采样：</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_18.png\" alt=\"IMG_18\" style=\"zoom:50%;\" /></p>\n<h3 id=\"3-4-GPT3的局限性\"><a href=\"#3-4-GPT3的局限性\" class=\"headerlink\" title=\"3.4 GPT3的局限性\"></a>3.4 GPT3的局限性</h3><ol>\n<li>当生成文本长度较长时，GPT-3 还是会出现各种问题，比如重复生成一段话，前后矛盾，逻辑衔接不好等等；</li>\n<li>模型和结构的局限性，对于某一些任务，比如填空类型的文本任务，使用单向的自回归语言模型确实存在一定的局限性，这时候如果同时考虑上文和下文的话，效果很可能会更好一些；</li>\n<li>预训练语言模型的通病，在训练时，语料中所有的词都被同等看待，对于一些虚词或无意义的词同样需要花费很多计算量去学习，无法区分学习重点；</li>\n<li>样本有效性或者利用率过低，训一个模型几乎要把整个互联网上的文本数据全都用起来，这与我们人类学习时所需要的成本存在非常大的差异，这方面也是未来人工智能研究的重点；</li>\n<li>有一个不太确定的点是，模型到底是在“学习”还是在“记忆”？我们当然希望它能够学习，但是在使用数据量如此大的情况下，很难去判断它到底是什么样的；</li>\n<li>众所周知，GPT-3 的训练和使用成本都太大了；</li>\n<li>GPT-3 跟很多深度学习模型一样，都是不可解释的，没办法知道模型内部到底是如何作出一系列决策的；</li>\n<li>训练数据中可能存在种族、性别等偏见，导致模型也会有这种偏见</li>\n</ol>\n<h1 id=\"4-InstructGPT\"><a href=\"#4-InstructGPT\" class=\"headerlink\" title=\"4 InstructGPT\"></a>4 InstructGPT</h1><h3 id=\"4-1-GPT存在的问题\"><a href=\"#4-1-GPT存在的问题\" class=\"headerlink\" title=\"4.1 GPT存在的问题\"></a>4.1 GPT存在的问题</h3><ul>\n<li>GPT的训练方式是采用LM的方法，是估计下一个时间步的词的概率分布：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_19.png\" alt=\"IMG_19\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>但是由于这是一个概率分布，所以模型的一些输入可能并不符合人类的预期，比如：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_20.png\" alt=\"IMG_20\" style=\"zoom:67%;\" /></p>\n<ul>\n<li><strong>对于上述问题的解决方案有两种：</strong><blockquote>\n<ol>\n<li>在训练数据上，构造更加好的问答数据集，但是所花费的人工成本极大，因为训练数据集很大</li>\n<li>引入一个“老师”，让老师对GPT生成的回答进行打分排序，告诉模型人类更期望哪种结果<strong>（这里的老师既可以是真人，也就是使用在线学习；也可以训练一个Reward Model来对模型结果自动打分排序）</strong></li>\n</ol>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"4-2-实现方案\"><a href=\"#4-2-实现方案\" class=\"headerlink\" title=\"4.2 实现方案\"></a>4.2 实现方案</h3><ul>\n<li>模型通过<strong>三个不同的数据集</strong>，完成了三个子任务：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_21.png\" alt=\"IMG_21\" style=\"zoom:55%;\" /></p>\n<ol>\n<li><strong>有监督微调（SFT）：</strong>通过常见的prompt和labeler编写的response，来对GPT-3进行LM任务微调</li>\n<li><strong>训练奖励模型（RM）：</strong>通过常见的prompt，让SFT微调后的GPT模型生成多个response，labeler对这些response进行排序。再使用这些prompt+ response对，输入GPT进行打分</li>\n<li><strong>强化学习（RL）：</strong>只需要prompt，不需要有监督，采用PPO算法，再次微调SFT微调后的模型</li>\n</ol>\n<h3 id=\"4-3-训练数据\"><a href=\"#4-3-训练数据\" class=\"headerlink\" title=\"4.3 训练数据\"></a>4.3 训练数据</h3><ul>\n<li>训练数据所用到的prompt来自两部分：<blockquote>\n<ol>\n<li>labeler先构造了一批prompt和对应的response，对GPT-3进行微调，然后上线内测</li>\n<li>将内测用户的prompt又收集起来，由labeler撰写response</li>\n</ol>\n</blockquote>\n</li>\n<li>然后将两部分数据分为三个子任务的数据集： </li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_22.png\" alt=\"IMG_22\" style=\"zoom: 67%;\" /></p>\n<p>注意，最后RL的数据集是只有prompt，且只包含收集的用户的prompt，而SFT和RM是两者都有</p>\n<h3 id=\"4-4-实现细节\"><a href=\"#4-4-实现细节\" class=\"headerlink\" title=\"4.4 实现细节\"></a>4.4 实现细节</h3><h5 id=\"4-4-1-SFT\"><a href=\"#4-4-1-SFT\" class=\"headerlink\" title=\"4.4.1 SFT\"></a>4.4.1 SFT</h5><ul>\n<li>方法很简单，就不多赘述</li>\n<li>值得注意的是，作者在SFT中一共训练了16个epoch，但是发现在第一个epoch后就过拟合了（这么大的模型用这么小的数据肯定过拟合）。<strong>但是由于这个模型并不是微调完就直接拿来用，所以过拟合也没关系。甚至更多的epoch甚至能产生更高的RM分数的输出</strong></li>\n</ul>\n<h5 id=\"4-4-2-RM\"><a href=\"#4-4-2-RM\" class=\"headerlink\" title=\"4.4.2 RM\"></a>4.4.2 RM</h5><ul>\n<li>先采用SFT后的模型，对一个prompt生成多个response，并对每一对prompt+response，让labeler进行排序：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_23.png\" alt=\"IMG_23\" style=\"zoom: 25%;\" /></p>\n<ul>\n<li>然后将SFT后的模型最后的输出层去掉，转而变为一个只有一个神经元的线性层</li>\n<li>将每一对prompt+response连结起来，输入该模型，最后输出相当于两者契合的logit分数。然后采用以下损失函数进行优化：<script type=\"math/tex; mode=display\">\n\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c}\nK \\\\\n2\n\\end{array}\\right)} E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_{\\theta}\\left(x, y_{w}\\right)-r_{\\theta}\\left(x, y_{l}\\right)\\right)\\right)\\right]</script>其中，K是每个prompt生成的response数量，<script type=\"math/tex\">y_w, y_l</script>分别是prompt输入x的输出response，且<script type=\"math/tex\">rank(y_w) \\ge rank(y_l)</script>，外层函数就相当于一个Logestic Regression</li>\n<li>RM是采用的6B的模型，因为作者发现<strong>大模型（比如175B）训练后期loss不稳定</strong></li>\n<li>此外，作者还提出了另一种方法：采用交叉熵，将排名第一的输出当作正样本，其他输出当作负样本，但是<strong>非常容易过拟合</strong></li>\n</ul>\n<h5 id=\"4-4-3-RL\"><a href=\"#4-4-3-RL\" class=\"headerlink\" title=\"4.4.3 RL\"></a>4.4.3 RL</h5><ul>\n<li>RL涉及三个模型：RM模型<script type=\"math/tex\">r_{\\theta}</script>、SFT模型<script type=\"math/tex\">\\pi^{SFT}</script>和我们最终想要得到的RL模型<script type=\"math/tex\">\\pi^{RL}</script>；以及两个数据集RL自身的数据集<script type=\"math/tex\">D_{RL}</script>和预训练时的一部分数据集<script type=\"math/tex\">D_{pretrain}</script></li>\n<li>优化目标如下：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{objective}(\\phi)= & E_{(x, y) \\sim D_{\\pi_{\\phi}^{\\mathrm{RL}}}}\\left[r_{\\theta}(x, y)-\\beta \\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\\n& \\gamma E_{x \\sim D_{\\text {prerrain }}}\\left[\\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(x)\\right)\\right]\n\\end{aligned}</script></li>\n<li>一开始<script type=\"math/tex\">\\pi^{RL}</script>是由<script type=\"math/tex\">\\pi^{SFT}</script>初始化得来的</li>\n<li>对于第一项，是想让<script type=\"math/tex\">\\pi^{RL}</script>的输出得到的RM分数尽可能高，并且在这个微调过程中，<script type=\"math/tex\">\\pi^{RL}</script>和<script type=\"math/tex\">\\pi^{SFT}</script>的差距不能过大，所以减去两者的KL散度来保证这个差距</li>\n<li>如果只使用第一项，方法就称作PPO。但是为了防止模型遗忘预训练时的知识，引入第二项，也就是预训练任务的优化目标，加入第二项后则称为PPO-ptx</li>\n</ul>\n<h3 id=\"4-5-实验结果\"><a href=\"#4-5-实验结果\" class=\"headerlink\" title=\"4.5 实验结果\"></a>4.5 实验结果</h3><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_24.png\" alt=\"IMG_24\" style=\"zoom: 80%;\" /></p>\n<h1 id=\"5-GPT-vs-BERT\"><a href=\"#5-GPT-vs-BERT\" class=\"headerlink\" title=\"5 GPT vs BERT\"></a>5 GPT vs BERT</h1><ul>\n<li>编码器和解码器的选取倒不是 GPT 和 BERT 的区别，它们的区别主要是预训练目标函数的选取，有人认为 GPT 选择的是一个更难的训练目标，它是根据前面的信息去预测下文，预测未来肯定是比完形填空难度要更大的。这也能从某种程度上解释了为什么相同规模的 GPT 和 BERT 模型，GPT 的效果要比 BERT 差。</li>\n<li>但是从另一个角度去想，如果能够把预测未来这个事情做好的话，它最终所能达到的效果的天花板一定是更高的，这可能也是 OpenAI 从一开始到现在一直坚持使用标准语言模型目标函数来做预训练模型的其中一个原因吧，当然这只是一种猜想。事实证明，从 GPT-3 开始，到最近的 ChatGPT，OpenAI 所取得的令人惊艳的效果也一定程度上证明了他们的选择的正确性。</li>\n</ul>\n"},{"title":"HMM和CRF","math":true,"date":"2022-06-13T16:00:00.000Z","_content":"\n\n\n# 1 隐马尔可夫模型\n\n- 隐马尔可夫模型（Hidden Markov Model, HMM）常用于序列标注问题，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于**概率图模型**（用图结构来描述变量之间的关系，属于生成式模型）\n- HMM属于[贝叶斯网](https://zlkqz.site/2022/09/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#6-EM%E7%AE%97%E6%B3%95)，其两个基本假设其实就是贝叶斯网的假设：**给定父节点集，贝叶斯网假设每个属性与他的非后裔属性独立**\n\n\n\n### 1.1 模型定义\n\n- 设$$Q=  \\{q_1, ..., q_N\\}$$是所有可能的状态的集合，$$V = \\{v_1, ..., v_M\\}$$是所有可能的观测的集合。而$$I = (i_1, ..., i_T)$$是状态序列，$$O = (o_1, ..., o_T)$$是对应的观测序列，模型定义了三种参数：\n\n> 1. **状态转移矩阵A：**\n>\n> $$\n> A = [a_{ij}]_{N \\times N}\n> $$\n>\n> 其中$$a_{ij}$$指在时刻t处于状态$$q_i$$的条件下转移到在t+1时刻状态为$$q_j$$的概率：\n> $$\n> a_{ij} = P(i_{t+1}=q_j|i_t=q_i)\n> $$\n>\n> 2. **观测概率矩阵B：**\n>\n> $$\n> B = [b_j(k)]_{N\\times N}\n> $$\n>\n> 其中$$b_j(k)$$指在t时刻处于状态$$q_j$$时生成观测$$v_k$$的概率：\n> $$\n> b_j(k) = P(o_t = v_k|i_t =  q_j)\n> $$\n>\n> 3. **初始状态概率向量$$\\pi$$：**\n>\n> $$\n> \\pi = (\\pi_i)\n> $$\n>\n> 其中$$\\pi_i$$是$$t=1$$时处于状态$$q_i$$的概率：\n> $$\n> \\pi = P(i_1 = q_i)\n> $$\n\n- 一般使用一个三元组表示HMM的参数：\n\n$$\n\\lambda = (A,B,\\pi)\n$$\n\n- 上面对参数的定义中，隐含了HMM的两个基本假设：\n\n> 1. **齐次马尔可夫性假设：**假设隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关：\n>\n> $$\n> P\\left(i_{t} \\mid i_{t-1}, o_{t-1}, \\cdots, i_{1}, o_{1}\\right)=P\\left(i_{t} \\mid i_{t-1}\\right), \\quad t=1,2, \\cdots, T\n> $$\n>\n> 2. **观测独立性假设：**假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：\n>\n> $$\n> P\\left(o_{t} \\mid i_{T}, o_{T}, i_{T-1}, o_{T-1}, \\cdots, i_{t+1}, o_{t+1}, i_{t}, i_{t-1}, o_{t-1}, \\cdots, i_{1}, o_{1}\\right)=P\\left(o_{t} \\mid i_{t}\\right)\n> $$\n>\n> **其实这两个假设就是贝叶斯网的假设，只不过结构特殊一点，是一个线性结构**\n\n\n\n\n\n### 1.2 概率计算\n\n- 概率计算即给定模型$$\\lambda = (A,B,\\pi)$$和观测序列$$O=(o_1, ..., o_T)$$，计算该观测序列出现的概率$$P(O|\\lambda)$$\n\n\n\n#### 1.2.1 直接计算\n\n- 直接计算即列举所有可能的状态序列$$I$$，计算：\n\n$$\n\\begin{aligned}\nP(O \\mid \\lambda) &=\\sum_{I} P(O \\mid I, \\lambda) P(I \\mid \\lambda) \\\\\n&=\\sum_{i_{1}, i_{2}, \\cdots, i_{T}} \\pi_{i_{1}} b_{i_{1}}\\left(o_{1}\\right) a_{i_{1} i_{2}} b_{i_{2}}\\left(o_{2}\\right) \\cdots a_{i_{T-i} i_{T}} b_{i_{T}}\\left(o_{T}\\right)\n\\end{aligned}\n$$\n\n- 但是这种方法计算量过大，复杂度为$$O(TN^T)$$，所以是不可行的\n\n\n\n#### 1.2.2 前向计算\n\n- 给定模型$$\\lambda$$，定义在时刻t时观测序列为$$o_1, ..., o_t$$，且当前状态为$$q_i$$的概率为前向概率：\n\n$$\n\\alpha_t(i) = P(o_1, ..., o_t, i_t=q_i|\\lambda)\n$$\n\n- 算法流程：\n\n给定模型$$\\lambda$$和观测序列$$O=(o_1, ..., o_T)$$\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028120947404.png\" alt=\"image-20221028120947404\" style=\"zoom:80%;\" />\n\n\n\n#### 1.2.3 后向计算\n\n- 给定模型$$\\lambda$$，定义在时刻t时刻状态为$$q_i$$的条件下，从t+1到T的部分观测序列为$$o_{t+1}, ..., o_T$$的概率为后向概率：\n\n$$\n\\beta_{t}(i)=P\\left(o_{t+1}, o_{t+2}, \\cdots, o_{T} \\mid i_{t}=q_{i}, \\lambda\\right)\n$$\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028122410576.png\" alt=\"image-20221028122410576\" style=\"zoom:75%;\" />\n\n\n\n#### 1.2.4 常用概率的计算\n\n1. **计算给定模型$$\\lambda$$和观测$$O$$，在t时刻处于$$q_i$$的概率：**\n\n$$\n\\gamma_t(i) = P(i_t = q_i|O, \\lambda)\n$$\n\n> - 首先运用贝叶斯公式：\n>\n> $$\n> \\gamma_{t}(i)=P\\left(i_{t}=q_{i} \\mid O, \\lambda\\right)=\\frac{P\\left(i_{t}=q_{i}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}\n> $$\n>\n> - 由上述的前向后向概率的定义可得：\n>\n> $$\n> \\alpha_t(i)\\beta_t(i) = P(i_t=q_i,O|\\lambda)\n> $$\n>\n> - 于是得到：\n>\n> $$\n> \\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)}\n> $$\n\n2. **计算给定模型$$\\lambda$$和观测$$O$$，在t时刻处于状态$$q_i$$并且在t+1时刻处于状态$$q_j$$的概率：**\n\n$$\n\\xi_{t}(i, j)=P\\left(i_{t}=q_{i}, i_{t+1}=q_{j} \\mid O, \\lambda\\right)\n$$\n\n> - 同样先运用贝叶斯公式：\n>\n> $$\n> \\xi_{t}(i, j)=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}\n> $$\n>\n> - 其中：\n>\n> $$\n> P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)=\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)\n> $$\n>\n> - 所以：\n>\n> $$\n> \\xi_{t}(i, j)=\\frac{\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}\n> $$\n\n\n\n### 1.3 学习算法\n\n- 在观测序列和状态序列都给定的时候，训练集足够大时，可以直接通过极大似然估计来估计模型参数，即**直接通过频数来估计概率**\n- 我们主要讨论的是只给定观测序列时的情况，**此时状态序列为隐变量**，所以使用[EM算法](https://zlkqz.site/2022/09/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#6-EM%E7%AE%97%E6%B3%95)\n\n- 首先是E步，即计算Q函数：\n\n$$\nQ(\\lambda, \\bar{\\lambda}) = \\sum_{I}\\log P(O,I|\\lambda)P(I|O,\\bar{\\lambda}) = \\sum_{I}\\log P(O,I|\\lambda)\\frac{P(O,I|\\bar{\\lambda})}{P(O|\\bar{\\lambda})}\n$$\n\n其中$$\\bar{\\lambda}$$是HMM参数的当前估计值，$$\\lambda$$是要极大化的值，作为下次迭代的新参数\n\n- 由于分母中的$$P(O|\\bar{\\lambda})$$和要更新的参数$$\\lambda$$无关，所以直接去掉，Q函数直接化为：\n\n$$\nQ(\\lambda, \\bar{\\lambda})=\\sum_{I} \\log P(O, I \\mid \\lambda) P(O, I \\mid \\bar{\\lambda})\n$$\n\n- 其中：\n\n$$\nP(O, I \\mid \\lambda)=\\pi_{i_{1}} b_{i_{1}}\\left(o_{1}\\right) a_{i_{i} i_{2}} b_{i_{2}}\\left(o_{2}\\right) \\cdots a_{i_{T-1} i_{T}} b_{i_{T}}\\left(o_{T}\\right)\n$$\n\n- 所以：\n\n$$\n\\begin{aligned}\nQ(\\lambda, \\bar{\\lambda})=& \\sum_{I} \\log \\pi_{i} P(O, I \\mid \\bar{\\lambda}) +\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i, i_{i}+1}\\right) P(O, I \\mid \\bar{\\lambda})+\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})\n\\end{aligned}\n$$\n\n- 然后是M步，即最大化Q函数，而最大化Q函数的问题，可以化为分别最大化上式中Q函数中的三项：\n\n> 1. 最大化$$\\sum_{I} \\log \\pi_{i} P(O, I \\mid \\bar{\\lambda})$$，可以先将其化为：\n>\n> $$\n> \\sum_{I} \\log \\pi_{i_{0}} P(O, I \\mid \\bar{\\lambda})=\\sum_{i=1}^{N} \\log \\pi_{i} P\\left(O, i_{1}=i \\mid \\bar{\\lambda}\\right)\n> $$\n>\n> 由于存在约束$$\\sum_{i=1}^N\\pi_i =1$$，所以可以使用拉格朗日乘子法进行求解，最后得到：\n> $$\n> \\pi_{i}=\\frac{P\\left(O, i_{1}=i \\mid \\bar{\\lambda}\\right)}{P(O \\mid \\bar{\\lambda})}\n> $$\n>\n> 2. 最大化$$\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i, i_{i}+1}\\right) P(O, I \\mid \\bar{\\lambda})$$，化为：\n>\n> $$\n> \\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i_{i} i_{t+1}}\\right) P(O, I \\mid \\bar{\\lambda})=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\sum_{t=1}^{T-1} \\log a_{i j} P\\left(O, i_{t}=i, i_{t+1}=j \\mid \\bar{\\lambda}\\right)\n> $$\n>\n> 运用拉格朗日乘子法，最后得到：\n> $$\n> a_{i j}=\\frac{\\sum_{t=1}^{T-1} P\\left(O, i_{t}=i, i_{t+1}=j \\mid \\bar{\\lambda}\\right)}{\\sum_{t=1}^{T-1} P\\left(O, i_{t}=i \\mid \\bar{\\lambda}\\right)}\n> $$\n>\n> 3. 最大化$$\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})$$，化为：\n>\n> $$\n> \\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})=\\sum_{j=1}^{N} \\sum_{t=1}^{T} \\log b_{j}\\left(o_{t}\\right) P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right)\n> $$\n>\n> 运用拉格朗日乘子法，最后得到：\n> $$\n> b_{j}(k)=\\frac{\\sum_{t=1}^{T} P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right) I\\left(o_{t}=v_{k}\\right)}{\\sum_{t=1}^{T} P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right)}\n> $$\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028132201642.png\" alt=\"image-20221028132201642\" style=\"zoom:67%;\" />\n\n\n\n### 1.4 预测算法\n\n- 序列模型常用**维特比算法**来进行预测，算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028132500929.png\" alt=\"image-20221028132500929\" style=\"zoom:67%;\" />\n\n\n\n\n\n# 2 条件随机场\n\n- 条件随机场（Conditional Random Field, CRF）和HMM一样，同样属于概率图模型，是给定一组输入随机变量条件下，另一组输出是随机变量的条件概率分布模型\n- 有一种特殊且最常用的CRF为线性链条件随机场，其结构和数学推导和HMM十分相似，但是仍有一些区别，稍后介绍\n\n\n\n### 2.1 马尔可夫随机场\n\n- **概率图模型**是由图结构来描述变量之间的关系的模型，而采用有向无环图结构被称为**贝叶斯网**，采用无向图则被称为**概率无向图模型或马尔可夫随机场**\n\n- 设有联合概率分布$$P(Y)$$，$$Y \\in \\mathcal{Y}$$是一组随机变量。由无向图$$G = (V,E)$$表示概率分布$$P(Y)$$，即在G中，结点$$v \\in V$$表示一个随机变量$$Y_v$$，$$Y = (Y_v)_{v \\in V}$$，而边$$e \\in E$$表示随机变量间的概率依赖关系\n- 首先定义无向图模型内的马尔可夫性：\n\n> 1. **成对马尔可夫性：**设u和v是图中任意两个没有边连接的结点，其对应的随机变量分别为$$Y_u, Y_v$$，其他所有结点为O，对应随机变量$$Y_O$$。成对马尔可夫性指给定$$Y_O$$的条件下，$$Y_u, Y_v$$是条件独立的：\n>\n> $$\n> P\\left(Y_{u}, Y_{v} \\mid Y_{o}\\right)=P\\left(Y_{u} \\mid Y_{o}\\right) P\\left(Y_{v} \\mid Y_{o}\\right)\n> $$\n>\n> 2. **局部马尔可夫性：**设v为图中任意一个结点，W是与v相连的所有结点，O是除v和W之外的所有结点。局部马尔可夫性指给定$$Y_W$$的条件下$$Y_v, Y_O$$是条件独立的：\n>\n> $$\n> P\\left(Y_{v}, Y_{o} \\mid Y_{W}\\right)=P\\left(Y_{v} \\mid Y_{W}\\right) P\\left(Y_{o} \\mid Y_{W}\\right)\n> $$\n>\n> 也可以为表示为：\n> $$\n> P(Y_v|Y_W) = P(Y_v|Y_W, Y_O)\n> $$\n>\n> 3. **全局马尔可夫性：**A、B是在图中被结点集合C分开的任意结点集合（如下图所示）。全局马尔可夫性指给定$$Y_C$$条件下$$Y_A,Y_B$$条件独立：\n>\n> $$\n> P\\left(Y_{A}, Y_{B} \\mid Y_{C}\\right)=P\\left(Y_{A} \\mid Y_{C}\\right) P\\left(Y_{B} \\mid Y_{C}\\right)\n> $$\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029144742544.png\" alt=\"image-20221029144742544\" style=\"zoom:67%;\" />\n>\n> **上述三种马尔可夫性是等价的**\n\n- 而马尔可夫随机场，不仅要满足使用无向图，还需要满足马尔可夫性。显然，CRF属于马尔可夫随机场\n\n\n\n### 2.2 团和极大团\n\n- 无向图中的任意一个强连通子集都称为**团（clique）**，而一个团不能再加任意一个结点使其仍为团，则这种团为**极大团（maximal clique）**\n\n- 联合概率分布可以用每个团的**势函数（potential function）**的乘积来表示，但是一个图中的团很多，且有些随机变量同时属于多个团，所以简化来说，可以直接使用**极大团的势函数成绩**：\n\n$$\nP(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right)\n$$\n\n其中C为极大团集，Z为规范化因子，$$\\Psi$$为势函数，要求势函数是严格正的，一般定义为指数函数：\n$$\n\\Psi_{C}\\left(Y_{C}\\right)=\\exp \\left\\{-E\\left(Y_{C}\\right)\\right\\}\n$$\n\n\n### 2.3 模型定义\n\n- 设随机变量$$X,Y$$，如果对任意结点v满足马尔可夫性（下式为局部马尔可夫性）：\n\n$$\nP\\left(Y_{v} \\mid X, Y_{w}, w \\neq v\\right)=P\\left(Y_{v} \\mid X, Y_{w}, w \\sim v\\right)\n$$\n\n则称条件概率$$P(Y|X)$$为条件随机场\n\n- 另外有CRF的特例：**线性链条件随机场**，满足马尔可夫性：\n\n$$\n\\begin{array}{c}\nP\\left(Y_{i} \\mid X, Y_{1}, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_{n}\\right)=P\\left(Y_{i} \\mid X, Y_{i-1}, Y_{i+1}\\right) \\\\\ni=1,2, \\cdots, n \\text { (在 } i=1 \\text { 和 } n \\text { 时只考虑单边) }\n\\end{array}\n$$\n\n则称条件概率$$P(Y|X)$$为条件随机场，图结构如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029152606453.png\" alt=\"image-20221029152606453\" style=\"zoom: 67%;\" />\n\n- CRF定义中没有要求X的结构，但是一般假设X和Y有相同的图结构，比如线性链条件随机场：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029152749755.png\" alt=\"image-20221029152749755\" style=\"zoom:67%;\" />\n\n在标注问题中（如NER），X表示输入观测序列，Y表示对应的输出标记序列或状态序列\n\n\n\n### 2.4 CRF的参数\n\n- 现在开始介绍的CRF都默认为线性链CRF\n- 前面说过，马尔科夫场的概率可以用极大团的势函数来表示：\n\n$$\nP(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right) = \\frac{1}{Z}\\exp \\sum_C-E\\left(Y_{C}\\right) = \\frac{1}{Z}\\exp \\sum_CF_C\\left(Y_{C}\\right)\n$$\n\n- 对于线性链CRF，每一个$$y_{i-1}, y_i$$构成一个极大团，所以：\n\n$$\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^TF_t(y_{t-1},y_t,x)\n$$\n\n其中T为时间步数，$$x$$为X的取值，是整个观测序列。**由于每个极大团的结构都相同**，所以可以**简化为每个极大团的势函数都一样**：\n$$\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^TF(y_{t-1},y_t,x)\n$$\n\n- 而对于每个$$F(y_{t-1},y_t,x)$$，可以表示为三个函数的和：\n\n$$\nF(y_{t-1},y_t,x) = F_1(y_{t-1}, x) + F_1(y_t, x) + F_2(y_{t-1}, y_t, x)\n$$\n\n其中$$F_1$$就称为**状态函数**，$$F_2$$称为**转移函数**。由于$$F_1(y_{t-1},x)$$在上一个时间步已经出现过了，所以可以直接去掉：\n$$\nF(y_{t-1},y_t,x) = F_1(y_t, x) + F_2(y_{t-1}, y_t, x)\n$$\n\n- 那么可以引入特征函数来定义$$F_1, F_2$$：\n\n$$\nF_1 = \\sum_{l=1}^{K_2}\\mu_ls_l(y_t, x) \\\\\nF_2 = \\sum_{k=1}^{K_1}\\lambda_kt_k(y_{t-1}, y_t, x)\n$$\n\n- 所以条件概率就表示为：\n\n$$\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^T(\\sum_{k=1}^{K_1}\\lambda_kt_k(y_{t-1}, y_t, x) + \\sum_{l=1}^{K_2}\\mu_ls_l(y_t, x))\n$$\n\n- 而可以把关于时间步的求和放入括号中：\n\n$$\nP(Y|X) = \\frac{1}{Z}\\exp(\\sum_{k=1}^{K_1}\\lambda_k\\sum_{t=1}^Tt_k(y_{t-1}, y_t, x)+\\sum_{l=1}^{K_2}\\mu_l\\sum_{t=1}^Ts_l(y_t, x))\n$$\n\n- 将两种特征函数和其权重合起来：\n\n$$\nf_{k}\\left(y_{t-1}, y_{t}, x\\right)=\\left\\{\\begin{array}{l}\nt_{k}\\left(y_{t-1}, y_{t}, x\\right), \\quad k=1,2, \\cdots, K_{1} \\\\\ns_{l}\\left(y_{t}, x\\right), \\quad k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.  \\\\\nw_{k}=\\left\\{\\begin{array}{ll}\n\\lambda_{k}, & k=1,2, \\cdots, K_{1} \\\\\n\\mu_{l}, & k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.\n$$\n\n然后对特征函数在各个时间步进行求和，记作：\n$$\nf_{k}(y, x)=\\sum_{t=1}^{T} f_{k}\\left(y_{t-1}, y_{t}, x\\right), \\quad k=1,2, \\cdots, K\n$$\n\n- 所以条件概率化简为：\n\n$$\nP(y \\mid x)=\\frac{1}{Z(x)} \\exp \\sum_{k=1}^{K} w_{k} f_{k}(y, x)\n$$\n\n- 上式采用向量化表示，引入：\n\n$$\n\\begin{array}{c}\nw=\\left(w_{1}, w_{2}, \\cdots, w_{K}\\right)^{\\mathrm{T}} \\\\\nF(y, x)=\\left(f_{1}(y, x), f_{2}(y, x), \\cdots, f_{K}(y, x)\\right)^{\\mathrm{T}}\n\\end{array}\n$$\n\n所以条件概率的向量化表示为：\n$$\n\\begin{array}{l}\nP_{w}(y \\mid x)=\\frac{\\exp (w \\cdot F(y, x))}{Z_{w}(x)} \\\\\nZ_{w}(x)=\\sum_{y} \\exp (w \\cdot F(y, x))\n\\end{array}\n$$\n**其中特征函数$$F(y,x)$$是事先设计好给出的，而学习的目标即学习权重$$w$$，最大化特征函数的总得分**\n\n\n\n### 2.5 概率计算\n\n- 和前面HMM一样，同样运用前向后向算法的变量来进行概率计算，首先定义一个矩阵，**为了方便讨论，我们又引入了两个时间步的状态序列$$y_0 = start, y_{T+1} = stop$$，实际前文的讨论中也隐含了$$y_0 = start$$：**\n\n$$\n\\begin{array}{c}\nM_{t}(x)=\\left[M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)\\right] \\\\\nM_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)=\\exp \\sum_{k=1}^{K} w_{k} f_{k}\\left(y_{t-1}, y_{t}, x\\right)\n\\end{array}\n$$\n\n设$$y_t$$有m个取值，则矩阵$$M_t(x)$$里面的$$y_t,y_{t-1}$$分别取不同的m个值，所以$$M_t(x)$$是一个$$m \\times m$$阶矩阵\n\n- 有了上述定义，可以将条件概率进一步写为矩阵形式：\n\n$$\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\prod_{t=1}^{T} M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)\n$$\n\n其中$$Z_w(x)$$是T+1个矩阵乘积的第(start, end)元素：\n$$\nZ_{w}(x)=\\left(M_{1}(x) M_{2}(x) \\cdots M_{T+1}(x)\\right)_{\\text {start,stop }}\n$$\n\n- 现在来定义**前向概率，$$\\alpha_t(y_t|x)$$为在时刻t时观测序列为$$x_1, ..., x_t$$，且当前状态为$$y_t$$的概率**：\n\n$$\n\\alpha_{t}\\left(y_{t} \\mid x\\right)=\\alpha_{t-1}\\left(y_{t-1} \\mid x\\right) M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right) \\quad t=1,...,T+1 \\\\\n\\alpha_{0}(y \\mid x)=\\left\\{\\begin{array}{ll}\n1, & y=\\operatorname{start} \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.\n$$\n\n因为$$y_t$$有m个取值，所以可以定义m维向量$$\\alpha_t(x)$$：\n$$\n\\alpha_{t}^{\\mathrm{T}}(x)=\\alpha_{t-1}^{\\mathrm{T}}(x) M_{t}(x) \\\\\n\\alpha_0(x) = \\mathbb{1}（单位向量）\n$$\n\n- 同样可以定义**后向概率，$$\\beta_t(y_t|x)$$为在时刻t观测序列为$$x_{t+1}, ..., x_T$$，且当前状态为$$y_t$$的概率：**\n\n$$\n\\beta_{t}\\left(y_{t} \\mid x\\right)=M_{t}\\left(y_{t}, y_{t+1} \\mid x\\right) \\beta_{t+1}\\left(y_{t+1} \\mid x\\right) \\\\\n\\beta_{t+1}\\left(y \\mid x\\right)=\\left\\{\\begin{array}{ll}\n1, & y=\\text { stop } \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.\n$$\n\n同样有向量形式：\n$$\n\\beta_{t}(x)=M_{t+1}(x) \\beta_{t+1}(x) \\\\\n\\beta_{T+1}(x) = \\mathbb{1}  (单位向量)\n$$\n\n- 得到了前后向概率，就可以得到**在时刻t状态是$$y_t$$的概率：**\n\n$$\nP\\left(Y_{t}=y_{t} \\mid x\\right)=\\frac{\\alpha_{t}\\left(y_{t} \\mid x\\right) \\beta_{t}\\left(y_{t} \\mid x\\right)}{Z(x)}\n$$\n\n- 以及**在时刻t-1状态是$$y_{t-1}$$并且在时刻t状态是$$y_t$$的概率：**\n\n$$\nP\\left(Y_{t-1}=y_{t-1}, Y_{t}=y_{t} \\mid x\\right)=\\frac{\\alpha_{t-1}\\left(y_{t-1} \\mid x\\right) M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right) \\beta_{t}\\left(y_{t} \\mid x\\right)}{Z(x)}\n$$\n\n- 上述两个式子的$$Z(x)$$可以用更简单的形式表达：\n\n$$\nZ(x) = \\alpha_n^T(x)\\mathbb{1} = \\mathbb{1}\\beta_1(x)\n$$\n\n\n\n### 2.6 学习和预测\n\n- 预测算法和HMM一样，使用**维特比算法**\n\n- 学习算法其实就是[最大熵模型的学习](https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#3-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)\n\n\n\n\n\n# 3 HMM和CRF的区别\n\n- CRF和HMM的数学推导几乎是一样的，但是差异在于：\n\n> 1. HMM属于贝叶斯网络，而CRF属于马尔可夫场，前者的假设和约束更加的严格，比如CRF并没有像HMM一样完全依赖上一步的状态，HMM的观测变量的生成是独立的\n> 2. 上述我们讨论的仅仅只是线性链CRF，但是CRF还可以有其他的图结构，并且**实际上可以任意选定特征函数的个数和形式，特征函数的不确定也是CRF能和深度学习融合的最主要原因，模型可以自己学习特征函数，并且不用显示地表达出来**\n\n\n\n\n\n# 4 CRF和深度学习模型的结合\n\n- 以BiLSTM+CRF做NER任务举例\n\n- 如果不用CRF而是直接在模型的后面接一个Softmax，鉴于所选取的基模型的强大的特征抽取能力，这已经可以有比较好的分类效果，**但是NER任务是存在一些约束的**，比如BIO格式中，B-Person后面不可能跟I-Organization。**Softmax的分类是每个时间步相互独立的，所以可能会出现上述的问题**\n\n- **而CRF层可以加入一些约束来保证最终预测结果是有效的，这些约束可以在训练数据时被CRF层自动学习得到**\n\n- 首先介绍一下模型结构：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-0650e1511e7d4419c9528a8d08ea61fd_720w.webp\" alt=\"img\" style=\"zoom:80%;\" />\n\nBiLSTM的输出经过Dense层，**转化为每个Label对应的score（图中浅黄色部分），这个score即CRF中的状态得分$$\\sum \\mu s$$**，然后将其输入CRF，CRF层维护了一个转移矩阵（Transition Matrix），这也是CRF层中需要学习的参数，假设总共有包括START和END在内的7个label，则转移矩阵为：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-2064e34cece3be4e852b1ace6bbca2ba_720w.webp\" alt=\"img\" style=\"zoom:67%;\" />\n\n**每个元素代表了相邻时间步之间进行状态转移的score，这个score即CRF中的转移得分$$\\sum\\lambda t$$**，并且上述矩阵已经学到了一些有用的约束：\n\n> 1. 句子的第一个单词应该是“B-” 或 “O”，而不是“I”。（从“START”->“I-Person 或 I-Organization”的转移分数很低）\n> 2. “B-label1 I-label2 I-label3…”，在该模式中，类别1,2,3应该是同一种实体类别。比如，“B-Person I-Person” 是正确的，而“B-Person I-Organization”则是错误的。（“B-Organization” -> “I-Person”的分数很低）\n> 3. O I-label”是错误的，命名实体的开头应该是“B-”而不是“I-”\n\n- 每个输入有N中可能的结果，即N条路径，比如（加粗的为真实路径）：\n\n> 1. START B-Person B-Person B-Person B-Person B-Person END\n> 2. START B-Person I-Person B-Person B-Person B-Person END\n> 3. **START B-Person I-Person O B-Organization O END**\n>\n> ......\n>\n>    N. O O O O O O O\n\n- 训练目标即最大化真实路径的得分，可得损失函数：\n\n$$\nLoss\n =-\\log \\frac{P_{\\text {Real Path }}}{P_{1}+P_{2}+\\ldots+P_{N}} \n =-\\log \\frac{e^{s_{\\text {Realpath }}}}{e^{s_{1}+e^{s_{2}}+\\ldots+e^{s_{N}}}}  \\\\\n =-\\left(S_{\\text {RealPath }}-\\log \\left(e^{S_{1}}+e^{S_{2}}+\\ldots+e^{S_{N}}\\right)\\right)\n$$\n\n其中$$S_i$$为一条路径对应的得分，是通过Softmax实现最大化的\n\n- 值得一提的是，计算分母中的所有路径的得分和$$-\\log (e^{S_1} + ... + e^{S_N})$$不需要列举所有可能路径，可以用一种动态规划的方法降低计算复杂度\n\n- 另外，在进行预测的时候，同样是使用维特比算法","source":"_posts/HMM和CRF.md","raw":"---\ntitle: HMM和CRF\nmath: true\ndate: 2022-6-14\n---\n\n\n\n# 1 隐马尔可夫模型\n\n- 隐马尔可夫模型（Hidden Markov Model, HMM）常用于序列标注问题，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于**概率图模型**（用图结构来描述变量之间的关系，属于生成式模型）\n- HMM属于[贝叶斯网](https://zlkqz.site/2022/09/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#6-EM%E7%AE%97%E6%B3%95)，其两个基本假设其实就是贝叶斯网的假设：**给定父节点集，贝叶斯网假设每个属性与他的非后裔属性独立**\n\n\n\n### 1.1 模型定义\n\n- 设$$Q=  \\{q_1, ..., q_N\\}$$是所有可能的状态的集合，$$V = \\{v_1, ..., v_M\\}$$是所有可能的观测的集合。而$$I = (i_1, ..., i_T)$$是状态序列，$$O = (o_1, ..., o_T)$$是对应的观测序列，模型定义了三种参数：\n\n> 1. **状态转移矩阵A：**\n>\n> $$\n> A = [a_{ij}]_{N \\times N}\n> $$\n>\n> 其中$$a_{ij}$$指在时刻t处于状态$$q_i$$的条件下转移到在t+1时刻状态为$$q_j$$的概率：\n> $$\n> a_{ij} = P(i_{t+1}=q_j|i_t=q_i)\n> $$\n>\n> 2. **观测概率矩阵B：**\n>\n> $$\n> B = [b_j(k)]_{N\\times N}\n> $$\n>\n> 其中$$b_j(k)$$指在t时刻处于状态$$q_j$$时生成观测$$v_k$$的概率：\n> $$\n> b_j(k) = P(o_t = v_k|i_t =  q_j)\n> $$\n>\n> 3. **初始状态概率向量$$\\pi$$：**\n>\n> $$\n> \\pi = (\\pi_i)\n> $$\n>\n> 其中$$\\pi_i$$是$$t=1$$时处于状态$$q_i$$的概率：\n> $$\n> \\pi = P(i_1 = q_i)\n> $$\n\n- 一般使用一个三元组表示HMM的参数：\n\n$$\n\\lambda = (A,B,\\pi)\n$$\n\n- 上面对参数的定义中，隐含了HMM的两个基本假设：\n\n> 1. **齐次马尔可夫性假设：**假设隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关：\n>\n> $$\n> P\\left(i_{t} \\mid i_{t-1}, o_{t-1}, \\cdots, i_{1}, o_{1}\\right)=P\\left(i_{t} \\mid i_{t-1}\\right), \\quad t=1,2, \\cdots, T\n> $$\n>\n> 2. **观测独立性假设：**假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：\n>\n> $$\n> P\\left(o_{t} \\mid i_{T}, o_{T}, i_{T-1}, o_{T-1}, \\cdots, i_{t+1}, o_{t+1}, i_{t}, i_{t-1}, o_{t-1}, \\cdots, i_{1}, o_{1}\\right)=P\\left(o_{t} \\mid i_{t}\\right)\n> $$\n>\n> **其实这两个假设就是贝叶斯网的假设，只不过结构特殊一点，是一个线性结构**\n\n\n\n\n\n### 1.2 概率计算\n\n- 概率计算即给定模型$$\\lambda = (A,B,\\pi)$$和观测序列$$O=(o_1, ..., o_T)$$，计算该观测序列出现的概率$$P(O|\\lambda)$$\n\n\n\n#### 1.2.1 直接计算\n\n- 直接计算即列举所有可能的状态序列$$I$$，计算：\n\n$$\n\\begin{aligned}\nP(O \\mid \\lambda) &=\\sum_{I} P(O \\mid I, \\lambda) P(I \\mid \\lambda) \\\\\n&=\\sum_{i_{1}, i_{2}, \\cdots, i_{T}} \\pi_{i_{1}} b_{i_{1}}\\left(o_{1}\\right) a_{i_{1} i_{2}} b_{i_{2}}\\left(o_{2}\\right) \\cdots a_{i_{T-i} i_{T}} b_{i_{T}}\\left(o_{T}\\right)\n\\end{aligned}\n$$\n\n- 但是这种方法计算量过大，复杂度为$$O(TN^T)$$，所以是不可行的\n\n\n\n#### 1.2.2 前向计算\n\n- 给定模型$$\\lambda$$，定义在时刻t时观测序列为$$o_1, ..., o_t$$，且当前状态为$$q_i$$的概率为前向概率：\n\n$$\n\\alpha_t(i) = P(o_1, ..., o_t, i_t=q_i|\\lambda)\n$$\n\n- 算法流程：\n\n给定模型$$\\lambda$$和观测序列$$O=(o_1, ..., o_T)$$\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028120947404.png\" alt=\"image-20221028120947404\" style=\"zoom:80%;\" />\n\n\n\n#### 1.2.3 后向计算\n\n- 给定模型$$\\lambda$$，定义在时刻t时刻状态为$$q_i$$的条件下，从t+1到T的部分观测序列为$$o_{t+1}, ..., o_T$$的概率为后向概率：\n\n$$\n\\beta_{t}(i)=P\\left(o_{t+1}, o_{t+2}, \\cdots, o_{T} \\mid i_{t}=q_{i}, \\lambda\\right)\n$$\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028122410576.png\" alt=\"image-20221028122410576\" style=\"zoom:75%;\" />\n\n\n\n#### 1.2.4 常用概率的计算\n\n1. **计算给定模型$$\\lambda$$和观测$$O$$，在t时刻处于$$q_i$$的概率：**\n\n$$\n\\gamma_t(i) = P(i_t = q_i|O, \\lambda)\n$$\n\n> - 首先运用贝叶斯公式：\n>\n> $$\n> \\gamma_{t}(i)=P\\left(i_{t}=q_{i} \\mid O, \\lambda\\right)=\\frac{P\\left(i_{t}=q_{i}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}\n> $$\n>\n> - 由上述的前向后向概率的定义可得：\n>\n> $$\n> \\alpha_t(i)\\beta_t(i) = P(i_t=q_i,O|\\lambda)\n> $$\n>\n> - 于是得到：\n>\n> $$\n> \\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)}\n> $$\n\n2. **计算给定模型$$\\lambda$$和观测$$O$$，在t时刻处于状态$$q_i$$并且在t+1时刻处于状态$$q_j$$的概率：**\n\n$$\n\\xi_{t}(i, j)=P\\left(i_{t}=q_{i}, i_{t+1}=q_{j} \\mid O, \\lambda\\right)\n$$\n\n> - 同样先运用贝叶斯公式：\n>\n> $$\n> \\xi_{t}(i, j)=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}\n> $$\n>\n> - 其中：\n>\n> $$\n> P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)=\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)\n> $$\n>\n> - 所以：\n>\n> $$\n> \\xi_{t}(i, j)=\\frac{\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}\n> $$\n\n\n\n### 1.3 学习算法\n\n- 在观测序列和状态序列都给定的时候，训练集足够大时，可以直接通过极大似然估计来估计模型参数，即**直接通过频数来估计概率**\n- 我们主要讨论的是只给定观测序列时的情况，**此时状态序列为隐变量**，所以使用[EM算法](https://zlkqz.site/2022/09/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#6-EM%E7%AE%97%E6%B3%95)\n\n- 首先是E步，即计算Q函数：\n\n$$\nQ(\\lambda, \\bar{\\lambda}) = \\sum_{I}\\log P(O,I|\\lambda)P(I|O,\\bar{\\lambda}) = \\sum_{I}\\log P(O,I|\\lambda)\\frac{P(O,I|\\bar{\\lambda})}{P(O|\\bar{\\lambda})}\n$$\n\n其中$$\\bar{\\lambda}$$是HMM参数的当前估计值，$$\\lambda$$是要极大化的值，作为下次迭代的新参数\n\n- 由于分母中的$$P(O|\\bar{\\lambda})$$和要更新的参数$$\\lambda$$无关，所以直接去掉，Q函数直接化为：\n\n$$\nQ(\\lambda, \\bar{\\lambda})=\\sum_{I} \\log P(O, I \\mid \\lambda) P(O, I \\mid \\bar{\\lambda})\n$$\n\n- 其中：\n\n$$\nP(O, I \\mid \\lambda)=\\pi_{i_{1}} b_{i_{1}}\\left(o_{1}\\right) a_{i_{i} i_{2}} b_{i_{2}}\\left(o_{2}\\right) \\cdots a_{i_{T-1} i_{T}} b_{i_{T}}\\left(o_{T}\\right)\n$$\n\n- 所以：\n\n$$\n\\begin{aligned}\nQ(\\lambda, \\bar{\\lambda})=& \\sum_{I} \\log \\pi_{i} P(O, I \\mid \\bar{\\lambda}) +\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i, i_{i}+1}\\right) P(O, I \\mid \\bar{\\lambda})+\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})\n\\end{aligned}\n$$\n\n- 然后是M步，即最大化Q函数，而最大化Q函数的问题，可以化为分别最大化上式中Q函数中的三项：\n\n> 1. 最大化$$\\sum_{I} \\log \\pi_{i} P(O, I \\mid \\bar{\\lambda})$$，可以先将其化为：\n>\n> $$\n> \\sum_{I} \\log \\pi_{i_{0}} P(O, I \\mid \\bar{\\lambda})=\\sum_{i=1}^{N} \\log \\pi_{i} P\\left(O, i_{1}=i \\mid \\bar{\\lambda}\\right)\n> $$\n>\n> 由于存在约束$$\\sum_{i=1}^N\\pi_i =1$$，所以可以使用拉格朗日乘子法进行求解，最后得到：\n> $$\n> \\pi_{i}=\\frac{P\\left(O, i_{1}=i \\mid \\bar{\\lambda}\\right)}{P(O \\mid \\bar{\\lambda})}\n> $$\n>\n> 2. 最大化$$\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i, i_{i}+1}\\right) P(O, I \\mid \\bar{\\lambda})$$，化为：\n>\n> $$\n> \\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i_{i} i_{t+1}}\\right) P(O, I \\mid \\bar{\\lambda})=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\sum_{t=1}^{T-1} \\log a_{i j} P\\left(O, i_{t}=i, i_{t+1}=j \\mid \\bar{\\lambda}\\right)\n> $$\n>\n> 运用拉格朗日乘子法，最后得到：\n> $$\n> a_{i j}=\\frac{\\sum_{t=1}^{T-1} P\\left(O, i_{t}=i, i_{t+1}=j \\mid \\bar{\\lambda}\\right)}{\\sum_{t=1}^{T-1} P\\left(O, i_{t}=i \\mid \\bar{\\lambda}\\right)}\n> $$\n>\n> 3. 最大化$$\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})$$，化为：\n>\n> $$\n> \\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})=\\sum_{j=1}^{N} \\sum_{t=1}^{T} \\log b_{j}\\left(o_{t}\\right) P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right)\n> $$\n>\n> 运用拉格朗日乘子法，最后得到：\n> $$\n> b_{j}(k)=\\frac{\\sum_{t=1}^{T} P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right) I\\left(o_{t}=v_{k}\\right)}{\\sum_{t=1}^{T} P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right)}\n> $$\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028132201642.png\" alt=\"image-20221028132201642\" style=\"zoom:67%;\" />\n\n\n\n### 1.4 预测算法\n\n- 序列模型常用**维特比算法**来进行预测，算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028132500929.png\" alt=\"image-20221028132500929\" style=\"zoom:67%;\" />\n\n\n\n\n\n# 2 条件随机场\n\n- 条件随机场（Conditional Random Field, CRF）和HMM一样，同样属于概率图模型，是给定一组输入随机变量条件下，另一组输出是随机变量的条件概率分布模型\n- 有一种特殊且最常用的CRF为线性链条件随机场，其结构和数学推导和HMM十分相似，但是仍有一些区别，稍后介绍\n\n\n\n### 2.1 马尔可夫随机场\n\n- **概率图模型**是由图结构来描述变量之间的关系的模型，而采用有向无环图结构被称为**贝叶斯网**，采用无向图则被称为**概率无向图模型或马尔可夫随机场**\n\n- 设有联合概率分布$$P(Y)$$，$$Y \\in \\mathcal{Y}$$是一组随机变量。由无向图$$G = (V,E)$$表示概率分布$$P(Y)$$，即在G中，结点$$v \\in V$$表示一个随机变量$$Y_v$$，$$Y = (Y_v)_{v \\in V}$$，而边$$e \\in E$$表示随机变量间的概率依赖关系\n- 首先定义无向图模型内的马尔可夫性：\n\n> 1. **成对马尔可夫性：**设u和v是图中任意两个没有边连接的结点，其对应的随机变量分别为$$Y_u, Y_v$$，其他所有结点为O，对应随机变量$$Y_O$$。成对马尔可夫性指给定$$Y_O$$的条件下，$$Y_u, Y_v$$是条件独立的：\n>\n> $$\n> P\\left(Y_{u}, Y_{v} \\mid Y_{o}\\right)=P\\left(Y_{u} \\mid Y_{o}\\right) P\\left(Y_{v} \\mid Y_{o}\\right)\n> $$\n>\n> 2. **局部马尔可夫性：**设v为图中任意一个结点，W是与v相连的所有结点，O是除v和W之外的所有结点。局部马尔可夫性指给定$$Y_W$$的条件下$$Y_v, Y_O$$是条件独立的：\n>\n> $$\n> P\\left(Y_{v}, Y_{o} \\mid Y_{W}\\right)=P\\left(Y_{v} \\mid Y_{W}\\right) P\\left(Y_{o} \\mid Y_{W}\\right)\n> $$\n>\n> 也可以为表示为：\n> $$\n> P(Y_v|Y_W) = P(Y_v|Y_W, Y_O)\n> $$\n>\n> 3. **全局马尔可夫性：**A、B是在图中被结点集合C分开的任意结点集合（如下图所示）。全局马尔可夫性指给定$$Y_C$$条件下$$Y_A,Y_B$$条件独立：\n>\n> $$\n> P\\left(Y_{A}, Y_{B} \\mid Y_{C}\\right)=P\\left(Y_{A} \\mid Y_{C}\\right) P\\left(Y_{B} \\mid Y_{C}\\right)\n> $$\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029144742544.png\" alt=\"image-20221029144742544\" style=\"zoom:67%;\" />\n>\n> **上述三种马尔可夫性是等价的**\n\n- 而马尔可夫随机场，不仅要满足使用无向图，还需要满足马尔可夫性。显然，CRF属于马尔可夫随机场\n\n\n\n### 2.2 团和极大团\n\n- 无向图中的任意一个强连通子集都称为**团（clique）**，而一个团不能再加任意一个结点使其仍为团，则这种团为**极大团（maximal clique）**\n\n- 联合概率分布可以用每个团的**势函数（potential function）**的乘积来表示，但是一个图中的团很多，且有些随机变量同时属于多个团，所以简化来说，可以直接使用**极大团的势函数成绩**：\n\n$$\nP(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right)\n$$\n\n其中C为极大团集，Z为规范化因子，$$\\Psi$$为势函数，要求势函数是严格正的，一般定义为指数函数：\n$$\n\\Psi_{C}\\left(Y_{C}\\right)=\\exp \\left\\{-E\\left(Y_{C}\\right)\\right\\}\n$$\n\n\n### 2.3 模型定义\n\n- 设随机变量$$X,Y$$，如果对任意结点v满足马尔可夫性（下式为局部马尔可夫性）：\n\n$$\nP\\left(Y_{v} \\mid X, Y_{w}, w \\neq v\\right)=P\\left(Y_{v} \\mid X, Y_{w}, w \\sim v\\right)\n$$\n\n则称条件概率$$P(Y|X)$$为条件随机场\n\n- 另外有CRF的特例：**线性链条件随机场**，满足马尔可夫性：\n\n$$\n\\begin{array}{c}\nP\\left(Y_{i} \\mid X, Y_{1}, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_{n}\\right)=P\\left(Y_{i} \\mid X, Y_{i-1}, Y_{i+1}\\right) \\\\\ni=1,2, \\cdots, n \\text { (在 } i=1 \\text { 和 } n \\text { 时只考虑单边) }\n\\end{array}\n$$\n\n则称条件概率$$P(Y|X)$$为条件随机场，图结构如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029152606453.png\" alt=\"image-20221029152606453\" style=\"zoom: 67%;\" />\n\n- CRF定义中没有要求X的结构，但是一般假设X和Y有相同的图结构，比如线性链条件随机场：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029152749755.png\" alt=\"image-20221029152749755\" style=\"zoom:67%;\" />\n\n在标注问题中（如NER），X表示输入观测序列，Y表示对应的输出标记序列或状态序列\n\n\n\n### 2.4 CRF的参数\n\n- 现在开始介绍的CRF都默认为线性链CRF\n- 前面说过，马尔科夫场的概率可以用极大团的势函数来表示：\n\n$$\nP(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right) = \\frac{1}{Z}\\exp \\sum_C-E\\left(Y_{C}\\right) = \\frac{1}{Z}\\exp \\sum_CF_C\\left(Y_{C}\\right)\n$$\n\n- 对于线性链CRF，每一个$$y_{i-1}, y_i$$构成一个极大团，所以：\n\n$$\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^TF_t(y_{t-1},y_t,x)\n$$\n\n其中T为时间步数，$$x$$为X的取值，是整个观测序列。**由于每个极大团的结构都相同**，所以可以**简化为每个极大团的势函数都一样**：\n$$\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^TF(y_{t-1},y_t,x)\n$$\n\n- 而对于每个$$F(y_{t-1},y_t,x)$$，可以表示为三个函数的和：\n\n$$\nF(y_{t-1},y_t,x) = F_1(y_{t-1}, x) + F_1(y_t, x) + F_2(y_{t-1}, y_t, x)\n$$\n\n其中$$F_1$$就称为**状态函数**，$$F_2$$称为**转移函数**。由于$$F_1(y_{t-1},x)$$在上一个时间步已经出现过了，所以可以直接去掉：\n$$\nF(y_{t-1},y_t,x) = F_1(y_t, x) + F_2(y_{t-1}, y_t, x)\n$$\n\n- 那么可以引入特征函数来定义$$F_1, F_2$$：\n\n$$\nF_1 = \\sum_{l=1}^{K_2}\\mu_ls_l(y_t, x) \\\\\nF_2 = \\sum_{k=1}^{K_1}\\lambda_kt_k(y_{t-1}, y_t, x)\n$$\n\n- 所以条件概率就表示为：\n\n$$\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^T(\\sum_{k=1}^{K_1}\\lambda_kt_k(y_{t-1}, y_t, x) + \\sum_{l=1}^{K_2}\\mu_ls_l(y_t, x))\n$$\n\n- 而可以把关于时间步的求和放入括号中：\n\n$$\nP(Y|X) = \\frac{1}{Z}\\exp(\\sum_{k=1}^{K_1}\\lambda_k\\sum_{t=1}^Tt_k(y_{t-1}, y_t, x)+\\sum_{l=1}^{K_2}\\mu_l\\sum_{t=1}^Ts_l(y_t, x))\n$$\n\n- 将两种特征函数和其权重合起来：\n\n$$\nf_{k}\\left(y_{t-1}, y_{t}, x\\right)=\\left\\{\\begin{array}{l}\nt_{k}\\left(y_{t-1}, y_{t}, x\\right), \\quad k=1,2, \\cdots, K_{1} \\\\\ns_{l}\\left(y_{t}, x\\right), \\quad k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.  \\\\\nw_{k}=\\left\\{\\begin{array}{ll}\n\\lambda_{k}, & k=1,2, \\cdots, K_{1} \\\\\n\\mu_{l}, & k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.\n$$\n\n然后对特征函数在各个时间步进行求和，记作：\n$$\nf_{k}(y, x)=\\sum_{t=1}^{T} f_{k}\\left(y_{t-1}, y_{t}, x\\right), \\quad k=1,2, \\cdots, K\n$$\n\n- 所以条件概率化简为：\n\n$$\nP(y \\mid x)=\\frac{1}{Z(x)} \\exp \\sum_{k=1}^{K} w_{k} f_{k}(y, x)\n$$\n\n- 上式采用向量化表示，引入：\n\n$$\n\\begin{array}{c}\nw=\\left(w_{1}, w_{2}, \\cdots, w_{K}\\right)^{\\mathrm{T}} \\\\\nF(y, x)=\\left(f_{1}(y, x), f_{2}(y, x), \\cdots, f_{K}(y, x)\\right)^{\\mathrm{T}}\n\\end{array}\n$$\n\n所以条件概率的向量化表示为：\n$$\n\\begin{array}{l}\nP_{w}(y \\mid x)=\\frac{\\exp (w \\cdot F(y, x))}{Z_{w}(x)} \\\\\nZ_{w}(x)=\\sum_{y} \\exp (w \\cdot F(y, x))\n\\end{array}\n$$\n**其中特征函数$$F(y,x)$$是事先设计好给出的，而学习的目标即学习权重$$w$$，最大化特征函数的总得分**\n\n\n\n### 2.5 概率计算\n\n- 和前面HMM一样，同样运用前向后向算法的变量来进行概率计算，首先定义一个矩阵，**为了方便讨论，我们又引入了两个时间步的状态序列$$y_0 = start, y_{T+1} = stop$$，实际前文的讨论中也隐含了$$y_0 = start$$：**\n\n$$\n\\begin{array}{c}\nM_{t}(x)=\\left[M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)\\right] \\\\\nM_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)=\\exp \\sum_{k=1}^{K} w_{k} f_{k}\\left(y_{t-1}, y_{t}, x\\right)\n\\end{array}\n$$\n\n设$$y_t$$有m个取值，则矩阵$$M_t(x)$$里面的$$y_t,y_{t-1}$$分别取不同的m个值，所以$$M_t(x)$$是一个$$m \\times m$$阶矩阵\n\n- 有了上述定义，可以将条件概率进一步写为矩阵形式：\n\n$$\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\prod_{t=1}^{T} M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)\n$$\n\n其中$$Z_w(x)$$是T+1个矩阵乘积的第(start, end)元素：\n$$\nZ_{w}(x)=\\left(M_{1}(x) M_{2}(x) \\cdots M_{T+1}(x)\\right)_{\\text {start,stop }}\n$$\n\n- 现在来定义**前向概率，$$\\alpha_t(y_t|x)$$为在时刻t时观测序列为$$x_1, ..., x_t$$，且当前状态为$$y_t$$的概率**：\n\n$$\n\\alpha_{t}\\left(y_{t} \\mid x\\right)=\\alpha_{t-1}\\left(y_{t-1} \\mid x\\right) M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right) \\quad t=1,...,T+1 \\\\\n\\alpha_{0}(y \\mid x)=\\left\\{\\begin{array}{ll}\n1, & y=\\operatorname{start} \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.\n$$\n\n因为$$y_t$$有m个取值，所以可以定义m维向量$$\\alpha_t(x)$$：\n$$\n\\alpha_{t}^{\\mathrm{T}}(x)=\\alpha_{t-1}^{\\mathrm{T}}(x) M_{t}(x) \\\\\n\\alpha_0(x) = \\mathbb{1}（单位向量）\n$$\n\n- 同样可以定义**后向概率，$$\\beta_t(y_t|x)$$为在时刻t观测序列为$$x_{t+1}, ..., x_T$$，且当前状态为$$y_t$$的概率：**\n\n$$\n\\beta_{t}\\left(y_{t} \\mid x\\right)=M_{t}\\left(y_{t}, y_{t+1} \\mid x\\right) \\beta_{t+1}\\left(y_{t+1} \\mid x\\right) \\\\\n\\beta_{t+1}\\left(y \\mid x\\right)=\\left\\{\\begin{array}{ll}\n1, & y=\\text { stop } \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.\n$$\n\n同样有向量形式：\n$$\n\\beta_{t}(x)=M_{t+1}(x) \\beta_{t+1}(x) \\\\\n\\beta_{T+1}(x) = \\mathbb{1}  (单位向量)\n$$\n\n- 得到了前后向概率，就可以得到**在时刻t状态是$$y_t$$的概率：**\n\n$$\nP\\left(Y_{t}=y_{t} \\mid x\\right)=\\frac{\\alpha_{t}\\left(y_{t} \\mid x\\right) \\beta_{t}\\left(y_{t} \\mid x\\right)}{Z(x)}\n$$\n\n- 以及**在时刻t-1状态是$$y_{t-1}$$并且在时刻t状态是$$y_t$$的概率：**\n\n$$\nP\\left(Y_{t-1}=y_{t-1}, Y_{t}=y_{t} \\mid x\\right)=\\frac{\\alpha_{t-1}\\left(y_{t-1} \\mid x\\right) M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right) \\beta_{t}\\left(y_{t} \\mid x\\right)}{Z(x)}\n$$\n\n- 上述两个式子的$$Z(x)$$可以用更简单的形式表达：\n\n$$\nZ(x) = \\alpha_n^T(x)\\mathbb{1} = \\mathbb{1}\\beta_1(x)\n$$\n\n\n\n### 2.6 学习和预测\n\n- 预测算法和HMM一样，使用**维特比算法**\n\n- 学习算法其实就是[最大熵模型的学习](https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#3-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)\n\n\n\n\n\n# 3 HMM和CRF的区别\n\n- CRF和HMM的数学推导几乎是一样的，但是差异在于：\n\n> 1. HMM属于贝叶斯网络，而CRF属于马尔可夫场，前者的假设和约束更加的严格，比如CRF并没有像HMM一样完全依赖上一步的状态，HMM的观测变量的生成是独立的\n> 2. 上述我们讨论的仅仅只是线性链CRF，但是CRF还可以有其他的图结构，并且**实际上可以任意选定特征函数的个数和形式，特征函数的不确定也是CRF能和深度学习融合的最主要原因，模型可以自己学习特征函数，并且不用显示地表达出来**\n\n\n\n\n\n# 4 CRF和深度学习模型的结合\n\n- 以BiLSTM+CRF做NER任务举例\n\n- 如果不用CRF而是直接在模型的后面接一个Softmax，鉴于所选取的基模型的强大的特征抽取能力，这已经可以有比较好的分类效果，**但是NER任务是存在一些约束的**，比如BIO格式中，B-Person后面不可能跟I-Organization。**Softmax的分类是每个时间步相互独立的，所以可能会出现上述的问题**\n\n- **而CRF层可以加入一些约束来保证最终预测结果是有效的，这些约束可以在训练数据时被CRF层自动学习得到**\n\n- 首先介绍一下模型结构：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-0650e1511e7d4419c9528a8d08ea61fd_720w.webp\" alt=\"img\" style=\"zoom:80%;\" />\n\nBiLSTM的输出经过Dense层，**转化为每个Label对应的score（图中浅黄色部分），这个score即CRF中的状态得分$$\\sum \\mu s$$**，然后将其输入CRF，CRF层维护了一个转移矩阵（Transition Matrix），这也是CRF层中需要学习的参数，假设总共有包括START和END在内的7个label，则转移矩阵为：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-2064e34cece3be4e852b1ace6bbca2ba_720w.webp\" alt=\"img\" style=\"zoom:67%;\" />\n\n**每个元素代表了相邻时间步之间进行状态转移的score，这个score即CRF中的转移得分$$\\sum\\lambda t$$**，并且上述矩阵已经学到了一些有用的约束：\n\n> 1. 句子的第一个单词应该是“B-” 或 “O”，而不是“I”。（从“START”->“I-Person 或 I-Organization”的转移分数很低）\n> 2. “B-label1 I-label2 I-label3…”，在该模式中，类别1,2,3应该是同一种实体类别。比如，“B-Person I-Person” 是正确的，而“B-Person I-Organization”则是错误的。（“B-Organization” -> “I-Person”的分数很低）\n> 3. O I-label”是错误的，命名实体的开头应该是“B-”而不是“I-”\n\n- 每个输入有N中可能的结果，即N条路径，比如（加粗的为真实路径）：\n\n> 1. START B-Person B-Person B-Person B-Person B-Person END\n> 2. START B-Person I-Person B-Person B-Person B-Person END\n> 3. **START B-Person I-Person O B-Organization O END**\n>\n> ......\n>\n>    N. O O O O O O O\n\n- 训练目标即最大化真实路径的得分，可得损失函数：\n\n$$\nLoss\n =-\\log \\frac{P_{\\text {Real Path }}}{P_{1}+P_{2}+\\ldots+P_{N}} \n =-\\log \\frac{e^{s_{\\text {Realpath }}}}{e^{s_{1}+e^{s_{2}}+\\ldots+e^{s_{N}}}}  \\\\\n =-\\left(S_{\\text {RealPath }}-\\log \\left(e^{S_{1}}+e^{S_{2}}+\\ldots+e^{S_{N}}\\right)\\right)\n$$\n\n其中$$S_i$$为一条路径对应的得分，是通过Softmax实现最大化的\n\n- 值得一提的是，计算分母中的所有路径的得分和$$-\\log (e^{S_1} + ... + e^{S_N})$$不需要列举所有可能路径，可以用一种动态规划的方法降低计算复杂度\n\n- 另外，在进行预测的时候，同样是使用维特比算法","slug":"HMM和CRF","published":1,"updated":"2022-12-20T06:22:33.900Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1c00067cszbpg55m6t","content":"<h1 id=\"1-隐马尔可夫模型\"><a href=\"#1-隐马尔可夫模型\" class=\"headerlink\" title=\"1 隐马尔可夫模型\"></a>1 隐马尔可夫模型</h1><ul>\n<li>隐马尔可夫模型（Hidden Markov Model, HMM）常用于序列标注问题，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于<strong>概率图模型</strong>（用图结构来描述变量之间的关系，属于生成式模型）</li>\n<li>HMM属于<a href=\"https://zlkqz.site/2022/09/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#6-EM%E7%AE%97%E6%B3%95\">贝叶斯网</a>，其两个基本假设其实就是贝叶斯网的假设：<strong>给定父节点集，贝叶斯网假设每个属性与他的非后裔属性独立</strong></li>\n</ul>\n<h3 id=\"1-1-模型定义\"><a href=\"#1-1-模型定义\" class=\"headerlink\" title=\"1.1 模型定义\"></a>1.1 模型定义</h3><ul>\n<li>设<script type=\"math/tex\">Q=  \\{q_1, ..., q_N\\}</script>是所有可能的状态的集合，<script type=\"math/tex\">V = \\{v_1, ..., v_M\\}</script>是所有可能的观测的集合。而<script type=\"math/tex\">I = (i_1, ..., i_T)</script>是状态序列，<script type=\"math/tex\">O = (o_1, ..., o_T)</script>是对应的观测序列，模型定义了三种参数：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>状态转移矩阵A：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\nA = [a_{ij}]_{N \\times N}</script><p>其中<script type=\"math/tex\">a_{ij}</script>指在时刻t处于状态<script type=\"math/tex\">q_i</script>的条件下转移到在t+1时刻状态为<script type=\"math/tex\">q_j</script>的概率：</p>\n<script type=\"math/tex; mode=display\">\na_{ij} = P(i_{t+1}=q_j|i_t=q_i)</script><ol>\n<li><strong>观测概率矩阵B：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\nB = [b_j(k)]_{N\\times N}</script><p>其中<script type=\"math/tex\">b_j(k)</script>指在t时刻处于状态<script type=\"math/tex\">q_j</script>时生成观测<script type=\"math/tex\">v_k</script>的概率：</p>\n<script type=\"math/tex; mode=display\">\nb_j(k) = P(o_t = v_k|i_t =  q_j)</script><ol>\n<li><strong>初始状态概率向量<script type=\"math/tex\">\\pi</script>：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\pi = (\\pi_i)</script><p>其中<script type=\"math/tex\">\\pi_i</script>是<script type=\"math/tex\">t=1</script>时处于状态<script type=\"math/tex\">q_i</script>的概率：</p>\n<script type=\"math/tex; mode=display\">\n\\pi = P(i_1 = q_i)</script></blockquote>\n<ul>\n<li>一般使用一个三元组表示HMM的参数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\lambda = (A,B,\\pi)</script><ul>\n<li>上面对参数的定义中，隐含了HMM的两个基本假设：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>齐次马尔可夫性假设：</strong>假设隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(i_{t} \\mid i_{t-1}, o_{t-1}, \\cdots, i_{1}, o_{1}\\right)=P\\left(i_{t} \\mid i_{t-1}\\right), \\quad t=1,2, \\cdots, T</script><ol>\n<li><strong>观测独立性假设：</strong>假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(o_{t} \\mid i_{T}, o_{T}, i_{T-1}, o_{T-1}, \\cdots, i_{t+1}, o_{t+1}, i_{t}, i_{t-1}, o_{t-1}, \\cdots, i_{1}, o_{1}\\right)=P\\left(o_{t} \\mid i_{t}\\right)</script><p><strong>其实这两个假设就是贝叶斯网的假设，只不过结构特殊一点，是一个线性结构</strong></p>\n</blockquote>\n<h3 id=\"1-2-概率计算\"><a href=\"#1-2-概率计算\" class=\"headerlink\" title=\"1.2 概率计算\"></a>1.2 概率计算</h3><ul>\n<li>概率计算即给定模型<script type=\"math/tex\">\\lambda = (A,B,\\pi)</script>和观测序列<script type=\"math/tex\">O=(o_1, ..., o_T)</script>，计算该观测序列出现的概率<script type=\"math/tex\">P(O|\\lambda)</script></li>\n</ul>\n<h4 id=\"1-2-1-直接计算\"><a href=\"#1-2-1-直接计算\" class=\"headerlink\" title=\"1.2.1 直接计算\"></a>1.2.1 直接计算</h4><ul>\n<li>直接计算即列举所有可能的状态序列<script type=\"math/tex\">I</script>，计算：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(O \\mid \\lambda) &=\\sum_{I} P(O \\mid I, \\lambda) P(I \\mid \\lambda) \\\\\n&=\\sum_{i_{1}, i_{2}, \\cdots, i_{T}} \\pi_{i_{1}} b_{i_{1}}\\left(o_{1}\\right) a_{i_{1} i_{2}} b_{i_{2}}\\left(o_{2}\\right) \\cdots a_{i_{T-i} i_{T}} b_{i_{T}}\\left(o_{T}\\right)\n\\end{aligned}</script><ul>\n<li>但是这种方法计算量过大，复杂度为<script type=\"math/tex\">O(TN^T)</script>，所以是不可行的</li>\n</ul>\n<h4 id=\"1-2-2-前向计算\"><a href=\"#1-2-2-前向计算\" class=\"headerlink\" title=\"1.2.2 前向计算\"></a>1.2.2 前向计算</h4><ul>\n<li>给定模型<script type=\"math/tex\">\\lambda</script>，定义在时刻t时观测序列为<script type=\"math/tex\">o_1, ..., o_t</script>，且当前状态为<script type=\"math/tex\">q_i</script>的概率为前向概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\alpha_t(i) = P(o_1, ..., o_t, i_t=q_i|\\lambda)</script><ul>\n<li>算法流程：</li>\n</ul>\n<p>给定模型<script type=\"math/tex\">\\lambda</script>和观测序列<script type=\"math/tex\">O=(o_1, ..., o_T)</script></p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028120947404.png\" alt=\"image-20221028120947404\" style=\"zoom:80%;\" /></p>\n<h4 id=\"1-2-3-后向计算\"><a href=\"#1-2-3-后向计算\" class=\"headerlink\" title=\"1.2.3 后向计算\"></a>1.2.3 后向计算</h4><ul>\n<li>给定模型<script type=\"math/tex\">\\lambda</script>，定义在时刻t时刻状态为<script type=\"math/tex\">q_i</script>的条件下，从t+1到T的部分观测序列为<script type=\"math/tex\">o_{t+1}, ..., o_T</script>的概率为后向概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\beta_{t}(i)=P\\left(o_{t+1}, o_{t+2}, \\cdots, o_{T} \\mid i_{t}=q_{i}, \\lambda\\right)</script><ul>\n<li>算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028122410576.png\" alt=\"image-20221028122410576\" style=\"zoom:75%;\" /></p>\n<h4 id=\"1-2-4-常用概率的计算\"><a href=\"#1-2-4-常用概率的计算\" class=\"headerlink\" title=\"1.2.4 常用概率的计算\"></a>1.2.4 常用概率的计算</h4><ol>\n<li><strong>计算给定模型<script type=\"math/tex\">\\lambda</script>和观测<script type=\"math/tex\">O</script>，在t时刻处于<script type=\"math/tex\">q_i</script>的概率：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\gamma_t(i) = P(i_t = q_i|O, \\lambda)</script><blockquote>\n<ul>\n<li>首先运用贝叶斯公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\gamma_{t}(i)=P\\left(i_{t}=q_{i} \\mid O, \\lambda\\right)=\\frac{P\\left(i_{t}=q_{i}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}</script><ul>\n<li>由上述的前向后向概率的定义可得：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\alpha_t(i)\\beta_t(i) = P(i_t=q_i,O|\\lambda)</script><ul>\n<li>于是得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)}</script></blockquote>\n<ol>\n<li><strong>计算给定模型<script type=\"math/tex\">\\lambda</script>和观测<script type=\"math/tex\">O</script>，在t时刻处于状态<script type=\"math/tex\">q_i</script>并且在t+1时刻处于状态<script type=\"math/tex\">q_j</script>的概率：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\xi_{t}(i, j)=P\\left(i_{t}=q_{i}, i_{t+1}=q_{j} \\mid O, \\lambda\\right)</script><blockquote>\n<ul>\n<li>同样先运用贝叶斯公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\xi_{t}(i, j)=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}</script><ul>\n<li>其中：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)=\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)</script><ul>\n<li>所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\xi_{t}(i, j)=\\frac{\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}</script></blockquote>\n<h3 id=\"1-3-学习算法\"><a href=\"#1-3-学习算法\" class=\"headerlink\" title=\"1.3 学习算法\"></a>1.3 学习算法</h3><ul>\n<li>在观测序列和状态序列都给定的时候，训练集足够大时，可以直接通过极大似然估计来估计模型参数，即<strong>直接通过频数来估计概率</strong></li>\n<li><p>我们主要讨论的是只给定观测序列时的情况，<strong>此时状态序列为隐变量</strong>，所以使用<a href=\"https://zlkqz.site/2022/09/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#6-EM%E7%AE%97%E6%B3%95\">EM算法</a></p>\n</li>\n<li><p>首先是E步，即计算Q函数：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nQ(\\lambda, \\bar{\\lambda}) = \\sum_{I}\\log P(O,I|\\lambda)P(I|O,\\bar{\\lambda}) = \\sum_{I}\\log P(O,I|\\lambda)\\frac{P(O,I|\\bar{\\lambda})}{P(O|\\bar{\\lambda})}</script><p>其中<script type=\"math/tex\">\\bar{\\lambda}</script>是HMM参数的当前估计值，<script type=\"math/tex\">\\lambda</script>是要极大化的值，作为下次迭代的新参数</p>\n<ul>\n<li>由于分母中的<script type=\"math/tex\">P(O|\\bar{\\lambda})</script>和要更新的参数<script type=\"math/tex\">\\lambda</script>无关，所以直接去掉，Q函数直接化为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nQ(\\lambda, \\bar{\\lambda})=\\sum_{I} \\log P(O, I \\mid \\lambda) P(O, I \\mid \\bar{\\lambda})</script><ul>\n<li>其中：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(O, I \\mid \\lambda)=\\pi_{i_{1}} b_{i_{1}}\\left(o_{1}\\right) a_{i_{i} i_{2}} b_{i_{2}}\\left(o_{2}\\right) \\cdots a_{i_{T-1} i_{T}} b_{i_{T}}\\left(o_{T}\\right)</script><ul>\n<li>所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nQ(\\lambda, \\bar{\\lambda})=& \\sum_{I} \\log \\pi_{i} P(O, I \\mid \\bar{\\lambda}) +\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i, i_{i}+1}\\right) P(O, I \\mid \\bar{\\lambda})+\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})\n\\end{aligned}</script><ul>\n<li>然后是M步，即最大化Q函数，而最大化Q函数的问题，可以化为分别最大化上式中Q函数中的三项：</li>\n</ul>\n<blockquote>\n<ol>\n<li>最大化<script type=\"math/tex\">\\sum_{I} \\log \\pi_{i} P(O, I \\mid \\bar{\\lambda})</script>，可以先将其化为：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\sum_{I} \\log \\pi_{i_{0}} P(O, I \\mid \\bar{\\lambda})=\\sum_{i=1}^{N} \\log \\pi_{i} P\\left(O, i_{1}=i \\mid \\bar{\\lambda}\\right)</script><p>由于存在约束<script type=\"math/tex\">\\sum_{i=1}^N\\pi_i =1</script>，所以可以使用拉格朗日乘子法进行求解，最后得到：</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{i}=\\frac{P\\left(O, i_{1}=i \\mid \\bar{\\lambda}\\right)}{P(O \\mid \\bar{\\lambda})}</script><ol>\n<li>最大化<script type=\"math/tex\">\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i, i_{i}+1}\\right) P(O, I \\mid \\bar{\\lambda})</script>，化为：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i_{i} i_{t+1}}\\right) P(O, I \\mid \\bar{\\lambda})=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\sum_{t=1}^{T-1} \\log a_{i j} P\\left(O, i_{t}=i, i_{t+1}=j \\mid \\bar{\\lambda}\\right)</script><p>运用拉格朗日乘子法，最后得到：</p>\n<script type=\"math/tex; mode=display\">\na_{i j}=\\frac{\\sum_{t=1}^{T-1} P\\left(O, i_{t}=i, i_{t+1}=j \\mid \\bar{\\lambda}\\right)}{\\sum_{t=1}^{T-1} P\\left(O, i_{t}=i \\mid \\bar{\\lambda}\\right)}</script><ol>\n<li>最大化<script type=\"math/tex\">\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})</script>，化为：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})=\\sum_{j=1}^{N} \\sum_{t=1}^{T} \\log b_{j}\\left(o_{t}\\right) P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right)</script><p>运用拉格朗日乘子法，最后得到：</p>\n<script type=\"math/tex; mode=display\">\nb_{j}(k)=\\frac{\\sum_{t=1}^{T} P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right) I\\left(o_{t}=v_{k}\\right)}{\\sum_{t=1}^{T} P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right)}</script></blockquote>\n<ul>\n<li>算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028132201642.png\" alt=\"image-20221028132201642\" style=\"zoom:67%;\" /></p>\n<h3 id=\"1-4-预测算法\"><a href=\"#1-4-预测算法\" class=\"headerlink\" title=\"1.4 预测算法\"></a>1.4 预测算法</h3><ul>\n<li>序列模型常用<strong>维特比算法</strong>来进行预测，算法流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028132500929.png\" alt=\"image-20221028132500929\" style=\"zoom:67%;\" /></p>\n<h1 id=\"2-条件随机场\"><a href=\"#2-条件随机场\" class=\"headerlink\" title=\"2 条件随机场\"></a>2 条件随机场</h1><ul>\n<li>条件随机场（Conditional Random Field, CRF）和HMM一样，同样属于概率图模型，是给定一组输入随机变量条件下，另一组输出是随机变量的条件概率分布模型</li>\n<li>有一种特殊且最常用的CRF为线性链条件随机场，其结构和数学推导和HMM十分相似，但是仍有一些区别，稍后介绍</li>\n</ul>\n<h3 id=\"2-1-马尔可夫随机场\"><a href=\"#2-1-马尔可夫随机场\" class=\"headerlink\" title=\"2.1 马尔可夫随机场\"></a>2.1 马尔可夫随机场</h3><ul>\n<li><p><strong>概率图模型</strong>是由图结构来描述变量之间的关系的模型，而采用有向无环图结构被称为<strong>贝叶斯网</strong>，采用无向图则被称为<strong>概率无向图模型或马尔可夫随机场</strong></p>\n</li>\n<li><p>设有联合概率分布<script type=\"math/tex\">P(Y)</script>，<script type=\"math/tex\">Y \\in \\mathcal{Y}</script>是一组随机变量。由无向图<script type=\"math/tex\">G = (V,E)</script>表示概率分布<script type=\"math/tex\">P(Y)</script>，即在G中，结点<script type=\"math/tex\">v \\in V</script>表示一个随机变量<script type=\"math/tex\">Y_v</script>，<script type=\"math/tex\">Y = (Y_v)_{v \\in V}</script>，而边<script type=\"math/tex\">e \\in E</script>表示随机变量间的概率依赖关系</p>\n</li>\n<li>首先定义无向图模型内的马尔可夫性：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>成对马尔可夫性：</strong>设u和v是图中任意两个没有边连接的结点，其对应的随机变量分别为<script type=\"math/tex\">Y_u, Y_v</script>，其他所有结点为O，对应随机变量<script type=\"math/tex\">Y_O</script>。成对马尔可夫性指给定<script type=\"math/tex\">Y_O</script>的条件下，<script type=\"math/tex\">Y_u, Y_v</script>是条件独立的：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{u}, Y_{v} \\mid Y_{o}\\right)=P\\left(Y_{u} \\mid Y_{o}\\right) P\\left(Y_{v} \\mid Y_{o}\\right)</script><ol>\n<li><strong>局部马尔可夫性：</strong>设v为图中任意一个结点，W是与v相连的所有结点，O是除v和W之外的所有结点。局部马尔可夫性指给定<script type=\"math/tex\">Y_W</script>的条件下<script type=\"math/tex\">Y_v, Y_O</script>是条件独立的：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{v}, Y_{o} \\mid Y_{W}\\right)=P\\left(Y_{v} \\mid Y_{W}\\right) P\\left(Y_{o} \\mid Y_{W}\\right)</script><p>也可以为表示为：</p>\n<script type=\"math/tex; mode=display\">\nP(Y_v|Y_W) = P(Y_v|Y_W, Y_O)</script><ol>\n<li><strong>全局马尔可夫性：</strong>A、B是在图中被结点集合C分开的任意结点集合（如下图所示）。全局马尔可夫性指给定<script type=\"math/tex\">Y_C</script>条件下<script type=\"math/tex\">Y_A,Y_B</script>条件独立：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{A}, Y_{B} \\mid Y_{C}\\right)=P\\left(Y_{A} \\mid Y_{C}\\right) P\\left(Y_{B} \\mid Y_{C}\\right)</script><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029144742544.png\" alt=\"image-20221029144742544\" style=\"zoom:67%;\" /></p>\n<p><strong>上述三种马尔可夫性是等价的</strong></p>\n</blockquote>\n<ul>\n<li>而马尔可夫随机场，不仅要满足使用无向图，还需要满足马尔可夫性。显然，CRF属于马尔可夫随机场</li>\n</ul>\n<h3 id=\"2-2-团和极大团\"><a href=\"#2-2-团和极大团\" class=\"headerlink\" title=\"2.2 团和极大团\"></a>2.2 团和极大团</h3><ul>\n<li><p>无向图中的任意一个强连通子集都称为<strong>团（clique）</strong>，而一个团不能再加任意一个结点使其仍为团，则这种团为<strong>极大团（maximal clique）</strong></p>\n</li>\n<li><p>联合概率分布可以用每个团的<strong>势函数（potential function）</strong>的乘积来表示，但是一个图中的团很多，且有些随机变量同时属于多个团，所以简化来说，可以直接使用<strong>极大团的势函数成绩</strong>：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right)</script><p>其中C为极大团集，Z为规范化因子，<script type=\"math/tex\">\\Psi</script>为势函数，要求势函数是严格正的，一般定义为指数函数：</p>\n<script type=\"math/tex; mode=display\">\n\\Psi_{C}\\left(Y_{C}\\right)=\\exp \\left\\{-E\\left(Y_{C}\\right)\\right\\}</script><h3 id=\"2-3-模型定义\"><a href=\"#2-3-模型定义\" class=\"headerlink\" title=\"2.3 模型定义\"></a>2.3 模型定义</h3><ul>\n<li>设随机变量<script type=\"math/tex\">X,Y</script>，如果对任意结点v满足马尔可夫性（下式为局部马尔可夫性）：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{v} \\mid X, Y_{w}, w \\neq v\\right)=P\\left(Y_{v} \\mid X, Y_{w}, w \\sim v\\right)</script><p>则称条件概率<script type=\"math/tex\">P(Y|X)</script>为条件随机场</p>\n<ul>\n<li>另外有CRF的特例：<strong>线性链条件随机场</strong>，满足马尔可夫性：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nP\\left(Y_{i} \\mid X, Y_{1}, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_{n}\\right)=P\\left(Y_{i} \\mid X, Y_{i-1}, Y_{i+1}\\right) \\\\\ni=1,2, \\cdots, n \\text { (在 } i=1 \\text { 和 } n \\text { 时只考虑单边) }\n\\end{array}</script><p>则称条件概率<script type=\"math/tex\">P(Y|X)</script>为条件随机场，图结构如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029152606453.png\" alt=\"image-20221029152606453\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>CRF定义中没有要求X的结构，但是一般假设X和Y有相同的图结构，比如线性链条件随机场：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029152749755.png\" alt=\"image-20221029152749755\" style=\"zoom:67%;\" /></p>\n<p>在标注问题中（如NER），X表示输入观测序列，Y表示对应的输出标记序列或状态序列</p>\n<h3 id=\"2-4-CRF的参数\"><a href=\"#2-4-CRF的参数\" class=\"headerlink\" title=\"2.4 CRF的参数\"></a>2.4 CRF的参数</h3><ul>\n<li>现在开始介绍的CRF都默认为线性链CRF</li>\n<li>前面说过，马尔科夫场的概率可以用极大团的势函数来表示：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right) = \\frac{1}{Z}\\exp \\sum_C-E\\left(Y_{C}\\right) = \\frac{1}{Z}\\exp \\sum_CF_C\\left(Y_{C}\\right)</script><ul>\n<li>对于线性链CRF，每一个<script type=\"math/tex\">y_{i-1}, y_i</script>构成一个极大团，所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^TF_t(y_{t-1},y_t,x)</script><p>其中T为时间步数，<script type=\"math/tex\">x</script>为X的取值，是整个观测序列。<strong>由于每个极大团的结构都相同</strong>，所以可以<strong>简化为每个极大团的势函数都一样</strong>：</p>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^TF(y_{t-1},y_t,x)</script><ul>\n<li>而对于每个<script type=\"math/tex\">F(y_{t-1},y_t,x)</script>，可以表示为三个函数的和：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF(y_{t-1},y_t,x) = F_1(y_{t-1}, x) + F_1(y_t, x) + F_2(y_{t-1}, y_t, x)</script><p>其中<script type=\"math/tex\">F_1</script>就称为<strong>状态函数</strong>，<script type=\"math/tex\">F_2</script>称为<strong>转移函数</strong>。由于<script type=\"math/tex\">F_1(y_{t-1},x)</script>在上一个时间步已经出现过了，所以可以直接去掉：</p>\n<script type=\"math/tex; mode=display\">\nF(y_{t-1},y_t,x) = F_1(y_t, x) + F_2(y_{t-1}, y_t, x)</script><ul>\n<li>那么可以引入特征函数来定义<script type=\"math/tex\">F_1, F_2</script>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF_1 = \\sum_{l=1}^{K_2}\\mu_ls_l(y_t, x) \\\\\nF_2 = \\sum_{k=1}^{K_1}\\lambda_kt_k(y_{t-1}, y_t, x)</script><ul>\n<li>所以条件概率就表示为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^T(\\sum_{k=1}^{K_1}\\lambda_kt_k(y_{t-1}, y_t, x) + \\sum_{l=1}^{K_2}\\mu_ls_l(y_t, x))</script><ul>\n<li>而可以把关于时间步的求和放入括号中：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\frac{1}{Z}\\exp(\\sum_{k=1}^{K_1}\\lambda_k\\sum_{t=1}^Tt_k(y_{t-1}, y_t, x)+\\sum_{l=1}^{K_2}\\mu_l\\sum_{t=1}^Ts_l(y_t, x))</script><ul>\n<li>将两种特征函数和其权重合起来：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{k}\\left(y_{t-1}, y_{t}, x\\right)=\\left\\{\\begin{array}{l}\nt_{k}\\left(y_{t-1}, y_{t}, x\\right), \\quad k=1,2, \\cdots, K_{1} \\\\\ns_{l}\\left(y_{t}, x\\right), \\quad k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.  \\\\\nw_{k}=\\left\\{\\begin{array}{ll}\n\\lambda_{k}, & k=1,2, \\cdots, K_{1} \\\\\n\\mu_{l}, & k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.</script><p>然后对特征函数在各个时间步进行求和，记作：</p>\n<script type=\"math/tex; mode=display\">\nf_{k}(y, x)=\\sum_{t=1}^{T} f_{k}\\left(y_{t-1}, y_{t}, x\\right), \\quad k=1,2, \\cdots, K</script><ul>\n<li>所以条件概率化简为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(y \\mid x)=\\frac{1}{Z(x)} \\exp \\sum_{k=1}^{K} w_{k} f_{k}(y, x)</script><ul>\n<li>上式采用向量化表示，引入：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nw=\\left(w_{1}, w_{2}, \\cdots, w_{K}\\right)^{\\mathrm{T}} \\\\\nF(y, x)=\\left(f_{1}(y, x), f_{2}(y, x), \\cdots, f_{K}(y, x)\\right)^{\\mathrm{T}}\n\\end{array}</script><p>所以条件概率的向量化表示为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nP_{w}(y \\mid x)=\\frac{\\exp (w \\cdot F(y, x))}{Z_{w}(x)} \\\\\nZ_{w}(x)=\\sum_{y} \\exp (w \\cdot F(y, x))\n\\end{array}</script><p><strong>其中特征函数<script type=\"math/tex\">F(y,x)</script>是事先设计好给出的，而学习的目标即学习权重<script type=\"math/tex\">w</script>，最大化特征函数的总得分</strong></p>\n<h3 id=\"2-5-概率计算\"><a href=\"#2-5-概率计算\" class=\"headerlink\" title=\"2.5 概率计算\"></a>2.5 概率计算</h3><ul>\n<li>和前面HMM一样，同样运用前向后向算法的变量来进行概率计算，首先定义一个矩阵，<strong>为了方便讨论，我们又引入了两个时间步的状态序列<script type=\"math/tex\">y_0 = start, y_{T+1} = stop</script>，实际前文的讨论中也隐含了<script type=\"math/tex\">y_0 = start</script>：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nM_{t}(x)=\\left[M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)\\right] \\\\\nM_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)=\\exp \\sum_{k=1}^{K} w_{k} f_{k}\\left(y_{t-1}, y_{t}, x\\right)\n\\end{array}</script><p>设<script type=\"math/tex\">y_t</script>有m个取值，则矩阵<script type=\"math/tex\">M_t(x)</script>里面的<script type=\"math/tex\">y_t,y_{t-1}</script>分别取不同的m个值，所以<script type=\"math/tex\">M_t(x)</script>是一个<script type=\"math/tex\">m \\times m</script>阶矩阵</p>\n<ul>\n<li>有了上述定义，可以将条件概率进一步写为矩阵形式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\prod_{t=1}^{T} M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)</script><p>其中<script type=\"math/tex\">Z_w(x)</script>是T+1个矩阵乘积的第(start, end)元素：</p>\n<script type=\"math/tex; mode=display\">\nZ_{w}(x)=\\left(M_{1}(x) M_{2}(x) \\cdots M_{T+1}(x)\\right)_{\\text {start,stop }}</script><ul>\n<li>现在来定义<strong>前向概率，<script type=\"math/tex\">\\alpha_t(y_t|x)</script>为在时刻t时观测序列为<script type=\"math/tex\">x_1, ..., x_t</script>，且当前状态为<script type=\"math/tex\">y_t</script>的概率</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\alpha_{t}\\left(y_{t} \\mid x\\right)=\\alpha_{t-1}\\left(y_{t-1} \\mid x\\right) M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right) \\quad t=1,...,T+1 \\\\\n\\alpha_{0}(y \\mid x)=\\left\\{\\begin{array}{ll}\n1, & y=\\operatorname{start} \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.</script><p>因为<script type=\"math/tex\">y_t</script>有m个取值，所以可以定义m维向量<script type=\"math/tex\">\\alpha_t(x)</script>：</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_{t}^{\\mathrm{T}}(x)=\\alpha_{t-1}^{\\mathrm{T}}(x) M_{t}(x) \\\\\n\\alpha_0(x) = \\mathbb{1}（单位向量）</script><ul>\n<li>同样可以定义<strong>后向概率，<script type=\"math/tex\">\\beta_t(y_t|x)</script>为在时刻t观测序列为<script type=\"math/tex\">x_{t+1}, ..., x_T</script>，且当前状态为<script type=\"math/tex\">y_t</script>的概率：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\beta_{t}\\left(y_{t} \\mid x\\right)=M_{t}\\left(y_{t}, y_{t+1} \\mid x\\right) \\beta_{t+1}\\left(y_{t+1} \\mid x\\right) \\\\\n\\beta_{t+1}\\left(y \\mid x\\right)=\\left\\{\\begin{array}{ll}\n1, & y=\\text { stop } \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.</script><p>同样有向量形式：</p>\n<script type=\"math/tex; mode=display\">\n\\beta_{t}(x)=M_{t+1}(x) \\beta_{t+1}(x) \\\\\n\\beta_{T+1}(x) = \\mathbb{1}  (单位向量)</script><ul>\n<li>得到了前后向概率，就可以得到<strong>在时刻t状态是<script type=\"math/tex\">y_t</script>的概率：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{t}=y_{t} \\mid x\\right)=\\frac{\\alpha_{t}\\left(y_{t} \\mid x\\right) \\beta_{t}\\left(y_{t} \\mid x\\right)}{Z(x)}</script><ul>\n<li>以及<strong>在时刻t-1状态是<script type=\"math/tex\">y_{t-1}</script>并且在时刻t状态是<script type=\"math/tex\">y_t</script>的概率：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{t-1}=y_{t-1}, Y_{t}=y_{t} \\mid x\\right)=\\frac{\\alpha_{t-1}\\left(y_{t-1} \\mid x\\right) M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right) \\beta_{t}\\left(y_{t} \\mid x\\right)}{Z(x)}</script><ul>\n<li>上述两个式子的<script type=\"math/tex\">Z(x)</script>可以用更简单的形式表达：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nZ(x) = \\alpha_n^T(x)\\mathbb{1} = \\mathbb{1}\\beta_1(x)</script><h3 id=\"2-6-学习和预测\"><a href=\"#2-6-学习和预测\" class=\"headerlink\" title=\"2.6 学习和预测\"></a>2.6 学习和预测</h3><ul>\n<li><p>预测算法和HMM一样，使用<strong>维特比算法</strong></p>\n</li>\n<li><p>学习算法其实就是<a href=\"https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#3-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95\">最大熵模型的学习</a></p>\n</li>\n</ul>\n<h1 id=\"3-HMM和CRF的区别\"><a href=\"#3-HMM和CRF的区别\" class=\"headerlink\" title=\"3 HMM和CRF的区别\"></a>3 HMM和CRF的区别</h1><ul>\n<li>CRF和HMM的数学推导几乎是一样的，但是差异在于：</li>\n</ul>\n<blockquote>\n<ol>\n<li>HMM属于贝叶斯网络，而CRF属于马尔可夫场，前者的假设和约束更加的严格，比如CRF并没有像HMM一样完全依赖上一步的状态，HMM的观测变量的生成是独立的</li>\n<li>上述我们讨论的仅仅只是线性链CRF，但是CRF还可以有其他的图结构，并且<strong>实际上可以任意选定特征函数的个数和形式，特征函数的不确定也是CRF能和深度学习融合的最主要原因，模型可以自己学习特征函数，并且不用显示地表达出来</strong></li>\n</ol>\n</blockquote>\n<h1 id=\"4-CRF和深度学习模型的结合\"><a href=\"#4-CRF和深度学习模型的结合\" class=\"headerlink\" title=\"4 CRF和深度学习模型的结合\"></a>4 CRF和深度学习模型的结合</h1><ul>\n<li><p>以BiLSTM+CRF做NER任务举例</p>\n</li>\n<li><p>如果不用CRF而是直接在模型的后面接一个Softmax，鉴于所选取的基模型的强大的特征抽取能力，这已经可以有比较好的分类效果，<strong>但是NER任务是存在一些约束的</strong>，比如BIO格式中，B-Person后面不可能跟I-Organization。<strong>Softmax的分类是每个时间步相互独立的，所以可能会出现上述的问题</strong></p>\n</li>\n<li><p><strong>而CRF层可以加入一些约束来保证最终预测结果是有效的，这些约束可以在训练数据时被CRF层自动学习得到</strong></p>\n</li>\n<li><p>首先介绍一下模型结构：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-0650e1511e7d4419c9528a8d08ea61fd_720w.webp\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<p>BiLSTM的输出经过Dense层，<strong>转化为每个Label对应的score（图中浅黄色部分），这个score即CRF中的状态得分<script type=\"math/tex\">\\sum \\mu s</script></strong>，然后将其输入CRF，CRF层维护了一个转移矩阵（Transition Matrix），这也是CRF层中需要学习的参数，假设总共有包括START和END在内的7个label，则转移矩阵为：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-2064e34cece3be4e852b1ace6bbca2ba_720w.webp\" alt=\"img\" style=\"zoom:67%;\" /></p>\n<p><strong>每个元素代表了相邻时间步之间进行状态转移的score，这个score即CRF中的转移得分<script type=\"math/tex\">\\sum\\lambda t</script></strong>，并且上述矩阵已经学到了一些有用的约束：</p>\n<blockquote>\n<ol>\n<li>句子的第一个单词应该是“B-” 或 “O”，而不是“I”。（从“START”-&gt;“I-Person 或 I-Organization”的转移分数很低）</li>\n<li>“B-label1 I-label2 I-label3…”，在该模式中，类别1,2,3应该是同一种实体类别。比如，“B-Person I-Person” 是正确的，而“B-Person I-Organization”则是错误的。（“B-Organization” -&gt; “I-Person”的分数很低）</li>\n<li>O I-label”是错误的，命名实体的开头应该是“B-”而不是“I-”</li>\n</ol>\n</blockquote>\n<ul>\n<li>每个输入有N中可能的结果，即N条路径，比如（加粗的为真实路径）：</li>\n</ul>\n<blockquote>\n<ol>\n<li>START B-Person B-Person B-Person B-Person B-Person END</li>\n<li>START B-Person I-Person B-Person B-Person B-Person END</li>\n<li><strong>START B-Person I-Person O B-Organization O END</strong></li>\n</ol>\n<p>……</p>\n<p>   N. O O O O O O O</p>\n</blockquote>\n<ul>\n<li>训练目标即最大化真实路径的得分，可得损失函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nLoss\n =-\\log \\frac{P_{\\text {Real Path }}}{P_{1}+P_{2}+\\ldots+P_{N}} \n =-\\log \\frac{e^{s_{\\text {Realpath }}}}{e^{s_{1}+e^{s_{2}}+\\ldots+e^{s_{N}}}}  \\\\\n =-\\left(S_{\\text {RealPath }}-\\log \\left(e^{S_{1}}+e^{S_{2}}+\\ldots+e^{S_{N}}\\right)\\right)</script><p>其中<script type=\"math/tex\">S_i</script>为一条路径对应的得分，是通过Softmax实现最大化的</p>\n<ul>\n<li><p>值得一提的是，计算分母中的所有路径的得分和<script type=\"math/tex\">-\\log (e^{S_1} + ... + e^{S_N})</script>不需要列举所有可能路径，可以用一种动态规划的方法降低计算复杂度</p>\n</li>\n<li><p>另外，在进行预测的时候，同样是使用维特比算法</p>\n</li>\n</ul>\n","site":{"data":{}},"wordcount":11663,"excerpt":"","more":"<h1 id=\"1-隐马尔可夫模型\"><a href=\"#1-隐马尔可夫模型\" class=\"headerlink\" title=\"1 隐马尔可夫模型\"></a>1 隐马尔可夫模型</h1><ul>\n<li>隐马尔可夫模型（Hidden Markov Model, HMM）常用于序列标注问题，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于<strong>概率图模型</strong>（用图结构来描述变量之间的关系，属于生成式模型）</li>\n<li>HMM属于<a href=\"https://zlkqz.site/2022/09/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#6-EM%E7%AE%97%E6%B3%95\">贝叶斯网</a>，其两个基本假设其实就是贝叶斯网的假设：<strong>给定父节点集，贝叶斯网假设每个属性与他的非后裔属性独立</strong></li>\n</ul>\n<h3 id=\"1-1-模型定义\"><a href=\"#1-1-模型定义\" class=\"headerlink\" title=\"1.1 模型定义\"></a>1.1 模型定义</h3><ul>\n<li>设<script type=\"math/tex\">Q=  \\{q_1, ..., q_N\\}</script>是所有可能的状态的集合，<script type=\"math/tex\">V = \\{v_1, ..., v_M\\}</script>是所有可能的观测的集合。而<script type=\"math/tex\">I = (i_1, ..., i_T)</script>是状态序列，<script type=\"math/tex\">O = (o_1, ..., o_T)</script>是对应的观测序列，模型定义了三种参数：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>状态转移矩阵A：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\nA = [a_{ij}]_{N \\times N}</script><p>其中<script type=\"math/tex\">a_{ij}</script>指在时刻t处于状态<script type=\"math/tex\">q_i</script>的条件下转移到在t+1时刻状态为<script type=\"math/tex\">q_j</script>的概率：</p>\n<script type=\"math/tex; mode=display\">\na_{ij} = P(i_{t+1}=q_j|i_t=q_i)</script><ol>\n<li><strong>观测概率矩阵B：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\nB = [b_j(k)]_{N\\times N}</script><p>其中<script type=\"math/tex\">b_j(k)</script>指在t时刻处于状态<script type=\"math/tex\">q_j</script>时生成观测<script type=\"math/tex\">v_k</script>的概率：</p>\n<script type=\"math/tex; mode=display\">\nb_j(k) = P(o_t = v_k|i_t =  q_j)</script><ol>\n<li><strong>初始状态概率向量<script type=\"math/tex\">\\pi</script>：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\pi = (\\pi_i)</script><p>其中<script type=\"math/tex\">\\pi_i</script>是<script type=\"math/tex\">t=1</script>时处于状态<script type=\"math/tex\">q_i</script>的概率：</p>\n<script type=\"math/tex; mode=display\">\n\\pi = P(i_1 = q_i)</script></blockquote>\n<ul>\n<li>一般使用一个三元组表示HMM的参数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\lambda = (A,B,\\pi)</script><ul>\n<li>上面对参数的定义中，隐含了HMM的两个基本假设：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>齐次马尔可夫性假设：</strong>假设隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(i_{t} \\mid i_{t-1}, o_{t-1}, \\cdots, i_{1}, o_{1}\\right)=P\\left(i_{t} \\mid i_{t-1}\\right), \\quad t=1,2, \\cdots, T</script><ol>\n<li><strong>观测独立性假设：</strong>假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(o_{t} \\mid i_{T}, o_{T}, i_{T-1}, o_{T-1}, \\cdots, i_{t+1}, o_{t+1}, i_{t}, i_{t-1}, o_{t-1}, \\cdots, i_{1}, o_{1}\\right)=P\\left(o_{t} \\mid i_{t}\\right)</script><p><strong>其实这两个假设就是贝叶斯网的假设，只不过结构特殊一点，是一个线性结构</strong></p>\n</blockquote>\n<h3 id=\"1-2-概率计算\"><a href=\"#1-2-概率计算\" class=\"headerlink\" title=\"1.2 概率计算\"></a>1.2 概率计算</h3><ul>\n<li>概率计算即给定模型<script type=\"math/tex\">\\lambda = (A,B,\\pi)</script>和观测序列<script type=\"math/tex\">O=(o_1, ..., o_T)</script>，计算该观测序列出现的概率<script type=\"math/tex\">P(O|\\lambda)</script></li>\n</ul>\n<h4 id=\"1-2-1-直接计算\"><a href=\"#1-2-1-直接计算\" class=\"headerlink\" title=\"1.2.1 直接计算\"></a>1.2.1 直接计算</h4><ul>\n<li>直接计算即列举所有可能的状态序列<script type=\"math/tex\">I</script>，计算：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(O \\mid \\lambda) &=\\sum_{I} P(O \\mid I, \\lambda) P(I \\mid \\lambda) \\\\\n&=\\sum_{i_{1}, i_{2}, \\cdots, i_{T}} \\pi_{i_{1}} b_{i_{1}}\\left(o_{1}\\right) a_{i_{1} i_{2}} b_{i_{2}}\\left(o_{2}\\right) \\cdots a_{i_{T-i} i_{T}} b_{i_{T}}\\left(o_{T}\\right)\n\\end{aligned}</script><ul>\n<li>但是这种方法计算量过大，复杂度为<script type=\"math/tex\">O(TN^T)</script>，所以是不可行的</li>\n</ul>\n<h4 id=\"1-2-2-前向计算\"><a href=\"#1-2-2-前向计算\" class=\"headerlink\" title=\"1.2.2 前向计算\"></a>1.2.2 前向计算</h4><ul>\n<li>给定模型<script type=\"math/tex\">\\lambda</script>，定义在时刻t时观测序列为<script type=\"math/tex\">o_1, ..., o_t</script>，且当前状态为<script type=\"math/tex\">q_i</script>的概率为前向概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\alpha_t(i) = P(o_1, ..., o_t, i_t=q_i|\\lambda)</script><ul>\n<li>算法流程：</li>\n</ul>\n<p>给定模型<script type=\"math/tex\">\\lambda</script>和观测序列<script type=\"math/tex\">O=(o_1, ..., o_T)</script></p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028120947404.png\" alt=\"image-20221028120947404\" style=\"zoom:80%;\" /></p>\n<h4 id=\"1-2-3-后向计算\"><a href=\"#1-2-3-后向计算\" class=\"headerlink\" title=\"1.2.3 后向计算\"></a>1.2.3 后向计算</h4><ul>\n<li>给定模型<script type=\"math/tex\">\\lambda</script>，定义在时刻t时刻状态为<script type=\"math/tex\">q_i</script>的条件下，从t+1到T的部分观测序列为<script type=\"math/tex\">o_{t+1}, ..., o_T</script>的概率为后向概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\beta_{t}(i)=P\\left(o_{t+1}, o_{t+2}, \\cdots, o_{T} \\mid i_{t}=q_{i}, \\lambda\\right)</script><ul>\n<li>算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028122410576.png\" alt=\"image-20221028122410576\" style=\"zoom:75%;\" /></p>\n<h4 id=\"1-2-4-常用概率的计算\"><a href=\"#1-2-4-常用概率的计算\" class=\"headerlink\" title=\"1.2.4 常用概率的计算\"></a>1.2.4 常用概率的计算</h4><ol>\n<li><strong>计算给定模型<script type=\"math/tex\">\\lambda</script>和观测<script type=\"math/tex\">O</script>，在t时刻处于<script type=\"math/tex\">q_i</script>的概率：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\gamma_t(i) = P(i_t = q_i|O, \\lambda)</script><blockquote>\n<ul>\n<li>首先运用贝叶斯公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\gamma_{t}(i)=P\\left(i_{t}=q_{i} \\mid O, \\lambda\\right)=\\frac{P\\left(i_{t}=q_{i}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}</script><ul>\n<li>由上述的前向后向概率的定义可得：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\alpha_t(i)\\beta_t(i) = P(i_t=q_i,O|\\lambda)</script><ul>\n<li>于是得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)}</script></blockquote>\n<ol>\n<li><strong>计算给定模型<script type=\"math/tex\">\\lambda</script>和观测<script type=\"math/tex\">O</script>，在t时刻处于状态<script type=\"math/tex\">q_i</script>并且在t+1时刻处于状态<script type=\"math/tex\">q_j</script>的概率：</strong></li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\xi_{t}(i, j)=P\\left(i_{t}=q_{i}, i_{t+1}=q_{j} \\mid O, \\lambda\\right)</script><blockquote>\n<ul>\n<li>同样先运用贝叶斯公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\xi_{t}(i, j)=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}</script><ul>\n<li>其中：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)=\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)</script><ul>\n<li>所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\xi_{t}(i, j)=\\frac{\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}</script></blockquote>\n<h3 id=\"1-3-学习算法\"><a href=\"#1-3-学习算法\" class=\"headerlink\" title=\"1.3 学习算法\"></a>1.3 学习算法</h3><ul>\n<li>在观测序列和状态序列都给定的时候，训练集足够大时，可以直接通过极大似然估计来估计模型参数，即<strong>直接通过频数来估计概率</strong></li>\n<li><p>我们主要讨论的是只给定观测序列时的情况，<strong>此时状态序列为隐变量</strong>，所以使用<a href=\"https://zlkqz.site/2022/09/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#6-EM%E7%AE%97%E6%B3%95\">EM算法</a></p>\n</li>\n<li><p>首先是E步，即计算Q函数：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nQ(\\lambda, \\bar{\\lambda}) = \\sum_{I}\\log P(O,I|\\lambda)P(I|O,\\bar{\\lambda}) = \\sum_{I}\\log P(O,I|\\lambda)\\frac{P(O,I|\\bar{\\lambda})}{P(O|\\bar{\\lambda})}</script><p>其中<script type=\"math/tex\">\\bar{\\lambda}</script>是HMM参数的当前估计值，<script type=\"math/tex\">\\lambda</script>是要极大化的值，作为下次迭代的新参数</p>\n<ul>\n<li>由于分母中的<script type=\"math/tex\">P(O|\\bar{\\lambda})</script>和要更新的参数<script type=\"math/tex\">\\lambda</script>无关，所以直接去掉，Q函数直接化为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nQ(\\lambda, \\bar{\\lambda})=\\sum_{I} \\log P(O, I \\mid \\lambda) P(O, I \\mid \\bar{\\lambda})</script><ul>\n<li>其中：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(O, I \\mid \\lambda)=\\pi_{i_{1}} b_{i_{1}}\\left(o_{1}\\right) a_{i_{i} i_{2}} b_{i_{2}}\\left(o_{2}\\right) \\cdots a_{i_{T-1} i_{T}} b_{i_{T}}\\left(o_{T}\\right)</script><ul>\n<li>所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nQ(\\lambda, \\bar{\\lambda})=& \\sum_{I} \\log \\pi_{i} P(O, I \\mid \\bar{\\lambda}) +\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i, i_{i}+1}\\right) P(O, I \\mid \\bar{\\lambda})+\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})\n\\end{aligned}</script><ul>\n<li>然后是M步，即最大化Q函数，而最大化Q函数的问题，可以化为分别最大化上式中Q函数中的三项：</li>\n</ul>\n<blockquote>\n<ol>\n<li>最大化<script type=\"math/tex\">\\sum_{I} \\log \\pi_{i} P(O, I \\mid \\bar{\\lambda})</script>，可以先将其化为：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\sum_{I} \\log \\pi_{i_{0}} P(O, I \\mid \\bar{\\lambda})=\\sum_{i=1}^{N} \\log \\pi_{i} P\\left(O, i_{1}=i \\mid \\bar{\\lambda}\\right)</script><p>由于存在约束<script type=\"math/tex\">\\sum_{i=1}^N\\pi_i =1</script>，所以可以使用拉格朗日乘子法进行求解，最后得到：</p>\n<script type=\"math/tex; mode=display\">\n\\pi_{i}=\\frac{P\\left(O, i_{1}=i \\mid \\bar{\\lambda}\\right)}{P(O \\mid \\bar{\\lambda})}</script><ol>\n<li>最大化<script type=\"math/tex\">\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i, i_{i}+1}\\right) P(O, I \\mid \\bar{\\lambda})</script>，化为：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\sum_{I}\\left(\\sum_{t=1}^{T-1} \\log a_{i_{i} i_{t+1}}\\right) P(O, I \\mid \\bar{\\lambda})=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\sum_{t=1}^{T-1} \\log a_{i j} P\\left(O, i_{t}=i, i_{t+1}=j \\mid \\bar{\\lambda}\\right)</script><p>运用拉格朗日乘子法，最后得到：</p>\n<script type=\"math/tex; mode=display\">\na_{i j}=\\frac{\\sum_{t=1}^{T-1} P\\left(O, i_{t}=i, i_{t+1}=j \\mid \\bar{\\lambda}\\right)}{\\sum_{t=1}^{T-1} P\\left(O, i_{t}=i \\mid \\bar{\\lambda}\\right)}</script><ol>\n<li>最大化<script type=\"math/tex\">\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})</script>，化为：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\sum_{I}\\left(\\sum_{t=1}^{T} \\log b_{i_{i}}\\left(o_{t}\\right)\\right) P(O, I \\mid \\bar{\\lambda})=\\sum_{j=1}^{N} \\sum_{t=1}^{T} \\log b_{j}\\left(o_{t}\\right) P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right)</script><p>运用拉格朗日乘子法，最后得到：</p>\n<script type=\"math/tex; mode=display\">\nb_{j}(k)=\\frac{\\sum_{t=1}^{T} P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right) I\\left(o_{t}=v_{k}\\right)}{\\sum_{t=1}^{T} P\\left(O, i_{t}=j \\mid \\bar{\\lambda}\\right)}</script></blockquote>\n<ul>\n<li>算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028132201642.png\" alt=\"image-20221028132201642\" style=\"zoom:67%;\" /></p>\n<h3 id=\"1-4-预测算法\"><a href=\"#1-4-预测算法\" class=\"headerlink\" title=\"1.4 预测算法\"></a>1.4 预测算法</h3><ul>\n<li>序列模型常用<strong>维特比算法</strong>来进行预测，算法流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221028132500929.png\" alt=\"image-20221028132500929\" style=\"zoom:67%;\" /></p>\n<h1 id=\"2-条件随机场\"><a href=\"#2-条件随机场\" class=\"headerlink\" title=\"2 条件随机场\"></a>2 条件随机场</h1><ul>\n<li>条件随机场（Conditional Random Field, CRF）和HMM一样，同样属于概率图模型，是给定一组输入随机变量条件下，另一组输出是随机变量的条件概率分布模型</li>\n<li>有一种特殊且最常用的CRF为线性链条件随机场，其结构和数学推导和HMM十分相似，但是仍有一些区别，稍后介绍</li>\n</ul>\n<h3 id=\"2-1-马尔可夫随机场\"><a href=\"#2-1-马尔可夫随机场\" class=\"headerlink\" title=\"2.1 马尔可夫随机场\"></a>2.1 马尔可夫随机场</h3><ul>\n<li><p><strong>概率图模型</strong>是由图结构来描述变量之间的关系的模型，而采用有向无环图结构被称为<strong>贝叶斯网</strong>，采用无向图则被称为<strong>概率无向图模型或马尔可夫随机场</strong></p>\n</li>\n<li><p>设有联合概率分布<script type=\"math/tex\">P(Y)</script>，<script type=\"math/tex\">Y \\in \\mathcal{Y}</script>是一组随机变量。由无向图<script type=\"math/tex\">G = (V,E)</script>表示概率分布<script type=\"math/tex\">P(Y)</script>，即在G中，结点<script type=\"math/tex\">v \\in V</script>表示一个随机变量<script type=\"math/tex\">Y_v</script>，<script type=\"math/tex\">Y = (Y_v)_{v \\in V}</script>，而边<script type=\"math/tex\">e \\in E</script>表示随机变量间的概率依赖关系</p>\n</li>\n<li>首先定义无向图模型内的马尔可夫性：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>成对马尔可夫性：</strong>设u和v是图中任意两个没有边连接的结点，其对应的随机变量分别为<script type=\"math/tex\">Y_u, Y_v</script>，其他所有结点为O，对应随机变量<script type=\"math/tex\">Y_O</script>。成对马尔可夫性指给定<script type=\"math/tex\">Y_O</script>的条件下，<script type=\"math/tex\">Y_u, Y_v</script>是条件独立的：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{u}, Y_{v} \\mid Y_{o}\\right)=P\\left(Y_{u} \\mid Y_{o}\\right) P\\left(Y_{v} \\mid Y_{o}\\right)</script><ol>\n<li><strong>局部马尔可夫性：</strong>设v为图中任意一个结点，W是与v相连的所有结点，O是除v和W之外的所有结点。局部马尔可夫性指给定<script type=\"math/tex\">Y_W</script>的条件下<script type=\"math/tex\">Y_v, Y_O</script>是条件独立的：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{v}, Y_{o} \\mid Y_{W}\\right)=P\\left(Y_{v} \\mid Y_{W}\\right) P\\left(Y_{o} \\mid Y_{W}\\right)</script><p>也可以为表示为：</p>\n<script type=\"math/tex; mode=display\">\nP(Y_v|Y_W) = P(Y_v|Y_W, Y_O)</script><ol>\n<li><strong>全局马尔可夫性：</strong>A、B是在图中被结点集合C分开的任意结点集合（如下图所示）。全局马尔可夫性指给定<script type=\"math/tex\">Y_C</script>条件下<script type=\"math/tex\">Y_A,Y_B</script>条件独立：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{A}, Y_{B} \\mid Y_{C}\\right)=P\\left(Y_{A} \\mid Y_{C}\\right) P\\left(Y_{B} \\mid Y_{C}\\right)</script><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029144742544.png\" alt=\"image-20221029144742544\" style=\"zoom:67%;\" /></p>\n<p><strong>上述三种马尔可夫性是等价的</strong></p>\n</blockquote>\n<ul>\n<li>而马尔可夫随机场，不仅要满足使用无向图，还需要满足马尔可夫性。显然，CRF属于马尔可夫随机场</li>\n</ul>\n<h3 id=\"2-2-团和极大团\"><a href=\"#2-2-团和极大团\" class=\"headerlink\" title=\"2.2 团和极大团\"></a>2.2 团和极大团</h3><ul>\n<li><p>无向图中的任意一个强连通子集都称为<strong>团（clique）</strong>，而一个团不能再加任意一个结点使其仍为团，则这种团为<strong>极大团（maximal clique）</strong></p>\n</li>\n<li><p>联合概率分布可以用每个团的<strong>势函数（potential function）</strong>的乘积来表示，但是一个图中的团很多，且有些随机变量同时属于多个团，所以简化来说，可以直接使用<strong>极大团的势函数成绩</strong>：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right)</script><p>其中C为极大团集，Z为规范化因子，<script type=\"math/tex\">\\Psi</script>为势函数，要求势函数是严格正的，一般定义为指数函数：</p>\n<script type=\"math/tex; mode=display\">\n\\Psi_{C}\\left(Y_{C}\\right)=\\exp \\left\\{-E\\left(Y_{C}\\right)\\right\\}</script><h3 id=\"2-3-模型定义\"><a href=\"#2-3-模型定义\" class=\"headerlink\" title=\"2.3 模型定义\"></a>2.3 模型定义</h3><ul>\n<li>设随机变量<script type=\"math/tex\">X,Y</script>，如果对任意结点v满足马尔可夫性（下式为局部马尔可夫性）：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{v} \\mid X, Y_{w}, w \\neq v\\right)=P\\left(Y_{v} \\mid X, Y_{w}, w \\sim v\\right)</script><p>则称条件概率<script type=\"math/tex\">P(Y|X)</script>为条件随机场</p>\n<ul>\n<li>另外有CRF的特例：<strong>线性链条件随机场</strong>，满足马尔可夫性：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nP\\left(Y_{i} \\mid X, Y_{1}, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_{n}\\right)=P\\left(Y_{i} \\mid X, Y_{i-1}, Y_{i+1}\\right) \\\\\ni=1,2, \\cdots, n \\text { (在 } i=1 \\text { 和 } n \\text { 时只考虑单边) }\n\\end{array}</script><p>则称条件概率<script type=\"math/tex\">P(Y|X)</script>为条件随机场，图结构如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029152606453.png\" alt=\"image-20221029152606453\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li>CRF定义中没有要求X的结构，但是一般假设X和Y有相同的图结构，比如线性链条件随机场：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221029152749755.png\" alt=\"image-20221029152749755\" style=\"zoom:67%;\" /></p>\n<p>在标注问题中（如NER），X表示输入观测序列，Y表示对应的输出标记序列或状态序列</p>\n<h3 id=\"2-4-CRF的参数\"><a href=\"#2-4-CRF的参数\" class=\"headerlink\" title=\"2.4 CRF的参数\"></a>2.4 CRF的参数</h3><ul>\n<li>现在开始介绍的CRF都默认为线性链CRF</li>\n<li>前面说过，马尔科夫场的概率可以用极大团的势函数来表示：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right) = \\frac{1}{Z}\\exp \\sum_C-E\\left(Y_{C}\\right) = \\frac{1}{Z}\\exp \\sum_CF_C\\left(Y_{C}\\right)</script><ul>\n<li>对于线性链CRF，每一个<script type=\"math/tex\">y_{i-1}, y_i</script>构成一个极大团，所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^TF_t(y_{t-1},y_t,x)</script><p>其中T为时间步数，<script type=\"math/tex\">x</script>为X的取值，是整个观测序列。<strong>由于每个极大团的结构都相同</strong>，所以可以<strong>简化为每个极大团的势函数都一样</strong>：</p>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^TF(y_{t-1},y_t,x)</script><ul>\n<li>而对于每个<script type=\"math/tex\">F(y_{t-1},y_t,x)</script>，可以表示为三个函数的和：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF(y_{t-1},y_t,x) = F_1(y_{t-1}, x) + F_1(y_t, x) + F_2(y_{t-1}, y_t, x)</script><p>其中<script type=\"math/tex\">F_1</script>就称为<strong>状态函数</strong>，<script type=\"math/tex\">F_2</script>称为<strong>转移函数</strong>。由于<script type=\"math/tex\">F_1(y_{t-1},x)</script>在上一个时间步已经出现过了，所以可以直接去掉：</p>\n<script type=\"math/tex; mode=display\">\nF(y_{t-1},y_t,x) = F_1(y_t, x) + F_2(y_{t-1}, y_t, x)</script><ul>\n<li>那么可以引入特征函数来定义<script type=\"math/tex\">F_1, F_2</script>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF_1 = \\sum_{l=1}^{K_2}\\mu_ls_l(y_t, x) \\\\\nF_2 = \\sum_{k=1}^{K_1}\\lambda_kt_k(y_{t-1}, y_t, x)</script><ul>\n<li>所以条件概率就表示为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\frac{1}{Z}\\exp \\sum_{t=1}^T(\\sum_{k=1}^{K_1}\\lambda_kt_k(y_{t-1}, y_t, x) + \\sum_{l=1}^{K_2}\\mu_ls_l(y_t, x))</script><ul>\n<li>而可以把关于时间步的求和放入括号中：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\frac{1}{Z}\\exp(\\sum_{k=1}^{K_1}\\lambda_k\\sum_{t=1}^Tt_k(y_{t-1}, y_t, x)+\\sum_{l=1}^{K_2}\\mu_l\\sum_{t=1}^Ts_l(y_t, x))</script><ul>\n<li>将两种特征函数和其权重合起来：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{k}\\left(y_{t-1}, y_{t}, x\\right)=\\left\\{\\begin{array}{l}\nt_{k}\\left(y_{t-1}, y_{t}, x\\right), \\quad k=1,2, \\cdots, K_{1} \\\\\ns_{l}\\left(y_{t}, x\\right), \\quad k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.  \\\\\nw_{k}=\\left\\{\\begin{array}{ll}\n\\lambda_{k}, & k=1,2, \\cdots, K_{1} \\\\\n\\mu_{l}, & k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.</script><p>然后对特征函数在各个时间步进行求和，记作：</p>\n<script type=\"math/tex; mode=display\">\nf_{k}(y, x)=\\sum_{t=1}^{T} f_{k}\\left(y_{t-1}, y_{t}, x\\right), \\quad k=1,2, \\cdots, K</script><ul>\n<li>所以条件概率化简为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(y \\mid x)=\\frac{1}{Z(x)} \\exp \\sum_{k=1}^{K} w_{k} f_{k}(y, x)</script><ul>\n<li>上式采用向量化表示，引入：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nw=\\left(w_{1}, w_{2}, \\cdots, w_{K}\\right)^{\\mathrm{T}} \\\\\nF(y, x)=\\left(f_{1}(y, x), f_{2}(y, x), \\cdots, f_{K}(y, x)\\right)^{\\mathrm{T}}\n\\end{array}</script><p>所以条件概率的向量化表示为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nP_{w}(y \\mid x)=\\frac{\\exp (w \\cdot F(y, x))}{Z_{w}(x)} \\\\\nZ_{w}(x)=\\sum_{y} \\exp (w \\cdot F(y, x))\n\\end{array}</script><p><strong>其中特征函数<script type=\"math/tex\">F(y,x)</script>是事先设计好给出的，而学习的目标即学习权重<script type=\"math/tex\">w</script>，最大化特征函数的总得分</strong></p>\n<h3 id=\"2-5-概率计算\"><a href=\"#2-5-概率计算\" class=\"headerlink\" title=\"2.5 概率计算\"></a>2.5 概率计算</h3><ul>\n<li>和前面HMM一样，同样运用前向后向算法的变量来进行概率计算，首先定义一个矩阵，<strong>为了方便讨论，我们又引入了两个时间步的状态序列<script type=\"math/tex\">y_0 = start, y_{T+1} = stop</script>，实际前文的讨论中也隐含了<script type=\"math/tex\">y_0 = start</script>：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nM_{t}(x)=\\left[M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)\\right] \\\\\nM_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)=\\exp \\sum_{k=1}^{K} w_{k} f_{k}\\left(y_{t-1}, y_{t}, x\\right)\n\\end{array}</script><p>设<script type=\"math/tex\">y_t</script>有m个取值，则矩阵<script type=\"math/tex\">M_t(x)</script>里面的<script type=\"math/tex\">y_t,y_{t-1}</script>分别取不同的m个值，所以<script type=\"math/tex\">M_t(x)</script>是一个<script type=\"math/tex\">m \\times m</script>阶矩阵</p>\n<ul>\n<li>有了上述定义，可以将条件概率进一步写为矩阵形式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\prod_{t=1}^{T} M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right)</script><p>其中<script type=\"math/tex\">Z_w(x)</script>是T+1个矩阵乘积的第(start, end)元素：</p>\n<script type=\"math/tex; mode=display\">\nZ_{w}(x)=\\left(M_{1}(x) M_{2}(x) \\cdots M_{T+1}(x)\\right)_{\\text {start,stop }}</script><ul>\n<li>现在来定义<strong>前向概率，<script type=\"math/tex\">\\alpha_t(y_t|x)</script>为在时刻t时观测序列为<script type=\"math/tex\">x_1, ..., x_t</script>，且当前状态为<script type=\"math/tex\">y_t</script>的概率</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\alpha_{t}\\left(y_{t} \\mid x\\right)=\\alpha_{t-1}\\left(y_{t-1} \\mid x\\right) M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right) \\quad t=1,...,T+1 \\\\\n\\alpha_{0}(y \\mid x)=\\left\\{\\begin{array}{ll}\n1, & y=\\operatorname{start} \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.</script><p>因为<script type=\"math/tex\">y_t</script>有m个取值，所以可以定义m维向量<script type=\"math/tex\">\\alpha_t(x)</script>：</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_{t}^{\\mathrm{T}}(x)=\\alpha_{t-1}^{\\mathrm{T}}(x) M_{t}(x) \\\\\n\\alpha_0(x) = \\mathbb{1}（单位向量）</script><ul>\n<li>同样可以定义<strong>后向概率，<script type=\"math/tex\">\\beta_t(y_t|x)</script>为在时刻t观测序列为<script type=\"math/tex\">x_{t+1}, ..., x_T</script>，且当前状态为<script type=\"math/tex\">y_t</script>的概率：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\beta_{t}\\left(y_{t} \\mid x\\right)=M_{t}\\left(y_{t}, y_{t+1} \\mid x\\right) \\beta_{t+1}\\left(y_{t+1} \\mid x\\right) \\\\\n\\beta_{t+1}\\left(y \\mid x\\right)=\\left\\{\\begin{array}{ll}\n1, & y=\\text { stop } \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.</script><p>同样有向量形式：</p>\n<script type=\"math/tex; mode=display\">\n\\beta_{t}(x)=M_{t+1}(x) \\beta_{t+1}(x) \\\\\n\\beta_{T+1}(x) = \\mathbb{1}  (单位向量)</script><ul>\n<li>得到了前后向概率，就可以得到<strong>在时刻t状态是<script type=\"math/tex\">y_t</script>的概率：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{t}=y_{t} \\mid x\\right)=\\frac{\\alpha_{t}\\left(y_{t} \\mid x\\right) \\beta_{t}\\left(y_{t} \\mid x\\right)}{Z(x)}</script><ul>\n<li>以及<strong>在时刻t-1状态是<script type=\"math/tex\">y_{t-1}</script>并且在时刻t状态是<script type=\"math/tex\">y_t</script>的概率：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(Y_{t-1}=y_{t-1}, Y_{t}=y_{t} \\mid x\\right)=\\frac{\\alpha_{t-1}\\left(y_{t-1} \\mid x\\right) M_{t}\\left(y_{t-1}, y_{t} \\mid x\\right) \\beta_{t}\\left(y_{t} \\mid x\\right)}{Z(x)}</script><ul>\n<li>上述两个式子的<script type=\"math/tex\">Z(x)</script>可以用更简单的形式表达：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nZ(x) = \\alpha_n^T(x)\\mathbb{1} = \\mathbb{1}\\beta_1(x)</script><h3 id=\"2-6-学习和预测\"><a href=\"#2-6-学习和预测\" class=\"headerlink\" title=\"2.6 学习和预测\"></a>2.6 学习和预测</h3><ul>\n<li><p>预测算法和HMM一样，使用<strong>维特比算法</strong></p>\n</li>\n<li><p>学习算法其实就是<a href=\"https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#3-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95\">最大熵模型的学习</a></p>\n</li>\n</ul>\n<h1 id=\"3-HMM和CRF的区别\"><a href=\"#3-HMM和CRF的区别\" class=\"headerlink\" title=\"3 HMM和CRF的区别\"></a>3 HMM和CRF的区别</h1><ul>\n<li>CRF和HMM的数学推导几乎是一样的，但是差异在于：</li>\n</ul>\n<blockquote>\n<ol>\n<li>HMM属于贝叶斯网络，而CRF属于马尔可夫场，前者的假设和约束更加的严格，比如CRF并没有像HMM一样完全依赖上一步的状态，HMM的观测变量的生成是独立的</li>\n<li>上述我们讨论的仅仅只是线性链CRF，但是CRF还可以有其他的图结构，并且<strong>实际上可以任意选定特征函数的个数和形式，特征函数的不确定也是CRF能和深度学习融合的最主要原因，模型可以自己学习特征函数，并且不用显示地表达出来</strong></li>\n</ol>\n</blockquote>\n<h1 id=\"4-CRF和深度学习模型的结合\"><a href=\"#4-CRF和深度学习模型的结合\" class=\"headerlink\" title=\"4 CRF和深度学习模型的结合\"></a>4 CRF和深度学习模型的结合</h1><ul>\n<li><p>以BiLSTM+CRF做NER任务举例</p>\n</li>\n<li><p>如果不用CRF而是直接在模型的后面接一个Softmax，鉴于所选取的基模型的强大的特征抽取能力，这已经可以有比较好的分类效果，<strong>但是NER任务是存在一些约束的</strong>，比如BIO格式中，B-Person后面不可能跟I-Organization。<strong>Softmax的分类是每个时间步相互独立的，所以可能会出现上述的问题</strong></p>\n</li>\n<li><p><strong>而CRF层可以加入一些约束来保证最终预测结果是有效的，这些约束可以在训练数据时被CRF层自动学习得到</strong></p>\n</li>\n<li><p>首先介绍一下模型结构：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-0650e1511e7d4419c9528a8d08ea61fd_720w.webp\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<p>BiLSTM的输出经过Dense层，<strong>转化为每个Label对应的score（图中浅黄色部分），这个score即CRF中的状态得分<script type=\"math/tex\">\\sum \\mu s</script></strong>，然后将其输入CRF，CRF层维护了一个转移矩阵（Transition Matrix），这也是CRF层中需要学习的参数，假设总共有包括START和END在内的7个label，则转移矩阵为：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-2064e34cece3be4e852b1ace6bbca2ba_720w.webp\" alt=\"img\" style=\"zoom:67%;\" /></p>\n<p><strong>每个元素代表了相邻时间步之间进行状态转移的score，这个score即CRF中的转移得分<script type=\"math/tex\">\\sum\\lambda t</script></strong>，并且上述矩阵已经学到了一些有用的约束：</p>\n<blockquote>\n<ol>\n<li>句子的第一个单词应该是“B-” 或 “O”，而不是“I”。（从“START”-&gt;“I-Person 或 I-Organization”的转移分数很低）</li>\n<li>“B-label1 I-label2 I-label3…”，在该模式中，类别1,2,3应该是同一种实体类别。比如，“B-Person I-Person” 是正确的，而“B-Person I-Organization”则是错误的。（“B-Organization” -&gt; “I-Person”的分数很低）</li>\n<li>O I-label”是错误的，命名实体的开头应该是“B-”而不是“I-”</li>\n</ol>\n</blockquote>\n<ul>\n<li>每个输入有N中可能的结果，即N条路径，比如（加粗的为真实路径）：</li>\n</ul>\n<blockquote>\n<ol>\n<li>START B-Person B-Person B-Person B-Person B-Person END</li>\n<li>START B-Person I-Person B-Person B-Person B-Person END</li>\n<li><strong>START B-Person I-Person O B-Organization O END</strong></li>\n</ol>\n<p>……</p>\n<p>   N. O O O O O O O</p>\n</blockquote>\n<ul>\n<li>训练目标即最大化真实路径的得分，可得损失函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nLoss\n =-\\log \\frac{P_{\\text {Real Path }}}{P_{1}+P_{2}+\\ldots+P_{N}} \n =-\\log \\frac{e^{s_{\\text {Realpath }}}}{e^{s_{1}+e^{s_{2}}+\\ldots+e^{s_{N}}}}  \\\\\n =-\\left(S_{\\text {RealPath }}-\\log \\left(e^{S_{1}}+e^{S_{2}}+\\ldots+e^{S_{N}}\\right)\\right)</script><p>其中<script type=\"math/tex\">S_i</script>为一条路径对应的得分，是通过Softmax实现最大化的</p>\n<ul>\n<li><p>值得一提的是，计算分母中的所有路径的得分和<script type=\"math/tex\">-\\log (e^{S_1} + ... + e^{S_N})</script>不需要列举所有可能路径，可以用一种动态规划的方法降低计算复杂度</p>\n</li>\n<li><p>另外，在进行预测的时候，同样是使用维特比算法</p>\n</li>\n</ul>\n"},{"title":"KNN总结","math":true,"date":"2022-03-24T16:00:00.000Z","_content":"\n\n\n# 1 基本思想\n\n- KNN模型是一种非参、惰性的模型，即并不需要显式的训练过程，也不需要参数，可以基于训练集直接进行预测\n- 给定训练集$$D = \\{(x_1, y_1), ... (x_N, y_N)\\}$$和要预测的输入$$x$$，在训练集中找到和$$x$$最近的k个样本，然后通过投票的方式来预测$$x$$的类别\n\n$$\ny=\\arg \\max _{c_{j}} \\sum_{x_{i} \\in N_{k}(x)} I\\left(y_{i}=c_{j}\\right), \\quad i=1,2, \\cdots, N ; \\quad j=1,2, \\cdots, K\n$$\n\n其中$$N_k$$为和$$x$$最近的k个样本\n\n- 样本之间的距离度量常用**欧氏距离**，但也可以为更一般的$$L_p,\\quad p=1,2,...,\\infty$$\n\n- 对于k值的选择具有很大的影响，在应用中，**k值一般取一个比较小的值**，通常用**交叉验证法**选取最优值\n\n\n\n\n\n# 2 kd树\n\n- 如果要把输入和每个样本都比较，复杂度是和训练及大小成正比的，所以采用kd树来降低复杂度\n\n\n\n### 2.1 kd树构造\n\n- 简单来说，就是**每次选取特征空间的一个维度，然后基于这个维度进行划分（一般选择当前样本在这个维度上的中位数进行划分）**，算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120149491.png\" alt=\"image-20221103120149491\" style=\"zoom: 50%;\" />\n\n- 举个栗子，给定特征空间$$T=\\left\\{(2,3)^{\\mathrm{T}},(5,4)^{\\mathrm{T}},(9,6)^{\\mathrm{T}},(4,7)^{\\mathrm{T}},(8,1)^{\\mathrm{T}},(7,2)^{\\mathrm{T}}\\right\\}$$，则会产生以下的kd树和空间划分：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120509325.png\" alt=\"image-20221103120509325\" style=\"zoom:50%;\" />\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120523272.png\" alt=\"image-20221103120523272\" style=\"zoom:50%;\" />\n\n\n\n### 2.2 kd树搜索\n\n- 构造好kd树后，就需要对kd树的结点进行搜索得到最近的k个样本，这里只说明搜索最近的样本（k=1），其他情况可以类推\n\n- 可以从上面的栗子中看到，**每次划分会产生一个超矩阵空间，并且叶结点对应的超矩阵空间是最小的**\n\n- 给定要预测的输入$$x$$，先根据kd树的划分依据，找到$$x$$对应的叶结点，然后将该叶节点对应的样本$$x'$$作为候选最近样本，那么**更近的样本，一定处于以x为球心，distance(x, x')为半径的超球面内部**，那么我们依次对每个超矩阵空间进行判定，**若超矩阵空间和超球面没有相交，那么最近邻样本一定不在该超矩阵空间中，则该空间中的所有样本就都不用判断了；反之若相交，则将该子树作为根节点，递归寻找**，算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103155306872.png\" alt=\"image-20221103155306872\" style=\"zoom:50%;\" />\n\n- 使用kd树使平均时间复杂度从$$O(N)$$降为了$$O(\\log N)$$，但是需要很大的存储空间\n\n\n\n\n\n# 3 一些结论\n\n1. **KNN对回归、分类都可以用，对数据没有假设，并且对异常点不敏感，但是样本不平衡的时候准确率较低，可以通过投票时引入权值来改善**\n2. **KNN每次选取最大方差的维度作为划分维度(方差越大，表示此维度上数据越分散)，这样一次划分的两组样本差别越大，搜索的时候可能会减少递归次数**\n\n3. **KNN要提取中位数，可以在算法最开始的时候，在每个维度进行一次排序并存储下来，而不需要每次选了维度再排序，提升性能**\n\n","source":"_posts/KNN.md","raw":"---\ntitle: KNN总结\nmath: true\ndate: 2022-3-25\n---\n\n\n\n# 1 基本思想\n\n- KNN模型是一种非参、惰性的模型，即并不需要显式的训练过程，也不需要参数，可以基于训练集直接进行预测\n- 给定训练集$$D = \\{(x_1, y_1), ... (x_N, y_N)\\}$$和要预测的输入$$x$$，在训练集中找到和$$x$$最近的k个样本，然后通过投票的方式来预测$$x$$的类别\n\n$$\ny=\\arg \\max _{c_{j}} \\sum_{x_{i} \\in N_{k}(x)} I\\left(y_{i}=c_{j}\\right), \\quad i=1,2, \\cdots, N ; \\quad j=1,2, \\cdots, K\n$$\n\n其中$$N_k$$为和$$x$$最近的k个样本\n\n- 样本之间的距离度量常用**欧氏距离**，但也可以为更一般的$$L_p,\\quad p=1,2,...,\\infty$$\n\n- 对于k值的选择具有很大的影响，在应用中，**k值一般取一个比较小的值**，通常用**交叉验证法**选取最优值\n\n\n\n\n\n# 2 kd树\n\n- 如果要把输入和每个样本都比较，复杂度是和训练及大小成正比的，所以采用kd树来降低复杂度\n\n\n\n### 2.1 kd树构造\n\n- 简单来说，就是**每次选取特征空间的一个维度，然后基于这个维度进行划分（一般选择当前样本在这个维度上的中位数进行划分）**，算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120149491.png\" alt=\"image-20221103120149491\" style=\"zoom: 50%;\" />\n\n- 举个栗子，给定特征空间$$T=\\left\\{(2,3)^{\\mathrm{T}},(5,4)^{\\mathrm{T}},(9,6)^{\\mathrm{T}},(4,7)^{\\mathrm{T}},(8,1)^{\\mathrm{T}},(7,2)^{\\mathrm{T}}\\right\\}$$，则会产生以下的kd树和空间划分：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120509325.png\" alt=\"image-20221103120509325\" style=\"zoom:50%;\" />\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120523272.png\" alt=\"image-20221103120523272\" style=\"zoom:50%;\" />\n\n\n\n### 2.2 kd树搜索\n\n- 构造好kd树后，就需要对kd树的结点进行搜索得到最近的k个样本，这里只说明搜索最近的样本（k=1），其他情况可以类推\n\n- 可以从上面的栗子中看到，**每次划分会产生一个超矩阵空间，并且叶结点对应的超矩阵空间是最小的**\n\n- 给定要预测的输入$$x$$，先根据kd树的划分依据，找到$$x$$对应的叶结点，然后将该叶节点对应的样本$$x'$$作为候选最近样本，那么**更近的样本，一定处于以x为球心，distance(x, x')为半径的超球面内部**，那么我们依次对每个超矩阵空间进行判定，**若超矩阵空间和超球面没有相交，那么最近邻样本一定不在该超矩阵空间中，则该空间中的所有样本就都不用判断了；反之若相交，则将该子树作为根节点，递归寻找**，算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103155306872.png\" alt=\"image-20221103155306872\" style=\"zoom:50%;\" />\n\n- 使用kd树使平均时间复杂度从$$O(N)$$降为了$$O(\\log N)$$，但是需要很大的存储空间\n\n\n\n\n\n# 3 一些结论\n\n1. **KNN对回归、分类都可以用，对数据没有假设，并且对异常点不敏感，但是样本不平衡的时候准确率较低，可以通过投票时引入权值来改善**\n2. **KNN每次选取最大方差的维度作为划分维度(方差越大，表示此维度上数据越分散)，这样一次划分的两组样本差别越大，搜索的时候可能会减少递归次数**\n\n3. **KNN要提取中位数，可以在算法最开始的时候，在每个维度进行一次排序并存储下来，而不需要每次选了维度再排序，提升性能**\n\n","slug":"KNN","published":1,"updated":"2022-12-20T06:19:22.075Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1c00077cszhxx4g90q","content":"<h1 id=\"1-基本思想\"><a href=\"#1-基本思想\" class=\"headerlink\" title=\"1 基本思想\"></a>1 基本思想</h1><ul>\n<li>KNN模型是一种非参、惰性的模型，即并不需要显式的训练过程，也不需要参数，可以基于训练集直接进行预测</li>\n<li>给定训练集<script type=\"math/tex\">D = \\{(x_1, y_1), ... (x_N, y_N)\\}</script>和要预测的输入<script type=\"math/tex\">x</script>，在训练集中找到和<script type=\"math/tex\">x</script>最近的k个样本，然后通过投票的方式来预测<script type=\"math/tex\">x</script>的类别</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ny=\\arg \\max _{c_{j}} \\sum_{x_{i} \\in N_{k}(x)} I\\left(y_{i}=c_{j}\\right), \\quad i=1,2, \\cdots, N ; \\quad j=1,2, \\cdots, K</script><p>其中<script type=\"math/tex\">N_k</script>为和<script type=\"math/tex\">x</script>最近的k个样本</p>\n<ul>\n<li><p>样本之间的距离度量常用<strong>欧氏距离</strong>，但也可以为更一般的<script type=\"math/tex\">L_p,\\quad p=1,2,...,\\infty</script></p>\n</li>\n<li><p>对于k值的选择具有很大的影响，在应用中，<strong>k值一般取一个比较小的值</strong>，通常用<strong>交叉验证法</strong>选取最优值</p>\n</li>\n</ul>\n<h1 id=\"2-kd树\"><a href=\"#2-kd树\" class=\"headerlink\" title=\"2 kd树\"></a>2 kd树</h1><ul>\n<li>如果要把输入和每个样本都比较，复杂度是和训练及大小成正比的，所以采用kd树来降低复杂度</li>\n</ul>\n<h3 id=\"2-1-kd树构造\"><a href=\"#2-1-kd树构造\" class=\"headerlink\" title=\"2.1 kd树构造\"></a>2.1 kd树构造</h3><ul>\n<li>简单来说，就是<strong>每次选取特征空间的一个维度，然后基于这个维度进行划分（一般选择当前样本在这个维度上的中位数进行划分）</strong>，算法流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120149491.png\" alt=\"image-20221103120149491\" style=\"zoom: 50%;\" /></p>\n<ul>\n<li>举个栗子，给定特征空间<script type=\"math/tex\">T=\\left\\{(2,3)^{\\mathrm{T}},(5,4)^{\\mathrm{T}},(9,6)^{\\mathrm{T}},(4,7)^{\\mathrm{T}},(8,1)^{\\mathrm{T}},(7,2)^{\\mathrm{T}}\\right\\}</script>，则会产生以下的kd树和空间划分：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120509325.png\" alt=\"image-20221103120509325\" style=\"zoom:50%;\" /></p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120523272.png\" alt=\"image-20221103120523272\" style=\"zoom:50%;\" /></p>\n<h3 id=\"2-2-kd树搜索\"><a href=\"#2-2-kd树搜索\" class=\"headerlink\" title=\"2.2 kd树搜索\"></a>2.2 kd树搜索</h3><ul>\n<li><p>构造好kd树后，就需要对kd树的结点进行搜索得到最近的k个样本，这里只说明搜索最近的样本（k=1），其他情况可以类推</p>\n</li>\n<li><p>可以从上面的栗子中看到，<strong>每次划分会产生一个超矩阵空间，并且叶结点对应的超矩阵空间是最小的</strong></p>\n</li>\n<li><p>给定要预测的输入<script type=\"math/tex\">x</script>，先根据kd树的划分依据，找到<script type=\"math/tex\">x</script>对应的叶结点，然后将该叶节点对应的样本<script type=\"math/tex\">x'</script>作为候选最近样本，那么<strong>更近的样本，一定处于以x为球心，distance(x, x’)为半径的超球面内部</strong>，那么我们依次对每个超矩阵空间进行判定，<strong>若超矩阵空间和超球面没有相交，那么最近邻样本一定不在该超矩阵空间中，则该空间中的所有样本就都不用判断了；反之若相交，则将该子树作为根节点，递归寻找</strong>，算法流程如下：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103155306872.png\" alt=\"image-20221103155306872\" style=\"zoom:50%;\" /></p>\n<ul>\n<li>使用kd树使平均时间复杂度从<script type=\"math/tex\">O(N)</script>降为了<script type=\"math/tex\">O(\\log N)</script>，但是需要很大的存储空间</li>\n</ul>\n<h1 id=\"3-一些结论\"><a href=\"#3-一些结论\" class=\"headerlink\" title=\"3 一些结论\"></a>3 一些结论</h1><ol>\n<li><strong>KNN对回归、分类都可以用，对数据没有假设，并且对异常点不敏感，但是样本不平衡的时候准确率较低，可以通过投票时引入权值来改善</strong></li>\n<li><p><strong>KNN每次选取最大方差的维度作为划分维度(方差越大，表示此维度上数据越分散)，这样一次划分的两组样本差别越大，搜索的时候可能会减少递归次数</strong></p>\n</li>\n<li><p><strong>KNN要提取中位数，可以在算法最开始的时候，在每个维度进行一次排序并存储下来，而不需要每次选了维度再排序，提升性能</strong></p>\n</li>\n</ol>\n","site":{"data":{}},"wordcount":1166,"excerpt":"","more":"<h1 id=\"1-基本思想\"><a href=\"#1-基本思想\" class=\"headerlink\" title=\"1 基本思想\"></a>1 基本思想</h1><ul>\n<li>KNN模型是一种非参、惰性的模型，即并不需要显式的训练过程，也不需要参数，可以基于训练集直接进行预测</li>\n<li>给定训练集<script type=\"math/tex\">D = \\{(x_1, y_1), ... (x_N, y_N)\\}</script>和要预测的输入<script type=\"math/tex\">x</script>，在训练集中找到和<script type=\"math/tex\">x</script>最近的k个样本，然后通过投票的方式来预测<script type=\"math/tex\">x</script>的类别</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ny=\\arg \\max _{c_{j}} \\sum_{x_{i} \\in N_{k}(x)} I\\left(y_{i}=c_{j}\\right), \\quad i=1,2, \\cdots, N ; \\quad j=1,2, \\cdots, K</script><p>其中<script type=\"math/tex\">N_k</script>为和<script type=\"math/tex\">x</script>最近的k个样本</p>\n<ul>\n<li><p>样本之间的距离度量常用<strong>欧氏距离</strong>，但也可以为更一般的<script type=\"math/tex\">L_p,\\quad p=1,2,...,\\infty</script></p>\n</li>\n<li><p>对于k值的选择具有很大的影响，在应用中，<strong>k值一般取一个比较小的值</strong>，通常用<strong>交叉验证法</strong>选取最优值</p>\n</li>\n</ul>\n<h1 id=\"2-kd树\"><a href=\"#2-kd树\" class=\"headerlink\" title=\"2 kd树\"></a>2 kd树</h1><ul>\n<li>如果要把输入和每个样本都比较，复杂度是和训练及大小成正比的，所以采用kd树来降低复杂度</li>\n</ul>\n<h3 id=\"2-1-kd树构造\"><a href=\"#2-1-kd树构造\" class=\"headerlink\" title=\"2.1 kd树构造\"></a>2.1 kd树构造</h3><ul>\n<li>简单来说，就是<strong>每次选取特征空间的一个维度，然后基于这个维度进行划分（一般选择当前样本在这个维度上的中位数进行划分）</strong>，算法流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120149491.png\" alt=\"image-20221103120149491\" style=\"zoom: 50%;\" /></p>\n<ul>\n<li>举个栗子，给定特征空间<script type=\"math/tex\">T=\\left\\{(2,3)^{\\mathrm{T}},(5,4)^{\\mathrm{T}},(9,6)^{\\mathrm{T}},(4,7)^{\\mathrm{T}},(8,1)^{\\mathrm{T}},(7,2)^{\\mathrm{T}}\\right\\}</script>，则会产生以下的kd树和空间划分：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120509325.png\" alt=\"image-20221103120509325\" style=\"zoom:50%;\" /></p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103120523272.png\" alt=\"image-20221103120523272\" style=\"zoom:50%;\" /></p>\n<h3 id=\"2-2-kd树搜索\"><a href=\"#2-2-kd树搜索\" class=\"headerlink\" title=\"2.2 kd树搜索\"></a>2.2 kd树搜索</h3><ul>\n<li><p>构造好kd树后，就需要对kd树的结点进行搜索得到最近的k个样本，这里只说明搜索最近的样本（k=1），其他情况可以类推</p>\n</li>\n<li><p>可以从上面的栗子中看到，<strong>每次划分会产生一个超矩阵空间，并且叶结点对应的超矩阵空间是最小的</strong></p>\n</li>\n<li><p>给定要预测的输入<script type=\"math/tex\">x</script>，先根据kd树的划分依据，找到<script type=\"math/tex\">x</script>对应的叶结点，然后将该叶节点对应的样本<script type=\"math/tex\">x'</script>作为候选最近样本，那么<strong>更近的样本，一定处于以x为球心，distance(x, x’)为半径的超球面内部</strong>，那么我们依次对每个超矩阵空间进行判定，<strong>若超矩阵空间和超球面没有相交，那么最近邻样本一定不在该超矩阵空间中，则该空间中的所有样本就都不用判断了；反之若相交，则将该子树作为根节点，递归寻找</strong>，算法流程如下：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221103155306872.png\" alt=\"image-20221103155306872\" style=\"zoom:50%;\" /></p>\n<ul>\n<li>使用kd树使平均时间复杂度从<script type=\"math/tex\">O(N)</script>降为了<script type=\"math/tex\">O(\\log N)</script>，但是需要很大的存储空间</li>\n</ul>\n<h1 id=\"3-一些结论\"><a href=\"#3-一些结论\" class=\"headerlink\" title=\"3 一些结论\"></a>3 一些结论</h1><ol>\n<li><strong>KNN对回归、分类都可以用，对数据没有假设，并且对异常点不敏感，但是样本不平衡的时候准确率较低，可以通过投票时引入权值来改善</strong></li>\n<li><p><strong>KNN每次选取最大方差的维度作为划分维度(方差越大，表示此维度上数据越分散)，这样一次划分的两组样本差别越大，搜索的时候可能会减少递归次数</strong></p>\n</li>\n<li><p><strong>KNN要提取中位数，可以在算法最开始的时候，在每个维度进行一次排序并存储下来，而不需要每次选了维度再排序，提升性能</strong></p>\n</li>\n</ol>\n"},{"title":"RNN基本概念","math":true,"date":"2021-12-05T16:00:00.000Z","_content":"\n\n\n- 与多层感知机和能有效**处理空间信息**的卷积神经网络不同，循环神经网络是为更好地**处理时序信息**而设计的。它**引⼊状态变量来存储过去的信息**，并⽤其**与当前的输⼊共同决定当前的输出**\n\n\n\n\n\n# 1 语言模型\n\n### 1.1 语言模型的计算\n\n- 我们可以把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。假设⼀段⻓度为 $T$ 的⽂本中的词 依次为 $w_1, w_2, . . . , w_T$，那么在离散的时间序列中，$w_t（1 ≤ t ≤ T）$可看作在时间步（time step）$ t$ 的输出或标签。给定⼀个⻓度为 $T$ 的词的序列 $w_1, w_2, . . . , w_T$，语⾔模型将计算该序列的概率：\n\n$$\nP(w_1, w_2, ...,w_T)\n$$\n\n- **由于 $w_1, w_2, . . . , w_T$ 是依次生成的，所以有：**\n\n$$\nP(w_1, w_2, ...,w_T) = \\prod_{t=1}^T P(w_t | w_1, ..., w_{t-1})\n$$\n\n- 为了计算语⾔模型，我们需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即**语⾔模型参数**。**词的概率可以通过该词在训练数据集中的相对词频来计算**\n\n\n\n### 1.2 n元语法\n\n- **当序列⻓度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加**\n- n元语法通过⻢尔可夫假设（**虽然并不⼀定成立**）简化了语⾔模型的计算。这⾥的**⻢尔可夫假设是指⼀个词的出现只与前⾯n个词相关**，即n阶⻢尔可夫链（Markov chain of order n）\n- 如果基于$n-1$阶马尔科夫链，我们可以将语言模型改写为：\n\n$$\nP(w_1, w_2, ...,w_T) \\approx \\prod_{t=1}^T P(w_t | w_{t - (n - 1)}, ..., w_{t-1})\n$$\n\n- 以上也叫**n元语法**（n-grams）。它是基于n−1阶⻢尔可夫链的概率语⾔模型\n\n- 当n较小时，n元语法往往并不准确。当n较⼤时，n元语法需要计算并存储⼤量的词频和多词相邻频率。**n权衡了计算复杂度和模型准确性**\n\n\n\n\n\n# 2 循环神经网络（RNN）\n\n- 在n元语法中，时间步t的词 $w_t$ 基于前⾯所有词的条件概率只考虑了最近时间步的n − 1个词。如果要考虑⽐t − (n − 1)更早时间步的词对$w_t$的可能影响，我们需要增⼤n。但这样模型参数的数量将随之呈指数级增⻓\n- 但是对于循环神经网络，它**并⾮刚性地记忆所有固定⻓度的序列，而是通过隐藏状态来存储之前时间步的信息**\n\n\n\n### 2.1 循环神经网络\n\n- 假设$$X_t \\in \\mathbb{R}^{n \\times d}$$是序列中时间步t的小批量输⼊， $$H_t \\in \\mathbb{R}^{n \\times h}$$是该时间步的隐藏变量。与多层感知机不同的是，这⾥我们保存上⼀时间步的隐藏变量$$H_{t−1}$$，并引⼊⼀个新的权重参数$$W_{hh} \\in \\mathbb{R}^{h×h}$$，该参数⽤来描述在当前时间步如何使⽤上⼀时间步的隐藏变量。时间步t的隐藏变量的计算由当前时间步的输⼊和上⼀时间步的隐藏变量共同决定：\n\n$$\n\\boldsymbol{H}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right)\n$$\n\n$\\phi$是激活函数\n\n- **这⾥的隐藏变量能够捕捉截⾄当前时间步的序列的历史信息**\n- 每个时间步还有一个对应的输出：\n\n$$\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}\n$$\n\n在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差\n\n- **即便在不同时间步，循环神经网络也始终使⽤W， b这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增⻓**\n\n![image-20211210101227481](https://s2.loli.net/2021/12/10/oAbwO43tDSTWpVg.png)\n\n- 隐藏状态中$$X_{t}W_{x h}+H_{t-1} W_{h h}$$的计算等价于$$X_t$$与$$H_{t−1}$$连结后的矩阵乘以$$W_{xh}$$与$$W_{hh}$$连结后的矩阵，实际上在代码实现中基本都是这么做的\n\n\n\n- 举一个栗子：基于字符级循环神经网络的语⾔模型\n\n![image-20211210101914126](https://s2.loli.net/2021/12/10/OYZwANsxl2oEQaI.png)\n\n**标签序列依次为输入序列的下一个**，并且在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差\n\n\n\n### 2.2 时序数据的采样\n\n- 时序数据的⼀个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想” “要” “有” “直” “升”。**该样本的标签序列为这些字符分别在训练集中的下⼀个字符**，即 “要” “有” “直” “升” “机”。\n- 我们有两种⽅式对时序数据进⾏采样，分别是随机采样和相邻采样\n\n\n\n#### 2.2.1 随机采样\n\n- 在随机采样中，每个样本是原始序列上任意截取的⼀段序列。\n\n- 相邻的两个随机小批量在原始序列上的位置不⼀定相毗邻。因此，我们⽆法⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态。**在训练模型时，每次随机采样前都需要重新初始化隐藏状态**\n\n#### 2.2.2 相邻采样\n\n- 除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就**可以⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态**，从而使下⼀个小批量的输出也取决于当前小批量的输⼊\n\n\n\n- **两种采样方式的区别：**\n  1. 采用相邻采样，在训练模型时，我们只需在每⼀个迭代周期开始时初始化隐藏状态\n  2. 当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同⼀迭代周期中，随着迭代次数的增加，梯度的**计算开销会越来越⼤**。为了使模型参数的梯度计算只依赖⼀次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来\n  3. **相邻采样不能并行运算**，因为必须等上一个小批量结束了，下一个小批量才能开始\n\n\n\n### 2.3 RNN的训练、预测\n\n#### 2.3.1 输入输出\n\n- 首先是RNN的输入输出，应该包含time_steps、batch_size、vocab_size（时间步、批量大小、词典大小）的维度，之所以有一个维度是词典大小，是因为我们会**把输入输出用独热向量**表示\n\n\n\n#### 2.3.2 预测\n\n- **在训练时，下一个时间步的输入可以：**\n\n  1. 上一个时间步的标签\n  2. 上一个时间步的表征（也就是输出）\n\n  采用第一种方法，**更容易收敛，但是泛化能力更差**，而第二种方法**更不易收敛，但是泛化能力更强**\n\n- **但是在预测时**，由于我们压根没有标签，所以只能用上述第二种方法。正是由于训练时和预测时干的事情都不一样，所以采用第一种方法泛化能力更差\n\n\n\n#### 2.3.3 困惑度\n\n- 我们通常使⽤困惑度（perplexity）来评价语⾔模型的好坏\n- 困惑度的基本思想是：**给测试集的句子赋予较高概率值的语言模型较好，当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型在测试集上的概率越高越好**\n- 现在给定测试集上的一个句子序列$$w_1, ..., w_t$$，那么其困惑度为：\n\n$$\nPP(w_1, ..., w_t) = P(w_1, ..., w_t)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P\\left(w_{1} w_{2} \\ldots w_{N})\\right.}}\n$$\n\n可以看到，我们的**目的是最大化获得测试集上的句子的概率**，而最小化困惑度其实就是最大化这个概率，另外这里使用了一个几何平均，其目的是：\n\n> 1. 因为每个字符的概率必然小于1，所以越长的句子的概率在连乘的情况下必然越小，所以为了对长短句公平，需要平均一下\n> 2. 因为**几何平均数**的特点是，如果有其中的一个概率是很小的，那么最终的结果就不可能很大，从而要求好的句子的每个字符（即每个时间步的输出）都要有基本让人满意的概率\n\n- 采用unigram，假设每个时间步的预测是相互独立的：\n\n$$\nPP(w_1, ..., w_t) = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^NP(w_i)}}\n$$\n\n这是uni-gram（即当前词不取决于前面的词）的困惑度计算公式，针对不同近似的N元语法又不同的计算公式，比如bi-gram、tri-gram等。另外，里面的$$P(w_i)$$直接取输出层经过Softmax的概率值即可\n\n- 特别的：\n\n1. 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1\n2. 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷\n3. 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数\n\n**显然，任何一个有效模型的困惑度必须小于类别个数（要不然模型的预测效果还不如随机点一个类别来的好）**\n\n\n\n#### 2.3.4 通过时间反向传播\n\n- 我们需要将循环神经网络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应⽤反向传播计算并存储梯度\n- 简单起⻅，我们考虑⼀个⽆偏差项的循环神经网络，且激活函数为恒等映射$（\\phi(x) = x）$。设时间步t的输⼊为单样本$x_t \\in R^d$，标签为$y_t$，那么隐藏状态$h_t \\in R^h$的计算表达式为:\n\n$$\nh_t = W_{hx}X_t + W_{hh}h_{t-1}\n$$\n\n输出层变量$o_t \\in \\mathbb{R}^q$为：\n$$\no_t = W_{qh}h_t\n$$\n设时间步t的损失为$\\ell(o_t, y_t)$。则总时间步数为T的损失函数L定义为：\n$$\nL = \\frac{1}{T}\\sum_{t = 1}^T \\ell(o_t, y_t)\n$$\n\n- 我们假设一共有3个时间步数，那么可以做出计算图：\n  ![image-20211210132156070](https://s2.loli.net/2021/12/10/Feq2PM3nCslpGO7.png)\n\n- 现在我们开始反向传播：\n\n易得：\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}=\\frac{\\partial \\ell\\left(\\boldsymbol{o}_{t}, y_{t}\\right)}{T \\cdot \\partial \\boldsymbol{o}_{t}}\n$$\n然后可以得到：\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{q h}}=\\sum_{t=1}^{T} \\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}, \\frac{\\partial \\boldsymbol{o}_{t}}{\\partial \\boldsymbol{W}_{q h}}\\right)=\\sum_{t=1}^{T} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}} \\boldsymbol{h}_{t}^{\\top} .\n$$\n接下来就是计算关于$$W_{hx}$$和$$W_{hh}$$的梯度，那么必然要求关于各时间步隐藏状态的梯度，我们注意到隐藏状态之间也存在依赖关系，，L只通过$$o_T$$依赖最终时间步T的隐藏状态$$h_T$$。因此，我们先计算⽬标函数有关最终时间步隐藏状态的梯度：\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{T}}=\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{T}}, \\frac{\\partial \\boldsymbol{o}_{T}}{\\partial \\boldsymbol{h}_{T}}\\right)=\\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T}}\n$$\n接下来对于时间步t < T，在图6.3中，$L$通过$h_{t+1}$和$o_t$依赖$h_t$。依据链式法则，⽬标函数有关时间步t < T的隐藏状态的梯度$\\partial L / \\partial h_t \\in R^h$需要按照时间步从⼤到小依次计算:\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t}}=\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}, \\frac{\\partial \\boldsymbol{h}_{t+1}}{\\partial \\boldsymbol{h}_{t}}\\right)+\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}, \\frac{\\partial \\boldsymbol{o}_{t}}{\\partial \\boldsymbol{h}_{t}}\\right)=\\boldsymbol{W}_{h h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}+\\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}} .\n$$\n对次递归公式展开，我们可以得到对任意时间步1 ≤ t ≤ T，我们可以得到⽬标函数有关隐藏状态梯度的通项公式：\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t}}=\\sum_{i=t}^{T}\\left(\\boldsymbol{W}_{h h}^{\\top}\\right)^{T-i} \\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T+t-i}}\n$$\n\n通过关于每个时间步隐藏状态的梯度，易求得关于$$W_{hx}$$和$$W_{hh}$$的梯度。\n\n- **由上式中的指数项可⻅，当时间步数T较⼤或者时间步t较小时，⽬标函数有关隐藏状态的梯度较容易出现衰减和爆炸**\n- RNN和一般的网络一样，我们在正向传播和反向传播的时候会进行一些数据的储存，避免重复计算\n\n\n\n#### 2.3.5 激活函数的选择\n\n- 对于下一个时间步隐藏状态的计算，激活函数一般选用Tanh或者ReLu。对输出的运算，一般做softmax运算。\n\n- 但是很多时候还是选用Tanh而不是ReLu，这一点在Hinton的[IRNN论文](https://arxiv.org/abs/1504.00941)里面是很明确的提到的：\n\n![img](https://pic2.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_720w.jpg?source=1940ef5c)\n\n**也就是说在RNN中直接把激活函数换成ReLu会导致非常大的输出值，因此它们可能更容易发生爆炸**\n\n- 下面具体来看一下：\n\n$$\n\\begin{array}{c}\n\\text { net }_{t}=U x_{t}+W h_{t-1} \\\\\nh_{t}=f\\left(\\text { net }_{t}\\right)\n\\end{array}\n$$\n\n**假设ReLu函数一直处于激活区域（即输入大于0）**，则有$$f(x) = x, net_t = Ux_t + W(Ux_{t-1} + Wh_{t-2})$$，继续将其展开，最终会包含多个W的连乘，如果W不是单位矩阵， 那么最终的结果将会区域0或无穷。\n\n但是在CNN中就不会发生这种问题，因为CNN中每一层的W是不同的，并且在初始化时它们是独立同分布的，因此可以相互抵消，在多层之后一般不会出现这种严重的数值问题\n\n我们再来看看反向传播的时候：\n$$\n\\frac{\\partial h_t}{\\partial h_{t-1}} = W\n$$\n所以：\n$$\n\\frac{\\partial h_t}{\\partial h_{1}} = W^n\n$$\n\n\n可以看到只要W不是单位矩阵，梯度会出现消失或者爆炸的现象\n\n而在使用tanh作为激活函数的时候：\n$$\n\\frac{\\partial h_{t}}{\\partial h_{t-1}}=\\left(1-h_{t}^{2}\\right) W\n$$\n可以证明，固定$$h_{t-1}$$时，$$\\left(1-h_{t}^{2}\\right) W$$是有界的（这个界可能不是1，但总归是有界的）\n\n- 综上所述：当采用ReLu作为激活函数时，只有W在单位矩阵附近时才能取得比较好的结果。**而使用tanh函数，每个相邻时间步之间的梯度是有界的，减少了一些梯度爆炸的可能性（注意是梯度爆炸，而没有解决梯度消失）。**但是ReLu也拥有自己的优点，那就是计算量更小，收敛更快，其实relu也可以配合梯度裁剪来使用，但是终归没有tanh好\n\n\n\n### 2.4 梯度裁剪\n\n- 循环神经网络中较容易出现梯度衰减或梯度爆炸。**为了应对梯度爆炸**，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成⼀个向量 $g$，并设裁剪的阈值是$\\theta$。裁剪后的梯度：\n\n$$\n\\min \\left(\\frac{\\theta}{\\|g\\|}, 1\\right) g\n$$\n\n的$L_2$范数不超过$\\theta$\n\n- **但是梯度裁剪无法应对梯度衰减**\n\n\n\n\n\n# 3 门控循环单元（GRU）\n\n- 我们发现，当时间步数较⼤或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但⽆法解决梯度衰减的问题。通常由于这个原因，**循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系**\n- 门控循环神经网络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较⼤的依赖关系。它通过可以学习的⻔来控制信息的流动。其中，门控循环单元（gated recurrent unit，GRU）是⼀种常⽤的⻔控循环神经网\n\n\n\n### 3.1 重置门和更新门\n\n- ⻔控循环单元中的重置⻔和更新⻔的输⼊均为当前时间步输⼊$$X_t$$与上⼀时间步隐藏状态$$H_{t−1}$$，输出由激活函数为sigmoid函数的全连接层计算得到。\n\n![image-20211210135706726](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20211210135706726.png)\n\n具体来说，重置门$R_t \\in \\mathbb{R}^{n \\times h}$和更新门$Z_t \\in \\mathbb{R}^{n \\times h}$的计算如下：\n$$\n\\begin{array}{l}\n\\boldsymbol{R}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x r}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h r}+\\boldsymbol{b}_{r}\\right) \\\\\n\\boldsymbol{Z}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x z}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h z}+\\boldsymbol{b}_{z}\\right)\n\\end{array}\n$$\n\n\n### 3.2 候选隐藏状态\n\n- 我们将当前时间步重置⻔的输出与上⼀时间步隐藏状态做按元素乘法（符号为$\\odot$）。如果重置⻔中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上⼀时间步的隐藏状态。如果元素值接近1，那么表⽰保留上⼀时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输⼊连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态\n\n- 具体来说，时间步t的候选隐藏状态$\\tilde{H}_t \\in \\mathbb{R}^{n \\times h}$的计算为：\n\n$$\n\\tilde{\\boldsymbol{H}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\left(\\boldsymbol{R}_{t} \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right),\n$$\n\n- **重置门控制了上⼀时间步的隐藏状态如何流⼊当前时间步的候选隐藏状态。而上⼀时间步的隐藏状态可能包含了时间序列截⾄上⼀时间步的全部历史信息。因此，重置门可以⽤来丢弃与预测⽆关的历史信息**\n\n- 最后，时间步t的隐藏状态$$H_t \\in \\mathbb{R}^{n×h}$$的计算使⽤当前时间步的更新门$$Z_t$$来对上⼀时间步的隐藏状态$$H_{t−1}$$和当前时间步的候选隐藏状态$$\\tilde{H}_t$$做组合：\n\n$$\n\\boldsymbol{H}_{t}=\\boldsymbol{Z}_{t} \\odot \\boldsymbol{H}_{t-1}+\\left(1-\\boldsymbol{Z}_{t}\\right) \\odot \\tilde{\\boldsymbol{H}}_{t}\n$$\n\n\n\n![image-20211210141824572](https://s2.loli.net/2021/12/10/L7O69pm3Yjzbno5.png)\n\n- **更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新**。假设更新⻔在时间步$t'$到$t（t' < t）$之间⼀直近似1。那么，**在时间步$t'$到$t$之间的输⼊信息⼏乎没有流⼊时间步$t$的隐藏状态$H_t$**。实际上，**这可以看作是较早时刻的隐藏状态$H_{t'−1}$⼀直通过时间保存并传递⾄当前时间步$t$**。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较⼤的依赖关系\n- 我们稍作总结：\n\n1. 重置⻔有助于捕捉时间序列⾥短期的依赖关系\n2. 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系\n3. 重置门和更新门为0、1的情况：\n\n| 对应情况                                 | 重置门 | 更新门 |\n| ---------------------------------------- | ------ | ------ |\n| 退化为一般的RNN                          | 1      | 0      |\n| 丢弃当前时间步的全部信息，只保留历史信息 | 0或1   | 1      |\n| 完全丢弃历史信息，只保留当前时间步的信息 | 0      | 0      |\n\n\n\n\n\n# 4 长短期记忆（LSTM）\n\n- 还有另外一种十分常用的门控循环神经网络：⻓短期记忆（long short-term memory，LSTM）\n- **LSTM之所以叫做长短期记忆，是因为里面的两个记忆变量：隐藏状态$$H_t$$负责短期记忆，记忆细胞$$C_t$$负责长期记忆（因为每个时间步中$$H_t$$相比于$$C_t$$更新的更多，所以负责短期）**\n\n\n\n### 4.1 输⼊门、遗忘门、输出门和候选记忆细胞\n\n- 与门控循环单元中的重置门和更新门⼀样，⻓短期记忆的门的输⼊均为当前时间步输⼊$$X_t$$与上⼀时间步隐藏状态$$H_{t−1}$$，输出由激活函数为sigmoid函数的全连接层计算得到\n- ⻓短期记忆需要计算候选记忆细胞$\\tilde{C}_t$。它的计算与上⾯介绍的3个⻔类似，但使⽤了值域在[−1, 1]的tanh函数作为激活函数:\n\n![image-20211210144009641](https://s2.loli.net/2021/12/10/jzn954wmEqRfyIg.png)\n\n- 时间步t的输⼊⻔$I_t \\in R^{n×h}$、遗忘⻔$F_t \\in R^{n×h}$和输出⻔$O_t \\in R^{n×h}$分别计算如下：\n\n$$\n\\begin{aligned}\n\\boldsymbol{I}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x i}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h i}+\\boldsymbol{b}_{i}\\right), \\\\\n\\boldsymbol{F}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x f}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h f}+\\boldsymbol{b}_{f}\\right), \\\\\n\\boldsymbol{O}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x o}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h o}+\\boldsymbol{b}_{o}\\right), \\\\\n\\tilde{\\boldsymbol{C}}_{t}&=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x c}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h c}+\\boldsymbol{b}_{c}\\right),\n\\end{aligned}\n$$\n\n\n\n### 4.2 记忆细胞\n\n- 当前时间步记忆细胞$C_t \\in R^{n\\times h}$的计算组合了上⼀时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘⻔和输⼊⻔来控制信息的流动：\n\n$$\n\\boldsymbol{C}_{t}=\\boldsymbol{F}_{t} \\odot \\boldsymbol{C}_{t-1}+\\boldsymbol{I}_{t} \\odot \\tilde{\\boldsymbol{C}}_{t}\n$$\n\n- 遗忘门控制上⼀时间步的记忆细胞$C_{t−1}$中的信息是否传递到当前时间步，而输⼊门则控制当前时间步的输⼊$X_t$通过候选记忆细胞$\\tilde{C}_t$如何流⼊当前时间步的记忆细胞。如果遗忘门⼀直近似1且输⼊门⼀直近似0，过去的记忆细胞将⼀直通过时间保存并传递⾄当前时间步\n\n\n\n### 4.3 隐藏状态\n\n- 有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$H_t \\in R^{n×h}$的信息的流动：\n\n$$\nH_t = O_t \\odot tanh(C_t)\n$$\n\n- 当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使⽤；当输出门近似0时，记忆细胞信息只⾃⼰保留\n\n![image-20211210145754660](https://s2.loli.net/2021/12/10/mlYhrVbA47MByvG.png)\n\n\n\n### 4.4 三种门的作用\n\n- **输入门控制当前计算的新状态以多大程度更新到记忆单元中**\n- **遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉**\n- **输出门控制当前的输出有多大程度取决于当前的记忆单元**\n- 在一个训练好的网络中：\n\n1. 当输入的序列中没有重要信息时，LSTM的遗忘门接近于1，输入门接近于0，此时过去的记忆会被保存，从而实现了长期记忆功能\n2. 当输入的序列中出现了重要的信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近1\n3. 当输入的序列中出现了重要的信息，且该信息意味着之前的记忆不再重要时，输入门的值接近1，而遗忘门的值接近于0，这样旧的记忆被遗忘，新的重要信息被记忆\n\n\n\n### 4.5 LSTM各模块激活函数\n\n- LSTM中，三种门都用的Sigmoid函数，生成候选记忆时，使用Tanh作为激活函数。值得注意的是，**这两个激活函数都是饱和的**，也就是说在输入达到一定值的情况下，输出就不发生明显变化了。如果使用的是非饱和的激活函数，例如ReLu，将难以实现门控的效果\n- Sigmoid的输出在0到1之间，符合门控的物理定义。且当输入较大或较小时，输出会非常接近1或0，从而保证门开或门关\n- 在生成候选记忆时，使用Tanh函数，是因为其输出在-1到1之间，这与大多数场景下特征分布是zero-centered吻合\n- 此为Tanh函数在输入为0附近相比于Sigmoid有更大的梯度，通常收敛更快\n\n\n\n# 5 门控机制是如何解决RNN的问题的\n\n- 上面我们说到，RNN最大的一个问题就是无法捕捉长距离依赖，而引入门控机制就是为了解决这个问题，这里我们以LSTM举例：\n\n$$\n\\begin{aligned}\nf_{t} &=\\sigma\\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\\right) \\\\\ni_{t} &=\\sigma\\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\\right) \\\\\no_{t} &=\\sigma\\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\\right) \\\\\n\\hat{c}_{t} &=\\tanh \\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\\right) \\\\\nc_{t} &=f_{t} \\circ c_{t-1}+i_{t} \\circ \\hat{c}_{t} \\\\\nh_{t} &=o_{t} \\circ \\tanh \\left(c_{t}\\right)\n\\end{aligned}\n$$\n\n- 我们可以像上面2.3.5激活函数的选择中一样，探讨$$\\frac{\\partial h_t}{\\partial h_{t-1}}$$，但是因为$$h_{t} =o_{t} \\circ \\tanh(c_t)$$，所以同样可以分析$$\\frac{c_t}{c_{t-1}}$$，而后者更简单些，所以分析后者：\n\n$$\n\\frac{\\partial c_{t}}{\\partial c_{t-1}}=f_{t}+c_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}}+\\hat{c} t \\frac{\\partial i_{t}}{\\partial c_{t-1}}+i_{t} \\frac{\\partial \\hat{c}_{t}}{\\partial c_{t-1}}\n$$\n\n- 上式总共有4项，**梯度主要取决于第一项$$f_t$$**，由于$$f_t \\in (0,1)$$，所以梯度爆炸的风险减小，而是否会发生梯度小时，取决于每个时间步的$$f_t$$是否接近1，但是这里有个自洽的结论：**如果我们的任务比较依赖于历史信息，那么$$f_t$$就会接近于1，这时候历史的梯度信息也正好不容易消失；如果$$f_t$$很接近于0，那么就说明我们的任务不依赖于历史信息，这时候就算梯度消失也无妨了。**\n\n- 而后面3项都不是决定梯度的主要因素，挑$$c_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}}$$来说明，带入$$h_{t-1} =o_{t-1} \\circ \\tanh(c_{t-1})$$，可以得到：\n\n$$\nc_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}} = c_{t-1} \\frac{\\partial f_{t}}{\\partial h_{t-1}}  \\frac{\\partial h_{t-1}}{\\partial c_{t-1}} = f_{t}\\left(1-f_{t}\\right) o_{t-1}\\left(1-\\tanh ^{2} c_{t-1}\\right) c_{t-1} U_{f}\n$$\n\n- 由于$$f_t,(1-f_t),o_{t-1} \\in (0,1)$$，并且可以证明$$|\\left(1-\\tanh ^{2} c_{t-1}\\right)c_{t-1}| <0.45$$，所以结果相当于$$U_f$$乘上四个门，结果会非常的小，所以不起主导作用。其他两项是同样的方法\n\n- **注意：LSTM和GRU只是很大的缓解了梯度消失，但是如果时间步很长，成千上万步，仍然会发生梯度消失，毕竟记忆单元压根存不了那么多信息**\n\n- 另外，一般LSTM要比GRU要好一些，但是到底好在哪里呢，在GRU中同样进行反向传播的分析：\n\n$$\n\\begin{aligned}\n\\frac{\\partial h_{t}}{\\partial h_{t-1}}=& 1-z_{t}-z_{t}\\left(1-z_{t}\\right) h_{t-1} U_{z}+z_{t}\\left(1-z_{t}\\right) \\hat{h}_{t} U_{z} \\\\\n&+\\left(1-\\hat{h}_{t}^{2}\\right) r_{t}\\left(1+\\left(1-r_{t}\\right) h_{t-1} U_{r}\\right) z_{t} U_{h}\n\\end{aligned}\n$$\n\n- 上式其主导作用的同样是$$1-z_t$$，但是后面的项相比于LSTM只有3个门的连乘（LSTM有4个），所以这些想对梯度的影响可能大一些，所以**LSTM的梯度要比GRU稳定一些，所以GRU更依赖于初始化**\n\n\n\n# 6 深度循环神经网络\n\n- 到⽬前为⽌介绍的循环神经网络只有⼀个单向的隐藏层，在深度学习应⽤⾥，我们通常会⽤到含有多个隐藏层的循环神经网络，也称作深度循环神经网络\n- 下图演⽰了⼀个有L个隐藏层的深度循环神经网络，**每个隐藏状态不断传递⾄当前层的下⼀时间步和当前时间步的下⼀层**\n\n![image-20211210150125737](https://s2.loli.net/2021/12/10/PUK7CRjyemEbzLM.png)\n\n- 第1隐藏层的隐藏状态和之前的计算⼀样：\n\n$$\n\\boldsymbol{H}_{t}^{(1)}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(1)}+\\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{h h}^{(1)}+\\boldsymbol{b}_{h}^{(1)}\\right)\n$$\n\n- 当1 < l ≤ L时，第l隐藏层的隐藏状态的表达式为:\n\n$$\n\\boldsymbol{H}_{t}^{(l)}=\\phi\\left(\\boldsymbol{H}_{t}^{(l-1)} \\boldsymbol{W}_{x h}^{(l)}+\\boldsymbol{H}_{t-1}^{(l)} \\boldsymbol{W}_{h h}^{(l)}+\\boldsymbol{b}_{h}^{(l)}\\right)\n$$\n\n- 最终，输出层的输出只需基于第L隐藏层的隐藏状态：\n\n$$\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t}^{(L)} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}\n$$\n\n- 同多层感知机⼀样，隐藏层个数L是超参数\n\n- **RNN中选择深度网络的原因和一般前馈神经网络一样，都是为了能够用来表征更复杂的情况**\n\n\n\n\n\n# 7 双向循环神经网络\n\n- 之前介绍的循环神经网络模型都是假设当前时间步是由前⾯的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后⾯时间步决定。例如，当我们写下⼀个句⼦时，可能会根据句⼦后⾯的词来修改句⼦前⾯的⽤词\n- **双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息**，下图演⽰了⼀个含单隐藏层的双向循环神经网络的架构\n\n![image-20211210151934211](https://s2.loli.net/2021/12/10/VtFmigZ58qw2aeb.png)\n\n- 具体来看，设时间步t正向隐藏状态为$\\overrightarrow{H}_t \\in R^{n×h}$（正向隐藏单元个数为h），反向隐藏状态为$\\overleftarrow{H}_t \\in R^{n×h}$（反向隐藏单元个数为h）。我们可以分别计算正向隐藏状态和反向隐藏状态：\n\n$$\n\\begin{array}{l}\n\\overrightarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(f)}+\\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{h h}^{(f)}+\\boldsymbol{b}_{h}^{(f)}\\right) \\\\\n\\overleftarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(b)}+\\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{h h}^{(b)}+\\boldsymbol{b}_{h}^{(b)}\\right)\n\\end{array}\n$$\n\n**然后我们连结两个⽅向的隐藏状态$\\overrightarrow{H}_t$和 $\\overleftarrow{H}_t$来得到隐藏状态$H_t \\in R^{n×2h}$**，并将其输⼊到输出层。 输出层计算输出$O_t \\in R^{n×q}$（输出个数为q）：\n$$\nO_t = H_tW_{hq} + b_q\n$$\n\n- **不同方向的隐藏状态的隐藏单元个数也可以不同**\n-  双向循环神经网络在每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊）\n\n\n\n\n\n# 8 Seq2Seq模型\n\n- 在许多时候，输入和输出都是不定长序列，这采用一般的RNN肯定是行不通的。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，例如：英语输⼊：“They”、“are”、“watching”、“.” ； 法语输出：“Ils”、“regardent”、“.”\n- 当输⼊和输出都是不定⻓序列时，我们可以使⽤seq2seq模型，本质上都⽤到了两个循环神经网络，分别叫做编码器和解码器。**编码器用来分析输⼊序列，解码器⽤来生成输出序列**\n\n![image-20211210170200879](https://s2.loli.net/2021/12/10/FQfVxdq2wuC7aSs.png)\n\n- < bos >（beginning of sequence）和 < eos >（end of sequence）分别表示序列的开始和结束\n- 编码器每个时间步的输⼊依次为英语句⼦中的单词、标点和特殊符号。上图中使⽤了编码器在最终时间步的隐藏状态作为输⼊句⼦的表征或编码信息。解码器在各个时间步中使⽤输⼊句⼦的编码信息和上个时间步的输出以及隐藏状态作为输⼊\n\n\n\n### 8.1 编码器\n\n- **编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量$c$，并在该背景变量中编码输⼊序列信息**。编码器可以使⽤循环神经网络\n- 编码器通过⾃定义函数q将各个时间步的隐藏状态变换为背景变量\n\n$$\nc = q(h_1, ...,h_T)\n$$\n\n- **也可以使⽤双向循环神经网络构造编码器**，在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊），并编码了整个序列的信息。\n\n\n\n### 8.2 解码器\n\n- 解码器就是通过编码器输出的背景向量$c$，和每一个时间步的输入，输出所需要的结果\n- **解码器在预测和训练时是不一样的**，我们先介绍**预测**时的解码器：\n\n编码器输出的背景变量$c$编码了整个输⼊序列$$x_1, . . . , x_T$$的信息。给定训练样本中的输出序列$$y_1, y_2, . . . , y_{T'}$$，对每个时间步$$t'$$（符号与输⼊序列或编码器的时间步$$t$$有区别），解码器输出$$y_{t'}$$的条件概率将基于之前的输出序列$$y_1, . . . , y_{t'−1}$$和背景变量$$c$$，即$$P(y_{t'} | y_1, . . . , y_{t'−1}, c)$$\n\n为此，我们可以使⽤另⼀个循环神经网络作为解码器。在输出序列的时间步$$t'$$，解码器将上⼀时间步的输出$$y_{t'−1}$$以及背景变量$$c$$作为输⼊，并将它们与上⼀时间步的隐藏状态$$s_{t'−1}$$变换为当前时间步的隐藏状态$$s_{t'}$$：\n$$\ns_{t'} = g(y_{t' -1}, c, s_{t' - 1})\n$$\n有了解码器的隐藏状态后，我们可以使⽤⾃定义的输出层和softmax运算来计算$$P(y_{t'} | y_1, . . . , y_{t'−1}, c)$$\n\n- **训练**时的解码器，每一时间步的输入序列可以是上一时间步的输出，也可以是上一时间步的真实标签序列。后者叫做**强制教学**\n\n根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率：\n$$\n\\begin{aligned}\nP\\left(y_{1}, \\ldots, y_{T^{\\prime}} \\mid x_{1}, \\ldots, x_{T}\\right) &=\\prod_{t^{\\prime}=1}^{T^{\\prime}} P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, x_{1}, \\ldots, x_{T}\\right) \\\\\n&=\\prod_{t^{\\prime}=1}^{T^{\\prime}} P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)\n\\end{aligned}\n$$\n并得到该输出序列的损失：\n$$\n-\\log P\\left(y_{1}, \\ldots, y_{T^{\\prime}} \\mid x_{1}, \\ldots, x_{T}\\right)=-\\sum_{t^{\\prime}=1}^{T^{\\prime}} \\log P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)\n$$\n在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数\n\n\n\n### 8.3 预测时的搜索方式\n\n#### 8.3.1 贪婪搜索\n\n- 贪婪搜索（greedy search）。对于输出序列任⼀时间步$t'$，我 们从$|Y|$（Y为词典）个词中搜索出条件概率最⼤的词：\n\n$$\ny_{t'} = argmax_{y \\in Y}P(y|y_1, ..., y_{t' - 1}, c)\n$$\n\n作为输出。⼀旦搜索出“< eos >”符号，或者输出序列⻓度已经达到了最⼤⻓度$T'$，便完成输出。\n\n- 我们将该条件概率最⼤的输出序列称为**最优输出序列**，**而贪婪搜索无法保证得到最优输出序列**，下面举个栗子：\n\n> 假设输出词典⾥⾯有“A” “B” “C” 和“< eos >”这4个词。下图中每个时间步下的4个数字分别代表了该时间步⽣成“A” “B” “C” 和“< eos >”这4个词的条件概率。在每个 间步，贪婪搜索选取条件概率最⼤的词。因此，将⽣成输出序列“A” “B” “C” “< eos >”。 该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 = 0.048\n\n![image-20211210180148791](https://s2.loli.net/2021/12/10/JUgILXFCjWviaQw.png)\n\n> 但是现在我们考虑下面一种情况，在时间步2中选取了条件概率第⼆⼤的词“C”，**由于之后时间步的概率值是基于前面的时间步的，所以时间步2结果选取的改变，会导致后续的概率发生改变**，如下图，所以我们现在的输出序列“A” “C” “B” “< eos >”的条件概率是0.5 × 0.3 × 0.6 × 0.6 = 0.054，大于贪婪搜索的概率，所以贪婪搜索得到的不是最优的\n\n![image-20211210180404095](https://s2.loli.net/2021/12/10/zSKDLamFWqseyuN.png)\n\n#### 8.3.2 穷举搜索\n\n- 我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最⼤的序列\n- **虽然穷举搜索可以得到最优输出序列，但它的计算开销$O(|Y|^{T'})$很容易过⼤**，而贪婪搜索的开销为$O(|Y|T')$，明显小于穷举搜索\n\n\n\n#### 8.3.3 束搜索\n\n- 束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个**束宽**（beam size）超参数。我们将它设为k。在时间步1时，选取当前时间步条件概率最⼤的k个词，分别组成k个候选输出序列的⾸词。在之后的每个时间步，基于上个时间步的k个候选输出序列，从k$ |Y|$个可能的输出序列中选取条件概率最⼤的k个，作为该时间步的候选输出序列。最终，**我们从各个时间步的候选输出序列中筛选出包含特殊符号“< eos >”的序列，并将它们中所有特殊符号“< eos >”后⾯的⼦序列舍弃**，得到最终候选输出序列的集合\n\n- 下面举个栗子：\n\n![image-20211210182131362](https://s2.loli.net/2021/12/10/QEBb3gUVxmuDS7O.png)\n\n第一个时间步找出概率最大的\"A\"和\"C\"，然后再根据\"A\"和\"C\"，在时间步2中寻找概率最大的2个，分别为\"AB\"和\"CE\"，再根据这两个，在时间步3，输出\"ABD\"和\"CED\"，最后减去< eos >符号以后的内容，得出输出序列\n\n- 在最终候选输出序列的集合中，我们取以下分数最⾼的序列作为输出序列：\n\n$$\n\\frac{1}{L^{\\alpha}} \\log P\\left(y_{1}, \\ldots, y_{L}\\right)=\\frac{1}{L^{\\alpha}} \\sum_{t^{\\prime}=1}^{L} \\log P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)\n$$\n\n其中$L$为最终候选序列⻓度，$\\alpha$⼀般可选为0.75。分⺟上的$L^{\\alpha}$是为了惩罚较⻓序列在以上分数中较多的对数相加项\n\n- 束搜索的计算开销为$O(k|Y|T')$，介于贪婪搜索和穷举搜索之间\n- **束搜索通过灵活的束宽来权衡计算开销和搜索质量**\n\n\n\n### 8.4 注意力机制\n\n#### 8.4.1 seq2seq中的注意力机制\n\n- **解码器在⽣成输出序列中的每⼀个词时可能只需利⽤输⼊序列某⼀部分的信息， 而不需要由整个输入序列生成的背景向量**，例如，在输出序列的时间步1，解码器可以主要依赖“They” “are” 的信息来⽣成“Ils”，在时间步2则主要使⽤来⾃“watching”的编码信息⽣成“regardent”，而不需要\"They are watching\"整个句子\n- 若没引入注意力机制，在实际使用中，那么会发现随着输入序列的增长，模型的性能发生了显著下降。**这是因为编码时输入序列的全部信息压缩到了一个向量中，随着序列的增长，越前面的词丢失越严重**。**同时，seq2seq模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词对应的源语言词的上下文信息和位置信息在编解码过程中丢失了**\n\n- 注意⼒机制通过对编码器所有时间步的隐藏状态做**加权平均**来得到背景变量。解码器在每⼀时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量\n\n![image-20211210184622950](https://s2.loli.net/2021/12/10/5kGV8uqeaYn3wlF.png)\n\n⾸先，函数a根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输⼊。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量\n\n- 具体来说，令编码器在时间步t的隐藏状态为$h_t$，且总时间步数为T。那么解码器在时间步$t'$（$t'$代表的是解码器的时间步数）的背景变量为所有编码器隐藏状态的加权平均：\n\n$$\n\\boldsymbol{c}_{t^{\\prime}}=\\sum_{t=1}^{T} \\alpha_{t^{\\prime} t} \\boldsymbol{h}_{t}\n$$\n\n其中给定$t'$时，权重$\\alpha_{t't}$在$t = 1, . . . , T$的值是⼀个概率分布。为了得到概率分布，我们可以使⽤softmax运算:\n$$\n\\alpha_{t^{\\prime} t}=\\frac{\\exp \\left(e_{t^{\\prime} t}\\right)}{\\sum_{k=1}^{T} \\exp \\left(e_{t^{\\prime} k}\\right)}, \\quad t=1, \\ldots, T\n$$\n由于$$e_{t't}$$同时取决于解码器的时间步$$t'$$和编码器的时间步t，我们不妨以解码器在时间步$$t'− 1$$的隐藏状态$$s_{t'−1}$$与编码器在时间步t的隐藏状态$$h_t$$为输⼊，并通过函数a计算$$e_{t't}$$：\n$$\ne_{t't} = a(s_{t'-1}, h_t)\n$$\n其中a是一个自定义函数，有多种选择。如果两个输⼊向量⻓度相同，⼀个简单的选择是计算它们的内积$a(s,h) = s^⊤h$\n\n- 我们对注意力机制可以有一个直观的理解：**在生成一个输出词时，会考虑每一个输入词和当前输出词的对应关系，对齐越好的词，会有越大的权重**\n- 还可以结合双向RNN和注意力机制，每个隐状态包含了$\\overleftarrow{h_t}$和$\\overrightarrow{h_t}$\n\n\n\n#### 8.4.2 矢量化计算\n\n- 我们还可以对注意⼒机制采⽤更⾼效的⽮量化计算。⼴义上，注意⼒机制的输⼊包括**查询项**以及**⼀⼀对应的键项和值项**，其中值项是需要加权平均的⼀组项。在加权平均中，**值项的权重来⾃查询项以及与该值项对应的键项的计算**\n\n- 在上面的例⼦中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。假设编码器和解码器的隐藏单元个数均为h，且函数$$a(s, h) = s^ ⊤h$$。若我们希望根据解码器单个隐藏状态$$s_{t'−1} \\in \\mathbb{R}^h$$和编码器所有隐藏状态$$h_t \\in \\mathbb{R}^h , t = 1, . . . , T$$来计算背景向量$$c_{t'} \\in \\mathbb{R}^h$$。我们可以将查询项矩阵$$Q \\in R^{1\\times h}$$设为$$s^⊤_{t'−1}$$，并令键项矩阵$$K \\in \\mathbb{R}^{T\\times h}$$和值项矩阵$$V \\in \\mathbb{R}^{T\\times h}$$相同且第t⾏均为$$h^⊤_t$$ 。此时，我们只需要通过⽮量化计算：\n\n$$\nsoftmax(QK^T)V\n$$\n\n即可算出转置后的背景向量$$c^⊤_{t'}$$。当查询项矩阵Q的⾏数为n时，上式将得到n行的输出矩阵。输出矩阵与查询项矩阵在相同行上⼀⼀对应。\n","source":"_posts/RNN.md","raw":"---\ntitle: RNN基本概念\nmath: true\ndate: 2021-12-6\n---\n\n\n\n- 与多层感知机和能有效**处理空间信息**的卷积神经网络不同，循环神经网络是为更好地**处理时序信息**而设计的。它**引⼊状态变量来存储过去的信息**，并⽤其**与当前的输⼊共同决定当前的输出**\n\n\n\n\n\n# 1 语言模型\n\n### 1.1 语言模型的计算\n\n- 我们可以把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。假设⼀段⻓度为 $T$ 的⽂本中的词 依次为 $w_1, w_2, . . . , w_T$，那么在离散的时间序列中，$w_t（1 ≤ t ≤ T）$可看作在时间步（time step）$ t$ 的输出或标签。给定⼀个⻓度为 $T$ 的词的序列 $w_1, w_2, . . . , w_T$，语⾔模型将计算该序列的概率：\n\n$$\nP(w_1, w_2, ...,w_T)\n$$\n\n- **由于 $w_1, w_2, . . . , w_T$ 是依次生成的，所以有：**\n\n$$\nP(w_1, w_2, ...,w_T) = \\prod_{t=1}^T P(w_t | w_1, ..., w_{t-1})\n$$\n\n- 为了计算语⾔模型，我们需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即**语⾔模型参数**。**词的概率可以通过该词在训练数据集中的相对词频来计算**\n\n\n\n### 1.2 n元语法\n\n- **当序列⻓度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加**\n- n元语法通过⻢尔可夫假设（**虽然并不⼀定成立**）简化了语⾔模型的计算。这⾥的**⻢尔可夫假设是指⼀个词的出现只与前⾯n个词相关**，即n阶⻢尔可夫链（Markov chain of order n）\n- 如果基于$n-1$阶马尔科夫链，我们可以将语言模型改写为：\n\n$$\nP(w_1, w_2, ...,w_T) \\approx \\prod_{t=1}^T P(w_t | w_{t - (n - 1)}, ..., w_{t-1})\n$$\n\n- 以上也叫**n元语法**（n-grams）。它是基于n−1阶⻢尔可夫链的概率语⾔模型\n\n- 当n较小时，n元语法往往并不准确。当n较⼤时，n元语法需要计算并存储⼤量的词频和多词相邻频率。**n权衡了计算复杂度和模型准确性**\n\n\n\n\n\n# 2 循环神经网络（RNN）\n\n- 在n元语法中，时间步t的词 $w_t$ 基于前⾯所有词的条件概率只考虑了最近时间步的n − 1个词。如果要考虑⽐t − (n − 1)更早时间步的词对$w_t$的可能影响，我们需要增⼤n。但这样模型参数的数量将随之呈指数级增⻓\n- 但是对于循环神经网络，它**并⾮刚性地记忆所有固定⻓度的序列，而是通过隐藏状态来存储之前时间步的信息**\n\n\n\n### 2.1 循环神经网络\n\n- 假设$$X_t \\in \\mathbb{R}^{n \\times d}$$是序列中时间步t的小批量输⼊， $$H_t \\in \\mathbb{R}^{n \\times h}$$是该时间步的隐藏变量。与多层感知机不同的是，这⾥我们保存上⼀时间步的隐藏变量$$H_{t−1}$$，并引⼊⼀个新的权重参数$$W_{hh} \\in \\mathbb{R}^{h×h}$$，该参数⽤来描述在当前时间步如何使⽤上⼀时间步的隐藏变量。时间步t的隐藏变量的计算由当前时间步的输⼊和上⼀时间步的隐藏变量共同决定：\n\n$$\n\\boldsymbol{H}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right)\n$$\n\n$\\phi$是激活函数\n\n- **这⾥的隐藏变量能够捕捉截⾄当前时间步的序列的历史信息**\n- 每个时间步还有一个对应的输出：\n\n$$\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}\n$$\n\n在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差\n\n- **即便在不同时间步，循环神经网络也始终使⽤W， b这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增⻓**\n\n![image-20211210101227481](https://s2.loli.net/2021/12/10/oAbwO43tDSTWpVg.png)\n\n- 隐藏状态中$$X_{t}W_{x h}+H_{t-1} W_{h h}$$的计算等价于$$X_t$$与$$H_{t−1}$$连结后的矩阵乘以$$W_{xh}$$与$$W_{hh}$$连结后的矩阵，实际上在代码实现中基本都是这么做的\n\n\n\n- 举一个栗子：基于字符级循环神经网络的语⾔模型\n\n![image-20211210101914126](https://s2.loli.net/2021/12/10/OYZwANsxl2oEQaI.png)\n\n**标签序列依次为输入序列的下一个**，并且在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差\n\n\n\n### 2.2 时序数据的采样\n\n- 时序数据的⼀个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想” “要” “有” “直” “升”。**该样本的标签序列为这些字符分别在训练集中的下⼀个字符**，即 “要” “有” “直” “升” “机”。\n- 我们有两种⽅式对时序数据进⾏采样，分别是随机采样和相邻采样\n\n\n\n#### 2.2.1 随机采样\n\n- 在随机采样中，每个样本是原始序列上任意截取的⼀段序列。\n\n- 相邻的两个随机小批量在原始序列上的位置不⼀定相毗邻。因此，我们⽆法⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态。**在训练模型时，每次随机采样前都需要重新初始化隐藏状态**\n\n#### 2.2.2 相邻采样\n\n- 除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就**可以⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态**，从而使下⼀个小批量的输出也取决于当前小批量的输⼊\n\n\n\n- **两种采样方式的区别：**\n  1. 采用相邻采样，在训练模型时，我们只需在每⼀个迭代周期开始时初始化隐藏状态\n  2. 当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同⼀迭代周期中，随着迭代次数的增加，梯度的**计算开销会越来越⼤**。为了使模型参数的梯度计算只依赖⼀次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来\n  3. **相邻采样不能并行运算**，因为必须等上一个小批量结束了，下一个小批量才能开始\n\n\n\n### 2.3 RNN的训练、预测\n\n#### 2.3.1 输入输出\n\n- 首先是RNN的输入输出，应该包含time_steps、batch_size、vocab_size（时间步、批量大小、词典大小）的维度，之所以有一个维度是词典大小，是因为我们会**把输入输出用独热向量**表示\n\n\n\n#### 2.3.2 预测\n\n- **在训练时，下一个时间步的输入可以：**\n\n  1. 上一个时间步的标签\n  2. 上一个时间步的表征（也就是输出）\n\n  采用第一种方法，**更容易收敛，但是泛化能力更差**，而第二种方法**更不易收敛，但是泛化能力更强**\n\n- **但是在预测时**，由于我们压根没有标签，所以只能用上述第二种方法。正是由于训练时和预测时干的事情都不一样，所以采用第一种方法泛化能力更差\n\n\n\n#### 2.3.3 困惑度\n\n- 我们通常使⽤困惑度（perplexity）来评价语⾔模型的好坏\n- 困惑度的基本思想是：**给测试集的句子赋予较高概率值的语言模型较好，当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型在测试集上的概率越高越好**\n- 现在给定测试集上的一个句子序列$$w_1, ..., w_t$$，那么其困惑度为：\n\n$$\nPP(w_1, ..., w_t) = P(w_1, ..., w_t)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P\\left(w_{1} w_{2} \\ldots w_{N})\\right.}}\n$$\n\n可以看到，我们的**目的是最大化获得测试集上的句子的概率**，而最小化困惑度其实就是最大化这个概率，另外这里使用了一个几何平均，其目的是：\n\n> 1. 因为每个字符的概率必然小于1，所以越长的句子的概率在连乘的情况下必然越小，所以为了对长短句公平，需要平均一下\n> 2. 因为**几何平均数**的特点是，如果有其中的一个概率是很小的，那么最终的结果就不可能很大，从而要求好的句子的每个字符（即每个时间步的输出）都要有基本让人满意的概率\n\n- 采用unigram，假设每个时间步的预测是相互独立的：\n\n$$\nPP(w_1, ..., w_t) = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^NP(w_i)}}\n$$\n\n这是uni-gram（即当前词不取决于前面的词）的困惑度计算公式，针对不同近似的N元语法又不同的计算公式，比如bi-gram、tri-gram等。另外，里面的$$P(w_i)$$直接取输出层经过Softmax的概率值即可\n\n- 特别的：\n\n1. 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1\n2. 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷\n3. 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数\n\n**显然，任何一个有效模型的困惑度必须小于类别个数（要不然模型的预测效果还不如随机点一个类别来的好）**\n\n\n\n#### 2.3.4 通过时间反向传播\n\n- 我们需要将循环神经网络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应⽤反向传播计算并存储梯度\n- 简单起⻅，我们考虑⼀个⽆偏差项的循环神经网络，且激活函数为恒等映射$（\\phi(x) = x）$。设时间步t的输⼊为单样本$x_t \\in R^d$，标签为$y_t$，那么隐藏状态$h_t \\in R^h$的计算表达式为:\n\n$$\nh_t = W_{hx}X_t + W_{hh}h_{t-1}\n$$\n\n输出层变量$o_t \\in \\mathbb{R}^q$为：\n$$\no_t = W_{qh}h_t\n$$\n设时间步t的损失为$\\ell(o_t, y_t)$。则总时间步数为T的损失函数L定义为：\n$$\nL = \\frac{1}{T}\\sum_{t = 1}^T \\ell(o_t, y_t)\n$$\n\n- 我们假设一共有3个时间步数，那么可以做出计算图：\n  ![image-20211210132156070](https://s2.loli.net/2021/12/10/Feq2PM3nCslpGO7.png)\n\n- 现在我们开始反向传播：\n\n易得：\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}=\\frac{\\partial \\ell\\left(\\boldsymbol{o}_{t}, y_{t}\\right)}{T \\cdot \\partial \\boldsymbol{o}_{t}}\n$$\n然后可以得到：\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{q h}}=\\sum_{t=1}^{T} \\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}, \\frac{\\partial \\boldsymbol{o}_{t}}{\\partial \\boldsymbol{W}_{q h}}\\right)=\\sum_{t=1}^{T} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}} \\boldsymbol{h}_{t}^{\\top} .\n$$\n接下来就是计算关于$$W_{hx}$$和$$W_{hh}$$的梯度，那么必然要求关于各时间步隐藏状态的梯度，我们注意到隐藏状态之间也存在依赖关系，，L只通过$$o_T$$依赖最终时间步T的隐藏状态$$h_T$$。因此，我们先计算⽬标函数有关最终时间步隐藏状态的梯度：\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{T}}=\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{T}}, \\frac{\\partial \\boldsymbol{o}_{T}}{\\partial \\boldsymbol{h}_{T}}\\right)=\\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T}}\n$$\n接下来对于时间步t < T，在图6.3中，$L$通过$h_{t+1}$和$o_t$依赖$h_t$。依据链式法则，⽬标函数有关时间步t < T的隐藏状态的梯度$\\partial L / \\partial h_t \\in R^h$需要按照时间步从⼤到小依次计算:\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t}}=\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}, \\frac{\\partial \\boldsymbol{h}_{t+1}}{\\partial \\boldsymbol{h}_{t}}\\right)+\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}, \\frac{\\partial \\boldsymbol{o}_{t}}{\\partial \\boldsymbol{h}_{t}}\\right)=\\boldsymbol{W}_{h h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}+\\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}} .\n$$\n对次递归公式展开，我们可以得到对任意时间步1 ≤ t ≤ T，我们可以得到⽬标函数有关隐藏状态梯度的通项公式：\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t}}=\\sum_{i=t}^{T}\\left(\\boldsymbol{W}_{h h}^{\\top}\\right)^{T-i} \\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T+t-i}}\n$$\n\n通过关于每个时间步隐藏状态的梯度，易求得关于$$W_{hx}$$和$$W_{hh}$$的梯度。\n\n- **由上式中的指数项可⻅，当时间步数T较⼤或者时间步t较小时，⽬标函数有关隐藏状态的梯度较容易出现衰减和爆炸**\n- RNN和一般的网络一样，我们在正向传播和反向传播的时候会进行一些数据的储存，避免重复计算\n\n\n\n#### 2.3.5 激活函数的选择\n\n- 对于下一个时间步隐藏状态的计算，激活函数一般选用Tanh或者ReLu。对输出的运算，一般做softmax运算。\n\n- 但是很多时候还是选用Tanh而不是ReLu，这一点在Hinton的[IRNN论文](https://arxiv.org/abs/1504.00941)里面是很明确的提到的：\n\n![img](https://pic2.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_720w.jpg?source=1940ef5c)\n\n**也就是说在RNN中直接把激活函数换成ReLu会导致非常大的输出值，因此它们可能更容易发生爆炸**\n\n- 下面具体来看一下：\n\n$$\n\\begin{array}{c}\n\\text { net }_{t}=U x_{t}+W h_{t-1} \\\\\nh_{t}=f\\left(\\text { net }_{t}\\right)\n\\end{array}\n$$\n\n**假设ReLu函数一直处于激活区域（即输入大于0）**，则有$$f(x) = x, net_t = Ux_t + W(Ux_{t-1} + Wh_{t-2})$$，继续将其展开，最终会包含多个W的连乘，如果W不是单位矩阵， 那么最终的结果将会区域0或无穷。\n\n但是在CNN中就不会发生这种问题，因为CNN中每一层的W是不同的，并且在初始化时它们是独立同分布的，因此可以相互抵消，在多层之后一般不会出现这种严重的数值问题\n\n我们再来看看反向传播的时候：\n$$\n\\frac{\\partial h_t}{\\partial h_{t-1}} = W\n$$\n所以：\n$$\n\\frac{\\partial h_t}{\\partial h_{1}} = W^n\n$$\n\n\n可以看到只要W不是单位矩阵，梯度会出现消失或者爆炸的现象\n\n而在使用tanh作为激活函数的时候：\n$$\n\\frac{\\partial h_{t}}{\\partial h_{t-1}}=\\left(1-h_{t}^{2}\\right) W\n$$\n可以证明，固定$$h_{t-1}$$时，$$\\left(1-h_{t}^{2}\\right) W$$是有界的（这个界可能不是1，但总归是有界的）\n\n- 综上所述：当采用ReLu作为激活函数时，只有W在单位矩阵附近时才能取得比较好的结果。**而使用tanh函数，每个相邻时间步之间的梯度是有界的，减少了一些梯度爆炸的可能性（注意是梯度爆炸，而没有解决梯度消失）。**但是ReLu也拥有自己的优点，那就是计算量更小，收敛更快，其实relu也可以配合梯度裁剪来使用，但是终归没有tanh好\n\n\n\n### 2.4 梯度裁剪\n\n- 循环神经网络中较容易出现梯度衰减或梯度爆炸。**为了应对梯度爆炸**，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成⼀个向量 $g$，并设裁剪的阈值是$\\theta$。裁剪后的梯度：\n\n$$\n\\min \\left(\\frac{\\theta}{\\|g\\|}, 1\\right) g\n$$\n\n的$L_2$范数不超过$\\theta$\n\n- **但是梯度裁剪无法应对梯度衰减**\n\n\n\n\n\n# 3 门控循环单元（GRU）\n\n- 我们发现，当时间步数较⼤或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但⽆法解决梯度衰减的问题。通常由于这个原因，**循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系**\n- 门控循环神经网络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较⼤的依赖关系。它通过可以学习的⻔来控制信息的流动。其中，门控循环单元（gated recurrent unit，GRU）是⼀种常⽤的⻔控循环神经网\n\n\n\n### 3.1 重置门和更新门\n\n- ⻔控循环单元中的重置⻔和更新⻔的输⼊均为当前时间步输⼊$$X_t$$与上⼀时间步隐藏状态$$H_{t−1}$$，输出由激活函数为sigmoid函数的全连接层计算得到。\n\n![image-20211210135706726](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20211210135706726.png)\n\n具体来说，重置门$R_t \\in \\mathbb{R}^{n \\times h}$和更新门$Z_t \\in \\mathbb{R}^{n \\times h}$的计算如下：\n$$\n\\begin{array}{l}\n\\boldsymbol{R}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x r}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h r}+\\boldsymbol{b}_{r}\\right) \\\\\n\\boldsymbol{Z}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x z}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h z}+\\boldsymbol{b}_{z}\\right)\n\\end{array}\n$$\n\n\n### 3.2 候选隐藏状态\n\n- 我们将当前时间步重置⻔的输出与上⼀时间步隐藏状态做按元素乘法（符号为$\\odot$）。如果重置⻔中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上⼀时间步的隐藏状态。如果元素值接近1，那么表⽰保留上⼀时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输⼊连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态\n\n- 具体来说，时间步t的候选隐藏状态$\\tilde{H}_t \\in \\mathbb{R}^{n \\times h}$的计算为：\n\n$$\n\\tilde{\\boldsymbol{H}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\left(\\boldsymbol{R}_{t} \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right),\n$$\n\n- **重置门控制了上⼀时间步的隐藏状态如何流⼊当前时间步的候选隐藏状态。而上⼀时间步的隐藏状态可能包含了时间序列截⾄上⼀时间步的全部历史信息。因此，重置门可以⽤来丢弃与预测⽆关的历史信息**\n\n- 最后，时间步t的隐藏状态$$H_t \\in \\mathbb{R}^{n×h}$$的计算使⽤当前时间步的更新门$$Z_t$$来对上⼀时间步的隐藏状态$$H_{t−1}$$和当前时间步的候选隐藏状态$$\\tilde{H}_t$$做组合：\n\n$$\n\\boldsymbol{H}_{t}=\\boldsymbol{Z}_{t} \\odot \\boldsymbol{H}_{t-1}+\\left(1-\\boldsymbol{Z}_{t}\\right) \\odot \\tilde{\\boldsymbol{H}}_{t}\n$$\n\n\n\n![image-20211210141824572](https://s2.loli.net/2021/12/10/L7O69pm3Yjzbno5.png)\n\n- **更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新**。假设更新⻔在时间步$t'$到$t（t' < t）$之间⼀直近似1。那么，**在时间步$t'$到$t$之间的输⼊信息⼏乎没有流⼊时间步$t$的隐藏状态$H_t$**。实际上，**这可以看作是较早时刻的隐藏状态$H_{t'−1}$⼀直通过时间保存并传递⾄当前时间步$t$**。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较⼤的依赖关系\n- 我们稍作总结：\n\n1. 重置⻔有助于捕捉时间序列⾥短期的依赖关系\n2. 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系\n3. 重置门和更新门为0、1的情况：\n\n| 对应情况                                 | 重置门 | 更新门 |\n| ---------------------------------------- | ------ | ------ |\n| 退化为一般的RNN                          | 1      | 0      |\n| 丢弃当前时间步的全部信息，只保留历史信息 | 0或1   | 1      |\n| 完全丢弃历史信息，只保留当前时间步的信息 | 0      | 0      |\n\n\n\n\n\n# 4 长短期记忆（LSTM）\n\n- 还有另外一种十分常用的门控循环神经网络：⻓短期记忆（long short-term memory，LSTM）\n- **LSTM之所以叫做长短期记忆，是因为里面的两个记忆变量：隐藏状态$$H_t$$负责短期记忆，记忆细胞$$C_t$$负责长期记忆（因为每个时间步中$$H_t$$相比于$$C_t$$更新的更多，所以负责短期）**\n\n\n\n### 4.1 输⼊门、遗忘门、输出门和候选记忆细胞\n\n- 与门控循环单元中的重置门和更新门⼀样，⻓短期记忆的门的输⼊均为当前时间步输⼊$$X_t$$与上⼀时间步隐藏状态$$H_{t−1}$$，输出由激活函数为sigmoid函数的全连接层计算得到\n- ⻓短期记忆需要计算候选记忆细胞$\\tilde{C}_t$。它的计算与上⾯介绍的3个⻔类似，但使⽤了值域在[−1, 1]的tanh函数作为激活函数:\n\n![image-20211210144009641](https://s2.loli.net/2021/12/10/jzn954wmEqRfyIg.png)\n\n- 时间步t的输⼊⻔$I_t \\in R^{n×h}$、遗忘⻔$F_t \\in R^{n×h}$和输出⻔$O_t \\in R^{n×h}$分别计算如下：\n\n$$\n\\begin{aligned}\n\\boldsymbol{I}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x i}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h i}+\\boldsymbol{b}_{i}\\right), \\\\\n\\boldsymbol{F}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x f}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h f}+\\boldsymbol{b}_{f}\\right), \\\\\n\\boldsymbol{O}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x o}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h o}+\\boldsymbol{b}_{o}\\right), \\\\\n\\tilde{\\boldsymbol{C}}_{t}&=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x c}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h c}+\\boldsymbol{b}_{c}\\right),\n\\end{aligned}\n$$\n\n\n\n### 4.2 记忆细胞\n\n- 当前时间步记忆细胞$C_t \\in R^{n\\times h}$的计算组合了上⼀时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘⻔和输⼊⻔来控制信息的流动：\n\n$$\n\\boldsymbol{C}_{t}=\\boldsymbol{F}_{t} \\odot \\boldsymbol{C}_{t-1}+\\boldsymbol{I}_{t} \\odot \\tilde{\\boldsymbol{C}}_{t}\n$$\n\n- 遗忘门控制上⼀时间步的记忆细胞$C_{t−1}$中的信息是否传递到当前时间步，而输⼊门则控制当前时间步的输⼊$X_t$通过候选记忆细胞$\\tilde{C}_t$如何流⼊当前时间步的记忆细胞。如果遗忘门⼀直近似1且输⼊门⼀直近似0，过去的记忆细胞将⼀直通过时间保存并传递⾄当前时间步\n\n\n\n### 4.3 隐藏状态\n\n- 有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$H_t \\in R^{n×h}$的信息的流动：\n\n$$\nH_t = O_t \\odot tanh(C_t)\n$$\n\n- 当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使⽤；当输出门近似0时，记忆细胞信息只⾃⼰保留\n\n![image-20211210145754660](https://s2.loli.net/2021/12/10/mlYhrVbA47MByvG.png)\n\n\n\n### 4.4 三种门的作用\n\n- **输入门控制当前计算的新状态以多大程度更新到记忆单元中**\n- **遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉**\n- **输出门控制当前的输出有多大程度取决于当前的记忆单元**\n- 在一个训练好的网络中：\n\n1. 当输入的序列中没有重要信息时，LSTM的遗忘门接近于1，输入门接近于0，此时过去的记忆会被保存，从而实现了长期记忆功能\n2. 当输入的序列中出现了重要的信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近1\n3. 当输入的序列中出现了重要的信息，且该信息意味着之前的记忆不再重要时，输入门的值接近1，而遗忘门的值接近于0，这样旧的记忆被遗忘，新的重要信息被记忆\n\n\n\n### 4.5 LSTM各模块激活函数\n\n- LSTM中，三种门都用的Sigmoid函数，生成候选记忆时，使用Tanh作为激活函数。值得注意的是，**这两个激活函数都是饱和的**，也就是说在输入达到一定值的情况下，输出就不发生明显变化了。如果使用的是非饱和的激活函数，例如ReLu，将难以实现门控的效果\n- Sigmoid的输出在0到1之间，符合门控的物理定义。且当输入较大或较小时，输出会非常接近1或0，从而保证门开或门关\n- 在生成候选记忆时，使用Tanh函数，是因为其输出在-1到1之间，这与大多数场景下特征分布是zero-centered吻合\n- 此为Tanh函数在输入为0附近相比于Sigmoid有更大的梯度，通常收敛更快\n\n\n\n# 5 门控机制是如何解决RNN的问题的\n\n- 上面我们说到，RNN最大的一个问题就是无法捕捉长距离依赖，而引入门控机制就是为了解决这个问题，这里我们以LSTM举例：\n\n$$\n\\begin{aligned}\nf_{t} &=\\sigma\\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\\right) \\\\\ni_{t} &=\\sigma\\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\\right) \\\\\no_{t} &=\\sigma\\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\\right) \\\\\n\\hat{c}_{t} &=\\tanh \\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\\right) \\\\\nc_{t} &=f_{t} \\circ c_{t-1}+i_{t} \\circ \\hat{c}_{t} \\\\\nh_{t} &=o_{t} \\circ \\tanh \\left(c_{t}\\right)\n\\end{aligned}\n$$\n\n- 我们可以像上面2.3.5激活函数的选择中一样，探讨$$\\frac{\\partial h_t}{\\partial h_{t-1}}$$，但是因为$$h_{t} =o_{t} \\circ \\tanh(c_t)$$，所以同样可以分析$$\\frac{c_t}{c_{t-1}}$$，而后者更简单些，所以分析后者：\n\n$$\n\\frac{\\partial c_{t}}{\\partial c_{t-1}}=f_{t}+c_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}}+\\hat{c} t \\frac{\\partial i_{t}}{\\partial c_{t-1}}+i_{t} \\frac{\\partial \\hat{c}_{t}}{\\partial c_{t-1}}\n$$\n\n- 上式总共有4项，**梯度主要取决于第一项$$f_t$$**，由于$$f_t \\in (0,1)$$，所以梯度爆炸的风险减小，而是否会发生梯度小时，取决于每个时间步的$$f_t$$是否接近1，但是这里有个自洽的结论：**如果我们的任务比较依赖于历史信息，那么$$f_t$$就会接近于1，这时候历史的梯度信息也正好不容易消失；如果$$f_t$$很接近于0，那么就说明我们的任务不依赖于历史信息，这时候就算梯度消失也无妨了。**\n\n- 而后面3项都不是决定梯度的主要因素，挑$$c_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}}$$来说明，带入$$h_{t-1} =o_{t-1} \\circ \\tanh(c_{t-1})$$，可以得到：\n\n$$\nc_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}} = c_{t-1} \\frac{\\partial f_{t}}{\\partial h_{t-1}}  \\frac{\\partial h_{t-1}}{\\partial c_{t-1}} = f_{t}\\left(1-f_{t}\\right) o_{t-1}\\left(1-\\tanh ^{2} c_{t-1}\\right) c_{t-1} U_{f}\n$$\n\n- 由于$$f_t,(1-f_t),o_{t-1} \\in (0,1)$$，并且可以证明$$|\\left(1-\\tanh ^{2} c_{t-1}\\right)c_{t-1}| <0.45$$，所以结果相当于$$U_f$$乘上四个门，结果会非常的小，所以不起主导作用。其他两项是同样的方法\n\n- **注意：LSTM和GRU只是很大的缓解了梯度消失，但是如果时间步很长，成千上万步，仍然会发生梯度消失，毕竟记忆单元压根存不了那么多信息**\n\n- 另外，一般LSTM要比GRU要好一些，但是到底好在哪里呢，在GRU中同样进行反向传播的分析：\n\n$$\n\\begin{aligned}\n\\frac{\\partial h_{t}}{\\partial h_{t-1}}=& 1-z_{t}-z_{t}\\left(1-z_{t}\\right) h_{t-1} U_{z}+z_{t}\\left(1-z_{t}\\right) \\hat{h}_{t} U_{z} \\\\\n&+\\left(1-\\hat{h}_{t}^{2}\\right) r_{t}\\left(1+\\left(1-r_{t}\\right) h_{t-1} U_{r}\\right) z_{t} U_{h}\n\\end{aligned}\n$$\n\n- 上式其主导作用的同样是$$1-z_t$$，但是后面的项相比于LSTM只有3个门的连乘（LSTM有4个），所以这些想对梯度的影响可能大一些，所以**LSTM的梯度要比GRU稳定一些，所以GRU更依赖于初始化**\n\n\n\n# 6 深度循环神经网络\n\n- 到⽬前为⽌介绍的循环神经网络只有⼀个单向的隐藏层，在深度学习应⽤⾥，我们通常会⽤到含有多个隐藏层的循环神经网络，也称作深度循环神经网络\n- 下图演⽰了⼀个有L个隐藏层的深度循环神经网络，**每个隐藏状态不断传递⾄当前层的下⼀时间步和当前时间步的下⼀层**\n\n![image-20211210150125737](https://s2.loli.net/2021/12/10/PUK7CRjyemEbzLM.png)\n\n- 第1隐藏层的隐藏状态和之前的计算⼀样：\n\n$$\n\\boldsymbol{H}_{t}^{(1)}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(1)}+\\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{h h}^{(1)}+\\boldsymbol{b}_{h}^{(1)}\\right)\n$$\n\n- 当1 < l ≤ L时，第l隐藏层的隐藏状态的表达式为:\n\n$$\n\\boldsymbol{H}_{t}^{(l)}=\\phi\\left(\\boldsymbol{H}_{t}^{(l-1)} \\boldsymbol{W}_{x h}^{(l)}+\\boldsymbol{H}_{t-1}^{(l)} \\boldsymbol{W}_{h h}^{(l)}+\\boldsymbol{b}_{h}^{(l)}\\right)\n$$\n\n- 最终，输出层的输出只需基于第L隐藏层的隐藏状态：\n\n$$\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t}^{(L)} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}\n$$\n\n- 同多层感知机⼀样，隐藏层个数L是超参数\n\n- **RNN中选择深度网络的原因和一般前馈神经网络一样，都是为了能够用来表征更复杂的情况**\n\n\n\n\n\n# 7 双向循环神经网络\n\n- 之前介绍的循环神经网络模型都是假设当前时间步是由前⾯的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后⾯时间步决定。例如，当我们写下⼀个句⼦时，可能会根据句⼦后⾯的词来修改句⼦前⾯的⽤词\n- **双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息**，下图演⽰了⼀个含单隐藏层的双向循环神经网络的架构\n\n![image-20211210151934211](https://s2.loli.net/2021/12/10/VtFmigZ58qw2aeb.png)\n\n- 具体来看，设时间步t正向隐藏状态为$\\overrightarrow{H}_t \\in R^{n×h}$（正向隐藏单元个数为h），反向隐藏状态为$\\overleftarrow{H}_t \\in R^{n×h}$（反向隐藏单元个数为h）。我们可以分别计算正向隐藏状态和反向隐藏状态：\n\n$$\n\\begin{array}{l}\n\\overrightarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(f)}+\\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{h h}^{(f)}+\\boldsymbol{b}_{h}^{(f)}\\right) \\\\\n\\overleftarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(b)}+\\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{h h}^{(b)}+\\boldsymbol{b}_{h}^{(b)}\\right)\n\\end{array}\n$$\n\n**然后我们连结两个⽅向的隐藏状态$\\overrightarrow{H}_t$和 $\\overleftarrow{H}_t$来得到隐藏状态$H_t \\in R^{n×2h}$**，并将其输⼊到输出层。 输出层计算输出$O_t \\in R^{n×q}$（输出个数为q）：\n$$\nO_t = H_tW_{hq} + b_q\n$$\n\n- **不同方向的隐藏状态的隐藏单元个数也可以不同**\n-  双向循环神经网络在每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊）\n\n\n\n\n\n# 8 Seq2Seq模型\n\n- 在许多时候，输入和输出都是不定长序列，这采用一般的RNN肯定是行不通的。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，例如：英语输⼊：“They”、“are”、“watching”、“.” ； 法语输出：“Ils”、“regardent”、“.”\n- 当输⼊和输出都是不定⻓序列时，我们可以使⽤seq2seq模型，本质上都⽤到了两个循环神经网络，分别叫做编码器和解码器。**编码器用来分析输⼊序列，解码器⽤来生成输出序列**\n\n![image-20211210170200879](https://s2.loli.net/2021/12/10/FQfVxdq2wuC7aSs.png)\n\n- < bos >（beginning of sequence）和 < eos >（end of sequence）分别表示序列的开始和结束\n- 编码器每个时间步的输⼊依次为英语句⼦中的单词、标点和特殊符号。上图中使⽤了编码器在最终时间步的隐藏状态作为输⼊句⼦的表征或编码信息。解码器在各个时间步中使⽤输⼊句⼦的编码信息和上个时间步的输出以及隐藏状态作为输⼊\n\n\n\n### 8.1 编码器\n\n- **编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量$c$，并在该背景变量中编码输⼊序列信息**。编码器可以使⽤循环神经网络\n- 编码器通过⾃定义函数q将各个时间步的隐藏状态变换为背景变量\n\n$$\nc = q(h_1, ...,h_T)\n$$\n\n- **也可以使⽤双向循环神经网络构造编码器**，在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊），并编码了整个序列的信息。\n\n\n\n### 8.2 解码器\n\n- 解码器就是通过编码器输出的背景向量$c$，和每一个时间步的输入，输出所需要的结果\n- **解码器在预测和训练时是不一样的**，我们先介绍**预测**时的解码器：\n\n编码器输出的背景变量$c$编码了整个输⼊序列$$x_1, . . . , x_T$$的信息。给定训练样本中的输出序列$$y_1, y_2, . . . , y_{T'}$$，对每个时间步$$t'$$（符号与输⼊序列或编码器的时间步$$t$$有区别），解码器输出$$y_{t'}$$的条件概率将基于之前的输出序列$$y_1, . . . , y_{t'−1}$$和背景变量$$c$$，即$$P(y_{t'} | y_1, . . . , y_{t'−1}, c)$$\n\n为此，我们可以使⽤另⼀个循环神经网络作为解码器。在输出序列的时间步$$t'$$，解码器将上⼀时间步的输出$$y_{t'−1}$$以及背景变量$$c$$作为输⼊，并将它们与上⼀时间步的隐藏状态$$s_{t'−1}$$变换为当前时间步的隐藏状态$$s_{t'}$$：\n$$\ns_{t'} = g(y_{t' -1}, c, s_{t' - 1})\n$$\n有了解码器的隐藏状态后，我们可以使⽤⾃定义的输出层和softmax运算来计算$$P(y_{t'} | y_1, . . . , y_{t'−1}, c)$$\n\n- **训练**时的解码器，每一时间步的输入序列可以是上一时间步的输出，也可以是上一时间步的真实标签序列。后者叫做**强制教学**\n\n根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率：\n$$\n\\begin{aligned}\nP\\left(y_{1}, \\ldots, y_{T^{\\prime}} \\mid x_{1}, \\ldots, x_{T}\\right) &=\\prod_{t^{\\prime}=1}^{T^{\\prime}} P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, x_{1}, \\ldots, x_{T}\\right) \\\\\n&=\\prod_{t^{\\prime}=1}^{T^{\\prime}} P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)\n\\end{aligned}\n$$\n并得到该输出序列的损失：\n$$\n-\\log P\\left(y_{1}, \\ldots, y_{T^{\\prime}} \\mid x_{1}, \\ldots, x_{T}\\right)=-\\sum_{t^{\\prime}=1}^{T^{\\prime}} \\log P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)\n$$\n在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数\n\n\n\n### 8.3 预测时的搜索方式\n\n#### 8.3.1 贪婪搜索\n\n- 贪婪搜索（greedy search）。对于输出序列任⼀时间步$t'$，我 们从$|Y|$（Y为词典）个词中搜索出条件概率最⼤的词：\n\n$$\ny_{t'} = argmax_{y \\in Y}P(y|y_1, ..., y_{t' - 1}, c)\n$$\n\n作为输出。⼀旦搜索出“< eos >”符号，或者输出序列⻓度已经达到了最⼤⻓度$T'$，便完成输出。\n\n- 我们将该条件概率最⼤的输出序列称为**最优输出序列**，**而贪婪搜索无法保证得到最优输出序列**，下面举个栗子：\n\n> 假设输出词典⾥⾯有“A” “B” “C” 和“< eos >”这4个词。下图中每个时间步下的4个数字分别代表了该时间步⽣成“A” “B” “C” 和“< eos >”这4个词的条件概率。在每个 间步，贪婪搜索选取条件概率最⼤的词。因此，将⽣成输出序列“A” “B” “C” “< eos >”。 该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 = 0.048\n\n![image-20211210180148791](https://s2.loli.net/2021/12/10/JUgILXFCjWviaQw.png)\n\n> 但是现在我们考虑下面一种情况，在时间步2中选取了条件概率第⼆⼤的词“C”，**由于之后时间步的概率值是基于前面的时间步的，所以时间步2结果选取的改变，会导致后续的概率发生改变**，如下图，所以我们现在的输出序列“A” “C” “B” “< eos >”的条件概率是0.5 × 0.3 × 0.6 × 0.6 = 0.054，大于贪婪搜索的概率，所以贪婪搜索得到的不是最优的\n\n![image-20211210180404095](https://s2.loli.net/2021/12/10/zSKDLamFWqseyuN.png)\n\n#### 8.3.2 穷举搜索\n\n- 我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最⼤的序列\n- **虽然穷举搜索可以得到最优输出序列，但它的计算开销$O(|Y|^{T'})$很容易过⼤**，而贪婪搜索的开销为$O(|Y|T')$，明显小于穷举搜索\n\n\n\n#### 8.3.3 束搜索\n\n- 束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个**束宽**（beam size）超参数。我们将它设为k。在时间步1时，选取当前时间步条件概率最⼤的k个词，分别组成k个候选输出序列的⾸词。在之后的每个时间步，基于上个时间步的k个候选输出序列，从k$ |Y|$个可能的输出序列中选取条件概率最⼤的k个，作为该时间步的候选输出序列。最终，**我们从各个时间步的候选输出序列中筛选出包含特殊符号“< eos >”的序列，并将它们中所有特殊符号“< eos >”后⾯的⼦序列舍弃**，得到最终候选输出序列的集合\n\n- 下面举个栗子：\n\n![image-20211210182131362](https://s2.loli.net/2021/12/10/QEBb3gUVxmuDS7O.png)\n\n第一个时间步找出概率最大的\"A\"和\"C\"，然后再根据\"A\"和\"C\"，在时间步2中寻找概率最大的2个，分别为\"AB\"和\"CE\"，再根据这两个，在时间步3，输出\"ABD\"和\"CED\"，最后减去< eos >符号以后的内容，得出输出序列\n\n- 在最终候选输出序列的集合中，我们取以下分数最⾼的序列作为输出序列：\n\n$$\n\\frac{1}{L^{\\alpha}} \\log P\\left(y_{1}, \\ldots, y_{L}\\right)=\\frac{1}{L^{\\alpha}} \\sum_{t^{\\prime}=1}^{L} \\log P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)\n$$\n\n其中$L$为最终候选序列⻓度，$\\alpha$⼀般可选为0.75。分⺟上的$L^{\\alpha}$是为了惩罚较⻓序列在以上分数中较多的对数相加项\n\n- 束搜索的计算开销为$O(k|Y|T')$，介于贪婪搜索和穷举搜索之间\n- **束搜索通过灵活的束宽来权衡计算开销和搜索质量**\n\n\n\n### 8.4 注意力机制\n\n#### 8.4.1 seq2seq中的注意力机制\n\n- **解码器在⽣成输出序列中的每⼀个词时可能只需利⽤输⼊序列某⼀部分的信息， 而不需要由整个输入序列生成的背景向量**，例如，在输出序列的时间步1，解码器可以主要依赖“They” “are” 的信息来⽣成“Ils”，在时间步2则主要使⽤来⾃“watching”的编码信息⽣成“regardent”，而不需要\"They are watching\"整个句子\n- 若没引入注意力机制，在实际使用中，那么会发现随着输入序列的增长，模型的性能发生了显著下降。**这是因为编码时输入序列的全部信息压缩到了一个向量中，随着序列的增长，越前面的词丢失越严重**。**同时，seq2seq模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词对应的源语言词的上下文信息和位置信息在编解码过程中丢失了**\n\n- 注意⼒机制通过对编码器所有时间步的隐藏状态做**加权平均**来得到背景变量。解码器在每⼀时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量\n\n![image-20211210184622950](https://s2.loli.net/2021/12/10/5kGV8uqeaYn3wlF.png)\n\n⾸先，函数a根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输⼊。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量\n\n- 具体来说，令编码器在时间步t的隐藏状态为$h_t$，且总时间步数为T。那么解码器在时间步$t'$（$t'$代表的是解码器的时间步数）的背景变量为所有编码器隐藏状态的加权平均：\n\n$$\n\\boldsymbol{c}_{t^{\\prime}}=\\sum_{t=1}^{T} \\alpha_{t^{\\prime} t} \\boldsymbol{h}_{t}\n$$\n\n其中给定$t'$时，权重$\\alpha_{t't}$在$t = 1, . . . , T$的值是⼀个概率分布。为了得到概率分布，我们可以使⽤softmax运算:\n$$\n\\alpha_{t^{\\prime} t}=\\frac{\\exp \\left(e_{t^{\\prime} t}\\right)}{\\sum_{k=1}^{T} \\exp \\left(e_{t^{\\prime} k}\\right)}, \\quad t=1, \\ldots, T\n$$\n由于$$e_{t't}$$同时取决于解码器的时间步$$t'$$和编码器的时间步t，我们不妨以解码器在时间步$$t'− 1$$的隐藏状态$$s_{t'−1}$$与编码器在时间步t的隐藏状态$$h_t$$为输⼊，并通过函数a计算$$e_{t't}$$：\n$$\ne_{t't} = a(s_{t'-1}, h_t)\n$$\n其中a是一个自定义函数，有多种选择。如果两个输⼊向量⻓度相同，⼀个简单的选择是计算它们的内积$a(s,h) = s^⊤h$\n\n- 我们对注意力机制可以有一个直观的理解：**在生成一个输出词时，会考虑每一个输入词和当前输出词的对应关系，对齐越好的词，会有越大的权重**\n- 还可以结合双向RNN和注意力机制，每个隐状态包含了$\\overleftarrow{h_t}$和$\\overrightarrow{h_t}$\n\n\n\n#### 8.4.2 矢量化计算\n\n- 我们还可以对注意⼒机制采⽤更⾼效的⽮量化计算。⼴义上，注意⼒机制的输⼊包括**查询项**以及**⼀⼀对应的键项和值项**，其中值项是需要加权平均的⼀组项。在加权平均中，**值项的权重来⾃查询项以及与该值项对应的键项的计算**\n\n- 在上面的例⼦中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。假设编码器和解码器的隐藏单元个数均为h，且函数$$a(s, h) = s^ ⊤h$$。若我们希望根据解码器单个隐藏状态$$s_{t'−1} \\in \\mathbb{R}^h$$和编码器所有隐藏状态$$h_t \\in \\mathbb{R}^h , t = 1, . . . , T$$来计算背景向量$$c_{t'} \\in \\mathbb{R}^h$$。我们可以将查询项矩阵$$Q \\in R^{1\\times h}$$设为$$s^⊤_{t'−1}$$，并令键项矩阵$$K \\in \\mathbb{R}^{T\\times h}$$和值项矩阵$$V \\in \\mathbb{R}^{T\\times h}$$相同且第t⾏均为$$h^⊤_t$$ 。此时，我们只需要通过⽮量化计算：\n\n$$\nsoftmax(QK^T)V\n$$\n\n即可算出转置后的背景向量$$c^⊤_{t'}$$。当查询项矩阵Q的⾏数为n时，上式将得到n行的输出矩阵。输出矩阵与查询项矩阵在相同行上⼀⼀对应。\n","slug":"RNN","published":1,"updated":"2023-03-29T16:13:33.246Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1d00087cszc90dcoso","content":"<ul>\n<li>与多层感知机和能有效<strong>处理空间信息</strong>的卷积神经网络不同，循环神经网络是为更好地<strong>处理时序信息</strong>而设计的。它<strong>引⼊状态变量来存储过去的信息</strong>，并⽤其<strong>与当前的输⼊共同决定当前的输出</strong></li>\n</ul>\n<h1 id=\"1-语言模型\"><a href=\"#1-语言模型\" class=\"headerlink\" title=\"1 语言模型\"></a>1 语言模型</h1><h3 id=\"1-1-语言模型的计算\"><a href=\"#1-1-语言模型的计算\" class=\"headerlink\" title=\"1.1 语言模型的计算\"></a>1.1 语言模型的计算</h3><ul>\n<li>我们可以把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。假设⼀段⻓度为 $T$ 的⽂本中的词 依次为 $w_1, w_2, . . . , w_T$，那么在离散的时间序列中，$w_t（1 ≤ t ≤ T）$可看作在时间步（time step）$ t$ 的输出或标签。给定⼀个⻓度为 $T$ 的词的序列 $w_1, w_2, . . . , w_T$，语⾔模型将计算该序列的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(w_1, w_2, ...,w_T)</script><ul>\n<li><strong>由于 $w_1, w_2, . . . , w_T$ 是依次生成的，所以有：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(w_1, w_2, ...,w_T) = \\prod_{t=1}^T P(w_t | w_1, ..., w_{t-1})</script><ul>\n<li>为了计算语⾔模型，我们需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即<strong>语⾔模型参数</strong>。<strong>词的概率可以通过该词在训练数据集中的相对词频来计算</strong></li>\n</ul>\n<h3 id=\"1-2-n元语法\"><a href=\"#1-2-n元语法\" class=\"headerlink\" title=\"1.2 n元语法\"></a>1.2 n元语法</h3><ul>\n<li><strong>当序列⻓度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加</strong></li>\n<li>n元语法通过⻢尔可夫假设（<strong>虽然并不⼀定成立</strong>）简化了语⾔模型的计算。这⾥的<strong>⻢尔可夫假设是指⼀个词的出现只与前⾯n个词相关</strong>，即n阶⻢尔可夫链（Markov chain of order n）</li>\n<li>如果基于$n-1$阶马尔科夫链，我们可以将语言模型改写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(w_1, w_2, ...,w_T) \\approx \\prod_{t=1}^T P(w_t | w_{t - (n - 1)}, ..., w_{t-1})</script><ul>\n<li><p>以上也叫<strong>n元语法</strong>（n-grams）。它是基于n−1阶⻢尔可夫链的概率语⾔模型</p>\n</li>\n<li><p>当n较小时，n元语法往往并不准确。当n较⼤时，n元语法需要计算并存储⼤量的词频和多词相邻频率。<strong>n权衡了计算复杂度和模型准确性</strong></p>\n</li>\n</ul>\n<h1 id=\"2-循环神经网络（RNN）\"><a href=\"#2-循环神经网络（RNN）\" class=\"headerlink\" title=\"2 循环神经网络（RNN）\"></a>2 循环神经网络（RNN）</h1><ul>\n<li>在n元语法中，时间步t的词 $w_t$ 基于前⾯所有词的条件概率只考虑了最近时间步的n − 1个词。如果要考虑⽐t − (n − 1)更早时间步的词对$w_t$的可能影响，我们需要增⼤n。但这样模型参数的数量将随之呈指数级增⻓</li>\n<li>但是对于循环神经网络，它<strong>并⾮刚性地记忆所有固定⻓度的序列，而是通过隐藏状态来存储之前时间步的信息</strong></li>\n</ul>\n<h3 id=\"2-1-循环神经网络\"><a href=\"#2-1-循环神经网络\" class=\"headerlink\" title=\"2.1 循环神经网络\"></a>2.1 循环神经网络</h3><ul>\n<li>假设<script type=\"math/tex\">X_t \\in \\mathbb{R}^{n \\times d}</script>是序列中时间步t的小批量输⼊， <script type=\"math/tex\">H_t \\in \\mathbb{R}^{n \\times h}</script>是该时间步的隐藏变量。与多层感知机不同的是，这⾥我们保存上⼀时间步的隐藏变量<script type=\"math/tex\">H_{t−1}</script>，并引⼊⼀个新的权重参数<script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h×h}</script>，该参数⽤来描述在当前时间步如何使⽤上⼀时间步的隐藏变量。时间步t的隐藏变量的计算由当前时间步的输⼊和上⼀时间步的隐藏变量共同决定：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{H}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right)</script><p>$\\phi$是激活函数</p>\n<ul>\n<li><strong>这⾥的隐藏变量能够捕捉截⾄当前时间步的序列的历史信息</strong></li>\n<li>每个时间步还有一个对应的输出：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}</script><p>在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差</p>\n<ul>\n<li><strong>即便在不同时间步，循环神经网络也始终使⽤W， b这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增⻓</strong></li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/oAbwO43tDSTWpVg.png\" alt=\"image-20211210101227481\"></p>\n<ul>\n<li>隐藏状态中<script type=\"math/tex\">X_{t}W_{x h}+H_{t-1} W_{h h}</script>的计算等价于<script type=\"math/tex\">X_t</script>与<script type=\"math/tex\">H_{t−1}</script>连结后的矩阵乘以<script type=\"math/tex\">W_{xh}</script>与<script type=\"math/tex\">W_{hh}</script>连结后的矩阵，实际上在代码实现中基本都是这么做的</li>\n</ul>\n<ul>\n<li>举一个栗子：基于字符级循环神经网络的语⾔模型</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/OYZwANsxl2oEQaI.png\" alt=\"image-20211210101914126\"></p>\n<p><strong>标签序列依次为输入序列的下一个</strong>，并且在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差</p>\n<h3 id=\"2-2-时序数据的采样\"><a href=\"#2-2-时序数据的采样\" class=\"headerlink\" title=\"2.2 时序数据的采样\"></a>2.2 时序数据的采样</h3><ul>\n<li>时序数据的⼀个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想” “要” “有” “直” “升”。<strong>该样本的标签序列为这些字符分别在训练集中的下⼀个字符</strong>，即 “要” “有” “直” “升” “机”。</li>\n<li>我们有两种⽅式对时序数据进⾏采样，分别是随机采样和相邻采样</li>\n</ul>\n<h4 id=\"2-2-1-随机采样\"><a href=\"#2-2-1-随机采样\" class=\"headerlink\" title=\"2.2.1 随机采样\"></a>2.2.1 随机采样</h4><ul>\n<li><p>在随机采样中，每个样本是原始序列上任意截取的⼀段序列。</p>\n</li>\n<li><p>相邻的两个随机小批量在原始序列上的位置不⼀定相毗邻。因此，我们⽆法⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态。<strong>在训练模型时，每次随机采样前都需要重新初始化隐藏状态</strong></p>\n</li>\n</ul>\n<h4 id=\"2-2-2-相邻采样\"><a href=\"#2-2-2-相邻采样\" class=\"headerlink\" title=\"2.2.2 相邻采样\"></a>2.2.2 相邻采样</h4><ul>\n<li>除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就<strong>可以⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态</strong>，从而使下⼀个小批量的输出也取决于当前小批量的输⼊</li>\n</ul>\n<ul>\n<li><strong>两种采样方式的区别：</strong><ol>\n<li>采用相邻采样，在训练模型时，我们只需在每⼀个迭代周期开始时初始化隐藏状态</li>\n<li>当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同⼀迭代周期中，随着迭代次数的增加，梯度的<strong>计算开销会越来越⼤</strong>。为了使模型参数的梯度计算只依赖⼀次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来</li>\n<li><strong>相邻采样不能并行运算</strong>，因为必须等上一个小批量结束了，下一个小批量才能开始</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"2-3-RNN的训练、预测\"><a href=\"#2-3-RNN的训练、预测\" class=\"headerlink\" title=\"2.3 RNN的训练、预测\"></a>2.3 RNN的训练、预测</h3><h4 id=\"2-3-1-输入输出\"><a href=\"#2-3-1-输入输出\" class=\"headerlink\" title=\"2.3.1 输入输出\"></a>2.3.1 输入输出</h4><ul>\n<li>首先是RNN的输入输出，应该包含time_steps、batch_size、vocab_size（时间步、批量大小、词典大小）的维度，之所以有一个维度是词典大小，是因为我们会<strong>把输入输出用独热向量</strong>表示</li>\n</ul>\n<h4 id=\"2-3-2-预测\"><a href=\"#2-3-2-预测\" class=\"headerlink\" title=\"2.3.2 预测\"></a>2.3.2 预测</h4><ul>\n<li><p><strong>在训练时，下一个时间步的输入可以：</strong></p>\n<ol>\n<li>上一个时间步的标签</li>\n<li>上一个时间步的表征（也就是输出）</li>\n</ol>\n<p>采用第一种方法，<strong>更容易收敛，但是泛化能力更差</strong>，而第二种方法<strong>更不易收敛，但是泛化能力更强</strong></p>\n</li>\n<li><p><strong>但是在预测时</strong>，由于我们压根没有标签，所以只能用上述第二种方法。正是由于训练时和预测时干的事情都不一样，所以采用第一种方法泛化能力更差</p>\n</li>\n</ul>\n<h4 id=\"2-3-3-困惑度\"><a href=\"#2-3-3-困惑度\" class=\"headerlink\" title=\"2.3.3 困惑度\"></a>2.3.3 困惑度</h4><ul>\n<li>我们通常使⽤困惑度（perplexity）来评价语⾔模型的好坏</li>\n<li>困惑度的基本思想是：<strong>给测试集的句子赋予较高概率值的语言模型较好，当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型在测试集上的概率越高越好</strong></li>\n<li>现在给定测试集上的一个句子序列<script type=\"math/tex\">w_1, ..., w_t</script>，那么其困惑度为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nPP(w_1, ..., w_t) = P(w_1, ..., w_t)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P\\left(w_{1} w_{2} \\ldots w_{N})\\right.}}</script><p>可以看到，我们的<strong>目的是最大化获得测试集上的句子的概率</strong>，而最小化困惑度其实就是最大化这个概率，另外这里使用了一个几何平均，其目的是：</p>\n<blockquote>\n<ol>\n<li>因为每个字符的概率必然小于1，所以越长的句子的概率在连乘的情况下必然越小，所以为了对长短句公平，需要平均一下</li>\n<li>因为<strong>几何平均数</strong>的特点是，如果有其中的一个概率是很小的，那么最终的结果就不可能很大，从而要求好的句子的每个字符（即每个时间步的输出）都要有基本让人满意的概率</li>\n</ol>\n</blockquote>\n<ul>\n<li>采用unigram，假设每个时间步的预测是相互独立的：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nPP(w_1, ..., w_t) = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^NP(w_i)}}</script><p>这是uni-gram（即当前词不取决于前面的词）的困惑度计算公式，针对不同近似的N元语法又不同的计算公式，比如bi-gram、tri-gram等。另外，里面的<script type=\"math/tex\">P(w_i)</script>直接取输出层经过Softmax的概率值即可</p>\n<ul>\n<li>特别的：</li>\n</ul>\n<ol>\n<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1</li>\n<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷</li>\n<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数</li>\n</ol>\n<p><strong>显然，任何一个有效模型的困惑度必须小于类别个数（要不然模型的预测效果还不如随机点一个类别来的好）</strong></p>\n<h4 id=\"2-3-4-通过时间反向传播\"><a href=\"#2-3-4-通过时间反向传播\" class=\"headerlink\" title=\"2.3.4 通过时间反向传播\"></a>2.3.4 通过时间反向传播</h4><ul>\n<li>我们需要将循环神经网络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应⽤反向传播计算并存储梯度</li>\n<li>简单起⻅，我们考虑⼀个⽆偏差项的循环神经网络，且激活函数为恒等映射$（\\phi(x) = x）$。设时间步t的输⼊为单样本$x_t \\in R^d$，标签为$y_t$，那么隐藏状态$h_t \\in R^h$的计算表达式为:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh_t = W_{hx}X_t + W_{hh}h_{t-1}</script><p>输出层变量$o_t \\in \\mathbb{R}^q$为：</p>\n<script type=\"math/tex; mode=display\">\no_t = W_{qh}h_t</script><p>设时间步t的损失为$\\ell(o_t, y_t)$。则总时间步数为T的损失函数L定义为：</p>\n<script type=\"math/tex; mode=display\">\nL = \\frac{1}{T}\\sum_{t = 1}^T \\ell(o_t, y_t)</script><ul>\n<li><p>我们假设一共有3个时间步数，那么可以做出计算图：<br><img src=\"https://s2.loli.net/2021/12/10/Feq2PM3nCslpGO7.png\" alt=\"image-20211210132156070\"></p>\n</li>\n<li><p>现在我们开始反向传播：</p>\n</li>\n</ul>\n<p>易得：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}=\\frac{\\partial \\ell\\left(\\boldsymbol{o}_{t}, y_{t}\\right)}{T \\cdot \\partial \\boldsymbol{o}_{t}}</script><p>然后可以得到：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{q h}}=\\sum_{t=1}^{T} \\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}, \\frac{\\partial \\boldsymbol{o}_{t}}{\\partial \\boldsymbol{W}_{q h}}\\right)=\\sum_{t=1}^{T} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}} \\boldsymbol{h}_{t}^{\\top} .</script><p>接下来就是计算关于<script type=\"math/tex\">W_{hx}</script>和<script type=\"math/tex\">W_{hh}</script>的梯度，那么必然要求关于各时间步隐藏状态的梯度，我们注意到隐藏状态之间也存在依赖关系，，L只通过<script type=\"math/tex\">o_T</script>依赖最终时间步T的隐藏状态<script type=\"math/tex\">h_T</script>。因此，我们先计算⽬标函数有关最终时间步隐藏状态的梯度：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{T}}=\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{T}}, \\frac{\\partial \\boldsymbol{o}_{T}}{\\partial \\boldsymbol{h}_{T}}\\right)=\\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T}}</script><p>接下来对于时间步t &lt; T，在图6.3中，$L$通过$h_{t+1}$和$o_t$依赖$h_t$。依据链式法则，⽬标函数有关时间步t &lt; T的隐藏状态的梯度$\\partial L / \\partial h_t \\in R^h$需要按照时间步从⼤到小依次计算:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t}}=\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}, \\frac{\\partial \\boldsymbol{h}_{t+1}}{\\partial \\boldsymbol{h}_{t}}\\right)+\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}, \\frac{\\partial \\boldsymbol{o}_{t}}{\\partial \\boldsymbol{h}_{t}}\\right)=\\boldsymbol{W}_{h h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}+\\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}} .</script><p>对次递归公式展开，我们可以得到对任意时间步1 ≤ t ≤ T，我们可以得到⽬标函数有关隐藏状态梯度的通项公式：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t}}=\\sum_{i=t}^{T}\\left(\\boldsymbol{W}_{h h}^{\\top}\\right)^{T-i} \\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T+t-i}}</script><p>通过关于每个时间步隐藏状态的梯度，易求得关于<script type=\"math/tex\">W_{hx}</script>和<script type=\"math/tex\">W_{hh}</script>的梯度。</p>\n<ul>\n<li><strong>由上式中的指数项可⻅，当时间步数T较⼤或者时间步t较小时，⽬标函数有关隐藏状态的梯度较容易出现衰减和爆炸</strong></li>\n<li>RNN和一般的网络一样，我们在正向传播和反向传播的时候会进行一些数据的储存，避免重复计算</li>\n</ul>\n<h4 id=\"2-3-5-激活函数的选择\"><a href=\"#2-3-5-激活函数的选择\" class=\"headerlink\" title=\"2.3.5 激活函数的选择\"></a>2.3.5 激活函数的选择</h4><ul>\n<li><p>对于下一个时间步隐藏状态的计算，激活函数一般选用Tanh或者ReLu。对输出的运算，一般做softmax运算。</p>\n</li>\n<li><p>但是很多时候还是选用Tanh而不是ReLu，这一点在Hinton的<a href=\"https://arxiv.org/abs/1504.00941\">IRNN论文</a>里面是很明确的提到的：</p>\n</li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_720w.jpg?source=1940ef5c\" alt=\"img\"></p>\n<p><strong>也就是说在RNN中直接把激活函数换成ReLu会导致非常大的输出值，因此它们可能更容易发生爆炸</strong></p>\n<ul>\n<li>下面具体来看一下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\text { net }_{t}=U x_{t}+W h_{t-1} \\\\\nh_{t}=f\\left(\\text { net }_{t}\\right)\n\\end{array}</script><p><strong>假设ReLu函数一直处于激活区域（即输入大于0）</strong>，则有<script type=\"math/tex\">f(x) = x, net_t = Ux_t + W(Ux_{t-1} + Wh_{t-2})</script>，继续将其展开，最终会包含多个W的连乘，如果W不是单位矩阵， 那么最终的结果将会区域0或无穷。</p>\n<p>但是在CNN中就不会发生这种问题，因为CNN中每一层的W是不同的，并且在初始化时它们是独立同分布的，因此可以相互抵消，在多层之后一般不会出现这种严重的数值问题</p>\n<p>我们再来看看反向传播的时候：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_t}{\\partial h_{t-1}} = W</script><p>所以：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_t}{\\partial h_{1}} = W^n</script><p>可以看到只要W不是单位矩阵，梯度会出现消失或者爆炸的现象</p>\n<p>而在使用tanh作为激活函数的时候：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_{t}}{\\partial h_{t-1}}=\\left(1-h_{t}^{2}\\right) W</script><p>可以证明，固定<script type=\"math/tex\">h_{t-1}</script>时，<script type=\"math/tex\">\\left(1-h_{t}^{2}\\right) W</script>是有界的（这个界可能不是1，但总归是有界的）</p>\n<ul>\n<li>综上所述：当采用ReLu作为激活函数时，只有W在单位矩阵附近时才能取得比较好的结果。<strong>而使用tanh函数，每个相邻时间步之间的梯度是有界的，减少了一些梯度爆炸的可能性（注意是梯度爆炸，而没有解决梯度消失）。</strong>但是ReLu也拥有自己的优点，那就是计算量更小，收敛更快，其实relu也可以配合梯度裁剪来使用，但是终归没有tanh好</li>\n</ul>\n<h3 id=\"2-4-梯度裁剪\"><a href=\"#2-4-梯度裁剪\" class=\"headerlink\" title=\"2.4 梯度裁剪\"></a>2.4 梯度裁剪</h3><ul>\n<li>循环神经网络中较容易出现梯度衰减或梯度爆炸。<strong>为了应对梯度爆炸</strong>，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成⼀个向量 $g$，并设裁剪的阈值是$\\theta$。裁剪后的梯度：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min \\left(\\frac{\\theta}{\\|g\\|}, 1\\right) g</script><p>的$L_2$范数不超过$\\theta$</p>\n<ul>\n<li><strong>但是梯度裁剪无法应对梯度衰减</strong></li>\n</ul>\n<h1 id=\"3-门控循环单元（GRU）\"><a href=\"#3-门控循环单元（GRU）\" class=\"headerlink\" title=\"3 门控循环单元（GRU）\"></a>3 门控循环单元（GRU）</h1><ul>\n<li>我们发现，当时间步数较⼤或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但⽆法解决梯度衰减的问题。通常由于这个原因，<strong>循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系</strong></li>\n<li>门控循环神经网络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较⼤的依赖关系。它通过可以学习的⻔来控制信息的流动。其中，门控循环单元（gated recurrent unit，GRU）是⼀种常⽤的⻔控循环神经网</li>\n</ul>\n<h3 id=\"3-1-重置门和更新门\"><a href=\"#3-1-重置门和更新门\" class=\"headerlink\" title=\"3.1 重置门和更新门\"></a>3.1 重置门和更新门</h3><ul>\n<li>⻔控循环单元中的重置⻔和更新⻔的输⼊均为当前时间步输⼊<script type=\"math/tex\">X_t</script>与上⼀时间步隐藏状态<script type=\"math/tex\">H_{t−1}</script>，输出由激活函数为sigmoid函数的全连接层计算得到。</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20211210135706726.png\" alt=\"image-20211210135706726\"></p>\n<p>具体来说，重置门$R_t \\in \\mathbb{R}^{n \\times h}$和更新门$Z_t \\in \\mathbb{R}^{n \\times h}$的计算如下：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\boldsymbol{R}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x r}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h r}+\\boldsymbol{b}_{r}\\right) \\\\\n\\boldsymbol{Z}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x z}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h z}+\\boldsymbol{b}_{z}\\right)\n\\end{array}</script><h3 id=\"3-2-候选隐藏状态\"><a href=\"#3-2-候选隐藏状态\" class=\"headerlink\" title=\"3.2 候选隐藏状态\"></a>3.2 候选隐藏状态</h3><ul>\n<li><p>我们将当前时间步重置⻔的输出与上⼀时间步隐藏状态做按元素乘法（符号为$\\odot$）。如果重置⻔中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上⼀时间步的隐藏状态。如果元素值接近1，那么表⽰保留上⼀时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输⼊连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态</p>\n</li>\n<li><p>具体来说，时间步t的候选隐藏状态$\\tilde{H}_t \\in \\mathbb{R}^{n \\times h}$的计算为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\tilde{\\boldsymbol{H}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\left(\\boldsymbol{R}_{t} \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right),</script><ul>\n<li><p><strong>重置门控制了上⼀时间步的隐藏状态如何流⼊当前时间步的候选隐藏状态。而上⼀时间步的隐藏状态可能包含了时间序列截⾄上⼀时间步的全部历史信息。因此，重置门可以⽤来丢弃与预测⽆关的历史信息</strong></p>\n</li>\n<li><p>最后，时间步t的隐藏状态<script type=\"math/tex\">H_t \\in \\mathbb{R}^{n×h}</script>的计算使⽤当前时间步的更新门<script type=\"math/tex\">Z_t</script>来对上⼀时间步的隐藏状态<script type=\"math/tex\">H_{t−1}</script>和当前时间步的候选隐藏状态<script type=\"math/tex\">\\tilde{H}_t</script>做组合：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{H}_{t}=\\boldsymbol{Z}_{t} \\odot \\boldsymbol{H}_{t-1}+\\left(1-\\boldsymbol{Z}_{t}\\right) \\odot \\tilde{\\boldsymbol{H}}_{t}</script><p><img src=\"https://s2.loli.net/2021/12/10/L7O69pm3Yjzbno5.png\" alt=\"image-20211210141824572\"></p>\n<ul>\n<li><strong>更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新</strong>。假设更新⻔在时间步$t’$到$t（t’ &lt; t）$之间⼀直近似1。那么，<strong>在时间步$t’$到$t$之间的输⼊信息⼏乎没有流⼊时间步$t$的隐藏状态$H_t$</strong>。实际上，<strong>这可以看作是较早时刻的隐藏状态$H_{t’−1}$⼀直通过时间保存并传递⾄当前时间步$t$</strong>。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较⼤的依赖关系</li>\n<li>我们稍作总结：</li>\n</ul>\n<ol>\n<li>重置⻔有助于捕捉时间序列⾥短期的依赖关系</li>\n<li>更新⻔有助于捕捉时间序列⾥⻓期的依赖关系</li>\n<li>重置门和更新门为0、1的情况：</li>\n</ol>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>对应情况</th>\n<th>重置门</th>\n<th>更新门</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>退化为一般的RNN</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>丢弃当前时间步的全部信息，只保留历史信息</td>\n<td>0或1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>完全丢弃历史信息，只保留当前时间步的信息</td>\n<td>0</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h1 id=\"4-长短期记忆（LSTM）\"><a href=\"#4-长短期记忆（LSTM）\" class=\"headerlink\" title=\"4 长短期记忆（LSTM）\"></a>4 长短期记忆（LSTM）</h1><ul>\n<li>还有另外一种十分常用的门控循环神经网络：⻓短期记忆（long short-term memory，LSTM）</li>\n<li><strong>LSTM之所以叫做长短期记忆，是因为里面的两个记忆变量：隐藏状态<script type=\"math/tex\">H_t</script>负责短期记忆，记忆细胞<script type=\"math/tex\">C_t</script>负责长期记忆（因为每个时间步中<script type=\"math/tex\">H_t</script>相比于<script type=\"math/tex\">C_t</script>更新的更多，所以负责短期）</strong></li>\n</ul>\n<h3 id=\"4-1-输⼊门、遗忘门、输出门和候选记忆细胞\"><a href=\"#4-1-输⼊门、遗忘门、输出门和候选记忆细胞\" class=\"headerlink\" title=\"4.1 输⼊门、遗忘门、输出门和候选记忆细胞\"></a>4.1 输⼊门、遗忘门、输出门和候选记忆细胞</h3><ul>\n<li>与门控循环单元中的重置门和更新门⼀样，⻓短期记忆的门的输⼊均为当前时间步输⼊<script type=\"math/tex\">X_t</script>与上⼀时间步隐藏状态<script type=\"math/tex\">H_{t−1}</script>，输出由激活函数为sigmoid函数的全连接层计算得到</li>\n<li>⻓短期记忆需要计算候选记忆细胞$\\tilde{C}_t$。它的计算与上⾯介绍的3个⻔类似，但使⽤了值域在[−1, 1]的tanh函数作为激活函数:</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/jzn954wmEqRfyIg.png\" alt=\"image-20211210144009641\"></p>\n<ul>\n<li>时间步t的输⼊⻔$I_t \\in R^{n×h}$、遗忘⻔$F_t \\in R^{n×h}$和输出⻔$O_t \\in R^{n×h}$分别计算如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\boldsymbol{I}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x i}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h i}+\\boldsymbol{b}_{i}\\right), \\\\\n\\boldsymbol{F}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x f}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h f}+\\boldsymbol{b}_{f}\\right), \\\\\n\\boldsymbol{O}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x o}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h o}+\\boldsymbol{b}_{o}\\right), \\\\\n\\tilde{\\boldsymbol{C}}_{t}&=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x c}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h c}+\\boldsymbol{b}_{c}\\right),\n\\end{aligned}</script><h3 id=\"4-2-记忆细胞\"><a href=\"#4-2-记忆细胞\" class=\"headerlink\" title=\"4.2 记忆细胞\"></a>4.2 记忆细胞</h3><ul>\n<li>当前时间步记忆细胞$C_t \\in R^{n\\times h}$的计算组合了上⼀时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘⻔和输⼊⻔来控制信息的流动：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{C}_{t}=\\boldsymbol{F}_{t} \\odot \\boldsymbol{C}_{t-1}+\\boldsymbol{I}_{t} \\odot \\tilde{\\boldsymbol{C}}_{t}</script><ul>\n<li>遗忘门控制上⼀时间步的记忆细胞$C_{t−1}$中的信息是否传递到当前时间步，而输⼊门则控制当前时间步的输⼊$X_t$通过候选记忆细胞$\\tilde{C}_t$如何流⼊当前时间步的记忆细胞。如果遗忘门⼀直近似1且输⼊门⼀直近似0，过去的记忆细胞将⼀直通过时间保存并传递⾄当前时间步</li>\n</ul>\n<h3 id=\"4-3-隐藏状态\"><a href=\"#4-3-隐藏状态\" class=\"headerlink\" title=\"4.3 隐藏状态\"></a>4.3 隐藏状态</h3><ul>\n<li>有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$H_t \\in R^{n×h}$的信息的流动：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH_t = O_t \\odot tanh(C_t)</script><ul>\n<li>当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使⽤；当输出门近似0时，记忆细胞信息只⾃⼰保留</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/mlYhrVbA47MByvG.png\" alt=\"image-20211210145754660\"></p>\n<h3 id=\"4-4-三种门的作用\"><a href=\"#4-4-三种门的作用\" class=\"headerlink\" title=\"4.4 三种门的作用\"></a>4.4 三种门的作用</h3><ul>\n<li><strong>输入门控制当前计算的新状态以多大程度更新到记忆单元中</strong></li>\n<li><strong>遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉</strong></li>\n<li><strong>输出门控制当前的输出有多大程度取决于当前的记忆单元</strong></li>\n<li>在一个训练好的网络中：</li>\n</ul>\n<ol>\n<li>当输入的序列中没有重要信息时，LSTM的遗忘门接近于1，输入门接近于0，此时过去的记忆会被保存，从而实现了长期记忆功能</li>\n<li>当输入的序列中出现了重要的信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近1</li>\n<li>当输入的序列中出现了重要的信息，且该信息意味着之前的记忆不再重要时，输入门的值接近1，而遗忘门的值接近于0，这样旧的记忆被遗忘，新的重要信息被记忆</li>\n</ol>\n<h3 id=\"4-5-LSTM各模块激活函数\"><a href=\"#4-5-LSTM各模块激活函数\" class=\"headerlink\" title=\"4.5 LSTM各模块激活函数\"></a>4.5 LSTM各模块激活函数</h3><ul>\n<li>LSTM中，三种门都用的Sigmoid函数，生成候选记忆时，使用Tanh作为激活函数。值得注意的是，<strong>这两个激活函数都是饱和的</strong>，也就是说在输入达到一定值的情况下，输出就不发生明显变化了。如果使用的是非饱和的激活函数，例如ReLu，将难以实现门控的效果</li>\n<li>Sigmoid的输出在0到1之间，符合门控的物理定义。且当输入较大或较小时，输出会非常接近1或0，从而保证门开或门关</li>\n<li>在生成候选记忆时，使用Tanh函数，是因为其输出在-1到1之间，这与大多数场景下特征分布是zero-centered吻合</li>\n<li>此为Tanh函数在输入为0附近相比于Sigmoid有更大的梯度，通常收敛更快</li>\n</ul>\n<h1 id=\"5-门控机制是如何解决RNN的问题的\"><a href=\"#5-门控机制是如何解决RNN的问题的\" class=\"headerlink\" title=\"5 门控机制是如何解决RNN的问题的\"></a>5 门控机制是如何解决RNN的问题的</h1><ul>\n<li>上面我们说到，RNN最大的一个问题就是无法捕捉长距离依赖，而引入门控机制就是为了解决这个问题，这里我们以LSTM举例：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nf_{t} &=\\sigma\\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\\right) \\\\\ni_{t} &=\\sigma\\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\\right) \\\\\no_{t} &=\\sigma\\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\\right) \\\\\n\\hat{c}_{t} &=\\tanh \\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\\right) \\\\\nc_{t} &=f_{t} \\circ c_{t-1}+i_{t} \\circ \\hat{c}_{t} \\\\\nh_{t} &=o_{t} \\circ \\tanh \\left(c_{t}\\right)\n\\end{aligned}</script><ul>\n<li>我们可以像上面2.3.5激活函数的选择中一样，探讨<script type=\"math/tex\">\\frac{\\partial h_t}{\\partial h_{t-1}}</script>，但是因为<script type=\"math/tex\">h_{t} =o_{t} \\circ \\tanh(c_t)</script>，所以同样可以分析<script type=\"math/tex\">\\frac{c_t}{c_{t-1}}</script>，而后者更简单些，所以分析后者：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial c_{t}}{\\partial c_{t-1}}=f_{t}+c_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}}+\\hat{c} t \\frac{\\partial i_{t}}{\\partial c_{t-1}}+i_{t} \\frac{\\partial \\hat{c}_{t}}{\\partial c_{t-1}}</script><ul>\n<li><p>上式总共有4项，<strong>梯度主要取决于第一项<script type=\"math/tex\">f_t</script></strong>，由于<script type=\"math/tex\">f_t \\in (0,1)</script>，所以梯度爆炸的风险减小，而是否会发生梯度小时，取决于每个时间步的<script type=\"math/tex\">f_t</script>是否接近1，但是这里有个自洽的结论：<strong>如果我们的任务比较依赖于历史信息，那么<script type=\"math/tex\">f_t</script>就会接近于1，这时候历史的梯度信息也正好不容易消失；如果<script type=\"math/tex\">f_t</script>很接近于0，那么就说明我们的任务不依赖于历史信息，这时候就算梯度消失也无妨了。</strong></p>\n</li>\n<li><p>而后面3项都不是决定梯度的主要因素，挑<script type=\"math/tex\">c_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}}</script>来说明，带入<script type=\"math/tex\">h_{t-1} =o_{t-1} \\circ \\tanh(c_{t-1})</script>，可以得到：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nc_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}} = c_{t-1} \\frac{\\partial f_{t}}{\\partial h_{t-1}}  \\frac{\\partial h_{t-1}}{\\partial c_{t-1}} = f_{t}\\left(1-f_{t}\\right) o_{t-1}\\left(1-\\tanh ^{2} c_{t-1}\\right) c_{t-1} U_{f}</script><ul>\n<li><p>由于<script type=\"math/tex\">f_t,(1-f_t),o_{t-1} \\in (0,1)</script>，并且可以证明<script type=\"math/tex\">|\\left(1-\\tanh ^{2} c_{t-1}\\right)c_{t-1}| <0.45</script>，所以结果相当于<script type=\"math/tex\">U_f</script>乘上四个门，结果会非常的小，所以不起主导作用。其他两项是同样的方法</p>\n</li>\n<li><p><strong>注意：LSTM和GRU只是很大的缓解了梯度消失，但是如果时间步很长，成千上万步，仍然会发生梯度消失，毕竟记忆单元压根存不了那么多信息</strong></p>\n</li>\n<li><p>另外，一般LSTM要比GRU要好一些，但是到底好在哪里呢，在GRU中同样进行反向传播的分析：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial h_{t}}{\\partial h_{t-1}}=& 1-z_{t}-z_{t}\\left(1-z_{t}\\right) h_{t-1} U_{z}+z_{t}\\left(1-z_{t}\\right) \\hat{h}_{t} U_{z} \\\\\n&+\\left(1-\\hat{h}_{t}^{2}\\right) r_{t}\\left(1+\\left(1-r_{t}\\right) h_{t-1} U_{r}\\right) z_{t} U_{h}\n\\end{aligned}</script><ul>\n<li>上式其主导作用的同样是<script type=\"math/tex\">1-z_t</script>，但是后面的项相比于LSTM只有3个门的连乘（LSTM有4个），所以这些想对梯度的影响可能大一些，所以<strong>LSTM的梯度要比GRU稳定一些，所以GRU更依赖于初始化</strong></li>\n</ul>\n<h1 id=\"6-深度循环神经网络\"><a href=\"#6-深度循环神经网络\" class=\"headerlink\" title=\"6 深度循环神经网络\"></a>6 深度循环神经网络</h1><ul>\n<li>到⽬前为⽌介绍的循环神经网络只有⼀个单向的隐藏层，在深度学习应⽤⾥，我们通常会⽤到含有多个隐藏层的循环神经网络，也称作深度循环神经网络</li>\n<li>下图演⽰了⼀个有L个隐藏层的深度循环神经网络，<strong>每个隐藏状态不断传递⾄当前层的下⼀时间步和当前时间步的下⼀层</strong></li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/PUK7CRjyemEbzLM.png\" alt=\"image-20211210150125737\"></p>\n<ul>\n<li>第1隐藏层的隐藏状态和之前的计算⼀样：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{H}_{t}^{(1)}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(1)}+\\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{h h}^{(1)}+\\boldsymbol{b}_{h}^{(1)}\\right)</script><ul>\n<li>当1 &lt; l ≤ L时，第l隐藏层的隐藏状态的表达式为:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{H}_{t}^{(l)}=\\phi\\left(\\boldsymbol{H}_{t}^{(l-1)} \\boldsymbol{W}_{x h}^{(l)}+\\boldsymbol{H}_{t-1}^{(l)} \\boldsymbol{W}_{h h}^{(l)}+\\boldsymbol{b}_{h}^{(l)}\\right)</script><ul>\n<li>最终，输出层的输出只需基于第L隐藏层的隐藏状态：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t}^{(L)} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}</script><ul>\n<li><p>同多层感知机⼀样，隐藏层个数L是超参数</p>\n</li>\n<li><p><strong>RNN中选择深度网络的原因和一般前馈神经网络一样，都是为了能够用来表征更复杂的情况</strong></p>\n</li>\n</ul>\n<h1 id=\"7-双向循环神经网络\"><a href=\"#7-双向循环神经网络\" class=\"headerlink\" title=\"7 双向循环神经网络\"></a>7 双向循环神经网络</h1><ul>\n<li>之前介绍的循环神经网络模型都是假设当前时间步是由前⾯的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后⾯时间步决定。例如，当我们写下⼀个句⼦时，可能会根据句⼦后⾯的词来修改句⼦前⾯的⽤词</li>\n<li><strong>双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息</strong>，下图演⽰了⼀个含单隐藏层的双向循环神经网络的架构</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/VtFmigZ58qw2aeb.png\" alt=\"image-20211210151934211\"></p>\n<ul>\n<li>具体来看，设时间步t正向隐藏状态为$\\overrightarrow{H}_t \\in R^{n×h}$（正向隐藏单元个数为h），反向隐藏状态为$\\overleftarrow{H}_t \\in R^{n×h}$（反向隐藏单元个数为h）。我们可以分别计算正向隐藏状态和反向隐藏状态：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\overrightarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(f)}+\\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{h h}^{(f)}+\\boldsymbol{b}_{h}^{(f)}\\right) \\\\\n\\overleftarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(b)}+\\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{h h}^{(b)}+\\boldsymbol{b}_{h}^{(b)}\\right)\n\\end{array}</script><p><strong>然后我们连结两个⽅向的隐藏状态$\\overrightarrow{H}_t$和 $\\overleftarrow{H}_t$来得到隐藏状态$H_t \\in R^{n×2h}$</strong>，并将其输⼊到输出层。 输出层计算输出$O_t \\in R^{n×q}$（输出个数为q）：</p>\n<script type=\"math/tex; mode=display\">\nO_t = H_tW_{hq} + b_q</script><ul>\n<li><strong>不同方向的隐藏状态的隐藏单元个数也可以不同</strong></li>\n<li>双向循环神经网络在每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊）</li>\n</ul>\n<h1 id=\"8-Seq2Seq模型\"><a href=\"#8-Seq2Seq模型\" class=\"headerlink\" title=\"8 Seq2Seq模型\"></a>8 Seq2Seq模型</h1><ul>\n<li>在许多时候，输入和输出都是不定长序列，这采用一般的RNN肯定是行不通的。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，例如：英语输⼊：“They”、“are”、“watching”、“.” ； 法语输出：“Ils”、“regardent”、“.”</li>\n<li>当输⼊和输出都是不定⻓序列时，我们可以使⽤seq2seq模型，本质上都⽤到了两个循环神经网络，分别叫做编码器和解码器。<strong>编码器用来分析输⼊序列，解码器⽤来生成输出序列</strong></li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/FQfVxdq2wuC7aSs.png\" alt=\"image-20211210170200879\"></p>\n<ul>\n<li>&lt; bos &gt;（beginning of sequence）和 &lt; eos &gt;（end of sequence）分别表示序列的开始和结束</li>\n<li>编码器每个时间步的输⼊依次为英语句⼦中的单词、标点和特殊符号。上图中使⽤了编码器在最终时间步的隐藏状态作为输⼊句⼦的表征或编码信息。解码器在各个时间步中使⽤输⼊句⼦的编码信息和上个时间步的输出以及隐藏状态作为输⼊</li>\n</ul>\n<h3 id=\"8-1-编码器\"><a href=\"#8-1-编码器\" class=\"headerlink\" title=\"8.1 编码器\"></a>8.1 编码器</h3><ul>\n<li><strong>编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量$c$，并在该背景变量中编码输⼊序列信息</strong>。编码器可以使⽤循环神经网络</li>\n<li>编码器通过⾃定义函数q将各个时间步的隐藏状态变换为背景变量</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nc = q(h_1, ...,h_T)</script><ul>\n<li><strong>也可以使⽤双向循环神经网络构造编码器</strong>，在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊），并编码了整个序列的信息。</li>\n</ul>\n<h3 id=\"8-2-解码器\"><a href=\"#8-2-解码器\" class=\"headerlink\" title=\"8.2 解码器\"></a>8.2 解码器</h3><ul>\n<li>解码器就是通过编码器输出的背景向量$c$，和每一个时间步的输入，输出所需要的结果</li>\n<li><strong>解码器在预测和训练时是不一样的</strong>，我们先介绍<strong>预测</strong>时的解码器：</li>\n</ul>\n<p>编码器输出的背景变量$c$编码了整个输⼊序列<script type=\"math/tex\">x_1, . . . , x_T</script>的信息。给定训练样本中的输出序列<script type=\"math/tex\">y_1, y_2, . . . , y_{T'}</script>，对每个时间步<script type=\"math/tex\">t'</script>（符号与输⼊序列或编码器的时间步<script type=\"math/tex\">t</script>有区别），解码器输出<script type=\"math/tex\">y_{t'}</script>的条件概率将基于之前的输出序列<script type=\"math/tex\">y_1, . . . , y_{t'−1}</script>和背景变量<script type=\"math/tex\">c</script>，即<script type=\"math/tex\">P(y_{t'} | y_1, . . . , y_{t'−1}, c)</script></p>\n<p>为此，我们可以使⽤另⼀个循环神经网络作为解码器。在输出序列的时间步<script type=\"math/tex\">t'</script>，解码器将上⼀时间步的输出<script type=\"math/tex\">y_{t'−1}</script>以及背景变量<script type=\"math/tex\">c</script>作为输⼊，并将它们与上⼀时间步的隐藏状态<script type=\"math/tex\">s_{t'−1}</script>变换为当前时间步的隐藏状态<script type=\"math/tex\">s_{t'}</script>：</p>\n<script type=\"math/tex; mode=display\">\ns_{t'} = g(y_{t' -1}, c, s_{t' - 1})</script><p>有了解码器的隐藏状态后，我们可以使⽤⾃定义的输出层和softmax运算来计算<script type=\"math/tex\">P(y_{t'} | y_1, . . . , y_{t'−1}, c)</script></p>\n<ul>\n<li><strong>训练</strong>时的解码器，每一时间步的输入序列可以是上一时间步的输出，也可以是上一时间步的真实标签序列。后者叫做<strong>强制教学</strong></li>\n</ul>\n<p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP\\left(y_{1}, \\ldots, y_{T^{\\prime}} \\mid x_{1}, \\ldots, x_{T}\\right) &=\\prod_{t^{\\prime}=1}^{T^{\\prime}} P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, x_{1}, \\ldots, x_{T}\\right) \\\\\n&=\\prod_{t^{\\prime}=1}^{T^{\\prime}} P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)\n\\end{aligned}</script><p>并得到该输出序列的损失：</p>\n<script type=\"math/tex; mode=display\">\n-\\log P\\left(y_{1}, \\ldots, y_{T^{\\prime}} \\mid x_{1}, \\ldots, x_{T}\\right)=-\\sum_{t^{\\prime}=1}^{T^{\\prime}} \\log P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)</script><p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数</p>\n<h3 id=\"8-3-预测时的搜索方式\"><a href=\"#8-3-预测时的搜索方式\" class=\"headerlink\" title=\"8.3 预测时的搜索方式\"></a>8.3 预测时的搜索方式</h3><h4 id=\"8-3-1-贪婪搜索\"><a href=\"#8-3-1-贪婪搜索\" class=\"headerlink\" title=\"8.3.1 贪婪搜索\"></a>8.3.1 贪婪搜索</h4><ul>\n<li>贪婪搜索（greedy search）。对于输出序列任⼀时间步$t’$，我 们从$|Y|$（Y为词典）个词中搜索出条件概率最⼤的词：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ny_{t'} = argmax_{y \\in Y}P(y|y_1, ..., y_{t' - 1}, c)</script><p>作为输出。⼀旦搜索出“&lt; eos &gt;”符号，或者输出序列⻓度已经达到了最⼤⻓度$T’$，便完成输出。</p>\n<ul>\n<li>我们将该条件概率最⼤的输出序列称为<strong>最优输出序列</strong>，<strong>而贪婪搜索无法保证得到最优输出序列</strong>，下面举个栗子：</li>\n</ul>\n<blockquote>\n<p>假设输出词典⾥⾯有“A” “B” “C” 和“&lt; eos &gt;”这4个词。下图中每个时间步下的4个数字分别代表了该时间步⽣成“A” “B” “C” 和“&lt; eos &gt;”这4个词的条件概率。在每个 间步，贪婪搜索选取条件概率最⼤的词。因此，将⽣成输出序列“A” “B” “C” “&lt; eos &gt;”。 该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 = 0.048</p>\n</blockquote>\n<p><img src=\"https://s2.loli.net/2021/12/10/JUgILXFCjWviaQw.png\" alt=\"image-20211210180148791\"></p>\n<blockquote>\n<p>但是现在我们考虑下面一种情况，在时间步2中选取了条件概率第⼆⼤的词“C”，<strong>由于之后时间步的概率值是基于前面的时间步的，所以时间步2结果选取的改变，会导致后续的概率发生改变</strong>，如下图，所以我们现在的输出序列“A” “C” “B” “&lt; eos &gt;”的条件概率是0.5 × 0.3 × 0.6 × 0.6 = 0.054，大于贪婪搜索的概率，所以贪婪搜索得到的不是最优的</p>\n</blockquote>\n<p><img src=\"https://s2.loli.net/2021/12/10/zSKDLamFWqseyuN.png\" alt=\"image-20211210180404095\"></p>\n<h4 id=\"8-3-2-穷举搜索\"><a href=\"#8-3-2-穷举搜索\" class=\"headerlink\" title=\"8.3.2 穷举搜索\"></a>8.3.2 穷举搜索</h4><ul>\n<li>我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最⼤的序列</li>\n<li><strong>虽然穷举搜索可以得到最优输出序列，但它的计算开销$O(|Y|^{T’})$很容易过⼤</strong>，而贪婪搜索的开销为$O(|Y|T’)$，明显小于穷举搜索</li>\n</ul>\n<h4 id=\"8-3-3-束搜索\"><a href=\"#8-3-3-束搜索\" class=\"headerlink\" title=\"8.3.3 束搜索\"></a>8.3.3 束搜索</h4><ul>\n<li><p>束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个<strong>束宽</strong>（beam size）超参数。我们将它设为k。在时间步1时，选取当前时间步条件概率最⼤的k个词，分别组成k个候选输出序列的⾸词。在之后的每个时间步，基于上个时间步的k个候选输出序列，从k$ |Y|$个可能的输出序列中选取条件概率最⼤的k个，作为该时间步的候选输出序列。最终，<strong>我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt; eos &gt;”的序列，并将它们中所有特殊符号“&lt; eos &gt;”后⾯的⼦序列舍弃</strong>，得到最终候选输出序列的集合</p>\n</li>\n<li><p>下面举个栗子：</p>\n</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/QEBb3gUVxmuDS7O.png\" alt=\"image-20211210182131362\"></p>\n<p>第一个时间步找出概率最大的”A”和”C”，然后再根据”A”和”C”，在时间步2中寻找概率最大的2个，分别为”AB”和”CE”，再根据这两个，在时间步3，输出”ABD”和”CED”，最后减去&lt; eos &gt;符号以后的内容，得出输出序列</p>\n<ul>\n<li>在最终候选输出序列的集合中，我们取以下分数最⾼的序列作为输出序列：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{L^{\\alpha}} \\log P\\left(y_{1}, \\ldots, y_{L}\\right)=\\frac{1}{L^{\\alpha}} \\sum_{t^{\\prime}=1}^{L} \\log P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)</script><p>其中$L$为最终候选序列⻓度，$\\alpha$⼀般可选为0.75。分⺟上的$L^{\\alpha}$是为了惩罚较⻓序列在以上分数中较多的对数相加项</p>\n<ul>\n<li>束搜索的计算开销为$O(k|Y|T’)$，介于贪婪搜索和穷举搜索之间</li>\n<li><strong>束搜索通过灵活的束宽来权衡计算开销和搜索质量</strong></li>\n</ul>\n<h3 id=\"8-4-注意力机制\"><a href=\"#8-4-注意力机制\" class=\"headerlink\" title=\"8.4 注意力机制\"></a>8.4 注意力机制</h3><h4 id=\"8-4-1-seq2seq中的注意力机制\"><a href=\"#8-4-1-seq2seq中的注意力机制\" class=\"headerlink\" title=\"8.4.1 seq2seq中的注意力机制\"></a>8.4.1 seq2seq中的注意力机制</h4><ul>\n<li><strong>解码器在⽣成输出序列中的每⼀个词时可能只需利⽤输⼊序列某⼀部分的信息， 而不需要由整个输入序列生成的背景向量</strong>，例如，在输出序列的时间步1，解码器可以主要依赖“They” “are” 的信息来⽣成“Ils”，在时间步2则主要使⽤来⾃“watching”的编码信息⽣成“regardent”，而不需要”They are watching”整个句子</li>\n<li><p>若没引入注意力机制，在实际使用中，那么会发现随着输入序列的增长，模型的性能发生了显著下降。<strong>这是因为编码时输入序列的全部信息压缩到了一个向量中，随着序列的增长，越前面的词丢失越严重</strong>。<strong>同时，seq2seq模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词对应的源语言词的上下文信息和位置信息在编解码过程中丢失了</strong></p>\n</li>\n<li><p>注意⼒机制通过对编码器所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量。解码器在每⼀时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量</p>\n</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/5kGV8uqeaYn3wlF.png\" alt=\"image-20211210184622950\"></p>\n<p>⾸先，函数a根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输⼊。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量</p>\n<ul>\n<li>具体来说，令编码器在时间步t的隐藏状态为$h_t$，且总时间步数为T。那么解码器在时间步$t’$（$t’$代表的是解码器的时间步数）的背景变量为所有编码器隐藏状态的加权平均：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{c}_{t^{\\prime}}=\\sum_{t=1}^{T} \\alpha_{t^{\\prime} t} \\boldsymbol{h}_{t}</script><p>其中给定$t’$时，权重$\\alpha_{t’t}$在$t = 1, . . . , T$的值是⼀个概率分布。为了得到概率分布，我们可以使⽤softmax运算:</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_{t^{\\prime} t}=\\frac{\\exp \\left(e_{t^{\\prime} t}\\right)}{\\sum_{k=1}^{T} \\exp \\left(e_{t^{\\prime} k}\\right)}, \\quad t=1, \\ldots, T</script><p>由于<script type=\"math/tex\">e_{t't}</script>同时取决于解码器的时间步<script type=\"math/tex\">t'</script>和编码器的时间步t，我们不妨以解码器在时间步<script type=\"math/tex\">t'− 1</script>的隐藏状态<script type=\"math/tex\">s_{t'−1}</script>与编码器在时间步t的隐藏状态<script type=\"math/tex\">h_t</script>为输⼊，并通过函数a计算<script type=\"math/tex\">e_{t't}</script>：</p>\n<script type=\"math/tex; mode=display\">\ne_{t't} = a(s_{t'-1}, h_t)</script><p>其中a是一个自定义函数，有多种选择。如果两个输⼊向量⻓度相同，⼀个简单的选择是计算它们的内积$a(s,h) = s^⊤h$</p>\n<ul>\n<li>我们对注意力机制可以有一个直观的理解：<strong>在生成一个输出词时，会考虑每一个输入词和当前输出词的对应关系，对齐越好的词，会有越大的权重</strong></li>\n<li>还可以结合双向RNN和注意力机制，每个隐状态包含了$\\overleftarrow{h_t}$和$\\overrightarrow{h_t}$</li>\n</ul>\n<h4 id=\"8-4-2-矢量化计算\"><a href=\"#8-4-2-矢量化计算\" class=\"headerlink\" title=\"8.4.2 矢量化计算\"></a>8.4.2 矢量化计算</h4><ul>\n<li><p>我们还可以对注意⼒机制采⽤更⾼效的⽮量化计算。⼴义上，注意⼒机制的输⼊包括<strong>查询项</strong>以及<strong>⼀⼀对应的键项和值项</strong>，其中值项是需要加权平均的⼀组项。在加权平均中，<strong>值项的权重来⾃查询项以及与该值项对应的键项的计算</strong></p>\n</li>\n<li><p>在上面的例⼦中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。假设编码器和解码器的隐藏单元个数均为h，且函数<script type=\"math/tex\">a(s, h) = s^ ⊤h</script>。若我们希望根据解码器单个隐藏状态<script type=\"math/tex\">s_{t'−1} \\in \\mathbb{R}^h</script>和编码器所有隐藏状态<script type=\"math/tex\">h_t \\in \\mathbb{R}^h , t = 1, . . . , T</script>来计算背景向量<script type=\"math/tex\">c_{t'} \\in \\mathbb{R}^h</script>。我们可以将查询项矩阵<script type=\"math/tex\">Q \\in R^{1\\times h}</script>设为<script type=\"math/tex\">s^⊤_{t'−1}</script>，并令键项矩阵<script type=\"math/tex\">K \\in \\mathbb{R}^{T\\times h}</script>和值项矩阵<script type=\"math/tex\">V \\in \\mathbb{R}^{T\\times h}</script>相同且第t⾏均为<script type=\"math/tex\">h^⊤_t</script> 。此时，我们只需要通过⽮量化计算：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nsoftmax(QK^T)V</script><p>即可算出转置后的背景向量<script type=\"math/tex\">c^⊤_{t'}</script>。当查询项矩阵Q的⾏数为n时，上式将得到n行的输出矩阵。输出矩阵与查询项矩阵在相同行上⼀⼀对应。</p>\n","site":{"data":{}},"wordcount":11656,"excerpt":"","more":"<ul>\n<li>与多层感知机和能有效<strong>处理空间信息</strong>的卷积神经网络不同，循环神经网络是为更好地<strong>处理时序信息</strong>而设计的。它<strong>引⼊状态变量来存储过去的信息</strong>，并⽤其<strong>与当前的输⼊共同决定当前的输出</strong></li>\n</ul>\n<h1 id=\"1-语言模型\"><a href=\"#1-语言模型\" class=\"headerlink\" title=\"1 语言模型\"></a>1 语言模型</h1><h3 id=\"1-1-语言模型的计算\"><a href=\"#1-1-语言模型的计算\" class=\"headerlink\" title=\"1.1 语言模型的计算\"></a>1.1 语言模型的计算</h3><ul>\n<li>我们可以把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。假设⼀段⻓度为 $T$ 的⽂本中的词 依次为 $w_1, w_2, . . . , w_T$，那么在离散的时间序列中，$w_t（1 ≤ t ≤ T）$可看作在时间步（time step）$ t$ 的输出或标签。给定⼀个⻓度为 $T$ 的词的序列 $w_1, w_2, . . . , w_T$，语⾔模型将计算该序列的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(w_1, w_2, ...,w_T)</script><ul>\n<li><strong>由于 $w_1, w_2, . . . , w_T$ 是依次生成的，所以有：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(w_1, w_2, ...,w_T) = \\prod_{t=1}^T P(w_t | w_1, ..., w_{t-1})</script><ul>\n<li>为了计算语⾔模型，我们需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即<strong>语⾔模型参数</strong>。<strong>词的概率可以通过该词在训练数据集中的相对词频来计算</strong></li>\n</ul>\n<h3 id=\"1-2-n元语法\"><a href=\"#1-2-n元语法\" class=\"headerlink\" title=\"1.2 n元语法\"></a>1.2 n元语法</h3><ul>\n<li><strong>当序列⻓度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加</strong></li>\n<li>n元语法通过⻢尔可夫假设（<strong>虽然并不⼀定成立</strong>）简化了语⾔模型的计算。这⾥的<strong>⻢尔可夫假设是指⼀个词的出现只与前⾯n个词相关</strong>，即n阶⻢尔可夫链（Markov chain of order n）</li>\n<li>如果基于$n-1$阶马尔科夫链，我们可以将语言模型改写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(w_1, w_2, ...,w_T) \\approx \\prod_{t=1}^T P(w_t | w_{t - (n - 1)}, ..., w_{t-1})</script><ul>\n<li><p>以上也叫<strong>n元语法</strong>（n-grams）。它是基于n−1阶⻢尔可夫链的概率语⾔模型</p>\n</li>\n<li><p>当n较小时，n元语法往往并不准确。当n较⼤时，n元语法需要计算并存储⼤量的词频和多词相邻频率。<strong>n权衡了计算复杂度和模型准确性</strong></p>\n</li>\n</ul>\n<h1 id=\"2-循环神经网络（RNN）\"><a href=\"#2-循环神经网络（RNN）\" class=\"headerlink\" title=\"2 循环神经网络（RNN）\"></a>2 循环神经网络（RNN）</h1><ul>\n<li>在n元语法中，时间步t的词 $w_t$ 基于前⾯所有词的条件概率只考虑了最近时间步的n − 1个词。如果要考虑⽐t − (n − 1)更早时间步的词对$w_t$的可能影响，我们需要增⼤n。但这样模型参数的数量将随之呈指数级增⻓</li>\n<li>但是对于循环神经网络，它<strong>并⾮刚性地记忆所有固定⻓度的序列，而是通过隐藏状态来存储之前时间步的信息</strong></li>\n</ul>\n<h3 id=\"2-1-循环神经网络\"><a href=\"#2-1-循环神经网络\" class=\"headerlink\" title=\"2.1 循环神经网络\"></a>2.1 循环神经网络</h3><ul>\n<li>假设<script type=\"math/tex\">X_t \\in \\mathbb{R}^{n \\times d}</script>是序列中时间步t的小批量输⼊， <script type=\"math/tex\">H_t \\in \\mathbb{R}^{n \\times h}</script>是该时间步的隐藏变量。与多层感知机不同的是，这⾥我们保存上⼀时间步的隐藏变量<script type=\"math/tex\">H_{t−1}</script>，并引⼊⼀个新的权重参数<script type=\"math/tex\">W_{hh} \\in \\mathbb{R}^{h×h}</script>，该参数⽤来描述在当前时间步如何使⽤上⼀时间步的隐藏变量。时间步t的隐藏变量的计算由当前时间步的输⼊和上⼀时间步的隐藏变量共同决定：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{H}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right)</script><p>$\\phi$是激活函数</p>\n<ul>\n<li><strong>这⾥的隐藏变量能够捕捉截⾄当前时间步的序列的历史信息</strong></li>\n<li>每个时间步还有一个对应的输出：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}</script><p>在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差</p>\n<ul>\n<li><strong>即便在不同时间步，循环神经网络也始终使⽤W， b这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增⻓</strong></li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/oAbwO43tDSTWpVg.png\" alt=\"image-20211210101227481\"></p>\n<ul>\n<li>隐藏状态中<script type=\"math/tex\">X_{t}W_{x h}+H_{t-1} W_{h h}</script>的计算等价于<script type=\"math/tex\">X_t</script>与<script type=\"math/tex\">H_{t−1}</script>连结后的矩阵乘以<script type=\"math/tex\">W_{xh}</script>与<script type=\"math/tex\">W_{hh}</script>连结后的矩阵，实际上在代码实现中基本都是这么做的</li>\n</ul>\n<ul>\n<li>举一个栗子：基于字符级循环神经网络的语⾔模型</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/OYZwANsxl2oEQaI.png\" alt=\"image-20211210101914126\"></p>\n<p><strong>标签序列依次为输入序列的下一个</strong>，并且在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差</p>\n<h3 id=\"2-2-时序数据的采样\"><a href=\"#2-2-时序数据的采样\" class=\"headerlink\" title=\"2.2 时序数据的采样\"></a>2.2 时序数据的采样</h3><ul>\n<li>时序数据的⼀个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想” “要” “有” “直” “升”。<strong>该样本的标签序列为这些字符分别在训练集中的下⼀个字符</strong>，即 “要” “有” “直” “升” “机”。</li>\n<li>我们有两种⽅式对时序数据进⾏采样，分别是随机采样和相邻采样</li>\n</ul>\n<h4 id=\"2-2-1-随机采样\"><a href=\"#2-2-1-随机采样\" class=\"headerlink\" title=\"2.2.1 随机采样\"></a>2.2.1 随机采样</h4><ul>\n<li><p>在随机采样中，每个样本是原始序列上任意截取的⼀段序列。</p>\n</li>\n<li><p>相邻的两个随机小批量在原始序列上的位置不⼀定相毗邻。因此，我们⽆法⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态。<strong>在训练模型时，每次随机采样前都需要重新初始化隐藏状态</strong></p>\n</li>\n</ul>\n<h4 id=\"2-2-2-相邻采样\"><a href=\"#2-2-2-相邻采样\" class=\"headerlink\" title=\"2.2.2 相邻采样\"></a>2.2.2 相邻采样</h4><ul>\n<li>除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就<strong>可以⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态</strong>，从而使下⼀个小批量的输出也取决于当前小批量的输⼊</li>\n</ul>\n<ul>\n<li><strong>两种采样方式的区别：</strong><ol>\n<li>采用相邻采样，在训练模型时，我们只需在每⼀个迭代周期开始时初始化隐藏状态</li>\n<li>当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同⼀迭代周期中，随着迭代次数的增加，梯度的<strong>计算开销会越来越⼤</strong>。为了使模型参数的梯度计算只依赖⼀次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来</li>\n<li><strong>相邻采样不能并行运算</strong>，因为必须等上一个小批量结束了，下一个小批量才能开始</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"2-3-RNN的训练、预测\"><a href=\"#2-3-RNN的训练、预测\" class=\"headerlink\" title=\"2.3 RNN的训练、预测\"></a>2.3 RNN的训练、预测</h3><h4 id=\"2-3-1-输入输出\"><a href=\"#2-3-1-输入输出\" class=\"headerlink\" title=\"2.3.1 输入输出\"></a>2.3.1 输入输出</h4><ul>\n<li>首先是RNN的输入输出，应该包含time_steps、batch_size、vocab_size（时间步、批量大小、词典大小）的维度，之所以有一个维度是词典大小，是因为我们会<strong>把输入输出用独热向量</strong>表示</li>\n</ul>\n<h4 id=\"2-3-2-预测\"><a href=\"#2-3-2-预测\" class=\"headerlink\" title=\"2.3.2 预测\"></a>2.3.2 预测</h4><ul>\n<li><p><strong>在训练时，下一个时间步的输入可以：</strong></p>\n<ol>\n<li>上一个时间步的标签</li>\n<li>上一个时间步的表征（也就是输出）</li>\n</ol>\n<p>采用第一种方法，<strong>更容易收敛，但是泛化能力更差</strong>，而第二种方法<strong>更不易收敛，但是泛化能力更强</strong></p>\n</li>\n<li><p><strong>但是在预测时</strong>，由于我们压根没有标签，所以只能用上述第二种方法。正是由于训练时和预测时干的事情都不一样，所以采用第一种方法泛化能力更差</p>\n</li>\n</ul>\n<h4 id=\"2-3-3-困惑度\"><a href=\"#2-3-3-困惑度\" class=\"headerlink\" title=\"2.3.3 困惑度\"></a>2.3.3 困惑度</h4><ul>\n<li>我们通常使⽤困惑度（perplexity）来评价语⾔模型的好坏</li>\n<li>困惑度的基本思想是：<strong>给测试集的句子赋予较高概率值的语言模型较好，当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型在测试集上的概率越高越好</strong></li>\n<li>现在给定测试集上的一个句子序列<script type=\"math/tex\">w_1, ..., w_t</script>，那么其困惑度为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nPP(w_1, ..., w_t) = P(w_1, ..., w_t)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P\\left(w_{1} w_{2} \\ldots w_{N})\\right.}}</script><p>可以看到，我们的<strong>目的是最大化获得测试集上的句子的概率</strong>，而最小化困惑度其实就是最大化这个概率，另外这里使用了一个几何平均，其目的是：</p>\n<blockquote>\n<ol>\n<li>因为每个字符的概率必然小于1，所以越长的句子的概率在连乘的情况下必然越小，所以为了对长短句公平，需要平均一下</li>\n<li>因为<strong>几何平均数</strong>的特点是，如果有其中的一个概率是很小的，那么最终的结果就不可能很大，从而要求好的句子的每个字符（即每个时间步的输出）都要有基本让人满意的概率</li>\n</ol>\n</blockquote>\n<ul>\n<li>采用unigram，假设每个时间步的预测是相互独立的：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nPP(w_1, ..., w_t) = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^NP(w_i)}}</script><p>这是uni-gram（即当前词不取决于前面的词）的困惑度计算公式，针对不同近似的N元语法又不同的计算公式，比如bi-gram、tri-gram等。另外，里面的<script type=\"math/tex\">P(w_i)</script>直接取输出层经过Softmax的概率值即可</p>\n<ul>\n<li>特别的：</li>\n</ul>\n<ol>\n<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1</li>\n<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷</li>\n<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数</li>\n</ol>\n<p><strong>显然，任何一个有效模型的困惑度必须小于类别个数（要不然模型的预测效果还不如随机点一个类别来的好）</strong></p>\n<h4 id=\"2-3-4-通过时间反向传播\"><a href=\"#2-3-4-通过时间反向传播\" class=\"headerlink\" title=\"2.3.4 通过时间反向传播\"></a>2.3.4 通过时间反向传播</h4><ul>\n<li>我们需要将循环神经网络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应⽤反向传播计算并存储梯度</li>\n<li>简单起⻅，我们考虑⼀个⽆偏差项的循环神经网络，且激活函数为恒等映射$（\\phi(x) = x）$。设时间步t的输⼊为单样本$x_t \\in R^d$，标签为$y_t$，那么隐藏状态$h_t \\in R^h$的计算表达式为:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh_t = W_{hx}X_t + W_{hh}h_{t-1}</script><p>输出层变量$o_t \\in \\mathbb{R}^q$为：</p>\n<script type=\"math/tex; mode=display\">\no_t = W_{qh}h_t</script><p>设时间步t的损失为$\\ell(o_t, y_t)$。则总时间步数为T的损失函数L定义为：</p>\n<script type=\"math/tex; mode=display\">\nL = \\frac{1}{T}\\sum_{t = 1}^T \\ell(o_t, y_t)</script><ul>\n<li><p>我们假设一共有3个时间步数，那么可以做出计算图：<br><img src=\"https://s2.loli.net/2021/12/10/Feq2PM3nCslpGO7.png\" alt=\"image-20211210132156070\"></p>\n</li>\n<li><p>现在我们开始反向传播：</p>\n</li>\n</ul>\n<p>易得：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}=\\frac{\\partial \\ell\\left(\\boldsymbol{o}_{t}, y_{t}\\right)}{T \\cdot \\partial \\boldsymbol{o}_{t}}</script><p>然后可以得到：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{W}_{q h}}=\\sum_{t=1}^{T} \\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}, \\frac{\\partial \\boldsymbol{o}_{t}}{\\partial \\boldsymbol{W}_{q h}}\\right)=\\sum_{t=1}^{T} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}} \\boldsymbol{h}_{t}^{\\top} .</script><p>接下来就是计算关于<script type=\"math/tex\">W_{hx}</script>和<script type=\"math/tex\">W_{hh}</script>的梯度，那么必然要求关于各时间步隐藏状态的梯度，我们注意到隐藏状态之间也存在依赖关系，，L只通过<script type=\"math/tex\">o_T</script>依赖最终时间步T的隐藏状态<script type=\"math/tex\">h_T</script>。因此，我们先计算⽬标函数有关最终时间步隐藏状态的梯度：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{T}}=\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{T}}, \\frac{\\partial \\boldsymbol{o}_{T}}{\\partial \\boldsymbol{h}_{T}}\\right)=\\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T}}</script><p>接下来对于时间步t &lt; T，在图6.3中，$L$通过$h_{t+1}$和$o_t$依赖$h_t$。依据链式法则，⽬标函数有关时间步t &lt; T的隐藏状态的梯度$\\partial L / \\partial h_t \\in R^h$需要按照时间步从⼤到小依次计算:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t}}=\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}, \\frac{\\partial \\boldsymbol{h}_{t+1}}{\\partial \\boldsymbol{h}_{t}}\\right)+\\operatorname{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}}, \\frac{\\partial \\boldsymbol{o}_{t}}{\\partial \\boldsymbol{h}_{t}}\\right)=\\boldsymbol{W}_{h h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}+\\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{t}} .</script><p>对次递归公式展开，我们可以得到对任意时间步1 ≤ t ≤ T，我们可以得到⽬标函数有关隐藏状态梯度的通项公式：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t}}=\\sum_{i=t}^{T}\\left(\\boldsymbol{W}_{h h}^{\\top}\\right)^{T-i} \\boldsymbol{W}_{q h}^{\\top} \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T+t-i}}</script><p>通过关于每个时间步隐藏状态的梯度，易求得关于<script type=\"math/tex\">W_{hx}</script>和<script type=\"math/tex\">W_{hh}</script>的梯度。</p>\n<ul>\n<li><strong>由上式中的指数项可⻅，当时间步数T较⼤或者时间步t较小时，⽬标函数有关隐藏状态的梯度较容易出现衰减和爆炸</strong></li>\n<li>RNN和一般的网络一样，我们在正向传播和反向传播的时候会进行一些数据的储存，避免重复计算</li>\n</ul>\n<h4 id=\"2-3-5-激活函数的选择\"><a href=\"#2-3-5-激活函数的选择\" class=\"headerlink\" title=\"2.3.5 激活函数的选择\"></a>2.3.5 激活函数的选择</h4><ul>\n<li><p>对于下一个时间步隐藏状态的计算，激活函数一般选用Tanh或者ReLu。对输出的运算，一般做softmax运算。</p>\n</li>\n<li><p>但是很多时候还是选用Tanh而不是ReLu，这一点在Hinton的<a href=\"https://arxiv.org/abs/1504.00941\">IRNN论文</a>里面是很明确的提到的：</p>\n</li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_720w.jpg?source=1940ef5c\" alt=\"img\"></p>\n<p><strong>也就是说在RNN中直接把激活函数换成ReLu会导致非常大的输出值，因此它们可能更容易发生爆炸</strong></p>\n<ul>\n<li>下面具体来看一下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\text { net }_{t}=U x_{t}+W h_{t-1} \\\\\nh_{t}=f\\left(\\text { net }_{t}\\right)\n\\end{array}</script><p><strong>假设ReLu函数一直处于激活区域（即输入大于0）</strong>，则有<script type=\"math/tex\">f(x) = x, net_t = Ux_t + W(Ux_{t-1} + Wh_{t-2})</script>，继续将其展开，最终会包含多个W的连乘，如果W不是单位矩阵， 那么最终的结果将会区域0或无穷。</p>\n<p>但是在CNN中就不会发生这种问题，因为CNN中每一层的W是不同的，并且在初始化时它们是独立同分布的，因此可以相互抵消，在多层之后一般不会出现这种严重的数值问题</p>\n<p>我们再来看看反向传播的时候：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_t}{\\partial h_{t-1}} = W</script><p>所以：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_t}{\\partial h_{1}} = W^n</script><p>可以看到只要W不是单位矩阵，梯度会出现消失或者爆炸的现象</p>\n<p>而在使用tanh作为激活函数的时候：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_{t}}{\\partial h_{t-1}}=\\left(1-h_{t}^{2}\\right) W</script><p>可以证明，固定<script type=\"math/tex\">h_{t-1}</script>时，<script type=\"math/tex\">\\left(1-h_{t}^{2}\\right) W</script>是有界的（这个界可能不是1，但总归是有界的）</p>\n<ul>\n<li>综上所述：当采用ReLu作为激活函数时，只有W在单位矩阵附近时才能取得比较好的结果。<strong>而使用tanh函数，每个相邻时间步之间的梯度是有界的，减少了一些梯度爆炸的可能性（注意是梯度爆炸，而没有解决梯度消失）。</strong>但是ReLu也拥有自己的优点，那就是计算量更小，收敛更快，其实relu也可以配合梯度裁剪来使用，但是终归没有tanh好</li>\n</ul>\n<h3 id=\"2-4-梯度裁剪\"><a href=\"#2-4-梯度裁剪\" class=\"headerlink\" title=\"2.4 梯度裁剪\"></a>2.4 梯度裁剪</h3><ul>\n<li>循环神经网络中较容易出现梯度衰减或梯度爆炸。<strong>为了应对梯度爆炸</strong>，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成⼀个向量 $g$，并设裁剪的阈值是$\\theta$。裁剪后的梯度：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min \\left(\\frac{\\theta}{\\|g\\|}, 1\\right) g</script><p>的$L_2$范数不超过$\\theta$</p>\n<ul>\n<li><strong>但是梯度裁剪无法应对梯度衰减</strong></li>\n</ul>\n<h1 id=\"3-门控循环单元（GRU）\"><a href=\"#3-门控循环单元（GRU）\" class=\"headerlink\" title=\"3 门控循环单元（GRU）\"></a>3 门控循环单元（GRU）</h1><ul>\n<li>我们发现，当时间步数较⼤或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但⽆法解决梯度衰减的问题。通常由于这个原因，<strong>循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系</strong></li>\n<li>门控循环神经网络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较⼤的依赖关系。它通过可以学习的⻔来控制信息的流动。其中，门控循环单元（gated recurrent unit，GRU）是⼀种常⽤的⻔控循环神经网</li>\n</ul>\n<h3 id=\"3-1-重置门和更新门\"><a href=\"#3-1-重置门和更新门\" class=\"headerlink\" title=\"3.1 重置门和更新门\"></a>3.1 重置门和更新门</h3><ul>\n<li>⻔控循环单元中的重置⻔和更新⻔的输⼊均为当前时间步输⼊<script type=\"math/tex\">X_t</script>与上⼀时间步隐藏状态<script type=\"math/tex\">H_{t−1}</script>，输出由激活函数为sigmoid函数的全连接层计算得到。</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20211210135706726.png\" alt=\"image-20211210135706726\"></p>\n<p>具体来说，重置门$R_t \\in \\mathbb{R}^{n \\times h}$和更新门$Z_t \\in \\mathbb{R}^{n \\times h}$的计算如下：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\boldsymbol{R}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x r}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h r}+\\boldsymbol{b}_{r}\\right) \\\\\n\\boldsymbol{Z}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x z}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h z}+\\boldsymbol{b}_{z}\\right)\n\\end{array}</script><h3 id=\"3-2-候选隐藏状态\"><a href=\"#3-2-候选隐藏状态\" class=\"headerlink\" title=\"3.2 候选隐藏状态\"></a>3.2 候选隐藏状态</h3><ul>\n<li><p>我们将当前时间步重置⻔的输出与上⼀时间步隐藏状态做按元素乘法（符号为$\\odot$）。如果重置⻔中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上⼀时间步的隐藏状态。如果元素值接近1，那么表⽰保留上⼀时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输⼊连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态</p>\n</li>\n<li><p>具体来说，时间步t的候选隐藏状态$\\tilde{H}_t \\in \\mathbb{R}^{n \\times h}$的计算为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\tilde{\\boldsymbol{H}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\left(\\boldsymbol{R}_{t} \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right),</script><ul>\n<li><p><strong>重置门控制了上⼀时间步的隐藏状态如何流⼊当前时间步的候选隐藏状态。而上⼀时间步的隐藏状态可能包含了时间序列截⾄上⼀时间步的全部历史信息。因此，重置门可以⽤来丢弃与预测⽆关的历史信息</strong></p>\n</li>\n<li><p>最后，时间步t的隐藏状态<script type=\"math/tex\">H_t \\in \\mathbb{R}^{n×h}</script>的计算使⽤当前时间步的更新门<script type=\"math/tex\">Z_t</script>来对上⼀时间步的隐藏状态<script type=\"math/tex\">H_{t−1}</script>和当前时间步的候选隐藏状态<script type=\"math/tex\">\\tilde{H}_t</script>做组合：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{H}_{t}=\\boldsymbol{Z}_{t} \\odot \\boldsymbol{H}_{t-1}+\\left(1-\\boldsymbol{Z}_{t}\\right) \\odot \\tilde{\\boldsymbol{H}}_{t}</script><p><img src=\"https://s2.loli.net/2021/12/10/L7O69pm3Yjzbno5.png\" alt=\"image-20211210141824572\"></p>\n<ul>\n<li><strong>更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新</strong>。假设更新⻔在时间步$t’$到$t（t’ &lt; t）$之间⼀直近似1。那么，<strong>在时间步$t’$到$t$之间的输⼊信息⼏乎没有流⼊时间步$t$的隐藏状态$H_t$</strong>。实际上，<strong>这可以看作是较早时刻的隐藏状态$H_{t’−1}$⼀直通过时间保存并传递⾄当前时间步$t$</strong>。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较⼤的依赖关系</li>\n<li>我们稍作总结：</li>\n</ul>\n<ol>\n<li>重置⻔有助于捕捉时间序列⾥短期的依赖关系</li>\n<li>更新⻔有助于捕捉时间序列⾥⻓期的依赖关系</li>\n<li>重置门和更新门为0、1的情况：</li>\n</ol>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>对应情况</th>\n<th>重置门</th>\n<th>更新门</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>退化为一般的RNN</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>丢弃当前时间步的全部信息，只保留历史信息</td>\n<td>0或1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>完全丢弃历史信息，只保留当前时间步的信息</td>\n<td>0</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h1 id=\"4-长短期记忆（LSTM）\"><a href=\"#4-长短期记忆（LSTM）\" class=\"headerlink\" title=\"4 长短期记忆（LSTM）\"></a>4 长短期记忆（LSTM）</h1><ul>\n<li>还有另外一种十分常用的门控循环神经网络：⻓短期记忆（long short-term memory，LSTM）</li>\n<li><strong>LSTM之所以叫做长短期记忆，是因为里面的两个记忆变量：隐藏状态<script type=\"math/tex\">H_t</script>负责短期记忆，记忆细胞<script type=\"math/tex\">C_t</script>负责长期记忆（因为每个时间步中<script type=\"math/tex\">H_t</script>相比于<script type=\"math/tex\">C_t</script>更新的更多，所以负责短期）</strong></li>\n</ul>\n<h3 id=\"4-1-输⼊门、遗忘门、输出门和候选记忆细胞\"><a href=\"#4-1-输⼊门、遗忘门、输出门和候选记忆细胞\" class=\"headerlink\" title=\"4.1 输⼊门、遗忘门、输出门和候选记忆细胞\"></a>4.1 输⼊门、遗忘门、输出门和候选记忆细胞</h3><ul>\n<li>与门控循环单元中的重置门和更新门⼀样，⻓短期记忆的门的输⼊均为当前时间步输⼊<script type=\"math/tex\">X_t</script>与上⼀时间步隐藏状态<script type=\"math/tex\">H_{t−1}</script>，输出由激活函数为sigmoid函数的全连接层计算得到</li>\n<li>⻓短期记忆需要计算候选记忆细胞$\\tilde{C}_t$。它的计算与上⾯介绍的3个⻔类似，但使⽤了值域在[−1, 1]的tanh函数作为激活函数:</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/jzn954wmEqRfyIg.png\" alt=\"image-20211210144009641\"></p>\n<ul>\n<li>时间步t的输⼊⻔$I_t \\in R^{n×h}$、遗忘⻔$F_t \\in R^{n×h}$和输出⻔$O_t \\in R^{n×h}$分别计算如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\boldsymbol{I}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x i}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h i}+\\boldsymbol{b}_{i}\\right), \\\\\n\\boldsymbol{F}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x f}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h f}+\\boldsymbol{b}_{f}\\right), \\\\\n\\boldsymbol{O}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x o}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h o}+\\boldsymbol{b}_{o}\\right), \\\\\n\\tilde{\\boldsymbol{C}}_{t}&=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x c}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h c}+\\boldsymbol{b}_{c}\\right),\n\\end{aligned}</script><h3 id=\"4-2-记忆细胞\"><a href=\"#4-2-记忆细胞\" class=\"headerlink\" title=\"4.2 记忆细胞\"></a>4.2 记忆细胞</h3><ul>\n<li>当前时间步记忆细胞$C_t \\in R^{n\\times h}$的计算组合了上⼀时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘⻔和输⼊⻔来控制信息的流动：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{C}_{t}=\\boldsymbol{F}_{t} \\odot \\boldsymbol{C}_{t-1}+\\boldsymbol{I}_{t} \\odot \\tilde{\\boldsymbol{C}}_{t}</script><ul>\n<li>遗忘门控制上⼀时间步的记忆细胞$C_{t−1}$中的信息是否传递到当前时间步，而输⼊门则控制当前时间步的输⼊$X_t$通过候选记忆细胞$\\tilde{C}_t$如何流⼊当前时间步的记忆细胞。如果遗忘门⼀直近似1且输⼊门⼀直近似0，过去的记忆细胞将⼀直通过时间保存并传递⾄当前时间步</li>\n</ul>\n<h3 id=\"4-3-隐藏状态\"><a href=\"#4-3-隐藏状态\" class=\"headerlink\" title=\"4.3 隐藏状态\"></a>4.3 隐藏状态</h3><ul>\n<li>有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$H_t \\in R^{n×h}$的信息的流动：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH_t = O_t \\odot tanh(C_t)</script><ul>\n<li>当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使⽤；当输出门近似0时，记忆细胞信息只⾃⼰保留</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/mlYhrVbA47MByvG.png\" alt=\"image-20211210145754660\"></p>\n<h3 id=\"4-4-三种门的作用\"><a href=\"#4-4-三种门的作用\" class=\"headerlink\" title=\"4.4 三种门的作用\"></a>4.4 三种门的作用</h3><ul>\n<li><strong>输入门控制当前计算的新状态以多大程度更新到记忆单元中</strong></li>\n<li><strong>遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉</strong></li>\n<li><strong>输出门控制当前的输出有多大程度取决于当前的记忆单元</strong></li>\n<li>在一个训练好的网络中：</li>\n</ul>\n<ol>\n<li>当输入的序列中没有重要信息时，LSTM的遗忘门接近于1，输入门接近于0，此时过去的记忆会被保存，从而实现了长期记忆功能</li>\n<li>当输入的序列中出现了重要的信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近1</li>\n<li>当输入的序列中出现了重要的信息，且该信息意味着之前的记忆不再重要时，输入门的值接近1，而遗忘门的值接近于0，这样旧的记忆被遗忘，新的重要信息被记忆</li>\n</ol>\n<h3 id=\"4-5-LSTM各模块激活函数\"><a href=\"#4-5-LSTM各模块激活函数\" class=\"headerlink\" title=\"4.5 LSTM各模块激活函数\"></a>4.5 LSTM各模块激活函数</h3><ul>\n<li>LSTM中，三种门都用的Sigmoid函数，生成候选记忆时，使用Tanh作为激活函数。值得注意的是，<strong>这两个激活函数都是饱和的</strong>，也就是说在输入达到一定值的情况下，输出就不发生明显变化了。如果使用的是非饱和的激活函数，例如ReLu，将难以实现门控的效果</li>\n<li>Sigmoid的输出在0到1之间，符合门控的物理定义。且当输入较大或较小时，输出会非常接近1或0，从而保证门开或门关</li>\n<li>在生成候选记忆时，使用Tanh函数，是因为其输出在-1到1之间，这与大多数场景下特征分布是zero-centered吻合</li>\n<li>此为Tanh函数在输入为0附近相比于Sigmoid有更大的梯度，通常收敛更快</li>\n</ul>\n<h1 id=\"5-门控机制是如何解决RNN的问题的\"><a href=\"#5-门控机制是如何解决RNN的问题的\" class=\"headerlink\" title=\"5 门控机制是如何解决RNN的问题的\"></a>5 门控机制是如何解决RNN的问题的</h1><ul>\n<li>上面我们说到，RNN最大的一个问题就是无法捕捉长距离依赖，而引入门控机制就是为了解决这个问题，这里我们以LSTM举例：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nf_{t} &=\\sigma\\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\\right) \\\\\ni_{t} &=\\sigma\\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\\right) \\\\\no_{t} &=\\sigma\\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\\right) \\\\\n\\hat{c}_{t} &=\\tanh \\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\\right) \\\\\nc_{t} &=f_{t} \\circ c_{t-1}+i_{t} \\circ \\hat{c}_{t} \\\\\nh_{t} &=o_{t} \\circ \\tanh \\left(c_{t}\\right)\n\\end{aligned}</script><ul>\n<li>我们可以像上面2.3.5激活函数的选择中一样，探讨<script type=\"math/tex\">\\frac{\\partial h_t}{\\partial h_{t-1}}</script>，但是因为<script type=\"math/tex\">h_{t} =o_{t} \\circ \\tanh(c_t)</script>，所以同样可以分析<script type=\"math/tex\">\\frac{c_t}{c_{t-1}}</script>，而后者更简单些，所以分析后者：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial c_{t}}{\\partial c_{t-1}}=f_{t}+c_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}}+\\hat{c} t \\frac{\\partial i_{t}}{\\partial c_{t-1}}+i_{t} \\frac{\\partial \\hat{c}_{t}}{\\partial c_{t-1}}</script><ul>\n<li><p>上式总共有4项，<strong>梯度主要取决于第一项<script type=\"math/tex\">f_t</script></strong>，由于<script type=\"math/tex\">f_t \\in (0,1)</script>，所以梯度爆炸的风险减小，而是否会发生梯度小时，取决于每个时间步的<script type=\"math/tex\">f_t</script>是否接近1，但是这里有个自洽的结论：<strong>如果我们的任务比较依赖于历史信息，那么<script type=\"math/tex\">f_t</script>就会接近于1，这时候历史的梯度信息也正好不容易消失；如果<script type=\"math/tex\">f_t</script>很接近于0，那么就说明我们的任务不依赖于历史信息，这时候就算梯度消失也无妨了。</strong></p>\n</li>\n<li><p>而后面3项都不是决定梯度的主要因素，挑<script type=\"math/tex\">c_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}}</script>来说明，带入<script type=\"math/tex\">h_{t-1} =o_{t-1} \\circ \\tanh(c_{t-1})</script>，可以得到：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nc_{t-1} \\frac{\\partial f_{t}}{\\partial c_{t-1}} = c_{t-1} \\frac{\\partial f_{t}}{\\partial h_{t-1}}  \\frac{\\partial h_{t-1}}{\\partial c_{t-1}} = f_{t}\\left(1-f_{t}\\right) o_{t-1}\\left(1-\\tanh ^{2} c_{t-1}\\right) c_{t-1} U_{f}</script><ul>\n<li><p>由于<script type=\"math/tex\">f_t,(1-f_t),o_{t-1} \\in (0,1)</script>，并且可以证明<script type=\"math/tex\">|\\left(1-\\tanh ^{2} c_{t-1}\\right)c_{t-1}| <0.45</script>，所以结果相当于<script type=\"math/tex\">U_f</script>乘上四个门，结果会非常的小，所以不起主导作用。其他两项是同样的方法</p>\n</li>\n<li><p><strong>注意：LSTM和GRU只是很大的缓解了梯度消失，但是如果时间步很长，成千上万步，仍然会发生梯度消失，毕竟记忆单元压根存不了那么多信息</strong></p>\n</li>\n<li><p>另外，一般LSTM要比GRU要好一些，但是到底好在哪里呢，在GRU中同样进行反向传播的分析：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial h_{t}}{\\partial h_{t-1}}=& 1-z_{t}-z_{t}\\left(1-z_{t}\\right) h_{t-1} U_{z}+z_{t}\\left(1-z_{t}\\right) \\hat{h}_{t} U_{z} \\\\\n&+\\left(1-\\hat{h}_{t}^{2}\\right) r_{t}\\left(1+\\left(1-r_{t}\\right) h_{t-1} U_{r}\\right) z_{t} U_{h}\n\\end{aligned}</script><ul>\n<li>上式其主导作用的同样是<script type=\"math/tex\">1-z_t</script>，但是后面的项相比于LSTM只有3个门的连乘（LSTM有4个），所以这些想对梯度的影响可能大一些，所以<strong>LSTM的梯度要比GRU稳定一些，所以GRU更依赖于初始化</strong></li>\n</ul>\n<h1 id=\"6-深度循环神经网络\"><a href=\"#6-深度循环神经网络\" class=\"headerlink\" title=\"6 深度循环神经网络\"></a>6 深度循环神经网络</h1><ul>\n<li>到⽬前为⽌介绍的循环神经网络只有⼀个单向的隐藏层，在深度学习应⽤⾥，我们通常会⽤到含有多个隐藏层的循环神经网络，也称作深度循环神经网络</li>\n<li>下图演⽰了⼀个有L个隐藏层的深度循环神经网络，<strong>每个隐藏状态不断传递⾄当前层的下⼀时间步和当前时间步的下⼀层</strong></li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/PUK7CRjyemEbzLM.png\" alt=\"image-20211210150125737\"></p>\n<ul>\n<li>第1隐藏层的隐藏状态和之前的计算⼀样：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{H}_{t}^{(1)}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(1)}+\\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{h h}^{(1)}+\\boldsymbol{b}_{h}^{(1)}\\right)</script><ul>\n<li>当1 &lt; l ≤ L时，第l隐藏层的隐藏状态的表达式为:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{H}_{t}^{(l)}=\\phi\\left(\\boldsymbol{H}_{t}^{(l-1)} \\boldsymbol{W}_{x h}^{(l)}+\\boldsymbol{H}_{t-1}^{(l)} \\boldsymbol{W}_{h h}^{(l)}+\\boldsymbol{b}_{h}^{(l)}\\right)</script><ul>\n<li>最终，输出层的输出只需基于第L隐藏层的隐藏状态：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t}^{(L)} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}</script><ul>\n<li><p>同多层感知机⼀样，隐藏层个数L是超参数</p>\n</li>\n<li><p><strong>RNN中选择深度网络的原因和一般前馈神经网络一样，都是为了能够用来表征更复杂的情况</strong></p>\n</li>\n</ul>\n<h1 id=\"7-双向循环神经网络\"><a href=\"#7-双向循环神经网络\" class=\"headerlink\" title=\"7 双向循环神经网络\"></a>7 双向循环神经网络</h1><ul>\n<li>之前介绍的循环神经网络模型都是假设当前时间步是由前⾯的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后⾯时间步决定。例如，当我们写下⼀个句⼦时，可能会根据句⼦后⾯的词来修改句⼦前⾯的⽤词</li>\n<li><strong>双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息</strong>，下图演⽰了⼀个含单隐藏层的双向循环神经网络的架构</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/VtFmigZ58qw2aeb.png\" alt=\"image-20211210151934211\"></p>\n<ul>\n<li>具体来看，设时间步t正向隐藏状态为$\\overrightarrow{H}_t \\in R^{n×h}$（正向隐藏单元个数为h），反向隐藏状态为$\\overleftarrow{H}_t \\in R^{n×h}$（反向隐藏单元个数为h）。我们可以分别计算正向隐藏状态和反向隐藏状态：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\overrightarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(f)}+\\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{h h}^{(f)}+\\boldsymbol{b}_{h}^{(f)}\\right) \\\\\n\\overleftarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(b)}+\\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{h h}^{(b)}+\\boldsymbol{b}_{h}^{(b)}\\right)\n\\end{array}</script><p><strong>然后我们连结两个⽅向的隐藏状态$\\overrightarrow{H}_t$和 $\\overleftarrow{H}_t$来得到隐藏状态$H_t \\in R^{n×2h}$</strong>，并将其输⼊到输出层。 输出层计算输出$O_t \\in R^{n×q}$（输出个数为q）：</p>\n<script type=\"math/tex; mode=display\">\nO_t = H_tW_{hq} + b_q</script><ul>\n<li><strong>不同方向的隐藏状态的隐藏单元个数也可以不同</strong></li>\n<li>双向循环神经网络在每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊）</li>\n</ul>\n<h1 id=\"8-Seq2Seq模型\"><a href=\"#8-Seq2Seq模型\" class=\"headerlink\" title=\"8 Seq2Seq模型\"></a>8 Seq2Seq模型</h1><ul>\n<li>在许多时候，输入和输出都是不定长序列，这采用一般的RNN肯定是行不通的。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，例如：英语输⼊：“They”、“are”、“watching”、“.” ； 法语输出：“Ils”、“regardent”、“.”</li>\n<li>当输⼊和输出都是不定⻓序列时，我们可以使⽤seq2seq模型，本质上都⽤到了两个循环神经网络，分别叫做编码器和解码器。<strong>编码器用来分析输⼊序列，解码器⽤来生成输出序列</strong></li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/FQfVxdq2wuC7aSs.png\" alt=\"image-20211210170200879\"></p>\n<ul>\n<li>&lt; bos &gt;（beginning of sequence）和 &lt; eos &gt;（end of sequence）分别表示序列的开始和结束</li>\n<li>编码器每个时间步的输⼊依次为英语句⼦中的单词、标点和特殊符号。上图中使⽤了编码器在最终时间步的隐藏状态作为输⼊句⼦的表征或编码信息。解码器在各个时间步中使⽤输⼊句⼦的编码信息和上个时间步的输出以及隐藏状态作为输⼊</li>\n</ul>\n<h3 id=\"8-1-编码器\"><a href=\"#8-1-编码器\" class=\"headerlink\" title=\"8.1 编码器\"></a>8.1 编码器</h3><ul>\n<li><strong>编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量$c$，并在该背景变量中编码输⼊序列信息</strong>。编码器可以使⽤循环神经网络</li>\n<li>编码器通过⾃定义函数q将各个时间步的隐藏状态变换为背景变量</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nc = q(h_1, ...,h_T)</script><ul>\n<li><strong>也可以使⽤双向循环神经网络构造编码器</strong>，在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊），并编码了整个序列的信息。</li>\n</ul>\n<h3 id=\"8-2-解码器\"><a href=\"#8-2-解码器\" class=\"headerlink\" title=\"8.2 解码器\"></a>8.2 解码器</h3><ul>\n<li>解码器就是通过编码器输出的背景向量$c$，和每一个时间步的输入，输出所需要的结果</li>\n<li><strong>解码器在预测和训练时是不一样的</strong>，我们先介绍<strong>预测</strong>时的解码器：</li>\n</ul>\n<p>编码器输出的背景变量$c$编码了整个输⼊序列<script type=\"math/tex\">x_1, . . . , x_T</script>的信息。给定训练样本中的输出序列<script type=\"math/tex\">y_1, y_2, . . . , y_{T'}</script>，对每个时间步<script type=\"math/tex\">t'</script>（符号与输⼊序列或编码器的时间步<script type=\"math/tex\">t</script>有区别），解码器输出<script type=\"math/tex\">y_{t'}</script>的条件概率将基于之前的输出序列<script type=\"math/tex\">y_1, . . . , y_{t'−1}</script>和背景变量<script type=\"math/tex\">c</script>，即<script type=\"math/tex\">P(y_{t'} | y_1, . . . , y_{t'−1}, c)</script></p>\n<p>为此，我们可以使⽤另⼀个循环神经网络作为解码器。在输出序列的时间步<script type=\"math/tex\">t'</script>，解码器将上⼀时间步的输出<script type=\"math/tex\">y_{t'−1}</script>以及背景变量<script type=\"math/tex\">c</script>作为输⼊，并将它们与上⼀时间步的隐藏状态<script type=\"math/tex\">s_{t'−1}</script>变换为当前时间步的隐藏状态<script type=\"math/tex\">s_{t'}</script>：</p>\n<script type=\"math/tex; mode=display\">\ns_{t'} = g(y_{t' -1}, c, s_{t' - 1})</script><p>有了解码器的隐藏状态后，我们可以使⽤⾃定义的输出层和softmax运算来计算<script type=\"math/tex\">P(y_{t'} | y_1, . . . , y_{t'−1}, c)</script></p>\n<ul>\n<li><strong>训练</strong>时的解码器，每一时间步的输入序列可以是上一时间步的输出，也可以是上一时间步的真实标签序列。后者叫做<strong>强制教学</strong></li>\n</ul>\n<p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP\\left(y_{1}, \\ldots, y_{T^{\\prime}} \\mid x_{1}, \\ldots, x_{T}\\right) &=\\prod_{t^{\\prime}=1}^{T^{\\prime}} P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, x_{1}, \\ldots, x_{T}\\right) \\\\\n&=\\prod_{t^{\\prime}=1}^{T^{\\prime}} P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)\n\\end{aligned}</script><p>并得到该输出序列的损失：</p>\n<script type=\"math/tex; mode=display\">\n-\\log P\\left(y_{1}, \\ldots, y_{T^{\\prime}} \\mid x_{1}, \\ldots, x_{T}\\right)=-\\sum_{t^{\\prime}=1}^{T^{\\prime}} \\log P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)</script><p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数</p>\n<h3 id=\"8-3-预测时的搜索方式\"><a href=\"#8-3-预测时的搜索方式\" class=\"headerlink\" title=\"8.3 预测时的搜索方式\"></a>8.3 预测时的搜索方式</h3><h4 id=\"8-3-1-贪婪搜索\"><a href=\"#8-3-1-贪婪搜索\" class=\"headerlink\" title=\"8.3.1 贪婪搜索\"></a>8.3.1 贪婪搜索</h4><ul>\n<li>贪婪搜索（greedy search）。对于输出序列任⼀时间步$t’$，我 们从$|Y|$（Y为词典）个词中搜索出条件概率最⼤的词：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ny_{t'} = argmax_{y \\in Y}P(y|y_1, ..., y_{t' - 1}, c)</script><p>作为输出。⼀旦搜索出“&lt; eos &gt;”符号，或者输出序列⻓度已经达到了最⼤⻓度$T’$，便完成输出。</p>\n<ul>\n<li>我们将该条件概率最⼤的输出序列称为<strong>最优输出序列</strong>，<strong>而贪婪搜索无法保证得到最优输出序列</strong>，下面举个栗子：</li>\n</ul>\n<blockquote>\n<p>假设输出词典⾥⾯有“A” “B” “C” 和“&lt; eos &gt;”这4个词。下图中每个时间步下的4个数字分别代表了该时间步⽣成“A” “B” “C” 和“&lt; eos &gt;”这4个词的条件概率。在每个 间步，贪婪搜索选取条件概率最⼤的词。因此，将⽣成输出序列“A” “B” “C” “&lt; eos &gt;”。 该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 = 0.048</p>\n</blockquote>\n<p><img src=\"https://s2.loli.net/2021/12/10/JUgILXFCjWviaQw.png\" alt=\"image-20211210180148791\"></p>\n<blockquote>\n<p>但是现在我们考虑下面一种情况，在时间步2中选取了条件概率第⼆⼤的词“C”，<strong>由于之后时间步的概率值是基于前面的时间步的，所以时间步2结果选取的改变，会导致后续的概率发生改变</strong>，如下图，所以我们现在的输出序列“A” “C” “B” “&lt; eos &gt;”的条件概率是0.5 × 0.3 × 0.6 × 0.6 = 0.054，大于贪婪搜索的概率，所以贪婪搜索得到的不是最优的</p>\n</blockquote>\n<p><img src=\"https://s2.loli.net/2021/12/10/zSKDLamFWqseyuN.png\" alt=\"image-20211210180404095\"></p>\n<h4 id=\"8-3-2-穷举搜索\"><a href=\"#8-3-2-穷举搜索\" class=\"headerlink\" title=\"8.3.2 穷举搜索\"></a>8.3.2 穷举搜索</h4><ul>\n<li>我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最⼤的序列</li>\n<li><strong>虽然穷举搜索可以得到最优输出序列，但它的计算开销$O(|Y|^{T’})$很容易过⼤</strong>，而贪婪搜索的开销为$O(|Y|T’)$，明显小于穷举搜索</li>\n</ul>\n<h4 id=\"8-3-3-束搜索\"><a href=\"#8-3-3-束搜索\" class=\"headerlink\" title=\"8.3.3 束搜索\"></a>8.3.3 束搜索</h4><ul>\n<li><p>束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个<strong>束宽</strong>（beam size）超参数。我们将它设为k。在时间步1时，选取当前时间步条件概率最⼤的k个词，分别组成k个候选输出序列的⾸词。在之后的每个时间步，基于上个时间步的k个候选输出序列，从k$ |Y|$个可能的输出序列中选取条件概率最⼤的k个，作为该时间步的候选输出序列。最终，<strong>我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt; eos &gt;”的序列，并将它们中所有特殊符号“&lt; eos &gt;”后⾯的⼦序列舍弃</strong>，得到最终候选输出序列的集合</p>\n</li>\n<li><p>下面举个栗子：</p>\n</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/QEBb3gUVxmuDS7O.png\" alt=\"image-20211210182131362\"></p>\n<p>第一个时间步找出概率最大的”A”和”C”，然后再根据”A”和”C”，在时间步2中寻找概率最大的2个，分别为”AB”和”CE”，再根据这两个，在时间步3，输出”ABD”和”CED”，最后减去&lt; eos &gt;符号以后的内容，得出输出序列</p>\n<ul>\n<li>在最终候选输出序列的集合中，我们取以下分数最⾼的序列作为输出序列：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{L^{\\alpha}} \\log P\\left(y_{1}, \\ldots, y_{L}\\right)=\\frac{1}{L^{\\alpha}} \\sum_{t^{\\prime}=1}^{L} \\log P\\left(y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, c\\right)</script><p>其中$L$为最终候选序列⻓度，$\\alpha$⼀般可选为0.75。分⺟上的$L^{\\alpha}$是为了惩罚较⻓序列在以上分数中较多的对数相加项</p>\n<ul>\n<li>束搜索的计算开销为$O(k|Y|T’)$，介于贪婪搜索和穷举搜索之间</li>\n<li><strong>束搜索通过灵活的束宽来权衡计算开销和搜索质量</strong></li>\n</ul>\n<h3 id=\"8-4-注意力机制\"><a href=\"#8-4-注意力机制\" class=\"headerlink\" title=\"8.4 注意力机制\"></a>8.4 注意力机制</h3><h4 id=\"8-4-1-seq2seq中的注意力机制\"><a href=\"#8-4-1-seq2seq中的注意力机制\" class=\"headerlink\" title=\"8.4.1 seq2seq中的注意力机制\"></a>8.4.1 seq2seq中的注意力机制</h4><ul>\n<li><strong>解码器在⽣成输出序列中的每⼀个词时可能只需利⽤输⼊序列某⼀部分的信息， 而不需要由整个输入序列生成的背景向量</strong>，例如，在输出序列的时间步1，解码器可以主要依赖“They” “are” 的信息来⽣成“Ils”，在时间步2则主要使⽤来⾃“watching”的编码信息⽣成“regardent”，而不需要”They are watching”整个句子</li>\n<li><p>若没引入注意力机制，在实际使用中，那么会发现随着输入序列的增长，模型的性能发生了显著下降。<strong>这是因为编码时输入序列的全部信息压缩到了一个向量中，随着序列的增长，越前面的词丢失越严重</strong>。<strong>同时，seq2seq模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词对应的源语言词的上下文信息和位置信息在编解码过程中丢失了</strong></p>\n</li>\n<li><p>注意⼒机制通过对编码器所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量。解码器在每⼀时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量</p>\n</li>\n</ul>\n<p><img src=\"https://s2.loli.net/2021/12/10/5kGV8uqeaYn3wlF.png\" alt=\"image-20211210184622950\"></p>\n<p>⾸先，函数a根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输⼊。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量</p>\n<ul>\n<li>具体来说，令编码器在时间步t的隐藏状态为$h_t$，且总时间步数为T。那么解码器在时间步$t’$（$t’$代表的是解码器的时间步数）的背景变量为所有编码器隐藏状态的加权平均：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{c}_{t^{\\prime}}=\\sum_{t=1}^{T} \\alpha_{t^{\\prime} t} \\boldsymbol{h}_{t}</script><p>其中给定$t’$时，权重$\\alpha_{t’t}$在$t = 1, . . . , T$的值是⼀个概率分布。为了得到概率分布，我们可以使⽤softmax运算:</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_{t^{\\prime} t}=\\frac{\\exp \\left(e_{t^{\\prime} t}\\right)}{\\sum_{k=1}^{T} \\exp \\left(e_{t^{\\prime} k}\\right)}, \\quad t=1, \\ldots, T</script><p>由于<script type=\"math/tex\">e_{t't}</script>同时取决于解码器的时间步<script type=\"math/tex\">t'</script>和编码器的时间步t，我们不妨以解码器在时间步<script type=\"math/tex\">t'− 1</script>的隐藏状态<script type=\"math/tex\">s_{t'−1}</script>与编码器在时间步t的隐藏状态<script type=\"math/tex\">h_t</script>为输⼊，并通过函数a计算<script type=\"math/tex\">e_{t't}</script>：</p>\n<script type=\"math/tex; mode=display\">\ne_{t't} = a(s_{t'-1}, h_t)</script><p>其中a是一个自定义函数，有多种选择。如果两个输⼊向量⻓度相同，⼀个简单的选择是计算它们的内积$a(s,h) = s^⊤h$</p>\n<ul>\n<li>我们对注意力机制可以有一个直观的理解：<strong>在生成一个输出词时，会考虑每一个输入词和当前输出词的对应关系，对齐越好的词，会有越大的权重</strong></li>\n<li>还可以结合双向RNN和注意力机制，每个隐状态包含了$\\overleftarrow{h_t}$和$\\overrightarrow{h_t}$</li>\n</ul>\n<h4 id=\"8-4-2-矢量化计算\"><a href=\"#8-4-2-矢量化计算\" class=\"headerlink\" title=\"8.4.2 矢量化计算\"></a>8.4.2 矢量化计算</h4><ul>\n<li><p>我们还可以对注意⼒机制采⽤更⾼效的⽮量化计算。⼴义上，注意⼒机制的输⼊包括<strong>查询项</strong>以及<strong>⼀⼀对应的键项和值项</strong>，其中值项是需要加权平均的⼀组项。在加权平均中，<strong>值项的权重来⾃查询项以及与该值项对应的键项的计算</strong></p>\n</li>\n<li><p>在上面的例⼦中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。假设编码器和解码器的隐藏单元个数均为h，且函数<script type=\"math/tex\">a(s, h) = s^ ⊤h</script>。若我们希望根据解码器单个隐藏状态<script type=\"math/tex\">s_{t'−1} \\in \\mathbb{R}^h</script>和编码器所有隐藏状态<script type=\"math/tex\">h_t \\in \\mathbb{R}^h , t = 1, . . . , T</script>来计算背景向量<script type=\"math/tex\">c_{t'} \\in \\mathbb{R}^h</script>。我们可以将查询项矩阵<script type=\"math/tex\">Q \\in R^{1\\times h}</script>设为<script type=\"math/tex\">s^⊤_{t'−1}</script>，并令键项矩阵<script type=\"math/tex\">K \\in \\mathbb{R}^{T\\times h}</script>和值项矩阵<script type=\"math/tex\">V \\in \\mathbb{R}^{T\\times h}</script>相同且第t⾏均为<script type=\"math/tex\">h^⊤_t</script> 。此时，我们只需要通过⽮量化计算：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nsoftmax(QK^T)V</script><p>即可算出转置后的背景向量<script type=\"math/tex\">c^⊤_{t'}</script>。当查询项矩阵Q的⾏数为n时，上式将得到n行的输出矩阵。输出矩阵与查询项矩阵在相同行上⼀⼀对应。</p>\n"},{"title":"RoBERTa总结","math":true,"date":"2022-08-08T16:00:00.000Z","_content":"\n\n\n- 作者评估了BERT中的各种超参和数据集大小以及训练策略等方面，发现**BERT是训练不足的**\n\n\n\n# 1  RoBERTa的主要改变\n\n- 作者提出了RoBERTa（A Robustly Optimized BERT Pretraining Approach），是BERT的变体，主要的更改如下：\n\n> 1. 用更多的数据和更大的batch size，训练更长的时间\n> 2. 去除了NSP\n> 3. 在更长的句子上训练\n> 4. 进行动态的mask\n\n- 在优化器方面也有细微改变，改变了最大学习率和warmup steps，并且把$$\\beta_2$$改为了0.98，在更大的batch size上训练时，得到了更稳定的结果\n\n\n\n\n\n# 2 训练策略的改变\n\n### 2.1 动态和静态mask\n\n- **两种mask策略：**\n\n> 1. 静态mask：原BERT中使用的方法，在数据预处理时，进行mask。在本实验中，为了避免出现每个epoch的mask的位置都相同，在40个epoch中进行了10次随机mask，这样每个mask实例只会出现4次\n> 2. 动态mask：在每次数据喂入模型时进行mask，这样每次mask的位置都不同\n\n- 实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809143220226.png\" alt=\"image-20220809143220226\" style=\"zoom:80%;\" />\n\n- **动态mask相对于静态mask得到了细微的提升（稍微有丁点卵用）**\n\n\n\n### 2.2 模型的输入形式和NSP任务\n\n- **使用了4种输入策略：**\n\n> 1. **SEGMENT-PAIR+NSP：**原BERT使用的方法，**输入是一个segment对，每个segment可以包括多个句子**，只要总长度小于512就行，带有NSP Loss\n> 2. **SENTENCE-PAIR+NSP：**和前者类似，**只是每个segment只能是单个句子**，同样带有NSP Loss。由于两个单句子一般长度都远远不足512，为了实验公平，作者适当的增加了其实验数据\n> 3. **FULL-SENTENCES：输入是从一个或多个文档中采样的一个full-sentence（可由多个句子组成）**，当采样到文档的最后时，可以继续采下一个文档的句子，但是需要在中间加一个额外的分隔token。**无NSP Loss**\n> 4. **DOC-SENTENCES：和前者类似，但是不能跨文档采样，无NSP Loss**。在靠近文档末尾采样，句子长度会较短，所以同样为了实验公平，适当的增加了数据\n\n- 实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809150834375.png\" alt=\"image-20220809150834375\" style=\"zoom:75%;\" />\n\n- 将前两种策略进行对比，**可以发现使用单个句子会降低表现，推测原因为：这样模型无法学到长范围的依赖关系**\n\n> We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.  \n\n- 再对比前两种和后两种策略，**可以发现移除NSP任务，也可以得到细微的提升**\n\n> Removing the NSP loss matches or slightly improves downstream task performance.\n\n- 对比后两种策略，**可以发现不跨文档比跨文档稍好一点**\n\n\n\n### 2.3 使用更大的Batch Size\n\n- 作者使用相同的数据量，但是不同的batch size，来探究batch size的影响，结构如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809152515016.png\" alt=\"image-20220809152515016\" style=\"zoom:80%;\" />\n\n图中bsz为batch size，ppl为困惑度\n\n\n\n### 2.4 分词方式\n\n- 原版BERT使用char-level的BPE（也就是wordpiece），而本文使用byte-level的BPE，增加了词典大小，和增加了一些参数量，表现还有细微的下降😅，但是基本相同\n\n> - 基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。\n>\n> - 基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。\n>\n>   \n>\n> - 当采用 bytes-level 的 BPE 之后，词表大小从3万（原始 BERT 的 char-level ）增加到5万。这分别为 BERT-base和 BERT-large增加了1500万和2000万额外的参数\n\n\n\n\n\n# 3 对比试验\n\n- 对于实验用于探究预训练数据量的多少和训练时间对表现的影响，实验采用Large的模型规模，并且RoBERTa采用：动态mask + FULL_SENTENCES without NSP + 大batch size + byte-level BPE，实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809164004895.png\" alt=\"image-20220809164004895\" style=\"zoom:75%;\" />\n\n- **可以看到在相同数据量条件下，RoBERTa相比于BERT有较大提升**\n\n- **并且增加数据量可以得到显著的提升，增加训练时间同样可以得到细微提升，并且没有造成过拟合**\n\n\n\n\n\n# 4 在特定下游任务的表现\n\n### 4.1 GLUE\n\n- 实验考虑了$$batch\\_size \\in \\{16, 32\\}$$，$$lr \\in \\{1e-5, 2e-5, 3e-5\\}$$，并在前6%的steps使用线性warm up，之后使用线性衰减至0，使用10个epochs，但是设置了early stop，实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170036407.png\" alt=\"image-20220809170036407\" style=\"zoom:75%;\" />\n\n\n\n### 4.2 SQuAD\n\n- 在本实验中，XLNET和BERT都加上了QA数据集，但是RoBERTa仅使用了SQuAD。并且XLNET使用了逐层不同的学习率，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170501845.png\" alt=\"image-20220809170501845\" style=\"zoom:70%;\" />\n\n\n\n### 4.3 RACE\n\n- 一个做阅读理解的数据集，让模型从4个答案中选出一个最合适的。结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170610509.png\" alt=\"image-20220809170610509\" style=\"zoom:80%;\" />\n\n\n\n\n\n# 5 超参\n\n- **预训练：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170735173.png\" alt=\"image-20220809170735173\" style=\"zoom:50%;\" />\n\n- **微调：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170753600.png\" alt=\"image-20220809170753600\" style=\"zoom:67%;\" />","source":"_posts/RoBERTa总结.md","raw":"---\ntitle: RoBERTa总结\nmath: true\ndate: 2022-8-9\n---\n\n\n\n- 作者评估了BERT中的各种超参和数据集大小以及训练策略等方面，发现**BERT是训练不足的**\n\n\n\n# 1  RoBERTa的主要改变\n\n- 作者提出了RoBERTa（A Robustly Optimized BERT Pretraining Approach），是BERT的变体，主要的更改如下：\n\n> 1. 用更多的数据和更大的batch size，训练更长的时间\n> 2. 去除了NSP\n> 3. 在更长的句子上训练\n> 4. 进行动态的mask\n\n- 在优化器方面也有细微改变，改变了最大学习率和warmup steps，并且把$$\\beta_2$$改为了0.98，在更大的batch size上训练时，得到了更稳定的结果\n\n\n\n\n\n# 2 训练策略的改变\n\n### 2.1 动态和静态mask\n\n- **两种mask策略：**\n\n> 1. 静态mask：原BERT中使用的方法，在数据预处理时，进行mask。在本实验中，为了避免出现每个epoch的mask的位置都相同，在40个epoch中进行了10次随机mask，这样每个mask实例只会出现4次\n> 2. 动态mask：在每次数据喂入模型时进行mask，这样每次mask的位置都不同\n\n- 实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809143220226.png\" alt=\"image-20220809143220226\" style=\"zoom:80%;\" />\n\n- **动态mask相对于静态mask得到了细微的提升（稍微有丁点卵用）**\n\n\n\n### 2.2 模型的输入形式和NSP任务\n\n- **使用了4种输入策略：**\n\n> 1. **SEGMENT-PAIR+NSP：**原BERT使用的方法，**输入是一个segment对，每个segment可以包括多个句子**，只要总长度小于512就行，带有NSP Loss\n> 2. **SENTENCE-PAIR+NSP：**和前者类似，**只是每个segment只能是单个句子**，同样带有NSP Loss。由于两个单句子一般长度都远远不足512，为了实验公平，作者适当的增加了其实验数据\n> 3. **FULL-SENTENCES：输入是从一个或多个文档中采样的一个full-sentence（可由多个句子组成）**，当采样到文档的最后时，可以继续采下一个文档的句子，但是需要在中间加一个额外的分隔token。**无NSP Loss**\n> 4. **DOC-SENTENCES：和前者类似，但是不能跨文档采样，无NSP Loss**。在靠近文档末尾采样，句子长度会较短，所以同样为了实验公平，适当的增加了数据\n\n- 实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809150834375.png\" alt=\"image-20220809150834375\" style=\"zoom:75%;\" />\n\n- 将前两种策略进行对比，**可以发现使用单个句子会降低表现，推测原因为：这样模型无法学到长范围的依赖关系**\n\n> We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.  \n\n- 再对比前两种和后两种策略，**可以发现移除NSP任务，也可以得到细微的提升**\n\n> Removing the NSP loss matches or slightly improves downstream task performance.\n\n- 对比后两种策略，**可以发现不跨文档比跨文档稍好一点**\n\n\n\n### 2.3 使用更大的Batch Size\n\n- 作者使用相同的数据量，但是不同的batch size，来探究batch size的影响，结构如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809152515016.png\" alt=\"image-20220809152515016\" style=\"zoom:80%;\" />\n\n图中bsz为batch size，ppl为困惑度\n\n\n\n### 2.4 分词方式\n\n- 原版BERT使用char-level的BPE（也就是wordpiece），而本文使用byte-level的BPE，增加了词典大小，和增加了一些参数量，表现还有细微的下降😅，但是基本相同\n\n> - 基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。\n>\n> - 基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。\n>\n>   \n>\n> - 当采用 bytes-level 的 BPE 之后，词表大小从3万（原始 BERT 的 char-level ）增加到5万。这分别为 BERT-base和 BERT-large增加了1500万和2000万额外的参数\n\n\n\n\n\n# 3 对比试验\n\n- 对于实验用于探究预训练数据量的多少和训练时间对表现的影响，实验采用Large的模型规模，并且RoBERTa采用：动态mask + FULL_SENTENCES without NSP + 大batch size + byte-level BPE，实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809164004895.png\" alt=\"image-20220809164004895\" style=\"zoom:75%;\" />\n\n- **可以看到在相同数据量条件下，RoBERTa相比于BERT有较大提升**\n\n- **并且增加数据量可以得到显著的提升，增加训练时间同样可以得到细微提升，并且没有造成过拟合**\n\n\n\n\n\n# 4 在特定下游任务的表现\n\n### 4.1 GLUE\n\n- 实验考虑了$$batch\\_size \\in \\{16, 32\\}$$，$$lr \\in \\{1e-5, 2e-5, 3e-5\\}$$，并在前6%的steps使用线性warm up，之后使用线性衰减至0，使用10个epochs，但是设置了early stop，实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170036407.png\" alt=\"image-20220809170036407\" style=\"zoom:75%;\" />\n\n\n\n### 4.2 SQuAD\n\n- 在本实验中，XLNET和BERT都加上了QA数据集，但是RoBERTa仅使用了SQuAD。并且XLNET使用了逐层不同的学习率，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170501845.png\" alt=\"image-20220809170501845\" style=\"zoom:70%;\" />\n\n\n\n### 4.3 RACE\n\n- 一个做阅读理解的数据集，让模型从4个答案中选出一个最合适的。结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170610509.png\" alt=\"image-20220809170610509\" style=\"zoom:80%;\" />\n\n\n\n\n\n# 5 超参\n\n- **预训练：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170735173.png\" alt=\"image-20220809170735173\" style=\"zoom:50%;\" />\n\n- **微调：**\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170753600.png\" alt=\"image-20220809170753600\" style=\"zoom:67%;\" />","slug":"RoBERTa总结","published":1,"updated":"2022-12-20T06:24:17.680Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1e00097csz5xr8fmfu","content":"<ul>\n<li>作者评估了BERT中的各种超参和数据集大小以及训练策略等方面，发现<strong>BERT是训练不足的</strong></li>\n</ul>\n<h1 id=\"1-RoBERTa的主要改变\"><a href=\"#1-RoBERTa的主要改变\" class=\"headerlink\" title=\"1  RoBERTa的主要改变\"></a>1  RoBERTa的主要改变</h1><ul>\n<li>作者提出了RoBERTa（A Robustly Optimized BERT Pretraining Approach），是BERT的变体，主要的更改如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li>用更多的数据和更大的batch size，训练更长的时间</li>\n<li>去除了NSP</li>\n<li>在更长的句子上训练</li>\n<li>进行动态的mask</li>\n</ol>\n</blockquote>\n<ul>\n<li>在优化器方面也有细微改变，改变了最大学习率和warmup steps，并且把<script type=\"math/tex\">\\beta_2</script>改为了0.98，在更大的batch size上训练时，得到了更稳定的结果</li>\n</ul>\n<h1 id=\"2-训练策略的改变\"><a href=\"#2-训练策略的改变\" class=\"headerlink\" title=\"2 训练策略的改变\"></a>2 训练策略的改变</h1><h3 id=\"2-1-动态和静态mask\"><a href=\"#2-1-动态和静态mask\" class=\"headerlink\" title=\"2.1 动态和静态mask\"></a>2.1 动态和静态mask</h3><ul>\n<li><strong>两种mask策略：</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li>静态mask：原BERT中使用的方法，在数据预处理时，进行mask。在本实验中，为了避免出现每个epoch的mask的位置都相同，在40个epoch中进行了10次随机mask，这样每个mask实例只会出现4次</li>\n<li>动态mask：在每次数据喂入模型时进行mask，这样每次mask的位置都不同</li>\n</ol>\n</blockquote>\n<ul>\n<li>实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809143220226.png\" alt=\"image-20220809143220226\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><strong>动态mask相对于静态mask得到了细微的提升（稍微有丁点卵用）</strong></li>\n</ul>\n<h3 id=\"2-2-模型的输入形式和NSP任务\"><a href=\"#2-2-模型的输入形式和NSP任务\" class=\"headerlink\" title=\"2.2 模型的输入形式和NSP任务\"></a>2.2 模型的输入形式和NSP任务</h3><ul>\n<li><strong>使用了4种输入策略：</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>SEGMENT-PAIR+NSP：</strong>原BERT使用的方法，<strong>输入是一个segment对，每个segment可以包括多个句子</strong>，只要总长度小于512就行，带有NSP Loss</li>\n<li><strong>SENTENCE-PAIR+NSP：</strong>和前者类似，<strong>只是每个segment只能是单个句子</strong>，同样带有NSP Loss。由于两个单句子一般长度都远远不足512，为了实验公平，作者适当的增加了其实验数据</li>\n<li><strong>FULL-SENTENCES：输入是从一个或多个文档中采样的一个full-sentence（可由多个句子组成）</strong>，当采样到文档的最后时，可以继续采下一个文档的句子，但是需要在中间加一个额外的分隔token。<strong>无NSP Loss</strong></li>\n<li><strong>DOC-SENTENCES：和前者类似，但是不能跨文档采样，无NSP Loss</strong>。在靠近文档末尾采样，句子长度会较短，所以同样为了实验公平，适当的增加了数据</li>\n</ol>\n</blockquote>\n<ul>\n<li>实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809150834375.png\" alt=\"image-20220809150834375\" style=\"zoom:75%;\" /></p>\n<ul>\n<li>将前两种策略进行对比，<strong>可以发现使用单个句子会降低表现，推测原因为：这样模型无法学到长范围的依赖关系</strong></li>\n</ul>\n<blockquote>\n<p>We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.  </p>\n</blockquote>\n<ul>\n<li>再对比前两种和后两种策略，<strong>可以发现移除NSP任务，也可以得到细微的提升</strong></li>\n</ul>\n<blockquote>\n<p>Removing the NSP loss matches or slightly improves downstream task performance.</p>\n</blockquote>\n<ul>\n<li>对比后两种策略，<strong>可以发现不跨文档比跨文档稍好一点</strong></li>\n</ul>\n<h3 id=\"2-3-使用更大的Batch-Size\"><a href=\"#2-3-使用更大的Batch-Size\" class=\"headerlink\" title=\"2.3 使用更大的Batch Size\"></a>2.3 使用更大的Batch Size</h3><ul>\n<li>作者使用相同的数据量，但是不同的batch size，来探究batch size的影响，结构如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809152515016.png\" alt=\"image-20220809152515016\" style=\"zoom:80%;\" /></p>\n<p>图中bsz为batch size，ppl为困惑度</p>\n<h3 id=\"2-4-分词方式\"><a href=\"#2-4-分词方式\" class=\"headerlink\" title=\"2.4 分词方式\"></a>2.4 分词方式</h3><ul>\n<li>原版BERT使用char-level的BPE（也就是wordpiece），而本文使用byte-level的BPE，增加了词典大小，和增加了一些参数量，表现还有细微的下降😅，但是基本相同</li>\n</ul>\n<blockquote>\n<ul>\n<li><p>基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。</p>\n</li>\n<li><p>基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。</p>\n</li>\n</ul>\n<ul>\n<li>当采用 bytes-level 的 BPE 之后，词表大小从3万（原始 BERT 的 char-level ）增加到5万。这分别为 BERT-base和 BERT-large增加了1500万和2000万额外的参数</li>\n</ul>\n</blockquote>\n<h1 id=\"3-对比试验\"><a href=\"#3-对比试验\" class=\"headerlink\" title=\"3 对比试验\"></a>3 对比试验</h1><ul>\n<li>对于实验用于探究预训练数据量的多少和训练时间对表现的影响，实验采用Large的模型规模，并且RoBERTa采用：动态mask + FULL_SENTENCES without NSP + 大batch size + byte-level BPE，实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809164004895.png\" alt=\"image-20220809164004895\" style=\"zoom:75%;\" /></p>\n<ul>\n<li><p><strong>可以看到在相同数据量条件下，RoBERTa相比于BERT有较大提升</strong></p>\n</li>\n<li><p><strong>并且增加数据量可以得到显著的提升，增加训练时间同样可以得到细微提升，并且没有造成过拟合</strong></p>\n</li>\n</ul>\n<h1 id=\"4-在特定下游任务的表现\"><a href=\"#4-在特定下游任务的表现\" class=\"headerlink\" title=\"4 在特定下游任务的表现\"></a>4 在特定下游任务的表现</h1><h3 id=\"4-1-GLUE\"><a href=\"#4-1-GLUE\" class=\"headerlink\" title=\"4.1 GLUE\"></a>4.1 GLUE</h3><ul>\n<li>实验考虑了<script type=\"math/tex\">batch\\_size \\in \\{16, 32\\}</script>，<script type=\"math/tex\">lr \\in \\{1e-5, 2e-5, 3e-5\\}</script>，并在前6%的steps使用线性warm up，之后使用线性衰减至0，使用10个epochs，但是设置了early stop，实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170036407.png\" alt=\"image-20220809170036407\" style=\"zoom:75%;\" /></p>\n<h3 id=\"4-2-SQuAD\"><a href=\"#4-2-SQuAD\" class=\"headerlink\" title=\"4.2 SQuAD\"></a>4.2 SQuAD</h3><ul>\n<li>在本实验中，XLNET和BERT都加上了QA数据集，但是RoBERTa仅使用了SQuAD。并且XLNET使用了逐层不同的学习率，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170501845.png\" alt=\"image-20220809170501845\" style=\"zoom:70%;\" /></p>\n<h3 id=\"4-3-RACE\"><a href=\"#4-3-RACE\" class=\"headerlink\" title=\"4.3 RACE\"></a>4.3 RACE</h3><ul>\n<li>一个做阅读理解的数据集，让模型从4个答案中选出一个最合适的。结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170610509.png\" alt=\"image-20220809170610509\" style=\"zoom:80%;\" /></p>\n<h1 id=\"5-超参\"><a href=\"#5-超参\" class=\"headerlink\" title=\"5 超参\"></a>5 超参</h1><ul>\n<li><strong>预训练：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170735173.png\" alt=\"image-20220809170735173\" style=\"zoom:50%;\" /></p>\n<ul>\n<li><strong>微调：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170753600.png\" alt=\"image-20220809170753600\" style=\"zoom:67%;\" /></p>\n","site":{"data":{}},"wordcount":2084,"excerpt":"","more":"<ul>\n<li>作者评估了BERT中的各种超参和数据集大小以及训练策略等方面，发现<strong>BERT是训练不足的</strong></li>\n</ul>\n<h1 id=\"1-RoBERTa的主要改变\"><a href=\"#1-RoBERTa的主要改变\" class=\"headerlink\" title=\"1  RoBERTa的主要改变\"></a>1  RoBERTa的主要改变</h1><ul>\n<li>作者提出了RoBERTa（A Robustly Optimized BERT Pretraining Approach），是BERT的变体，主要的更改如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li>用更多的数据和更大的batch size，训练更长的时间</li>\n<li>去除了NSP</li>\n<li>在更长的句子上训练</li>\n<li>进行动态的mask</li>\n</ol>\n</blockquote>\n<ul>\n<li>在优化器方面也有细微改变，改变了最大学习率和warmup steps，并且把<script type=\"math/tex\">\\beta_2</script>改为了0.98，在更大的batch size上训练时，得到了更稳定的结果</li>\n</ul>\n<h1 id=\"2-训练策略的改变\"><a href=\"#2-训练策略的改变\" class=\"headerlink\" title=\"2 训练策略的改变\"></a>2 训练策略的改变</h1><h3 id=\"2-1-动态和静态mask\"><a href=\"#2-1-动态和静态mask\" class=\"headerlink\" title=\"2.1 动态和静态mask\"></a>2.1 动态和静态mask</h3><ul>\n<li><strong>两种mask策略：</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li>静态mask：原BERT中使用的方法，在数据预处理时，进行mask。在本实验中，为了避免出现每个epoch的mask的位置都相同，在40个epoch中进行了10次随机mask，这样每个mask实例只会出现4次</li>\n<li>动态mask：在每次数据喂入模型时进行mask，这样每次mask的位置都不同</li>\n</ol>\n</blockquote>\n<ul>\n<li>实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809143220226.png\" alt=\"image-20220809143220226\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><strong>动态mask相对于静态mask得到了细微的提升（稍微有丁点卵用）</strong></li>\n</ul>\n<h3 id=\"2-2-模型的输入形式和NSP任务\"><a href=\"#2-2-模型的输入形式和NSP任务\" class=\"headerlink\" title=\"2.2 模型的输入形式和NSP任务\"></a>2.2 模型的输入形式和NSP任务</h3><ul>\n<li><strong>使用了4种输入策略：</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>SEGMENT-PAIR+NSP：</strong>原BERT使用的方法，<strong>输入是一个segment对，每个segment可以包括多个句子</strong>，只要总长度小于512就行，带有NSP Loss</li>\n<li><strong>SENTENCE-PAIR+NSP：</strong>和前者类似，<strong>只是每个segment只能是单个句子</strong>，同样带有NSP Loss。由于两个单句子一般长度都远远不足512，为了实验公平，作者适当的增加了其实验数据</li>\n<li><strong>FULL-SENTENCES：输入是从一个或多个文档中采样的一个full-sentence（可由多个句子组成）</strong>，当采样到文档的最后时，可以继续采下一个文档的句子，但是需要在中间加一个额外的分隔token。<strong>无NSP Loss</strong></li>\n<li><strong>DOC-SENTENCES：和前者类似，但是不能跨文档采样，无NSP Loss</strong>。在靠近文档末尾采样，句子长度会较短，所以同样为了实验公平，适当的增加了数据</li>\n</ol>\n</blockquote>\n<ul>\n<li>实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809150834375.png\" alt=\"image-20220809150834375\" style=\"zoom:75%;\" /></p>\n<ul>\n<li>将前两种策略进行对比，<strong>可以发现使用单个句子会降低表现，推测原因为：这样模型无法学到长范围的依赖关系</strong></li>\n</ul>\n<blockquote>\n<p>We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.  </p>\n</blockquote>\n<ul>\n<li>再对比前两种和后两种策略，<strong>可以发现移除NSP任务，也可以得到细微的提升</strong></li>\n</ul>\n<blockquote>\n<p>Removing the NSP loss matches or slightly improves downstream task performance.</p>\n</blockquote>\n<ul>\n<li>对比后两种策略，<strong>可以发现不跨文档比跨文档稍好一点</strong></li>\n</ul>\n<h3 id=\"2-3-使用更大的Batch-Size\"><a href=\"#2-3-使用更大的Batch-Size\" class=\"headerlink\" title=\"2.3 使用更大的Batch Size\"></a>2.3 使用更大的Batch Size</h3><ul>\n<li>作者使用相同的数据量，但是不同的batch size，来探究batch size的影响，结构如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809152515016.png\" alt=\"image-20220809152515016\" style=\"zoom:80%;\" /></p>\n<p>图中bsz为batch size，ppl为困惑度</p>\n<h3 id=\"2-4-分词方式\"><a href=\"#2-4-分词方式\" class=\"headerlink\" title=\"2.4 分词方式\"></a>2.4 分词方式</h3><ul>\n<li>原版BERT使用char-level的BPE（也就是wordpiece），而本文使用byte-level的BPE，增加了词典大小，和增加了一些参数量，表现还有细微的下降😅，但是基本相同</li>\n</ul>\n<blockquote>\n<ul>\n<li><p>基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。</p>\n</li>\n<li><p>基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。</p>\n</li>\n</ul>\n<ul>\n<li>当采用 bytes-level 的 BPE 之后，词表大小从3万（原始 BERT 的 char-level ）增加到5万。这分别为 BERT-base和 BERT-large增加了1500万和2000万额外的参数</li>\n</ul>\n</blockquote>\n<h1 id=\"3-对比试验\"><a href=\"#3-对比试验\" class=\"headerlink\" title=\"3 对比试验\"></a>3 对比试验</h1><ul>\n<li>对于实验用于探究预训练数据量的多少和训练时间对表现的影响，实验采用Large的模型规模，并且RoBERTa采用：动态mask + FULL_SENTENCES without NSP + 大batch size + byte-level BPE，实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809164004895.png\" alt=\"image-20220809164004895\" style=\"zoom:75%;\" /></p>\n<ul>\n<li><p><strong>可以看到在相同数据量条件下，RoBERTa相比于BERT有较大提升</strong></p>\n</li>\n<li><p><strong>并且增加数据量可以得到显著的提升，增加训练时间同样可以得到细微提升，并且没有造成过拟合</strong></p>\n</li>\n</ul>\n<h1 id=\"4-在特定下游任务的表现\"><a href=\"#4-在特定下游任务的表现\" class=\"headerlink\" title=\"4 在特定下游任务的表现\"></a>4 在特定下游任务的表现</h1><h3 id=\"4-1-GLUE\"><a href=\"#4-1-GLUE\" class=\"headerlink\" title=\"4.1 GLUE\"></a>4.1 GLUE</h3><ul>\n<li>实验考虑了<script type=\"math/tex\">batch\\_size \\in \\{16, 32\\}</script>，<script type=\"math/tex\">lr \\in \\{1e-5, 2e-5, 3e-5\\}</script>，并在前6%的steps使用线性warm up，之后使用线性衰减至0，使用10个epochs，但是设置了early stop，实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170036407.png\" alt=\"image-20220809170036407\" style=\"zoom:75%;\" /></p>\n<h3 id=\"4-2-SQuAD\"><a href=\"#4-2-SQuAD\" class=\"headerlink\" title=\"4.2 SQuAD\"></a>4.2 SQuAD</h3><ul>\n<li>在本实验中，XLNET和BERT都加上了QA数据集，但是RoBERTa仅使用了SQuAD。并且XLNET使用了逐层不同的学习率，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170501845.png\" alt=\"image-20220809170501845\" style=\"zoom:70%;\" /></p>\n<h3 id=\"4-3-RACE\"><a href=\"#4-3-RACE\" class=\"headerlink\" title=\"4.3 RACE\"></a>4.3 RACE</h3><ul>\n<li>一个做阅读理解的数据集，让模型从4个答案中选出一个最合适的。结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170610509.png\" alt=\"image-20220809170610509\" style=\"zoom:80%;\" /></p>\n<h1 id=\"5-超参\"><a href=\"#5-超参\" class=\"headerlink\" title=\"5 超参\"></a>5 超参</h1><ul>\n<li><strong>预训练：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170735173.png\" alt=\"image-20220809170735173\" style=\"zoom:50%;\" /></p>\n<ul>\n<li><strong>微调：</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220809170753600.png\" alt=\"image-20220809170753600\" style=\"zoom:67%;\" /></p>\n"},{"title":"NLP基础","math":true,"date":"2022-01-07T16:00:00.000Z","_content":"\n\n\n# 1 词嵌入（word embedding）\n\n- 词向量是⽤来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）\n\n\n\n### 1.1 词嵌入相比于one-hot向量的优点\n\n- 虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，**one-hot词 向量⽆法准确表达不同词之间的相似度**，如我们常常使⽤的余弦相似度。对于向量$$x, y \\in R^d$$，它 们的余弦相似度是它们之间夹⻆的余弦值：\n\n$$\n\\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{y}}{\\|\\boldsymbol{x}\\|\\|\\boldsymbol{y}\\|} \\in[-1,1]\n$$\n\n由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。\n\nword2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表示成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。word2vec⼯具包含了两个模型，即**跳字模型（skip-gram）** 和**连续词袋模型（continuous bag of words，CBOW）**\n\n\n\n### 1.2 跳字模型（skip-gram）\n\n- 跳字模型假设基于某个词来生成它在⽂本序列周围的词\n- 举个例子，假设⽂本序列是 “the” “man” “loves” “his” “son”。以“loves”作为中⼼词，设背景窗口大小为2。如下图所示，跳字模型所关⼼的是，给定中⼼词“loves”，生成与它距离不超过2个词的背景词 “the” “man” “his” “son”的条件概率，即：\n\n$$\nP(the, man, his, son | loves)\n$$\n\n假设给定中⼼词的情况下，背景词的生成是相互独⽴的，那么上式可以改写成：\n$$\nP(the| loves) \\times P(man | loves) \\times P(his | loves) \\times P(son | loves)\n$$\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125151903218.png\" alt=\"image-20220125151903218\" style=\"zoom: 80%;\" />\n\n- 在跳字模型中，**每个词被表示成两个$$d$$维向量**，⽤来计算条件概率。假设这个词在词典中索引为$$i$$， 当它为中心词时向量表示为$$v_i \\in R^d$$，而为背景词时向量表示为$$u_i \\in R^d$$。设中⼼词$$w_c$$在词典中索引为$$c$$，背景词$$w_o$$在词典中索引为$$o$$，**给定中⼼词$$w_c$$生成背景词$$w_o$$的条件概率可以通过对向量内积做softmax运算而得到：**\n\n$$\nP\\left(w_{o} \\mid w_{c}\\right)=\\frac{\\exp \\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{v}_{c}\\right)},\n$$\n\n其中词典索引集$$V = \\{0, 1, . . . , |V|−1\\}$$\n\n- 假设给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$。 假设给定中⼼词的情况下背景词的生成相互独⽴，当背景窗口大小为m时，跳字模型的**似然函数**即给定任⼀中⼼词生成所有背景词的概率：\n\n$$\n\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(w^{(t+j)} \\mid w^{(t)}\\right),\n$$\n\n这⾥小于1或大于T的时间步可以被忽略\n\n\n\n- **跳字模型的训练：**\n\n>- **跳字模型的参数是每个词所对应的中心词向量和背景词向量**\n>- 先把把最大似然函数取负对数：\n>\n>$$\n>-\\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P\\left(w^{(t+j)} \\mid w^{(t)}\\right)\n>$$\n>\n>如果使⽤随机梯度下降，那么在每⼀次迭代⾥我们随机采样⼀个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数\n>\n>- 由Softmax的运算结果可以得到：\n>\n>$$\n>\\log P\\left(w_{o} \\mid w_{c}\\right)=\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}-\\log \\left(\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{v}_{c}\\right)\\right)\n>$$\n>\n>那么我们可以求得上式关于各中心词向量和背景词向量的梯度，比如关于$$v_c$$的梯度：\n>$$\n>\\frac{\\partial \\log P\\left(w_{o} \\mid w_{c}\\right)}{\\partial v_{c}}=\\boldsymbol{u}_{o}-\\sum_{j \\in \\mathcal{V}} P\\left(w_{j} \\mid w_{c}\\right) \\boldsymbol{u}_{j} .\n>$$\n>训练结束后，对于词典中的任⼀索引为$$i$$的词，我们均得到该词作为中⼼词和背景词的两组词向量$$v_i$$和$$u_i$$。在⾃然语⾔处理应⽤中，**⼀般使⽤跳字模型的中心词向量作为词的表征向量**\n\n\n\n- **跳字模型的具体实现：**\n\n> - 可以用一个input-hidden-output的三层神经网络来建模上面的skip-model：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526195633208.jpg\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\" />\n>\n> - 输入的表示：输入层中每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1,其余维度的值均为0\n>\n> - 网络中传播的前向过程：输出层向量的值可以通过**隐含层向量（K维，即每一个词向量的维度）**，以及连接隐藏层和输出层之间的**KxN维权重矩阵**计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算每一个单词的生成概率。\n> - **input层和hidden层之间的N$$\\times$$K权重矩阵即N个词的中心词向量，从input层到hidden层的运算即对中心词向量的提取，具体见下图。而hidden层和output层之间的K$$\\times$$N权重矩阵即N个词的背景词向量，hidden层到output层的运算即实现$$u^T_ov_c$$，然后再在最后做一个softmax**\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526202027163.jpg\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\" />\n>\n> 左边矩阵是input，右边矩阵是input到hidden的权重矩阵，即N个词的中心词向量，由于输入是ont-hot向量，所以运算结果其实就是对中心词向量的一个提取\n>\n> \n>\n> - 在代码实现中是直接使用一个**嵌入层（Embedding层）**，此层的权值形状为**词典大小$$\\times$$每个词的维度**，可以直接将中心词和背景词对应的one-hot向量转化为词向量。以skip-gram举例，中心词的one-hot向量形状为(批量大小，1)，背景词为(批量大小，词典大小)，两者皆通过嵌入层转化成词向量后形状分别为(批量大小，1，每个词维度)、(批量大小，词典大小，每个词维度)。然后通过**小批量乘法**实现中心词向量和背景词向量的相乘$$u^T_ov_c$$，得到形状为(批量大小，词典大小)的结果，然后进行Softmax\n> - 上面的背景词one-hot向量第二维度不一定要为词典大小，如果我们采用**负采样**，则应该改为**正负样本和的大小**，具体可看下方负采样原理\n> - 现在我们来解释一下**小批量乘法**，假设第⼀个小批量包含n个形状为$$a \\times b$$的矩阵$$X_1, . . . , X_n$$，第⼆个小批量包含n个形状为$$b \\times c$$的矩阵$$Y_1, . . . , Y_n$$。 这两个小批量的矩阵乘法输出为n个形状为$$a \\times c$$的矩阵$$X_1Y_1, . . . , X_nY_n$$\n\n\n\n### 1.3 连续词袋模型（CBOW）\n\n- 连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，**连续词袋模型假设基于某中⼼词在 ⽂本序列前后的背景词来生成该中心词**\n\n- 如下图，在同样的⽂本序列“the” “man” “loves” “his” “son” ⾥，以“loves”作为中⼼词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词 “the” “man” “his” “son”生成中心词“loves”的条件概率，也就是：\n\n$$\nP(loves|the,man,his,son)\n$$\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125172417014.png\" alt=\"image-20220125172417014\" style=\"zoom:80%;\" />\n\n- 因为连续词袋模型的背景词有多个，我们**将这些背景词向量取平均**，然后使⽤和跳字模型⼀样的 ⽅法来计算条件概率\n\n  设$$v_i \\in R^d$$和$$u_i \\in R^d$$分别表示词典中索引为$$i$$的词作为背景词和中⼼词的向 量（**注意符号的含义与跳字模型中的相反**）。设中⼼词$$w_c$$在词典中索引为$$c$$，背景词$$w_{o1} , . . . , w_{o2m}$$在 词典中索引为$$o_1, . . . , o_{2m}$$，那么给定背景词生成中⼼词的条件概率：\n  $$\n  .P\\left(w_{c} \\mid w_{o_{1}}, \\ldots, w_{o_{2 m}}\\right)=\\frac{\\exp \\left(\\frac{1}{2 m} \\boldsymbol{u}_{c}^{\\top}\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right)\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\frac{1}{2 m} \\boldsymbol{u}_{i}^{\\top}\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right)\\right)}\n  $$\n  设$$W_o = \\{w_{o1} , . . . , w_{o2m}\\}$$，$$\\overline{\\boldsymbol{v}}_{o}=\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right) /(2 m)$$，可将上式简化为：\n  $$\n  P\\left(w_{c} \\mid \\mathcal{W}_{o}\\right)=\\frac{\\exp \\left(\\boldsymbol{u}_{c}^{\\top} \\overline{\\boldsymbol{v}}_{o}\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\overline{\\boldsymbol{v}}_{o}\\right)}\n  $$\n\n- 给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$，背景窗口大小为m。连续词袋模型的似然函数是由背景词生成任⼀中⼼词的概率：\n\n$$\n\\prod_{t=1}^{T} P\\left(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}\\right)\n$$\n\n- 接着再像跳字模型一样求似然函数的负对数，然后再求关于背景向量和中心向量的梯度。**同跳字模型不⼀样的⼀点在于，我们⼀般使⽤连续词袋模型的背景词向量作为词的表征向量**\n\n\n\n- **跳字模型更适合大型语料库，CBOW更适合小型的语料库**\n\n### 1.4 近似训练\n\n- 无论是跳字模型还是连续词袋模型，在求梯度的时候，由于条件概率使⽤了softmax运 算，每⼀步的梯度计算都包含词典大小数⽬的项的累加。对于含⼏⼗万或上百万词的较大词典， 每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练⽅法，即**负采样（negative sampling）**或**层序softmax（hierarchical softmax）**\n\n##### 1.4.1 高频词抽样\n\n- 首先介绍一下对高频词进行抽样（也称⼆次采样(subsampling)）。对于文本中的高频词，比如\"the\"，不进行抽样会带来两个问题：\n\n> 1. 当我们得到成对的单词训练样本时，**(\"fox\", \"the\") 这样的训练样本并不会给我们提供关于“fox”更多的语义信息**，因为“the”在每个单词的上下文中几乎都会出现\n> 2. 由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，...）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数\n\n- Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：**对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关，词频越高被删除的概率就越大**\n\n\n\n##### 1.4.2 负采样（negative sampling）\n\n- **负采样（negative sampling）**解决了因为词典大小过大的计算开销问题。不同于原本每个训练样本更新所有的权重，**负采样每次让一个训练样本仅仅更新一小部分的权重**，这样就会降低梯度下降过程中的计算量。\n- 当我们用训练样本 ( input word: \"fox\"，output word: \"quick\") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们**期望输出为0的神经元结点**所对应的单词我们称为**“negative” word**。\n- 当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。**（在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。）**\n\n- **每个单词被选为“negative words”的概率计算公式与其出现的频次有关，词频越高，被选上的概率就越高**，论文中给的公式是：\n\n$$\nP\\left(w_{i}\\right)=\\frac{f\\left(w_{i}\\right)^{3 / 4}}{\\sum_{j=0}^{n}\\left(f\\left(w_{j}\\right)^{3 / 4}\\right)}\n$$\n\n- **负采样概述：**\n\n> - 负采样修改了原来的⽬标函数。给定中⼼词$$w_c$$的⼀个背景窗口，我们把背景词$$w_o$$出现在该背景窗口看作⼀个事件，并将该事件的概率计算为：\n>\n> $$\n> P\\left(D=1 \\mid w_{c}, w_{o}\\right)=\\sigma\\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)\n> $$\n>\n> - 我们先考虑最大化⽂本序列中所有该事件的联合概率来训练词向量。具体来说，给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$且背景窗口大小为m，考虑最大化联合概率：\n>\n> $$\n> \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(D=1 \\mid w^{(t)}, w^{(t+j)}\\right)\n> $$\n>\n> 然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为⽆穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫⽆意义。**负采样通过采样并添加负类样本使⽬标函数更有意义。**\n>\n> - 在采集了K个负样本后，我们可以将所求概率近似为：\n>\n> $$\n> P\\left(w^{(t+j)} \\mid w^{(t)}\\right)=P\\left(D=1 \\mid w^{(t)}, w^{(t+j)}\\right) \\prod_{k=1, w_{k} \\sim P(w)}^{K} P\\left(D=0 \\mid w^{(t)}, w_{k}\\right)\n> $$\n>\n> - 取负对数后变成了：\n>\n> $$\n> -\\log P\\left(w^{(t+j)} \\mid w^{(t)}\\right)=-\\log \\sigma\\left(\\boldsymbol{u}_{i_{t+j}}^{\\top} v_{i_{t}}\\right)-\\sum_{k=1, w_{k} \\sim P(w)}^{K} \\log \\sigma\\left(-\\boldsymbol{u}_{h_{k}}^{\\top} v_{i_{t}}\\right)\n> $$\n>\n> 现在，**训练中每⼀步的梯度计算开销不再与词典大小相关，而与K线性相关**。当K取较小的常数时，负采样在每⼀步的梯度计算开销较小。\n\n- **负采样的具体实现：**\n\n> - 在实际应用中，我们并没有严格的将不在窗口中的词定义为负样本，而是根据词频直接随机取负样本，这样的实验结果是更优的。\n>\n> - **设词典大小为$$\\mathcal{V}$$，将一根长为1的线段分为$$\\mathcal{V}$$段，每段分别对应一个词，每个词对应的长度就是上面所写的每个词对应的频率$$P(w_i)$$，然后取一个M（M>>$$\\mathcal{V}$$），论文中M取的是$$10^8$$，将长度为1的线段均分为M份（如下图），然后取K个负样本即在该线段上取K个点，取到的点在哪个词的对应线段就取哪个词**\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211153340493.png\" alt=\"image-20220211153340493\" style=\"zoom:67%;\" />\n\n\n\n##### 1.4.3 层序Softmax（hierarchical softmax）\n\n- 层序softmax是另⼀种近似训练法。它使⽤了⼆叉树这⼀数据结构，树的每个叶结点代表词典$$\\mathcal{V}$$中的每个词。\n- <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220210181128975.png\" alt=\"image-20220210181128975\" style=\"zoom: 80%;\" />\n\n假设$$L(w)$$为从⼆叉树的根结点到词$$w$$的叶结点的路径（包括根结点和叶结点）上的结点数。 设$$n(w, j)$$为该路径上第$$j$$个结点，并设该结点的背景词向量为$$u_{n(w,j)}$$。以上图为例，$$L(w_3) = 4$$ 层序softmax将跳字模型中的条件概率近似表示为：\n$$\nP\\left(w_{o} \\mid w_{c}\\right)=\\prod_{j=1}^{L\\left(w_{o}\\right)-1} \\sigma\\left([[ n\\left(w_{o}, j+1\\right)=\\operatorname{leftChild}\\left(n\\left(w_{o}, j\\right)\\right) ]] \\cdot \\boldsymbol{u}_{n\\left(w_{o}, j\\right)}^{\\top} \\boldsymbol{v}_{c}\\right),\n$$\n其中如果判断x为真，[[x]] = 1；反之[[x]] = −1。\n\n例如让我们计算上图中给定词$$w_c$$生成词$$w_3$$的条件概率：\n$$\nP\\left(w_{3} \\mid w_{c}\\right)=\\sigma\\left(\\boldsymbol{u}_{n\\left(w_{3}, 1\\right)}^{\\top} \\boldsymbol{v}_{c}\\right) \\cdot \\sigma\\left(-\\boldsymbol{u}_{n\\left(w_{3}, 2\\right)}^{\\top} \\boldsymbol{v}_{c}\\right) \\cdot \\sigma\\left(\\boldsymbol{u}_{n\\left(w_{3}, 3\\right)}^{\\top} \\boldsymbol{v}_{c}\\right)\n$$\n\n- **推导过程：**\n\n> 设$$d_{2}^{w}, d_{3}^{w}, \\cdots, d_{l w}^{w} \\in\\{0,1\\}$$为词$$w$$的编码，$$d_j^w$$代表$$w$$路径上的第$$j$$个节点对应的编码（根节点无编码），则：\n> $$\n> P(w_0|w_c) = \\prod_{j=2}^{L(w)} P\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right)\n> $$\n> 其中的每一项都是一个Logistic回归：\n> $$\n> P\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right) = \\left\\{\\begin{array}{ll}\n> \\sigma\\left(v_c^T u_{n_(w, j)}\\right), & d_{j}^{w}=0 \\\\\n> 1-\\sigma\\left(v_c^T u_{n_(w, j)}\\right), & d_{j}^{w}=1\n> \\end{array}\\right.\n> $$\n> 可以将两式合并，并且由于$$\\sigma(x) + \\sigma(-x) = 1$$，可以将上式转化为：\n> $$\n> P\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right)=\\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]^{1-d_{j}^{w}} \\cdot\\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]^{d^{w}}\n> $$\n> 我们取目标函数的对数：\n> $$\n> \\mathcal{L} = \\sum_{w \\in \\mathcal{V}}\\log P(Context(w) | w) = \\sum_{w \\in \\mathcal{V}} \\sum_{j=2}^{L(w)}\\left\\{\\left(1-d_{j}^{w}\\right) \\cdot \\log \\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]+d_{j}^{w} \\cdot \\log \\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]\\right\\}\n> $$\n> **要最大化上述目标函数是比较困难的，但是我们可以最大化每一子项（即下式），达到局部最大化的效果**：\n> $$\n> \\mathcal{L}(w, j) = \\left\\{\\left(1-d_{j}^{w}\\right) \\cdot \\log \\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]+d_{j}^{w} \\cdot \\log \\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]\\right\\}\n> $$\n> **这样就可以将计算复杂度降为$$O(log_2 |\\mathcal{V}|)$$，当词典$$\\mathcal{V}$$很大时，层序softmax在训练中每⼀步的梯度计算开销相较未使用近似训练时大幅降低。**\n>\n> 显然，上式的$$d_j^w$$是用来**判断下一个节点是否是本节点的左孩子的**，其作用是为了满足给定任意中心词$$w_c$$，生成背景词的条件概率和为1：\n> $$\n> \\sum_{w \\in \\mathcal{V}} P\\left(w \\mid w_{c}\\right)=1\n> $$\n> 这样才能使似然函数不至于训练到无穷大（和负采样要添加负样本是一样的原因），并且符合概率和为1\n\n- **层序Softmax中的二叉树是使用的哈夫曼树，可以使搜索次数达到最小**\n\n\n\n# 2 子词嵌入（fastText）\n\n- 英语单词通常有其内部结构和形成⽅式。例如，我们可以从“dog” “dogs”和“dogcatcher”的 字面上推测它们的关系。这些词都有同⼀个词根“dog”，但使⽤不同的后缀来改变词的含义。而 且，这个关联可以推⼴⾄其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关 系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。这方面的研究称为构词学\n- 在word2vec中，我们并没有直接利⽤构词学中的信息。⽆论是在跳字模型还是连续词袋模型中， 我们都将形态不同的单词⽤不同的向量来表示。例如，“dog”和“dogs”分别⽤两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌⼊（subword embedding）的⽅法，从而试图将构词信息引⼊word2vec中的跳字模型\n\n- 在fastText中，每个中⼼词被表⽰成子词的集合。举个例子，例如单词\"where\"，⾸先，我们在单词的**⾸尾分别添加特殊字符“<” 和  “>” 以区分作为前后缀的子词**。然后，**将单词当成⼀个由字符构成的序列来提取n元语法**。例如，当n = 3时，我们得到所有⻓度为3的子词：\"<wh\"、\"whe\"、\"her\"、\"ere\"、\"re>\"以及特殊子词\"where\"。\n- 在fastText中，对于⼀个词w，我们将它所有⻓度在3 ∼ 6的子词和特殊子词的并集记为$$\\mathcal{G}_{w}$$。**那么词典则是所有词的子词集合的并集**。假设词典中子词$$g$$的向量为$$z_g$$，那么跳字模型中词w的作为中⼼词的向量$$v_w$$则表⽰成：\n\n$$\nv_{w}=\\sum_{g \\in \\mathcal{G}_{w}} z_{g}\n$$\n\n- fastText的其余部分同跳字模型⼀致，不在此重复。可以看到，**与跳字模型相⽐，fastText中词典 规模更大，造成模型参数更多**，同时⼀个词的向量需要对所有子词向量求和，继而导致**计算复杂度更高**。但与此同时，**较生僻的复杂单词，甚⾄是词典中没有的单词，可能会从同它结构类似的 其他词那⾥获取更好的词向量表示。**\n\n\n\n# 3 全局向量的词嵌入（GloVe）\n\n- 在词嵌入中，交叉熵损失函数有时并不是一个好的选择。一方面，正如上面所说，**会带来整个词典大小的累加项，带来过大的计算开销**。另一方面，**词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。**\n- GloVe模型既使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。为了做到这一点GloVe模型引入了**共现矩阵（Cooccurrence Probabilities Matrix）**\n\n\n\n### 3.1 共现矩阵\n\n- 假设：\n> 1. 共现矩阵为$$X$$，$$X$$中的元素$$X_{ij}$$为语料库中$$word_i$$上下文中出现$$word_j$$的次数\n> 2. $$X_i = \\sum_k X_{ik}$$是出现在$$word_i$$上下文中所有词的总次数\n> 3. $$P_{ij} = P(j|i) = \\frac{X_{ij}}{X_i}$$为$$word_j$$出现在$$word_i$$上下文的概率\n\n- 下面我们来举个例子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235403111.png\" alt=\"image-20220211235403111\" style=\"zoom:80%;\" />\n\n设Ratio = $$\\frac{P_{ik}}{P_{jk}}$$，，从上面的例子中我们可以总结出：\n\n![image-20220211235553030](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235553030.png)\n\n**所以Ratio可以反应word之间的相关性，而GloVe就是利用了这个值**\n\n假设$$i, j, k$$三者的词向量都已经得到$$w_i, w_j, w_k$$，那么我们现在就要找一个函数F，使得：\n$$\nF\\left(w_{i}, w_{j}, w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}\n$$\n\n\n### 3.2 损失函数推导\n\n- **上述等式的右边是通过统计得到的已知量，左侧的三个词向量$$w_i, w_j, w_k$$是模型要求的量，F是未知的，如果能够将F的形式确定下来， 那么我们就可以通过优化算法求解词向量了，下面是求解过程**\n\n- $$\\frac{P_{i k}}{P_{j k}}$$考察了$$i,j,k$$三个词的相关性，不妨先只考虑$$i,j$$两个词的词向量$$w_i, w_j$$的相关性，向量相关关系的度量可以用两个向量的差，所以上述等式可以先改为：\n\n$$\nF\\left((w_{i}-w_{j}), w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}\n$$\n\n- 由于右边是标量，左边是向量，所以可以联想到使用向量的内积转化为标量：\n\n$$\nF\\left(\\left(w_{i}-w_{j}\\right)^{T} w_{k}\\right)=F\\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}\n$$\n\n- 因为左边是差，右边是商，所以将F设为exp运算，将差转化为商：\n\n$$\n\\exp \\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\\right)=\\frac{\\exp \\left(w_{i}^{T} w_{k}\\right)}{\\exp \\left(w_{j}^{T} w_{k}\\right)}=\\frac{P_{i k}}{P_{j k}}\n$$\n\n- 现在只需让分子分母分别相等就能成立：\n\n$$\n\\exp \\left(w_{i}^{T} w_{k}\\right)= \\alpha P_{i k}, \\exp \\left(w_{j}^{T} w_{k}\\right)= \\alpha P_{j k}，\\quad  \\alpha为常数\n$$\n\n- 现在只需要在整个词料库中考察$$\\exp \\left(w_{i}^{T} w_{k}\\right)= \\alpha P_{i k}= \\alpha \\frac{X_{i k}}{X_{i}}$$，即：\n\n$$\nw_{i}^{T} w_{k}=\\log \\left(\\alpha \\frac{X_{i k}}{X_{i}}\\right)=\\log X_{i k}-\\log X_{i} + log\\alpha\n$$\n\n- 所以我们需要设置一个偏差项$$b_i$$来拟合$$log X_i - log \\alpha$$，由于如果上式$$i,k$$位置交换，左式的值不变，但是右式的值会变，所以我们还要设置一个偏差项$$b_k$$，即：\n\n$$\nw_{i}^{T} w_{k}  + b_i + b_k=log X_{ik}\n$$\n\n- 上面公式只是理想状态下，实际中只能要求两者接近\t从而就有了代价函数：\n\n$$\nJ=\\sum_{i, k}\\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\\log X_{i k}\\right)^{2}\n$$\n\n- 如果两个词共同出现的次数越多，那么其在代价函数中的影响就越大，所以要设计一个权重函数：\n\n$$\nJ=\\sum_{i, k} f\\left(X_{i k}\\right)\\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\\log X_{i k}\\right)^{2}\n$$\n\n> 对于函数f要满足一下条件：\n>\n> 1. 如果两个词没有共同出现过，那么权重就是0，即f(0) = 0\n> 2. 两个词共同出现次数越多，那么权重就越大，所以f是一个递增函数\n> 3. 由一些于高频词（$$X_{ij}较大$$，如\"the\"）的权重很大，但是实际上作用并不是很大，所以f(x)对于较大的x不能取太大的值\n>\n> 综合上方条件，论文提出了一下函数：\n> $$\n> f(x)=\\left\\{\\begin{array}{r}\n> \\left(\\frac{x}{x_{\\max }}\\right)^{\\alpha}, \\text { if } x<x_{\\max } \\\\\n> 1, \\text { otherwise }\n> \\end{array}\\right.\n> $$\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212005820238.png\" alt=\"image-20220212005820238\" style=\"zoom:67%;\" />\n>\n> 作者认为$$x_{max} = 100, \\alpha = \\frac{3}{4}$$比较合适\n\n- 值得强调的是，如果词$$w_i$$出现在词$$w_j$$的背景窗口里，那么词$$w_j$$也会出现在词$$w_i$$的背景窗口⾥。也就是说，$$x_{ij} = x_{ji}$$。不同于**word2vec中拟合的是非对称的条件概率$$p_{ij}$$，GloVe模型拟合的是对称的$$\\log X _{ij}$$**。因此，**任意词的中心词向量和背景词向量在GloVe模型中是等价的。但由于初始化值的不同，同⼀个词最终学习到的两组词向量可能不同**。当学习得到所有词向量以后，GloVe模型使⽤**中心词向量与背景词向量之和作为该词的最终词向量，这样做是为了提高鲁棒性（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）**\n\n\n\n\n\n# 4 词嵌入应用\n\n- 下列应用均为在训练好的预训练模型上进行训练的\n\n\n\n\n### 4.1 求近义词和类比词\n\n- 求近义词可以运用词向量之间的余弦相似度\n- 求类比词，例如，“man”（男人）: “woman”（女人）:: “son”（儿子）: “daughter”（女儿）是⼀个类⽐例子。**对于类比关系中的4个词 a : b :: c : d，给定前3个词a、b和c，求d。设词w的词向量为vec(w)。求类比词的思路是vec(a) - vec(b) = vec(c) - vec(d)，来求出vec(d)的近似值，然后寻找和其最相似的几个词向量**\n\n\n\n### 4.2 使用循环神经网络进行文本情感分类\n\n- ⽂本分类是⾃然语⾔处理的⼀个常⻅任务，它把⼀段不定⻓的⽂本序列变换为⽂本的类别。本节关 注它的⼀个子问题：使⽤⽂本情感分类来分析⽂本作者的情绪。这个问题也叫**情感分析（sentiment analysis）**\n- 本节我们使用电影评论的情感分析（二分类，positive和negtive的情感），在数据处理的时候，一条评论为一个样本，我们要先对评论进行分词（一般都是**基于空格分词**），并且由于每条评论的长度不一样，无法组成小批量，所以我们需要通过**截断或者补充来使长度定长**\n- 对于模型的设计，先需要一个**嵌入层将文本转化为词向量**，再通过**双向循环网络对特征序列进⼀步编码得到序列信息**，然后将**最初时间步和最终时间步的隐藏状态连结**，再传入全连接层输出\n- 注意由于是预训练的模型，所以**嵌入层的模型参数是不需要更新的**，直接由训练好的模型参数导入即可。但是**导入的模型的词向量维度要和预先设置的嵌入层词向量维度一致**\n\n\n\n### 4.3 使用卷积神经网络（textCNN）进行文本情感分类：\n\n- 我们可以将⽂本数据看作只有⼀个维度的时间序列，并很⾃然地使⽤循环神经⽹络来表征这样的数据。其实，我们也可以将⽂本当作⼀维图像，从而可以⽤⼀维卷积神经⽹络来捕捉临近词之间的关联\n\n- **textCNN基于的假设是：一个词和其相邻的词（即卷积核中的词）是相关的**\n\n\n\n##### 4.3.1 一维卷积层\n\n- 和二维卷积层一样，一维卷积层卷积窗口从输⼊数组的最左⽅开始，按从左往右的顺序，依次在输⼊数组上滑动，举个例子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155418239.png\" alt=\"image-20220212155418239\" style=\"zoom:67%;\" />\n\n输出中的2是由输入的前两个和核逐个相乘得到的：0x1+1x2=2，然后窗口往后滑动一格，以此类推\n\n对于多通道操作也是和二维卷积层一样的：\n![image-20220212155747478](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155747478.png)\n\n每个通道分别操作，然后各通道结果相加。如果想要输出多通道，则核还要增加一个维度\n\n\n\n##### 4.3.2 时序最大池化层\n\n- 假设输⼊包含多个通道，各通道由不同时间步上的数值组成，**各通道的输出即该通道所有时间步中最⼤的数值**。因此，**时序最⼤池化层的输⼊在各个通道上的时间步数可以不同**\n- 为提升计算性能，我们常常将不同⻓度的时序样本组成⼀个小批量，**并通过在较短序列后附加特殊字符（如0）令批量中各时序样本⻓度相同**。这些⼈为添加的特殊字符当然是⽆意义的。由于时序最⼤池化的主要⽬的是抓取时序中最重要的特征，它通常能**使模型不受⼈为添加字符的影响**\n\n\n\n##### 4.3.3 textCNN模型\n\n- textCNN模型主要使⽤了⼀维卷积层和时序最⼤池化层。假设输⼊的⽂本序列由n个词组成，每 个词⽤d维的词向量表⽰。那么输⼊样本的宽为n，⾼为1，输⼊通道数为d。textCNN的计算主要分为以下⼏步：\n\n> 1. 定义多个⼀维卷积核，并使⽤这些卷积核对输⼊分别做卷积计算。宽度不同的卷积核可能 会捕捉到不同个数的相邻词的相关性\n> 2. 对输出的所有通道分别做时序最⼤池化，再将这些通道的池化输出值连结为向量。\n> 3. 通过全连接层将连结后的向量变换为有关各类别的输出。这⼀步可以使⽤丢弃层应对过拟合。\n\n- 举个例子，这⾥的输⼊是⼀个有11个词的句子，每个词⽤6维词向量表⽰。因此输⼊序列的宽为11，输⼊通道数为6。给定2个⼀维卷积核，核宽分别为2和4，输出 通道数分别设为4和5\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212161751535.png\" alt=\"image-20220212161751535\" style=\"zoom: 67%;\" />\n\n**尽管每个通道的宽不同，我们依然可以对各个通道做时序最⼤池化， 并将9个通道的池化输出连结成⼀个9维向量**。最终，使⽤全连接将9维向量变换为2维输出，即正⾯情感和负⾯情感的预测\n\n\n\n\n\n# 5 机器翻译\n\n- 我们使用seq2seq模型进行机器翻译，原理具体可看[seq2seq](https://zlkqz.top/2021/12/10/RNN/#7-Seq2Seq%E6%A8%A1%E5%9E%8B)\n\n- 这里我们只介绍一下如何评价翻译结果。评价机器翻译结果通常使⽤**BLEU（Bilingual Evaluation Understudy）**。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。\n- 具体来说，设词数为n的⼦序列的精度为$$p_n$$。它是**预测序列与标签序列匹配词数为n的⼦序列的数量与预测序列中词数为n的⼦序列的数量之⽐**。举个例子，假设标签序列为A、B、C、D、E、F， 预测序列为A、B、B、C、D，那么$$p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$$。设$$len_{label}$$和$$len_{pred}$$分 别为标签序列和预测序列的词数，那么，BLEU的定义为：\n\n$$\n\\exp \\left(\\min \\left(0,1-\\frac{\\text { len }_{\\text {label }}}{\\text { len }_{\\text {pred }}}\\right)\\right) \\prod_{n=1}^{k} p_{n}^{1 / 2^{n}}\n$$\n\n其中k是我们希望匹配的⼦序列的最⼤词数。可以看到当预测序列和标签序列完全⼀致时， BLEU为1。\n\n- 设计思想：\n\n> 因为匹配较⻓⼦序列⽐匹配较短⼦序列更难，BLEU对匹配较⻓⼦序列的精度赋予了更⼤权重，具体是在上式的$$p_n^{1/2^n}$$中，当序列较长，即n较大时，由于$$p_n \\in [0, 1]$$，所以$$1/2^n$$后会变大，并且n越大越接近1，即给较长序列更大的权重\n>\n> 并且由于较短序列一般会有比较大的$$p_n$$，所以连乘项前面的系数是为了惩罚较短的输出而设置的。举个例子，当$$len_{pred}$$较小时，连乘项前面的系数就会小于1，以此来减少较短序列的权重。\n","source":"_posts/NLP基础.md","raw":"---\ntitle: NLP基础\nmath: true\ndate: 2022-1-8\n---\n\n\n\n# 1 词嵌入（word embedding）\n\n- 词向量是⽤来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）\n\n\n\n### 1.1 词嵌入相比于one-hot向量的优点\n\n- 虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，**one-hot词 向量⽆法准确表达不同词之间的相似度**，如我们常常使⽤的余弦相似度。对于向量$$x, y \\in R^d$$，它 们的余弦相似度是它们之间夹⻆的余弦值：\n\n$$\n\\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{y}}{\\|\\boldsymbol{x}\\|\\|\\boldsymbol{y}\\|} \\in[-1,1]\n$$\n\n由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。\n\nword2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表示成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。word2vec⼯具包含了两个模型，即**跳字模型（skip-gram）** 和**连续词袋模型（continuous bag of words，CBOW）**\n\n\n\n### 1.2 跳字模型（skip-gram）\n\n- 跳字模型假设基于某个词来生成它在⽂本序列周围的词\n- 举个例子，假设⽂本序列是 “the” “man” “loves” “his” “son”。以“loves”作为中⼼词，设背景窗口大小为2。如下图所示，跳字模型所关⼼的是，给定中⼼词“loves”，生成与它距离不超过2个词的背景词 “the” “man” “his” “son”的条件概率，即：\n\n$$\nP(the, man, his, son | loves)\n$$\n\n假设给定中⼼词的情况下，背景词的生成是相互独⽴的，那么上式可以改写成：\n$$\nP(the| loves) \\times P(man | loves) \\times P(his | loves) \\times P(son | loves)\n$$\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125151903218.png\" alt=\"image-20220125151903218\" style=\"zoom: 80%;\" />\n\n- 在跳字模型中，**每个词被表示成两个$$d$$维向量**，⽤来计算条件概率。假设这个词在词典中索引为$$i$$， 当它为中心词时向量表示为$$v_i \\in R^d$$，而为背景词时向量表示为$$u_i \\in R^d$$。设中⼼词$$w_c$$在词典中索引为$$c$$，背景词$$w_o$$在词典中索引为$$o$$，**给定中⼼词$$w_c$$生成背景词$$w_o$$的条件概率可以通过对向量内积做softmax运算而得到：**\n\n$$\nP\\left(w_{o} \\mid w_{c}\\right)=\\frac{\\exp \\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{v}_{c}\\right)},\n$$\n\n其中词典索引集$$V = \\{0, 1, . . . , |V|−1\\}$$\n\n- 假设给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$。 假设给定中⼼词的情况下背景词的生成相互独⽴，当背景窗口大小为m时，跳字模型的**似然函数**即给定任⼀中⼼词生成所有背景词的概率：\n\n$$\n\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(w^{(t+j)} \\mid w^{(t)}\\right),\n$$\n\n这⾥小于1或大于T的时间步可以被忽略\n\n\n\n- **跳字模型的训练：**\n\n>- **跳字模型的参数是每个词所对应的中心词向量和背景词向量**\n>- 先把把最大似然函数取负对数：\n>\n>$$\n>-\\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P\\left(w^{(t+j)} \\mid w^{(t)}\\right)\n>$$\n>\n>如果使⽤随机梯度下降，那么在每⼀次迭代⾥我们随机采样⼀个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数\n>\n>- 由Softmax的运算结果可以得到：\n>\n>$$\n>\\log P\\left(w_{o} \\mid w_{c}\\right)=\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}-\\log \\left(\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{v}_{c}\\right)\\right)\n>$$\n>\n>那么我们可以求得上式关于各中心词向量和背景词向量的梯度，比如关于$$v_c$$的梯度：\n>$$\n>\\frac{\\partial \\log P\\left(w_{o} \\mid w_{c}\\right)}{\\partial v_{c}}=\\boldsymbol{u}_{o}-\\sum_{j \\in \\mathcal{V}} P\\left(w_{j} \\mid w_{c}\\right) \\boldsymbol{u}_{j} .\n>$$\n>训练结束后，对于词典中的任⼀索引为$$i$$的词，我们均得到该词作为中⼼词和背景词的两组词向量$$v_i$$和$$u_i$$。在⾃然语⾔处理应⽤中，**⼀般使⽤跳字模型的中心词向量作为词的表征向量**\n\n\n\n- **跳字模型的具体实现：**\n\n> - 可以用一个input-hidden-output的三层神经网络来建模上面的skip-model：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526195633208.jpg\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\" />\n>\n> - 输入的表示：输入层中每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1,其余维度的值均为0\n>\n> - 网络中传播的前向过程：输出层向量的值可以通过**隐含层向量（K维，即每一个词向量的维度）**，以及连接隐藏层和输出层之间的**KxN维权重矩阵**计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算每一个单词的生成概率。\n> - **input层和hidden层之间的N$$\\times$$K权重矩阵即N个词的中心词向量，从input层到hidden层的运算即对中心词向量的提取，具体见下图。而hidden层和output层之间的K$$\\times$$N权重矩阵即N个词的背景词向量，hidden层到output层的运算即实现$$u^T_ov_c$$，然后再在最后做一个softmax**\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526202027163.jpg\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\" />\n>\n> 左边矩阵是input，右边矩阵是input到hidden的权重矩阵，即N个词的中心词向量，由于输入是ont-hot向量，所以运算结果其实就是对中心词向量的一个提取\n>\n> \n>\n> - 在代码实现中是直接使用一个**嵌入层（Embedding层）**，此层的权值形状为**词典大小$$\\times$$每个词的维度**，可以直接将中心词和背景词对应的one-hot向量转化为词向量。以skip-gram举例，中心词的one-hot向量形状为(批量大小，1)，背景词为(批量大小，词典大小)，两者皆通过嵌入层转化成词向量后形状分别为(批量大小，1，每个词维度)、(批量大小，词典大小，每个词维度)。然后通过**小批量乘法**实现中心词向量和背景词向量的相乘$$u^T_ov_c$$，得到形状为(批量大小，词典大小)的结果，然后进行Softmax\n> - 上面的背景词one-hot向量第二维度不一定要为词典大小，如果我们采用**负采样**，则应该改为**正负样本和的大小**，具体可看下方负采样原理\n> - 现在我们来解释一下**小批量乘法**，假设第⼀个小批量包含n个形状为$$a \\times b$$的矩阵$$X_1, . . . , X_n$$，第⼆个小批量包含n个形状为$$b \\times c$$的矩阵$$Y_1, . . . , Y_n$$。 这两个小批量的矩阵乘法输出为n个形状为$$a \\times c$$的矩阵$$X_1Y_1, . . . , X_nY_n$$\n\n\n\n### 1.3 连续词袋模型（CBOW）\n\n- 连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，**连续词袋模型假设基于某中⼼词在 ⽂本序列前后的背景词来生成该中心词**\n\n- 如下图，在同样的⽂本序列“the” “man” “loves” “his” “son” ⾥，以“loves”作为中⼼词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词 “the” “man” “his” “son”生成中心词“loves”的条件概率，也就是：\n\n$$\nP(loves|the,man,his,son)\n$$\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125172417014.png\" alt=\"image-20220125172417014\" style=\"zoom:80%;\" />\n\n- 因为连续词袋模型的背景词有多个，我们**将这些背景词向量取平均**，然后使⽤和跳字模型⼀样的 ⽅法来计算条件概率\n\n  设$$v_i \\in R^d$$和$$u_i \\in R^d$$分别表示词典中索引为$$i$$的词作为背景词和中⼼词的向 量（**注意符号的含义与跳字模型中的相反**）。设中⼼词$$w_c$$在词典中索引为$$c$$，背景词$$w_{o1} , . . . , w_{o2m}$$在 词典中索引为$$o_1, . . . , o_{2m}$$，那么给定背景词生成中⼼词的条件概率：\n  $$\n  .P\\left(w_{c} \\mid w_{o_{1}}, \\ldots, w_{o_{2 m}}\\right)=\\frac{\\exp \\left(\\frac{1}{2 m} \\boldsymbol{u}_{c}^{\\top}\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right)\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\frac{1}{2 m} \\boldsymbol{u}_{i}^{\\top}\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right)\\right)}\n  $$\n  设$$W_o = \\{w_{o1} , . . . , w_{o2m}\\}$$，$$\\overline{\\boldsymbol{v}}_{o}=\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right) /(2 m)$$，可将上式简化为：\n  $$\n  P\\left(w_{c} \\mid \\mathcal{W}_{o}\\right)=\\frac{\\exp \\left(\\boldsymbol{u}_{c}^{\\top} \\overline{\\boldsymbol{v}}_{o}\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\overline{\\boldsymbol{v}}_{o}\\right)}\n  $$\n\n- 给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$，背景窗口大小为m。连续词袋模型的似然函数是由背景词生成任⼀中⼼词的概率：\n\n$$\n\\prod_{t=1}^{T} P\\left(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}\\right)\n$$\n\n- 接着再像跳字模型一样求似然函数的负对数，然后再求关于背景向量和中心向量的梯度。**同跳字模型不⼀样的⼀点在于，我们⼀般使⽤连续词袋模型的背景词向量作为词的表征向量**\n\n\n\n- **跳字模型更适合大型语料库，CBOW更适合小型的语料库**\n\n### 1.4 近似训练\n\n- 无论是跳字模型还是连续词袋模型，在求梯度的时候，由于条件概率使⽤了softmax运 算，每⼀步的梯度计算都包含词典大小数⽬的项的累加。对于含⼏⼗万或上百万词的较大词典， 每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练⽅法，即**负采样（negative sampling）**或**层序softmax（hierarchical softmax）**\n\n##### 1.4.1 高频词抽样\n\n- 首先介绍一下对高频词进行抽样（也称⼆次采样(subsampling)）。对于文本中的高频词，比如\"the\"，不进行抽样会带来两个问题：\n\n> 1. 当我们得到成对的单词训练样本时，**(\"fox\", \"the\") 这样的训练样本并不会给我们提供关于“fox”更多的语义信息**，因为“the”在每个单词的上下文中几乎都会出现\n> 2. 由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，...）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数\n\n- Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：**对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关，词频越高被删除的概率就越大**\n\n\n\n##### 1.4.2 负采样（negative sampling）\n\n- **负采样（negative sampling）**解决了因为词典大小过大的计算开销问题。不同于原本每个训练样本更新所有的权重，**负采样每次让一个训练样本仅仅更新一小部分的权重**，这样就会降低梯度下降过程中的计算量。\n- 当我们用训练样本 ( input word: \"fox\"，output word: \"quick\") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们**期望输出为0的神经元结点**所对应的单词我们称为**“negative” word**。\n- 当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。**（在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。）**\n\n- **每个单词被选为“negative words”的概率计算公式与其出现的频次有关，词频越高，被选上的概率就越高**，论文中给的公式是：\n\n$$\nP\\left(w_{i}\\right)=\\frac{f\\left(w_{i}\\right)^{3 / 4}}{\\sum_{j=0}^{n}\\left(f\\left(w_{j}\\right)^{3 / 4}\\right)}\n$$\n\n- **负采样概述：**\n\n> - 负采样修改了原来的⽬标函数。给定中⼼词$$w_c$$的⼀个背景窗口，我们把背景词$$w_o$$出现在该背景窗口看作⼀个事件，并将该事件的概率计算为：\n>\n> $$\n> P\\left(D=1 \\mid w_{c}, w_{o}\\right)=\\sigma\\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)\n> $$\n>\n> - 我们先考虑最大化⽂本序列中所有该事件的联合概率来训练词向量。具体来说，给定⼀个⻓度为T的⽂本序列，设时间步t的词为$$w^{(t)}$$且背景窗口大小为m，考虑最大化联合概率：\n>\n> $$\n> \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(D=1 \\mid w^{(t)}, w^{(t+j)}\\right)\n> $$\n>\n> 然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为⽆穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫⽆意义。**负采样通过采样并添加负类样本使⽬标函数更有意义。**\n>\n> - 在采集了K个负样本后，我们可以将所求概率近似为：\n>\n> $$\n> P\\left(w^{(t+j)} \\mid w^{(t)}\\right)=P\\left(D=1 \\mid w^{(t)}, w^{(t+j)}\\right) \\prod_{k=1, w_{k} \\sim P(w)}^{K} P\\left(D=0 \\mid w^{(t)}, w_{k}\\right)\n> $$\n>\n> - 取负对数后变成了：\n>\n> $$\n> -\\log P\\left(w^{(t+j)} \\mid w^{(t)}\\right)=-\\log \\sigma\\left(\\boldsymbol{u}_{i_{t+j}}^{\\top} v_{i_{t}}\\right)-\\sum_{k=1, w_{k} \\sim P(w)}^{K} \\log \\sigma\\left(-\\boldsymbol{u}_{h_{k}}^{\\top} v_{i_{t}}\\right)\n> $$\n>\n> 现在，**训练中每⼀步的梯度计算开销不再与词典大小相关，而与K线性相关**。当K取较小的常数时，负采样在每⼀步的梯度计算开销较小。\n\n- **负采样的具体实现：**\n\n> - 在实际应用中，我们并没有严格的将不在窗口中的词定义为负样本，而是根据词频直接随机取负样本，这样的实验结果是更优的。\n>\n> - **设词典大小为$$\\mathcal{V}$$，将一根长为1的线段分为$$\\mathcal{V}$$段，每段分别对应一个词，每个词对应的长度就是上面所写的每个词对应的频率$$P(w_i)$$，然后取一个M（M>>$$\\mathcal{V}$$），论文中M取的是$$10^8$$，将长度为1的线段均分为M份（如下图），然后取K个负样本即在该线段上取K个点，取到的点在哪个词的对应线段就取哪个词**\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211153340493.png\" alt=\"image-20220211153340493\" style=\"zoom:67%;\" />\n\n\n\n##### 1.4.3 层序Softmax（hierarchical softmax）\n\n- 层序softmax是另⼀种近似训练法。它使⽤了⼆叉树这⼀数据结构，树的每个叶结点代表词典$$\\mathcal{V}$$中的每个词。\n- <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220210181128975.png\" alt=\"image-20220210181128975\" style=\"zoom: 80%;\" />\n\n假设$$L(w)$$为从⼆叉树的根结点到词$$w$$的叶结点的路径（包括根结点和叶结点）上的结点数。 设$$n(w, j)$$为该路径上第$$j$$个结点，并设该结点的背景词向量为$$u_{n(w,j)}$$。以上图为例，$$L(w_3) = 4$$ 层序softmax将跳字模型中的条件概率近似表示为：\n$$\nP\\left(w_{o} \\mid w_{c}\\right)=\\prod_{j=1}^{L\\left(w_{o}\\right)-1} \\sigma\\left([[ n\\left(w_{o}, j+1\\right)=\\operatorname{leftChild}\\left(n\\left(w_{o}, j\\right)\\right) ]] \\cdot \\boldsymbol{u}_{n\\left(w_{o}, j\\right)}^{\\top} \\boldsymbol{v}_{c}\\right),\n$$\n其中如果判断x为真，[[x]] = 1；反之[[x]] = −1。\n\n例如让我们计算上图中给定词$$w_c$$生成词$$w_3$$的条件概率：\n$$\nP\\left(w_{3} \\mid w_{c}\\right)=\\sigma\\left(\\boldsymbol{u}_{n\\left(w_{3}, 1\\right)}^{\\top} \\boldsymbol{v}_{c}\\right) \\cdot \\sigma\\left(-\\boldsymbol{u}_{n\\left(w_{3}, 2\\right)}^{\\top} \\boldsymbol{v}_{c}\\right) \\cdot \\sigma\\left(\\boldsymbol{u}_{n\\left(w_{3}, 3\\right)}^{\\top} \\boldsymbol{v}_{c}\\right)\n$$\n\n- **推导过程：**\n\n> 设$$d_{2}^{w}, d_{3}^{w}, \\cdots, d_{l w}^{w} \\in\\{0,1\\}$$为词$$w$$的编码，$$d_j^w$$代表$$w$$路径上的第$$j$$个节点对应的编码（根节点无编码），则：\n> $$\n> P(w_0|w_c) = \\prod_{j=2}^{L(w)} P\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right)\n> $$\n> 其中的每一项都是一个Logistic回归：\n> $$\n> P\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right) = \\left\\{\\begin{array}{ll}\n> \\sigma\\left(v_c^T u_{n_(w, j)}\\right), & d_{j}^{w}=0 \\\\\n> 1-\\sigma\\left(v_c^T u_{n_(w, j)}\\right), & d_{j}^{w}=1\n> \\end{array}\\right.\n> $$\n> 可以将两式合并，并且由于$$\\sigma(x) + \\sigma(-x) = 1$$，可以将上式转化为：\n> $$\n> P\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right)=\\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]^{1-d_{j}^{w}} \\cdot\\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]^{d^{w}}\n> $$\n> 我们取目标函数的对数：\n> $$\n> \\mathcal{L} = \\sum_{w \\in \\mathcal{V}}\\log P(Context(w) | w) = \\sum_{w \\in \\mathcal{V}} \\sum_{j=2}^{L(w)}\\left\\{\\left(1-d_{j}^{w}\\right) \\cdot \\log \\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]+d_{j}^{w} \\cdot \\log \\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]\\right\\}\n> $$\n> **要最大化上述目标函数是比较困难的，但是我们可以最大化每一子项（即下式），达到局部最大化的效果**：\n> $$\n> \\mathcal{L}(w, j) = \\left\\{\\left(1-d_{j}^{w}\\right) \\cdot \\log \\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]+d_{j}^{w} \\cdot \\log \\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]\\right\\}\n> $$\n> **这样就可以将计算复杂度降为$$O(log_2 |\\mathcal{V}|)$$，当词典$$\\mathcal{V}$$很大时，层序softmax在训练中每⼀步的梯度计算开销相较未使用近似训练时大幅降低。**\n>\n> 显然，上式的$$d_j^w$$是用来**判断下一个节点是否是本节点的左孩子的**，其作用是为了满足给定任意中心词$$w_c$$，生成背景词的条件概率和为1：\n> $$\n> \\sum_{w \\in \\mathcal{V}} P\\left(w \\mid w_{c}\\right)=1\n> $$\n> 这样才能使似然函数不至于训练到无穷大（和负采样要添加负样本是一样的原因），并且符合概率和为1\n\n- **层序Softmax中的二叉树是使用的哈夫曼树，可以使搜索次数达到最小**\n\n\n\n# 2 子词嵌入（fastText）\n\n- 英语单词通常有其内部结构和形成⽅式。例如，我们可以从“dog” “dogs”和“dogcatcher”的 字面上推测它们的关系。这些词都有同⼀个词根“dog”，但使⽤不同的后缀来改变词的含义。而 且，这个关联可以推⼴⾄其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关 系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。这方面的研究称为构词学\n- 在word2vec中，我们并没有直接利⽤构词学中的信息。⽆论是在跳字模型还是连续词袋模型中， 我们都将形态不同的单词⽤不同的向量来表示。例如，“dog”和“dogs”分别⽤两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌⼊（subword embedding）的⽅法，从而试图将构词信息引⼊word2vec中的跳字模型\n\n- 在fastText中，每个中⼼词被表⽰成子词的集合。举个例子，例如单词\"where\"，⾸先，我们在单词的**⾸尾分别添加特殊字符“<” 和  “>” 以区分作为前后缀的子词**。然后，**将单词当成⼀个由字符构成的序列来提取n元语法**。例如，当n = 3时，我们得到所有⻓度为3的子词：\"<wh\"、\"whe\"、\"her\"、\"ere\"、\"re>\"以及特殊子词\"where\"。\n- 在fastText中，对于⼀个词w，我们将它所有⻓度在3 ∼ 6的子词和特殊子词的并集记为$$\\mathcal{G}_{w}$$。**那么词典则是所有词的子词集合的并集**。假设词典中子词$$g$$的向量为$$z_g$$，那么跳字模型中词w的作为中⼼词的向量$$v_w$$则表⽰成：\n\n$$\nv_{w}=\\sum_{g \\in \\mathcal{G}_{w}} z_{g}\n$$\n\n- fastText的其余部分同跳字模型⼀致，不在此重复。可以看到，**与跳字模型相⽐，fastText中词典 规模更大，造成模型参数更多**，同时⼀个词的向量需要对所有子词向量求和，继而导致**计算复杂度更高**。但与此同时，**较生僻的复杂单词，甚⾄是词典中没有的单词，可能会从同它结构类似的 其他词那⾥获取更好的词向量表示。**\n\n\n\n# 3 全局向量的词嵌入（GloVe）\n\n- 在词嵌入中，交叉熵损失函数有时并不是一个好的选择。一方面，正如上面所说，**会带来整个词典大小的累加项，带来过大的计算开销**。另一方面，**词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。**\n- GloVe模型既使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。为了做到这一点GloVe模型引入了**共现矩阵（Cooccurrence Probabilities Matrix）**\n\n\n\n### 3.1 共现矩阵\n\n- 假设：\n> 1. 共现矩阵为$$X$$，$$X$$中的元素$$X_{ij}$$为语料库中$$word_i$$上下文中出现$$word_j$$的次数\n> 2. $$X_i = \\sum_k X_{ik}$$是出现在$$word_i$$上下文中所有词的总次数\n> 3. $$P_{ij} = P(j|i) = \\frac{X_{ij}}{X_i}$$为$$word_j$$出现在$$word_i$$上下文的概率\n\n- 下面我们来举个例子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235403111.png\" alt=\"image-20220211235403111\" style=\"zoom:80%;\" />\n\n设Ratio = $$\\frac{P_{ik}}{P_{jk}}$$，，从上面的例子中我们可以总结出：\n\n![image-20220211235553030](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235553030.png)\n\n**所以Ratio可以反应word之间的相关性，而GloVe就是利用了这个值**\n\n假设$$i, j, k$$三者的词向量都已经得到$$w_i, w_j, w_k$$，那么我们现在就要找一个函数F，使得：\n$$\nF\\left(w_{i}, w_{j}, w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}\n$$\n\n\n### 3.2 损失函数推导\n\n- **上述等式的右边是通过统计得到的已知量，左侧的三个词向量$$w_i, w_j, w_k$$是模型要求的量，F是未知的，如果能够将F的形式确定下来， 那么我们就可以通过优化算法求解词向量了，下面是求解过程**\n\n- $$\\frac{P_{i k}}{P_{j k}}$$考察了$$i,j,k$$三个词的相关性，不妨先只考虑$$i,j$$两个词的词向量$$w_i, w_j$$的相关性，向量相关关系的度量可以用两个向量的差，所以上述等式可以先改为：\n\n$$\nF\\left((w_{i}-w_{j}), w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}\n$$\n\n- 由于右边是标量，左边是向量，所以可以联想到使用向量的内积转化为标量：\n\n$$\nF\\left(\\left(w_{i}-w_{j}\\right)^{T} w_{k}\\right)=F\\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}\n$$\n\n- 因为左边是差，右边是商，所以将F设为exp运算，将差转化为商：\n\n$$\n\\exp \\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\\right)=\\frac{\\exp \\left(w_{i}^{T} w_{k}\\right)}{\\exp \\left(w_{j}^{T} w_{k}\\right)}=\\frac{P_{i k}}{P_{j k}}\n$$\n\n- 现在只需让分子分母分别相等就能成立：\n\n$$\n\\exp \\left(w_{i}^{T} w_{k}\\right)= \\alpha P_{i k}, \\exp \\left(w_{j}^{T} w_{k}\\right)= \\alpha P_{j k}，\\quad  \\alpha为常数\n$$\n\n- 现在只需要在整个词料库中考察$$\\exp \\left(w_{i}^{T} w_{k}\\right)= \\alpha P_{i k}= \\alpha \\frac{X_{i k}}{X_{i}}$$，即：\n\n$$\nw_{i}^{T} w_{k}=\\log \\left(\\alpha \\frac{X_{i k}}{X_{i}}\\right)=\\log X_{i k}-\\log X_{i} + log\\alpha\n$$\n\n- 所以我们需要设置一个偏差项$$b_i$$来拟合$$log X_i - log \\alpha$$，由于如果上式$$i,k$$位置交换，左式的值不变，但是右式的值会变，所以我们还要设置一个偏差项$$b_k$$，即：\n\n$$\nw_{i}^{T} w_{k}  + b_i + b_k=log X_{ik}\n$$\n\n- 上面公式只是理想状态下，实际中只能要求两者接近\t从而就有了代价函数：\n\n$$\nJ=\\sum_{i, k}\\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\\log X_{i k}\\right)^{2}\n$$\n\n- 如果两个词共同出现的次数越多，那么其在代价函数中的影响就越大，所以要设计一个权重函数：\n\n$$\nJ=\\sum_{i, k} f\\left(X_{i k}\\right)\\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\\log X_{i k}\\right)^{2}\n$$\n\n> 对于函数f要满足一下条件：\n>\n> 1. 如果两个词没有共同出现过，那么权重就是0，即f(0) = 0\n> 2. 两个词共同出现次数越多，那么权重就越大，所以f是一个递增函数\n> 3. 由一些于高频词（$$X_{ij}较大$$，如\"the\"）的权重很大，但是实际上作用并不是很大，所以f(x)对于较大的x不能取太大的值\n>\n> 综合上方条件，论文提出了一下函数：\n> $$\n> f(x)=\\left\\{\\begin{array}{r}\n> \\left(\\frac{x}{x_{\\max }}\\right)^{\\alpha}, \\text { if } x<x_{\\max } \\\\\n> 1, \\text { otherwise }\n> \\end{array}\\right.\n> $$\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212005820238.png\" alt=\"image-20220212005820238\" style=\"zoom:67%;\" />\n>\n> 作者认为$$x_{max} = 100, \\alpha = \\frac{3}{4}$$比较合适\n\n- 值得强调的是，如果词$$w_i$$出现在词$$w_j$$的背景窗口里，那么词$$w_j$$也会出现在词$$w_i$$的背景窗口⾥。也就是说，$$x_{ij} = x_{ji}$$。不同于**word2vec中拟合的是非对称的条件概率$$p_{ij}$$，GloVe模型拟合的是对称的$$\\log X _{ij}$$**。因此，**任意词的中心词向量和背景词向量在GloVe模型中是等价的。但由于初始化值的不同，同⼀个词最终学习到的两组词向量可能不同**。当学习得到所有词向量以后，GloVe模型使⽤**中心词向量与背景词向量之和作为该词的最终词向量，这样做是为了提高鲁棒性（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）**\n\n\n\n\n\n# 4 词嵌入应用\n\n- 下列应用均为在训练好的预训练模型上进行训练的\n\n\n\n\n### 4.1 求近义词和类比词\n\n- 求近义词可以运用词向量之间的余弦相似度\n- 求类比词，例如，“man”（男人）: “woman”（女人）:: “son”（儿子）: “daughter”（女儿）是⼀个类⽐例子。**对于类比关系中的4个词 a : b :: c : d，给定前3个词a、b和c，求d。设词w的词向量为vec(w)。求类比词的思路是vec(a) - vec(b) = vec(c) - vec(d)，来求出vec(d)的近似值，然后寻找和其最相似的几个词向量**\n\n\n\n### 4.2 使用循环神经网络进行文本情感分类\n\n- ⽂本分类是⾃然语⾔处理的⼀个常⻅任务，它把⼀段不定⻓的⽂本序列变换为⽂本的类别。本节关 注它的⼀个子问题：使⽤⽂本情感分类来分析⽂本作者的情绪。这个问题也叫**情感分析（sentiment analysis）**\n- 本节我们使用电影评论的情感分析（二分类，positive和negtive的情感），在数据处理的时候，一条评论为一个样本，我们要先对评论进行分词（一般都是**基于空格分词**），并且由于每条评论的长度不一样，无法组成小批量，所以我们需要通过**截断或者补充来使长度定长**\n- 对于模型的设计，先需要一个**嵌入层将文本转化为词向量**，再通过**双向循环网络对特征序列进⼀步编码得到序列信息**，然后将**最初时间步和最终时间步的隐藏状态连结**，再传入全连接层输出\n- 注意由于是预训练的模型，所以**嵌入层的模型参数是不需要更新的**，直接由训练好的模型参数导入即可。但是**导入的模型的词向量维度要和预先设置的嵌入层词向量维度一致**\n\n\n\n### 4.3 使用卷积神经网络（textCNN）进行文本情感分类：\n\n- 我们可以将⽂本数据看作只有⼀个维度的时间序列，并很⾃然地使⽤循环神经⽹络来表征这样的数据。其实，我们也可以将⽂本当作⼀维图像，从而可以⽤⼀维卷积神经⽹络来捕捉临近词之间的关联\n\n- **textCNN基于的假设是：一个词和其相邻的词（即卷积核中的词）是相关的**\n\n\n\n##### 4.3.1 一维卷积层\n\n- 和二维卷积层一样，一维卷积层卷积窗口从输⼊数组的最左⽅开始，按从左往右的顺序，依次在输⼊数组上滑动，举个例子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155418239.png\" alt=\"image-20220212155418239\" style=\"zoom:67%;\" />\n\n输出中的2是由输入的前两个和核逐个相乘得到的：0x1+1x2=2，然后窗口往后滑动一格，以此类推\n\n对于多通道操作也是和二维卷积层一样的：\n![image-20220212155747478](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155747478.png)\n\n每个通道分别操作，然后各通道结果相加。如果想要输出多通道，则核还要增加一个维度\n\n\n\n##### 4.3.2 时序最大池化层\n\n- 假设输⼊包含多个通道，各通道由不同时间步上的数值组成，**各通道的输出即该通道所有时间步中最⼤的数值**。因此，**时序最⼤池化层的输⼊在各个通道上的时间步数可以不同**\n- 为提升计算性能，我们常常将不同⻓度的时序样本组成⼀个小批量，**并通过在较短序列后附加特殊字符（如0）令批量中各时序样本⻓度相同**。这些⼈为添加的特殊字符当然是⽆意义的。由于时序最⼤池化的主要⽬的是抓取时序中最重要的特征，它通常能**使模型不受⼈为添加字符的影响**\n\n\n\n##### 4.3.3 textCNN模型\n\n- textCNN模型主要使⽤了⼀维卷积层和时序最⼤池化层。假设输⼊的⽂本序列由n个词组成，每 个词⽤d维的词向量表⽰。那么输⼊样本的宽为n，⾼为1，输⼊通道数为d。textCNN的计算主要分为以下⼏步：\n\n> 1. 定义多个⼀维卷积核，并使⽤这些卷积核对输⼊分别做卷积计算。宽度不同的卷积核可能 会捕捉到不同个数的相邻词的相关性\n> 2. 对输出的所有通道分别做时序最⼤池化，再将这些通道的池化输出值连结为向量。\n> 3. 通过全连接层将连结后的向量变换为有关各类别的输出。这⼀步可以使⽤丢弃层应对过拟合。\n\n- 举个例子，这⾥的输⼊是⼀个有11个词的句子，每个词⽤6维词向量表⽰。因此输⼊序列的宽为11，输⼊通道数为6。给定2个⼀维卷积核，核宽分别为2和4，输出 通道数分别设为4和5\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212161751535.png\" alt=\"image-20220212161751535\" style=\"zoom: 67%;\" />\n\n**尽管每个通道的宽不同，我们依然可以对各个通道做时序最⼤池化， 并将9个通道的池化输出连结成⼀个9维向量**。最终，使⽤全连接将9维向量变换为2维输出，即正⾯情感和负⾯情感的预测\n\n\n\n\n\n# 5 机器翻译\n\n- 我们使用seq2seq模型进行机器翻译，原理具体可看[seq2seq](https://zlkqz.top/2021/12/10/RNN/#7-Seq2Seq%E6%A8%A1%E5%9E%8B)\n\n- 这里我们只介绍一下如何评价翻译结果。评价机器翻译结果通常使⽤**BLEU（Bilingual Evaluation Understudy）**。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。\n- 具体来说，设词数为n的⼦序列的精度为$$p_n$$。它是**预测序列与标签序列匹配词数为n的⼦序列的数量与预测序列中词数为n的⼦序列的数量之⽐**。举个例子，假设标签序列为A、B、C、D、E、F， 预测序列为A、B、B、C、D，那么$$p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$$。设$$len_{label}$$和$$len_{pred}$$分 别为标签序列和预测序列的词数，那么，BLEU的定义为：\n\n$$\n\\exp \\left(\\min \\left(0,1-\\frac{\\text { len }_{\\text {label }}}{\\text { len }_{\\text {pred }}}\\right)\\right) \\prod_{n=1}^{k} p_{n}^{1 / 2^{n}}\n$$\n\n其中k是我们希望匹配的⼦序列的最⼤词数。可以看到当预测序列和标签序列完全⼀致时， BLEU为1。\n\n- 设计思想：\n\n> 因为匹配较⻓⼦序列⽐匹配较短⼦序列更难，BLEU对匹配较⻓⼦序列的精度赋予了更⼤权重，具体是在上式的$$p_n^{1/2^n}$$中，当序列较长，即n较大时，由于$$p_n \\in [0, 1]$$，所以$$1/2^n$$后会变大，并且n越大越接近1，即给较长序列更大的权重\n>\n> 并且由于较短序列一般会有比较大的$$p_n$$，所以连乘项前面的系数是为了惩罚较短的输出而设置的。举个例子，当$$len_{pred}$$较小时，连乘项前面的系数就会小于1，以此来减少较短序列的权重。\n","slug":"NLP基础","published":1,"updated":"2023-02-06T08:43:59.278Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1e000a7cszguz490n6","content":"<h1 id=\"1-词嵌入（word-embedding）\"><a href=\"#1-词嵌入（word-embedding）\" class=\"headerlink\" title=\"1 词嵌入（word embedding）\"></a>1 词嵌入（word embedding）</h1><ul>\n<li>词向量是⽤来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）</li>\n</ul>\n<h3 id=\"1-1-词嵌入相比于one-hot向量的优点\"><a href=\"#1-1-词嵌入相比于one-hot向量的优点\" class=\"headerlink\" title=\"1.1 词嵌入相比于one-hot向量的优点\"></a>1.1 词嵌入相比于one-hot向量的优点</h3><ul>\n<li>虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，<strong>one-hot词 向量⽆法准确表达不同词之间的相似度</strong>，如我们常常使⽤的余弦相似度。对于向量<script type=\"math/tex\">x, y \\in R^d</script>，它 们的余弦相似度是它们之间夹⻆的余弦值：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{y}}{\\|\\boldsymbol{x}\\|\\|\\boldsymbol{y}\\|} \\in[-1,1]</script><p>由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。</p>\n<p>word2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表示成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。word2vec⼯具包含了两个模型，即<strong>跳字模型（skip-gram）</strong> 和<strong>连续词袋模型（continuous bag of words，CBOW）</strong></p>\n<h3 id=\"1-2-跳字模型（skip-gram）\"><a href=\"#1-2-跳字模型（skip-gram）\" class=\"headerlink\" title=\"1.2 跳字模型（skip-gram）\"></a>1.2 跳字模型（skip-gram）</h3><ul>\n<li>跳字模型假设基于某个词来生成它在⽂本序列周围的词</li>\n<li>举个例子，假设⽂本序列是 “the” “man” “loves” “his” “son”。以“loves”作为中⼼词，设背景窗口大小为2。如下图所示，跳字模型所关⼼的是，给定中⼼词“loves”，生成与它距离不超过2个词的背景词 “the” “man” “his” “son”的条件概率，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(the, man, his, son | loves)</script><p>假设给定中⼼词的情况下，背景词的生成是相互独⽴的，那么上式可以改写成：</p>\n<script type=\"math/tex; mode=display\">\nP(the| loves) \\times P(man | loves) \\times P(his | loves) \\times P(son | loves)</script><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125151903218.png\" alt=\"image-20220125151903218\" style=\"zoom: 80%;\" /></p>\n<ul>\n<li>在跳字模型中，<strong>每个词被表示成两个<script type=\"math/tex\">d</script>维向量</strong>，⽤来计算条件概率。假设这个词在词典中索引为<script type=\"math/tex\">i</script>， 当它为中心词时向量表示为<script type=\"math/tex\">v_i \\in R^d</script>，而为背景词时向量表示为<script type=\"math/tex\">u_i \\in R^d</script>。设中⼼词<script type=\"math/tex\">w_c</script>在词典中索引为<script type=\"math/tex\">c</script>，背景词<script type=\"math/tex\">w_o</script>在词典中索引为<script type=\"math/tex\">o</script>，<strong>给定中⼼词<script type=\"math/tex\">w_c</script>生成背景词<script type=\"math/tex\">w_o</script>的条件概率可以通过对向量内积做softmax运算而得到：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{o} \\mid w_{c}\\right)=\\frac{\\exp \\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{v}_{c}\\right)},</script><p>其中词典索引集<script type=\"math/tex\">V = \\{0, 1, . . . , |V|−1\\}</script></p>\n<ul>\n<li>假设给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type=\"math/tex\">w^{(t)}</script>。 假设给定中⼼词的情况下背景词的生成相互独⽴，当背景窗口大小为m时，跳字模型的<strong>似然函数</strong>即给定任⼀中⼼词生成所有背景词的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(w^{(t+j)} \\mid w^{(t)}\\right),</script><p>这⾥小于1或大于T的时间步可以被忽略</p>\n<ul>\n<li><strong>跳字模型的训练：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li><strong>跳字模型的参数是每个词所对应的中心词向量和背景词向量</strong></li>\n<li>先把把最大似然函数取负对数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P\\left(w^{(t+j)} \\mid w^{(t)}\\right)</script><p>如果使⽤随机梯度下降，那么在每⼀次迭代⾥我们随机采样⼀个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数</p>\n<ul>\n<li>由Softmax的运算结果可以得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\log P\\left(w_{o} \\mid w_{c}\\right)=\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}-\\log \\left(\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{v}_{c}\\right)\\right)</script><p>那么我们可以求得上式关于各中心词向量和背景词向量的梯度，比如关于<script type=\"math/tex\">v_c</script>的梯度：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\log P\\left(w_{o} \\mid w_{c}\\right)}{\\partial v_{c}}=\\boldsymbol{u}_{o}-\\sum_{j \\in \\mathcal{V}} P\\left(w_{j} \\mid w_{c}\\right) \\boldsymbol{u}_{j} .</script><p>训练结束后，对于词典中的任⼀索引为<script type=\"math/tex\">i</script>的词，我们均得到该词作为中⼼词和背景词的两组词向量<script type=\"math/tex\">v_i</script>和<script type=\"math/tex\">u_i</script>。在⾃然语⾔处理应⽤中，<strong>⼀般使⽤跳字模型的中心词向量作为词的表征向量</strong></p>\n</blockquote>\n<ul>\n<li><strong>跳字模型的具体实现：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>可以用一个input-hidden-output的三层神经网络来建模上面的skip-model：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526195633208.jpg\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\" /></p>\n<ul>\n<li><p>输入的表示：输入层中每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1,其余维度的值均为0</p>\n</li>\n<li><p>网络中传播的前向过程：输出层向量的值可以通过<strong>隐含层向量（K维，即每一个词向量的维度）</strong>，以及连接隐藏层和输出层之间的<strong>KxN维权重矩阵</strong>计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算每一个单词的生成概率。</p>\n</li>\n<li><strong>input层和hidden层之间的N<script type=\"math/tex\">\\times</script>K权重矩阵即N个词的中心词向量，从input层到hidden层的运算即对中心词向量的提取，具体见下图。而hidden层和output层之间的K<script type=\"math/tex\">\\times</script>N权重矩阵即N个词的背景词向量，hidden层到output层的运算即实现<script type=\"math/tex\">u^T_ov_c</script>，然后再在最后做一个softmax</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526202027163.jpg\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\" /></p>\n<p>左边矩阵是input，右边矩阵是input到hidden的权重矩阵，即N个词的中心词向量，由于输入是ont-hot向量，所以运算结果其实就是对中心词向量的一个提取</p>\n<ul>\n<li>在代码实现中是直接使用一个<strong>嵌入层（Embedding层）</strong>，此层的权值形状为<strong>词典大小<script type=\"math/tex\">\\times</script>每个词的维度</strong>，可以直接将中心词和背景词对应的one-hot向量转化为词向量。以skip-gram举例，中心词的one-hot向量形状为(批量大小，1)，背景词为(批量大小，词典大小)，两者皆通过嵌入层转化成词向量后形状分别为(批量大小，1，每个词维度)、(批量大小，词典大小，每个词维度)。然后通过<strong>小批量乘法</strong>实现中心词向量和背景词向量的相乘<script type=\"math/tex\">u^T_ov_c</script>，得到形状为(批量大小，词典大小)的结果，然后进行Softmax</li>\n<li>上面的背景词one-hot向量第二维度不一定要为词典大小，如果我们采用<strong>负采样</strong>，则应该改为<strong>正负样本和的大小</strong>，具体可看下方负采样原理</li>\n<li>现在我们来解释一下<strong>小批量乘法</strong>，假设第⼀个小批量包含n个形状为<script type=\"math/tex\">a \\times b</script>的矩阵<script type=\"math/tex\">X_1, . . . , X_n</script>，第⼆个小批量包含n个形状为<script type=\"math/tex\">b \\times c</script>的矩阵<script type=\"math/tex\">Y_1, . . . , Y_n</script>。 这两个小批量的矩阵乘法输出为n个形状为<script type=\"math/tex\">a \\times c</script>的矩阵<script type=\"math/tex\">X_1Y_1, . . . , X_nY_n</script></li>\n</ul>\n</blockquote>\n<h3 id=\"1-3-连续词袋模型（CBOW）\"><a href=\"#1-3-连续词袋模型（CBOW）\" class=\"headerlink\" title=\"1.3 连续词袋模型（CBOW）\"></a>1.3 连续词袋模型（CBOW）</h3><ul>\n<li><p>连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，<strong>连续词袋模型假设基于某中⼼词在 ⽂本序列前后的背景词来生成该中心词</strong></p>\n</li>\n<li><p>如下图，在同样的⽂本序列“the” “man” “loves” “his” “son” ⾥，以“loves”作为中⼼词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词 “the” “man” “his” “son”生成中心词“loves”的条件概率，也就是：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(loves|the,man,his,son)</script><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125172417014.png\" alt=\"image-20220125172417014\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><p>因为连续词袋模型的背景词有多个，我们<strong>将这些背景词向量取平均</strong>，然后使⽤和跳字模型⼀样的 ⽅法来计算条件概率</p>\n<p>设<script type=\"math/tex\">v_i \\in R^d</script>和<script type=\"math/tex\">u_i \\in R^d</script>分别表示词典中索引为<script type=\"math/tex\">i</script>的词作为背景词和中⼼词的向 量（<strong>注意符号的含义与跳字模型中的相反</strong>）。设中⼼词<script type=\"math/tex\">w_c</script>在词典中索引为<script type=\"math/tex\">c</script>，背景词<script type=\"math/tex\">w_{o1} , . . . , w_{o2m}</script>在 词典中索引为<script type=\"math/tex\">o_1, . . . , o_{2m}</script>，那么给定背景词生成中⼼词的条件概率：</p>\n<script type=\"math/tex; mode=display\">\n.P\\left(w_{c} \\mid w_{o_{1}}, \\ldots, w_{o_{2 m}}\\right)=\\frac{\\exp \\left(\\frac{1}{2 m} \\boldsymbol{u}_{c}^{\\top}\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right)\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\frac{1}{2 m} \\boldsymbol{u}_{i}^{\\top}\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right)\\right)}</script><p>设<script type=\"math/tex\">W_o = \\{w_{o1} , . . . , w_{o2m}\\}</script>，<script type=\"math/tex\">\\overline{\\boldsymbol{v}}_{o}=\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right) /(2 m)</script>，可将上式简化为：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{c} \\mid \\mathcal{W}_{o}\\right)=\\frac{\\exp \\left(\\boldsymbol{u}_{c}^{\\top} \\overline{\\boldsymbol{v}}_{o}\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\overline{\\boldsymbol{v}}_{o}\\right)}</script></li>\n<li><p>给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type=\"math/tex\">w^{(t)}</script>，背景窗口大小为m。连续词袋模型的似然函数是由背景词生成任⼀中⼼词的概率：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\prod_{t=1}^{T} P\\left(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}\\right)</script><ul>\n<li>接着再像跳字模型一样求似然函数的负对数，然后再求关于背景向量和中心向量的梯度。<strong>同跳字模型不⼀样的⼀点在于，我们⼀般使⽤连续词袋模型的背景词向量作为词的表征向量</strong></li>\n</ul>\n<ul>\n<li><strong>跳字模型更适合大型语料库，CBOW更适合小型的语料库</strong></li>\n</ul>\n<h3 id=\"1-4-近似训练\"><a href=\"#1-4-近似训练\" class=\"headerlink\" title=\"1.4 近似训练\"></a>1.4 近似训练</h3><ul>\n<li>无论是跳字模型还是连续词袋模型，在求梯度的时候，由于条件概率使⽤了softmax运 算，每⼀步的梯度计算都包含词典大小数⽬的项的累加。对于含⼏⼗万或上百万词的较大词典， 每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练⽅法，即<strong>负采样（negative sampling）</strong>或<strong>层序softmax（hierarchical softmax）</strong></li>\n</ul>\n<h5 id=\"1-4-1-高频词抽样\"><a href=\"#1-4-1-高频词抽样\" class=\"headerlink\" title=\"1.4.1 高频词抽样\"></a>1.4.1 高频词抽样</h5><ul>\n<li>首先介绍一下对高频词进行抽样（也称⼆次采样(subsampling)）。对于文本中的高频词，比如”the”，不进行抽样会带来两个问题：</li>\n</ul>\n<blockquote>\n<ol>\n<li>当我们得到成对的单词训练样本时，<strong>(“fox”, “the”) 这样的训练样本并不会给我们提供关于“fox”更多的语义信息</strong>，因为“the”在每个单词的上下文中几乎都会出现</li>\n<li>由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数</li>\n</ol>\n</blockquote>\n<ul>\n<li>Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：<strong>对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关，词频越高被删除的概率就越大</strong></li>\n</ul>\n<h5 id=\"1-4-2-负采样（negative-sampling）\"><a href=\"#1-4-2-负采样（negative-sampling）\" class=\"headerlink\" title=\"1.4.2 负采样（negative sampling）\"></a>1.4.2 负采样（negative sampling）</h5><ul>\n<li><strong>负采样（negative sampling）</strong>解决了因为词典大小过大的计算开销问题。不同于原本每个训练样本更新所有的权重，<strong>负采样每次让一个训练样本仅仅更新一小部分的权重</strong>，这样就会降低梯度下降过程中的计算量。</li>\n<li>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们<strong>期望输出为0的神经元结点</strong>所对应的单词我们称为<strong>“negative” word</strong>。</li>\n<li><p>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。<strong>（在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。）</strong></p>\n</li>\n<li><p><strong>每个单词被选为“negative words”的概率计算公式与其出现的频次有关，词频越高，被选上的概率就越高</strong>，论文中给的公式是：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{i}\\right)=\\frac{f\\left(w_{i}\\right)^{3 / 4}}{\\sum_{j=0}^{n}\\left(f\\left(w_{j}\\right)^{3 / 4}\\right)}</script><ul>\n<li><strong>负采样概述：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>负采样修改了原来的⽬标函数。给定中⼼词<script type=\"math/tex\">w_c</script>的⼀个背景窗口，我们把背景词<script type=\"math/tex\">w_o</script>出现在该背景窗口看作⼀个事件，并将该事件的概率计算为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(D=1 \\mid w_{c}, w_{o}\\right)=\\sigma\\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)</script><ul>\n<li>我们先考虑最大化⽂本序列中所有该事件的联合概率来训练词向量。具体来说，给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type=\"math/tex\">w^{(t)}</script>且背景窗口大小为m，考虑最大化联合概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(D=1 \\mid w^{(t)}, w^{(t+j)}\\right)</script><p>然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为⽆穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫⽆意义。<strong>负采样通过采样并添加负类样本使⽬标函数更有意义。</strong></p>\n<ul>\n<li>在采集了K个负样本后，我们可以将所求概率近似为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(w^{(t+j)} \\mid w^{(t)}\\right)=P\\left(D=1 \\mid w^{(t)}, w^{(t+j)}\\right) \\prod_{k=1, w_{k} \\sim P(w)}^{K} P\\left(D=0 \\mid w^{(t)}, w_{k}\\right)</script><ul>\n<li>取负对数后变成了：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\log P\\left(w^{(t+j)} \\mid w^{(t)}\\right)=-\\log \\sigma\\left(\\boldsymbol{u}_{i_{t+j}}^{\\top} v_{i_{t}}\\right)-\\sum_{k=1, w_{k} \\sim P(w)}^{K} \\log \\sigma\\left(-\\boldsymbol{u}_{h_{k}}^{\\top} v_{i_{t}}\\right)</script><p>现在，<strong>训练中每⼀步的梯度计算开销不再与词典大小相关，而与K线性相关</strong>。当K取较小的常数时，负采样在每⼀步的梯度计算开销较小。</p>\n</blockquote>\n<ul>\n<li><strong>负采样的具体实现：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li><p>在实际应用中，我们并没有严格的将不在窗口中的词定义为负样本，而是根据词频直接随机取负样本，这样的实验结果是更优的。</p>\n</li>\n<li><p><strong>设词典大小为<script type=\"math/tex\">\\mathcal{V}</script>，将一根长为1的线段分为<script type=\"math/tex\">\\mathcal{V}</script>段，每段分别对应一个词，每个词对应的长度就是上面所写的每个词对应的频率<script type=\"math/tex\">P(w_i)</script>，然后取一个M（M&gt;&gt;<script type=\"math/tex\">\\mathcal{V}</script>），论文中M取的是<script type=\"math/tex\">10^8</script>，将长度为1的线段均分为M份（如下图），然后取K个负样本即在该线段上取K个点，取到的点在哪个词的对应线段就取哪个词</strong></p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211153340493.png\" alt=\"image-20220211153340493\" style=\"zoom:67%;\" /></p>\n</blockquote>\n<h5 id=\"1-4-3-层序Softmax（hierarchical-softmax）\"><a href=\"#1-4-3-层序Softmax（hierarchical-softmax）\" class=\"headerlink\" title=\"1.4.3 层序Softmax（hierarchical softmax）\"></a>1.4.3 层序Softmax（hierarchical softmax）</h5><ul>\n<li>层序softmax是另⼀种近似训练法。它使⽤了⼆叉树这⼀数据结构，树的每个叶结点代表词典<script type=\"math/tex\">\\mathcal{V}</script>中的每个词。</li>\n<li><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220210181128975.png\" alt=\"image-20220210181128975\" style=\"zoom: 80%;\" /></li>\n</ul>\n<p>假设<script type=\"math/tex\">L(w)</script>为从⼆叉树的根结点到词<script type=\"math/tex\">w</script>的叶结点的路径（包括根结点和叶结点）上的结点数。 设<script type=\"math/tex\">n(w, j)</script>为该路径上第<script type=\"math/tex\">j</script>个结点，并设该结点的背景词向量为<script type=\"math/tex\">u_{n(w,j)}</script>。以上图为例，<script type=\"math/tex\">L(w_3) = 4</script> 层序softmax将跳字模型中的条件概率近似表示为：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{o} \\mid w_{c}\\right)=\\prod_{j=1}^{L\\left(w_{o}\\right)-1} \\sigma\\left([[ n\\left(w_{o}, j+1\\right)=\\operatorname{leftChild}\\left(n\\left(w_{o}, j\\right)\\right) ]] \\cdot \\boldsymbol{u}_{n\\left(w_{o}, j\\right)}^{\\top} \\boldsymbol{v}_{c}\\right),</script><p>其中如果判断x为真，[[x]] = 1；反之[[x]] = −1。</p>\n<p>例如让我们计算上图中给定词<script type=\"math/tex\">w_c</script>生成词<script type=\"math/tex\">w_3</script>的条件概率：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{3} \\mid w_{c}\\right)=\\sigma\\left(\\boldsymbol{u}_{n\\left(w_{3}, 1\\right)}^{\\top} \\boldsymbol{v}_{c}\\right) \\cdot \\sigma\\left(-\\boldsymbol{u}_{n\\left(w_{3}, 2\\right)}^{\\top} \\boldsymbol{v}_{c}\\right) \\cdot \\sigma\\left(\\boldsymbol{u}_{n\\left(w_{3}, 3\\right)}^{\\top} \\boldsymbol{v}_{c}\\right)</script><ul>\n<li><strong>推导过程：</strong></li>\n</ul>\n<blockquote>\n<p>设<script type=\"math/tex\">d_{2}^{w}, d_{3}^{w}, \\cdots, d_{l w}^{w} \\in\\{0,1\\}</script>为词<script type=\"math/tex\">w</script>的编码，<script type=\"math/tex\">d_j^w</script>代表<script type=\"math/tex\">w</script>路径上的第<script type=\"math/tex\">j</script>个节点对应的编码（根节点无编码），则：</p>\n<script type=\"math/tex; mode=display\">\nP(w_0|w_c) = \\prod_{j=2}^{L(w)} P\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right)</script><p>其中的每一项都是一个Logistic回归：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right) = \\left\\{\\begin{array}{ll}\n\\sigma\\left(v_c^T u_{n_(w, j)}\\right), & d_{j}^{w}=0 \\\\\n1-\\sigma\\left(v_c^T u_{n_(w, j)}\\right), & d_{j}^{w}=1\n\\end{array}\\right.</script><p>可以将两式合并，并且由于<script type=\"math/tex\">\\sigma(x) + \\sigma(-x) = 1</script>，可以将上式转化为：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right)=\\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]^{1-d_{j}^{w}} \\cdot\\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]^{d^{w}}</script><p>我们取目标函数的对数：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{L} = \\sum_{w \\in \\mathcal{V}}\\log P(Context(w) | w) = \\sum_{w \\in \\mathcal{V}} \\sum_{j=2}^{L(w)}\\left\\{\\left(1-d_{j}^{w}\\right) \\cdot \\log \\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]+d_{j}^{w} \\cdot \\log \\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]\\right\\}</script><p><strong>要最大化上述目标函数是比较困难的，但是我们可以最大化每一子项（即下式），达到局部最大化的效果</strong>：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{L}(w, j) = \\left\\{\\left(1-d_{j}^{w}\\right) \\cdot \\log \\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]+d_{j}^{w} \\cdot \\log \\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]\\right\\}</script><p><strong>这样就可以将计算复杂度降为<script type=\"math/tex\">O(log_2 |\\mathcal{V}|)</script>，当词典<script type=\"math/tex\">\\mathcal{V}</script>很大时，层序softmax在训练中每⼀步的梯度计算开销相较未使用近似训练时大幅降低。</strong></p>\n<p>显然，上式的<script type=\"math/tex\">d_j^w</script>是用来<strong>判断下一个节点是否是本节点的左孩子的</strong>，其作用是为了满足给定任意中心词<script type=\"math/tex\">w_c</script>，生成背景词的条件概率和为1：</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{w \\in \\mathcal{V}} P\\left(w \\mid w_{c}\\right)=1</script><p>这样才能使似然函数不至于训练到无穷大（和负采样要添加负样本是一样的原因），并且符合概率和为1</p>\n</blockquote>\n<ul>\n<li><strong>层序Softmax中的二叉树是使用的哈夫曼树，可以使搜索次数达到最小</strong></li>\n</ul>\n<h1 id=\"2-子词嵌入（fastText）\"><a href=\"#2-子词嵌入（fastText）\" class=\"headerlink\" title=\"2 子词嵌入（fastText）\"></a>2 子词嵌入（fastText）</h1><ul>\n<li>英语单词通常有其内部结构和形成⽅式。例如，我们可以从“dog” “dogs”和“dogcatcher”的 字面上推测它们的关系。这些词都有同⼀个词根“dog”，但使⽤不同的后缀来改变词的含义。而 且，这个关联可以推⼴⾄其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关 系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。这方面的研究称为构词学</li>\n<li><p>在word2vec中，我们并没有直接利⽤构词学中的信息。⽆论是在跳字模型还是连续词袋模型中， 我们都将形态不同的单词⽤不同的向量来表示。例如，“dog”和“dogs”分别⽤两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌⼊（subword embedding）的⽅法，从而试图将构词信息引⼊word2vec中的跳字模型</p>\n</li>\n<li><p>在fastText中，每个中⼼词被表⽰成子词的集合。举个例子，例如单词”where”，⾸先，我们在单词的<strong>⾸尾分别添加特殊字符“&lt;” 和  “&gt;” 以区分作为前后缀的子词</strong>。然后，<strong>将单词当成⼀个由字符构成的序列来提取n元语法</strong>。例如，当n = 3时，我们得到所有⻓度为3的子词：”<wh\"、\"whe\"、\"her\"、\"ere\"、\"re>“以及特殊子词”where”。</p>\n</li>\n<li>在fastText中，对于⼀个词w，我们将它所有⻓度在3 ∼ 6的子词和特殊子词的并集记为<script type=\"math/tex\">\\mathcal{G}_{w}</script>。<strong>那么词典则是所有词的子词集合的并集</strong>。假设词典中子词<script type=\"math/tex\">g</script>的向量为<script type=\"math/tex\">z_g</script>，那么跳字模型中词w的作为中⼼词的向量<script type=\"math/tex\">v_w</script>则表⽰成：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nv_{w}=\\sum_{g \\in \\mathcal{G}_{w}} z_{g}</script><ul>\n<li>fastText的其余部分同跳字模型⼀致，不在此重复。可以看到，<strong>与跳字模型相⽐，fastText中词典 规模更大，造成模型参数更多</strong>，同时⼀个词的向量需要对所有子词向量求和，继而导致<strong>计算复杂度更高</strong>。但与此同时，<strong>较生僻的复杂单词，甚⾄是词典中没有的单词，可能会从同它结构类似的 其他词那⾥获取更好的词向量表示。</strong></li>\n</ul>\n<h1 id=\"3-全局向量的词嵌入（GloVe）\"><a href=\"#3-全局向量的词嵌入（GloVe）\" class=\"headerlink\" title=\"3 全局向量的词嵌入（GloVe）\"></a>3 全局向量的词嵌入（GloVe）</h1><ul>\n<li>在词嵌入中，交叉熵损失函数有时并不是一个好的选择。一方面，正如上面所说，<strong>会带来整个词典大小的累加项，带来过大的计算开销</strong>。另一方面，<strong>词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。</strong></li>\n<li>GloVe模型既使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。为了做到这一点GloVe模型引入了<strong>共现矩阵（Cooccurrence Probabilities Matrix）</strong></li>\n</ul>\n<h3 id=\"3-1-共现矩阵\"><a href=\"#3-1-共现矩阵\" class=\"headerlink\" title=\"3.1 共现矩阵\"></a>3.1 共现矩阵</h3><ul>\n<li><p>假设：</p>\n<blockquote>\n<ol>\n<li>共现矩阵为<script type=\"math/tex\">X</script>，<script type=\"math/tex\">X</script>中的元素<script type=\"math/tex\">X_{ij}</script>为语料库中<script type=\"math/tex\">word_i</script>上下文中出现<script type=\"math/tex\">word_j</script>的次数</li>\n<li><script type=\"math/tex\">X_i = \\sum_k X_{ik}</script>是出现在<script type=\"math/tex\">word_i</script>上下文中所有词的总次数</li>\n<li><script type=\"math/tex\">P_{ij} = P(j|i) = \\frac{X_{ij}}{X_i}</script>为<script type=\"math/tex\">word_j</script>出现在<script type=\"math/tex\">word_i</script>上下文的概率</li>\n</ol>\n</blockquote>\n</li>\n<li><p>下面我们来举个例子：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235403111.png\" alt=\"image-20220211235403111\" style=\"zoom:80%;\" /></p>\n<p>设Ratio = <script type=\"math/tex\">\\frac{P_{ik}}{P_{jk}}</script>，，从上面的例子中我们可以总结出：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235553030.png\" alt=\"image-20220211235553030\"></p>\n<p><strong>所以Ratio可以反应word之间的相关性，而GloVe就是利用了这个值</strong></p>\n<p>假设<script type=\"math/tex\">i, j, k</script>三者的词向量都已经得到<script type=\"math/tex\">w_i, w_j, w_k</script>，那么我们现在就要找一个函数F，使得：</p>\n<script type=\"math/tex; mode=display\">\nF\\left(w_{i}, w_{j}, w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}</script><h3 id=\"3-2-损失函数推导\"><a href=\"#3-2-损失函数推导\" class=\"headerlink\" title=\"3.2 损失函数推导\"></a>3.2 损失函数推导</h3><ul>\n<li><p><strong>上述等式的右边是通过统计得到的已知量，左侧的三个词向量<script type=\"math/tex\">w_i, w_j, w_k</script>是模型要求的量，F是未知的，如果能够将F的形式确定下来， 那么我们就可以通过优化算法求解词向量了，下面是求解过程</strong></p>\n</li>\n<li><p><script type=\"math/tex\">\\frac{P_{i k}}{P_{j k}}</script>考察了<script type=\"math/tex\">i,j,k</script>三个词的相关性，不妨先只考虑<script type=\"math/tex\">i,j</script>两个词的词向量<script type=\"math/tex\">w_i, w_j</script>的相关性，向量相关关系的度量可以用两个向量的差，所以上述等式可以先改为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF\\left((w_{i}-w_{j}), w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}</script><ul>\n<li>由于右边是标量，左边是向量，所以可以联想到使用向量的内积转化为标量：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF\\left(\\left(w_{i}-w_{j}\\right)^{T} w_{k}\\right)=F\\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}</script><ul>\n<li>因为左边是差，右边是商，所以将F设为exp运算，将差转化为商：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\exp \\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\\right)=\\frac{\\exp \\left(w_{i}^{T} w_{k}\\right)}{\\exp \\left(w_{j}^{T} w_{k}\\right)}=\\frac{P_{i k}}{P_{j k}}</script><ul>\n<li>现在只需让分子分母分别相等就能成立：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\exp \\left(w_{i}^{T} w_{k}\\right)= \\alpha P_{i k}, \\exp \\left(w_{j}^{T} w_{k}\\right)= \\alpha P_{j k}，\\quad  \\alpha为常数</script><ul>\n<li>现在只需要在整个词料库中考察<script type=\"math/tex\">\\exp \\left(w_{i}^{T} w_{k}\\right)= \\alpha P_{i k}= \\alpha \\frac{X_{i k}}{X_{i}}</script>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nw_{i}^{T} w_{k}=\\log \\left(\\alpha \\frac{X_{i k}}{X_{i}}\\right)=\\log X_{i k}-\\log X_{i} + log\\alpha</script><ul>\n<li>所以我们需要设置一个偏差项<script type=\"math/tex\">b_i</script>来拟合<script type=\"math/tex\">log X_i - log \\alpha</script>，由于如果上式<script type=\"math/tex\">i,k</script>位置交换，左式的值不变，但是右式的值会变，所以我们还要设置一个偏差项<script type=\"math/tex\">b_k</script>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nw_{i}^{T} w_{k}  + b_i + b_k=log X_{ik}</script><ul>\n<li>上面公式只是理想状态下，实际中只能要求两者接近    从而就有了代价函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ=\\sum_{i, k}\\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\\log X_{i k}\\right)^{2}</script><ul>\n<li>如果两个词共同出现的次数越多，那么其在代价函数中的影响就越大，所以要设计一个权重函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ=\\sum_{i, k} f\\left(X_{i k}\\right)\\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\\log X_{i k}\\right)^{2}</script><blockquote>\n<p>对于函数f要满足一下条件：</p>\n<ol>\n<li>如果两个词没有共同出现过，那么权重就是0，即f(0) = 0</li>\n<li>两个词共同出现次数越多，那么权重就越大，所以f是一个递增函数</li>\n<li>由一些于高频词（<script type=\"math/tex\">X_{ij}较大</script>，如”the”）的权重很大，但是实际上作用并不是很大，所以f(x)对于较大的x不能取太大的值</li>\n</ol>\n<p>综合上方条件，论文提出了一下函数：</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\left\\{\\begin{array}{r}\n\\left(\\frac{x}{x_{\\max }}\\right)^{\\alpha}, \\text { if } x<x_{\\max } \\\\\n1, \\text { otherwise }\n\\end{array}\\right.</script><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212005820238.png\" alt=\"image-20220212005820238\" style=\"zoom:67%;\" /></p>\n<p>作者认为<script type=\"math/tex\">x_{max} = 100, \\alpha = \\frac{3}{4}</script>比较合适</p>\n</blockquote>\n<ul>\n<li>值得强调的是，如果词<script type=\"math/tex\">w_i</script>出现在词<script type=\"math/tex\">w_j</script>的背景窗口里，那么词<script type=\"math/tex\">w_j</script>也会出现在词<script type=\"math/tex\">w_i</script>的背景窗口⾥。也就是说，<script type=\"math/tex\">x_{ij} = x_{ji}</script>。不同于<strong>word2vec中拟合的是非对称的条件概率<script type=\"math/tex\">p_{ij}</script>，GloVe模型拟合的是对称的<script type=\"math/tex\">\\log X _{ij}</script></strong>。因此，<strong>任意词的中心词向量和背景词向量在GloVe模型中是等价的。但由于初始化值的不同，同⼀个词最终学习到的两组词向量可能不同</strong>。当学习得到所有词向量以后，GloVe模型使⽤<strong>中心词向量与背景词向量之和作为该词的最终词向量，这样做是为了提高鲁棒性（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）</strong></li>\n</ul>\n<h1 id=\"4-词嵌入应用\"><a href=\"#4-词嵌入应用\" class=\"headerlink\" title=\"4 词嵌入应用\"></a>4 词嵌入应用</h1><ul>\n<li>下列应用均为在训练好的预训练模型上进行训练的</li>\n</ul>\n<h3 id=\"4-1-求近义词和类比词\"><a href=\"#4-1-求近义词和类比词\" class=\"headerlink\" title=\"4.1 求近义词和类比词\"></a>4.1 求近义词和类比词</h3><ul>\n<li>求近义词可以运用词向量之间的余弦相似度</li>\n<li>求类比词，例如，“man”（男人）: “woman”（女人）:: “son”（儿子）: “daughter”（女儿）是⼀个类⽐例子。<strong>对于类比关系中的4个词 a : b :: c : d，给定前3个词a、b和c，求d。设词w的词向量为vec(w)。求类比词的思路是vec(a) - vec(b) = vec(c) - vec(d)，来求出vec(d)的近似值，然后寻找和其最相似的几个词向量</strong></li>\n</ul>\n<h3 id=\"4-2-使用循环神经网络进行文本情感分类\"><a href=\"#4-2-使用循环神经网络进行文本情感分类\" class=\"headerlink\" title=\"4.2 使用循环神经网络进行文本情感分类\"></a>4.2 使用循环神经网络进行文本情感分类</h3><ul>\n<li>⽂本分类是⾃然语⾔处理的⼀个常⻅任务，它把⼀段不定⻓的⽂本序列变换为⽂本的类别。本节关 注它的⼀个子问题：使⽤⽂本情感分类来分析⽂本作者的情绪。这个问题也叫<strong>情感分析（sentiment analysis）</strong></li>\n<li>本节我们使用电影评论的情感分析（二分类，positive和negtive的情感），在数据处理的时候，一条评论为一个样本，我们要先对评论进行分词（一般都是<strong>基于空格分词</strong>），并且由于每条评论的长度不一样，无法组成小批量，所以我们需要通过<strong>截断或者补充来使长度定长</strong></li>\n<li>对于模型的设计，先需要一个<strong>嵌入层将文本转化为词向量</strong>，再通过<strong>双向循环网络对特征序列进⼀步编码得到序列信息</strong>，然后将<strong>最初时间步和最终时间步的隐藏状态连结</strong>，再传入全连接层输出</li>\n<li>注意由于是预训练的模型，所以<strong>嵌入层的模型参数是不需要更新的</strong>，直接由训练好的模型参数导入即可。但是<strong>导入的模型的词向量维度要和预先设置的嵌入层词向量维度一致</strong></li>\n</ul>\n<h3 id=\"4-3-使用卷积神经网络（textCNN）进行文本情感分类：\"><a href=\"#4-3-使用卷积神经网络（textCNN）进行文本情感分类：\" class=\"headerlink\" title=\"4.3 使用卷积神经网络（textCNN）进行文本情感分类：\"></a>4.3 使用卷积神经网络（textCNN）进行文本情感分类：</h3><ul>\n<li><p>我们可以将⽂本数据看作只有⼀个维度的时间序列，并很⾃然地使⽤循环神经⽹络来表征这样的数据。其实，我们也可以将⽂本当作⼀维图像，从而可以⽤⼀维卷积神经⽹络来捕捉临近词之间的关联</p>\n</li>\n<li><p><strong>textCNN基于的假设是：一个词和其相邻的词（即卷积核中的词）是相关的</strong></p>\n</li>\n</ul>\n<h5 id=\"4-3-1-一维卷积层\"><a href=\"#4-3-1-一维卷积层\" class=\"headerlink\" title=\"4.3.1 一维卷积层\"></a>4.3.1 一维卷积层</h5><ul>\n<li>和二维卷积层一样，一维卷积层卷积窗口从输⼊数组的最左⽅开始，按从左往右的顺序，依次在输⼊数组上滑动，举个例子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155418239.png\" alt=\"image-20220212155418239\" style=\"zoom:67%;\" /></p>\n<p>输出中的2是由输入的前两个和核逐个相乘得到的：0x1+1x2=2，然后窗口往后滑动一格，以此类推</p>\n<p>对于多通道操作也是和二维卷积层一样的：<br><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155747478.png\" alt=\"image-20220212155747478\"></p>\n<p>每个通道分别操作，然后各通道结果相加。如果想要输出多通道，则核还要增加一个维度</p>\n<h5 id=\"4-3-2-时序最大池化层\"><a href=\"#4-3-2-时序最大池化层\" class=\"headerlink\" title=\"4.3.2 时序最大池化层\"></a>4.3.2 时序最大池化层</h5><ul>\n<li>假设输⼊包含多个通道，各通道由不同时间步上的数值组成，<strong>各通道的输出即该通道所有时间步中最⼤的数值</strong>。因此，<strong>时序最⼤池化层的输⼊在各个通道上的时间步数可以不同</strong></li>\n<li>为提升计算性能，我们常常将不同⻓度的时序样本组成⼀个小批量，<strong>并通过在较短序列后附加特殊字符（如0）令批量中各时序样本⻓度相同</strong>。这些⼈为添加的特殊字符当然是⽆意义的。由于时序最⼤池化的主要⽬的是抓取时序中最重要的特征，它通常能<strong>使模型不受⼈为添加字符的影响</strong></li>\n</ul>\n<h5 id=\"4-3-3-textCNN模型\"><a href=\"#4-3-3-textCNN模型\" class=\"headerlink\" title=\"4.3.3 textCNN模型\"></a>4.3.3 textCNN模型</h5><ul>\n<li>textCNN模型主要使⽤了⼀维卷积层和时序最⼤池化层。假设输⼊的⽂本序列由n个词组成，每 个词⽤d维的词向量表⽰。那么输⼊样本的宽为n，⾼为1，输⼊通道数为d。textCNN的计算主要分为以下⼏步：</li>\n</ul>\n<blockquote>\n<ol>\n<li>定义多个⼀维卷积核，并使⽤这些卷积核对输⼊分别做卷积计算。宽度不同的卷积核可能 会捕捉到不同个数的相邻词的相关性</li>\n<li>对输出的所有通道分别做时序最⼤池化，再将这些通道的池化输出值连结为向量。</li>\n<li>通过全连接层将连结后的向量变换为有关各类别的输出。这⼀步可以使⽤丢弃层应对过拟合。</li>\n</ol>\n</blockquote>\n<ul>\n<li>举个例子，这⾥的输⼊是⼀个有11个词的句子，每个词⽤6维词向量表⽰。因此输⼊序列的宽为11，输⼊通道数为6。给定2个⼀维卷积核，核宽分别为2和4，输出 通道数分别设为4和5</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212161751535.png\" alt=\"image-20220212161751535\" style=\"zoom: 67%;\" /></p>\n<p><strong>尽管每个通道的宽不同，我们依然可以对各个通道做时序最⼤池化， 并将9个通道的池化输出连结成⼀个9维向量</strong>。最终，使⽤全连接将9维向量变换为2维输出，即正⾯情感和负⾯情感的预测</p>\n<h1 id=\"5-机器翻译\"><a href=\"#5-机器翻译\" class=\"headerlink\" title=\"5 机器翻译\"></a>5 机器翻译</h1><ul>\n<li><p>我们使用seq2seq模型进行机器翻译，原理具体可看<a href=\"https://zlkqz.top/2021/12/10/RNN/#7-Seq2Seq%E6%A8%A1%E5%9E%8B\">seq2seq</a></p>\n</li>\n<li><p>这里我们只介绍一下如何评价翻译结果。评价机器翻译结果通常使⽤<strong>BLEU（Bilingual Evaluation Understudy）</strong>。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。</p>\n</li>\n<li>具体来说，设词数为n的⼦序列的精度为<script type=\"math/tex\">p_n</script>。它是<strong>预测序列与标签序列匹配词数为n的⼦序列的数量与预测序列中词数为n的⼦序列的数量之⽐</strong>。举个例子，假设标签序列为A、B、C、D、E、F， 预测序列为A、B、B、C、D，那么<script type=\"math/tex\">p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0</script>。设<script type=\"math/tex\">len_{label}</script>和<script type=\"math/tex\">len_{pred}</script>分 别为标签序列和预测序列的词数，那么，BLEU的定义为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\exp \\left(\\min \\left(0,1-\\frac{\\text { len }_{\\text {label }}}{\\text { len }_{\\text {pred }}}\\right)\\right) \\prod_{n=1}^{k} p_{n}^{1 / 2^{n}}</script><p>其中k是我们希望匹配的⼦序列的最⼤词数。可以看到当预测序列和标签序列完全⼀致时， BLEU为1。</p>\n<ul>\n<li>设计思想：</li>\n</ul>\n<blockquote>\n<p>因为匹配较⻓⼦序列⽐匹配较短⼦序列更难，BLEU对匹配较⻓⼦序列的精度赋予了更⼤权重，具体是在上式的<script type=\"math/tex\">p_n^{1/2^n}</script>中，当序列较长，即n较大时，由于<script type=\"math/tex\">p_n \\in [0, 1]</script>，所以<script type=\"math/tex\">1/2^n</script>后会变大，并且n越大越接近1，即给较长序列更大的权重</p>\n<p>并且由于较短序列一般会有比较大的<script type=\"math/tex\">p_n</script>，所以连乘项前面的系数是为了惩罚较短的输出而设置的。举个例子，当<script type=\"math/tex\">len_{pred}</script>较小时，连乘项前面的系数就会小于1，以此来减少较短序列的权重。</p>\n</blockquote>\n","site":{"data":{}},"wordcount":11731,"excerpt":"","more":"<h1 id=\"1-词嵌入（word-embedding）\"><a href=\"#1-词嵌入（word-embedding）\" class=\"headerlink\" title=\"1 词嵌入（word embedding）\"></a>1 词嵌入（word embedding）</h1><ul>\n<li>词向量是⽤来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）</li>\n</ul>\n<h3 id=\"1-1-词嵌入相比于one-hot向量的优点\"><a href=\"#1-1-词嵌入相比于one-hot向量的优点\" class=\"headerlink\" title=\"1.1 词嵌入相比于one-hot向量的优点\"></a>1.1 词嵌入相比于one-hot向量的优点</h3><ul>\n<li>虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，<strong>one-hot词 向量⽆法准确表达不同词之间的相似度</strong>，如我们常常使⽤的余弦相似度。对于向量<script type=\"math/tex\">x, y \\in R^d</script>，它 们的余弦相似度是它们之间夹⻆的余弦值：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{y}}{\\|\\boldsymbol{x}\\|\\|\\boldsymbol{y}\\|} \\in[-1,1]</script><p>由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。</p>\n<p>word2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表示成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。word2vec⼯具包含了两个模型，即<strong>跳字模型（skip-gram）</strong> 和<strong>连续词袋模型（continuous bag of words，CBOW）</strong></p>\n<h3 id=\"1-2-跳字模型（skip-gram）\"><a href=\"#1-2-跳字模型（skip-gram）\" class=\"headerlink\" title=\"1.2 跳字模型（skip-gram）\"></a>1.2 跳字模型（skip-gram）</h3><ul>\n<li>跳字模型假设基于某个词来生成它在⽂本序列周围的词</li>\n<li>举个例子，假设⽂本序列是 “the” “man” “loves” “his” “son”。以“loves”作为中⼼词，设背景窗口大小为2。如下图所示，跳字模型所关⼼的是，给定中⼼词“loves”，生成与它距离不超过2个词的背景词 “the” “man” “his” “son”的条件概率，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(the, man, his, son | loves)</script><p>假设给定中⼼词的情况下，背景词的生成是相互独⽴的，那么上式可以改写成：</p>\n<script type=\"math/tex; mode=display\">\nP(the| loves) \\times P(man | loves) \\times P(his | loves) \\times P(son | loves)</script><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125151903218.png\" alt=\"image-20220125151903218\" style=\"zoom: 80%;\" /></p>\n<ul>\n<li>在跳字模型中，<strong>每个词被表示成两个<script type=\"math/tex\">d</script>维向量</strong>，⽤来计算条件概率。假设这个词在词典中索引为<script type=\"math/tex\">i</script>， 当它为中心词时向量表示为<script type=\"math/tex\">v_i \\in R^d</script>，而为背景词时向量表示为<script type=\"math/tex\">u_i \\in R^d</script>。设中⼼词<script type=\"math/tex\">w_c</script>在词典中索引为<script type=\"math/tex\">c</script>，背景词<script type=\"math/tex\">w_o</script>在词典中索引为<script type=\"math/tex\">o</script>，<strong>给定中⼼词<script type=\"math/tex\">w_c</script>生成背景词<script type=\"math/tex\">w_o</script>的条件概率可以通过对向量内积做softmax运算而得到：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{o} \\mid w_{c}\\right)=\\frac{\\exp \\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{v}_{c}\\right)},</script><p>其中词典索引集<script type=\"math/tex\">V = \\{0, 1, . . . , |V|−1\\}</script></p>\n<ul>\n<li>假设给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type=\"math/tex\">w^{(t)}</script>。 假设给定中⼼词的情况下背景词的生成相互独⽴，当背景窗口大小为m时，跳字模型的<strong>似然函数</strong>即给定任⼀中⼼词生成所有背景词的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(w^{(t+j)} \\mid w^{(t)}\\right),</script><p>这⾥小于1或大于T的时间步可以被忽略</p>\n<ul>\n<li><strong>跳字模型的训练：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li><strong>跳字模型的参数是每个词所对应的中心词向量和背景词向量</strong></li>\n<li>先把把最大似然函数取负对数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P\\left(w^{(t+j)} \\mid w^{(t)}\\right)</script><p>如果使⽤随机梯度下降，那么在每⼀次迭代⾥我们随机采样⼀个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数</p>\n<ul>\n<li>由Softmax的运算结果可以得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\log P\\left(w_{o} \\mid w_{c}\\right)=\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}-\\log \\left(\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{v}_{c}\\right)\\right)</script><p>那么我们可以求得上式关于各中心词向量和背景词向量的梯度，比如关于<script type=\"math/tex\">v_c</script>的梯度：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\log P\\left(w_{o} \\mid w_{c}\\right)}{\\partial v_{c}}=\\boldsymbol{u}_{o}-\\sum_{j \\in \\mathcal{V}} P\\left(w_{j} \\mid w_{c}\\right) \\boldsymbol{u}_{j} .</script><p>训练结束后，对于词典中的任⼀索引为<script type=\"math/tex\">i</script>的词，我们均得到该词作为中⼼词和背景词的两组词向量<script type=\"math/tex\">v_i</script>和<script type=\"math/tex\">u_i</script>。在⾃然语⾔处理应⽤中，<strong>⼀般使⽤跳字模型的中心词向量作为词的表征向量</strong></p>\n</blockquote>\n<ul>\n<li><strong>跳字模型的具体实现：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>可以用一个input-hidden-output的三层神经网络来建模上面的skip-model：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526195633208.jpg\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\" /></p>\n<ul>\n<li><p>输入的表示：输入层中每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1,其余维度的值均为0</p>\n</li>\n<li><p>网络中传播的前向过程：输出层向量的值可以通过<strong>隐含层向量（K维，即每一个词向量的维度）</strong>，以及连接隐藏层和输出层之间的<strong>KxN维权重矩阵</strong>计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算每一个单词的生成概率。</p>\n</li>\n<li><strong>input层和hidden层之间的N<script type=\"math/tex\">\\times</script>K权重矩阵即N个词的中心词向量，从input层到hidden层的运算即对中心词向量的提取，具体见下图。而hidden层和output层之间的K<script type=\"math/tex\">\\times</script>N权重矩阵即N个词的背景词向量，hidden层到output层的运算即实现<script type=\"math/tex\">u^T_ov_c</script>，然后再在最后做一个softmax</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20190526202027163.jpg\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\" /></p>\n<p>左边矩阵是input，右边矩阵是input到hidden的权重矩阵，即N个词的中心词向量，由于输入是ont-hot向量，所以运算结果其实就是对中心词向量的一个提取</p>\n<ul>\n<li>在代码实现中是直接使用一个<strong>嵌入层（Embedding层）</strong>，此层的权值形状为<strong>词典大小<script type=\"math/tex\">\\times</script>每个词的维度</strong>，可以直接将中心词和背景词对应的one-hot向量转化为词向量。以skip-gram举例，中心词的one-hot向量形状为(批量大小，1)，背景词为(批量大小，词典大小)，两者皆通过嵌入层转化成词向量后形状分别为(批量大小，1，每个词维度)、(批量大小，词典大小，每个词维度)。然后通过<strong>小批量乘法</strong>实现中心词向量和背景词向量的相乘<script type=\"math/tex\">u^T_ov_c</script>，得到形状为(批量大小，词典大小)的结果，然后进行Softmax</li>\n<li>上面的背景词one-hot向量第二维度不一定要为词典大小，如果我们采用<strong>负采样</strong>，则应该改为<strong>正负样本和的大小</strong>，具体可看下方负采样原理</li>\n<li>现在我们来解释一下<strong>小批量乘法</strong>，假设第⼀个小批量包含n个形状为<script type=\"math/tex\">a \\times b</script>的矩阵<script type=\"math/tex\">X_1, . . . , X_n</script>，第⼆个小批量包含n个形状为<script type=\"math/tex\">b \\times c</script>的矩阵<script type=\"math/tex\">Y_1, . . . , Y_n</script>。 这两个小批量的矩阵乘法输出为n个形状为<script type=\"math/tex\">a \\times c</script>的矩阵<script type=\"math/tex\">X_1Y_1, . . . , X_nY_n</script></li>\n</ul>\n</blockquote>\n<h3 id=\"1-3-连续词袋模型（CBOW）\"><a href=\"#1-3-连续词袋模型（CBOW）\" class=\"headerlink\" title=\"1.3 连续词袋模型（CBOW）\"></a>1.3 连续词袋模型（CBOW）</h3><ul>\n<li><p>连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，<strong>连续词袋模型假设基于某中⼼词在 ⽂本序列前后的背景词来生成该中心词</strong></p>\n</li>\n<li><p>如下图，在同样的⽂本序列“the” “man” “loves” “his” “son” ⾥，以“loves”作为中⼼词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词 “the” “man” “his” “son”生成中心词“loves”的条件概率，也就是：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(loves|the,man,his,son)</script><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220125172417014.png\" alt=\"image-20220125172417014\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><p>因为连续词袋模型的背景词有多个，我们<strong>将这些背景词向量取平均</strong>，然后使⽤和跳字模型⼀样的 ⽅法来计算条件概率</p>\n<p>设<script type=\"math/tex\">v_i \\in R^d</script>和<script type=\"math/tex\">u_i \\in R^d</script>分别表示词典中索引为<script type=\"math/tex\">i</script>的词作为背景词和中⼼词的向 量（<strong>注意符号的含义与跳字模型中的相反</strong>）。设中⼼词<script type=\"math/tex\">w_c</script>在词典中索引为<script type=\"math/tex\">c</script>，背景词<script type=\"math/tex\">w_{o1} , . . . , w_{o2m}</script>在 词典中索引为<script type=\"math/tex\">o_1, . . . , o_{2m}</script>，那么给定背景词生成中⼼词的条件概率：</p>\n<script type=\"math/tex; mode=display\">\n.P\\left(w_{c} \\mid w_{o_{1}}, \\ldots, w_{o_{2 m}}\\right)=\\frac{\\exp \\left(\\frac{1}{2 m} \\boldsymbol{u}_{c}^{\\top}\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right)\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\frac{1}{2 m} \\boldsymbol{u}_{i}^{\\top}\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right)\\right)}</script><p>设<script type=\"math/tex\">W_o = \\{w_{o1} , . . . , w_{o2m}\\}</script>，<script type=\"math/tex\">\\overline{\\boldsymbol{v}}_{o}=\\left(\\boldsymbol{v}_{o_{1}}+\\ldots+\\boldsymbol{v}_{o_{2 m}}\\right) /(2 m)</script>，可将上式简化为：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{c} \\mid \\mathcal{W}_{o}\\right)=\\frac{\\exp \\left(\\boldsymbol{u}_{c}^{\\top} \\overline{\\boldsymbol{v}}_{o}\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp \\left(\\boldsymbol{u}_{i}^{\\top} \\overline{\\boldsymbol{v}}_{o}\\right)}</script></li>\n<li><p>给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type=\"math/tex\">w^{(t)}</script>，背景窗口大小为m。连续词袋模型的似然函数是由背景词生成任⼀中⼼词的概率：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\prod_{t=1}^{T} P\\left(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}\\right)</script><ul>\n<li>接着再像跳字模型一样求似然函数的负对数，然后再求关于背景向量和中心向量的梯度。<strong>同跳字模型不⼀样的⼀点在于，我们⼀般使⽤连续词袋模型的背景词向量作为词的表征向量</strong></li>\n</ul>\n<ul>\n<li><strong>跳字模型更适合大型语料库，CBOW更适合小型的语料库</strong></li>\n</ul>\n<h3 id=\"1-4-近似训练\"><a href=\"#1-4-近似训练\" class=\"headerlink\" title=\"1.4 近似训练\"></a>1.4 近似训练</h3><ul>\n<li>无论是跳字模型还是连续词袋模型，在求梯度的时候，由于条件概率使⽤了softmax运 算，每⼀步的梯度计算都包含词典大小数⽬的项的累加。对于含⼏⼗万或上百万词的较大词典， 每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练⽅法，即<strong>负采样（negative sampling）</strong>或<strong>层序softmax（hierarchical softmax）</strong></li>\n</ul>\n<h5 id=\"1-4-1-高频词抽样\"><a href=\"#1-4-1-高频词抽样\" class=\"headerlink\" title=\"1.4.1 高频词抽样\"></a>1.4.1 高频词抽样</h5><ul>\n<li>首先介绍一下对高频词进行抽样（也称⼆次采样(subsampling)）。对于文本中的高频词，比如”the”，不进行抽样会带来两个问题：</li>\n</ul>\n<blockquote>\n<ol>\n<li>当我们得到成对的单词训练样本时，<strong>(“fox”, “the”) 这样的训练样本并不会给我们提供关于“fox”更多的语义信息</strong>，因为“the”在每个单词的上下文中几乎都会出现</li>\n<li>由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数</li>\n</ol>\n</blockquote>\n<ul>\n<li>Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：<strong>对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关，词频越高被删除的概率就越大</strong></li>\n</ul>\n<h5 id=\"1-4-2-负采样（negative-sampling）\"><a href=\"#1-4-2-负采样（negative-sampling）\" class=\"headerlink\" title=\"1.4.2 负采样（negative sampling）\"></a>1.4.2 负采样（negative sampling）</h5><ul>\n<li><strong>负采样（negative sampling）</strong>解决了因为词典大小过大的计算开销问题。不同于原本每个训练样本更新所有的权重，<strong>负采样每次让一个训练样本仅仅更新一小部分的权重</strong>，这样就会降低梯度下降过程中的计算量。</li>\n<li>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们<strong>期望输出为0的神经元结点</strong>所对应的单词我们称为<strong>“negative” word</strong>。</li>\n<li><p>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。<strong>（在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。）</strong></p>\n</li>\n<li><p><strong>每个单词被选为“negative words”的概率计算公式与其出现的频次有关，词频越高，被选上的概率就越高</strong>，论文中给的公式是：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{i}\\right)=\\frac{f\\left(w_{i}\\right)^{3 / 4}}{\\sum_{j=0}^{n}\\left(f\\left(w_{j}\\right)^{3 / 4}\\right)}</script><ul>\n<li><strong>负采样概述：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>负采样修改了原来的⽬标函数。给定中⼼词<script type=\"math/tex\">w_c</script>的⼀个背景窗口，我们把背景词<script type=\"math/tex\">w_o</script>出现在该背景窗口看作⼀个事件，并将该事件的概率计算为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(D=1 \\mid w_{c}, w_{o}\\right)=\\sigma\\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)</script><ul>\n<li>我们先考虑最大化⽂本序列中所有该事件的联合概率来训练词向量。具体来说，给定⼀个⻓度为T的⽂本序列，设时间步t的词为<script type=\"math/tex\">w^{(t)}</script>且背景窗口大小为m，考虑最大化联合概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(D=1 \\mid w^{(t)}, w^{(t+j)}\\right)</script><p>然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为⽆穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫⽆意义。<strong>负采样通过采样并添加负类样本使⽬标函数更有意义。</strong></p>\n<ul>\n<li>在采集了K个负样本后，我们可以将所求概率近似为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP\\left(w^{(t+j)} \\mid w^{(t)}\\right)=P\\left(D=1 \\mid w^{(t)}, w^{(t+j)}\\right) \\prod_{k=1, w_{k} \\sim P(w)}^{K} P\\left(D=0 \\mid w^{(t)}, w_{k}\\right)</script><ul>\n<li>取负对数后变成了：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\log P\\left(w^{(t+j)} \\mid w^{(t)}\\right)=-\\log \\sigma\\left(\\boldsymbol{u}_{i_{t+j}}^{\\top} v_{i_{t}}\\right)-\\sum_{k=1, w_{k} \\sim P(w)}^{K} \\log \\sigma\\left(-\\boldsymbol{u}_{h_{k}}^{\\top} v_{i_{t}}\\right)</script><p>现在，<strong>训练中每⼀步的梯度计算开销不再与词典大小相关，而与K线性相关</strong>。当K取较小的常数时，负采样在每⼀步的梯度计算开销较小。</p>\n</blockquote>\n<ul>\n<li><strong>负采样的具体实现：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li><p>在实际应用中，我们并没有严格的将不在窗口中的词定义为负样本，而是根据词频直接随机取负样本，这样的实验结果是更优的。</p>\n</li>\n<li><p><strong>设词典大小为<script type=\"math/tex\">\\mathcal{V}</script>，将一根长为1的线段分为<script type=\"math/tex\">\\mathcal{V}</script>段，每段分别对应一个词，每个词对应的长度就是上面所写的每个词对应的频率<script type=\"math/tex\">P(w_i)</script>，然后取一个M（M&gt;&gt;<script type=\"math/tex\">\\mathcal{V}</script>），论文中M取的是<script type=\"math/tex\">10^8</script>，将长度为1的线段均分为M份（如下图），然后取K个负样本即在该线段上取K个点，取到的点在哪个词的对应线段就取哪个词</strong></p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211153340493.png\" alt=\"image-20220211153340493\" style=\"zoom:67%;\" /></p>\n</blockquote>\n<h5 id=\"1-4-3-层序Softmax（hierarchical-softmax）\"><a href=\"#1-4-3-层序Softmax（hierarchical-softmax）\" class=\"headerlink\" title=\"1.4.3 层序Softmax（hierarchical softmax）\"></a>1.4.3 层序Softmax（hierarchical softmax）</h5><ul>\n<li>层序softmax是另⼀种近似训练法。它使⽤了⼆叉树这⼀数据结构，树的每个叶结点代表词典<script type=\"math/tex\">\\mathcal{V}</script>中的每个词。</li>\n<li><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220210181128975.png\" alt=\"image-20220210181128975\" style=\"zoom: 80%;\" /></li>\n</ul>\n<p>假设<script type=\"math/tex\">L(w)</script>为从⼆叉树的根结点到词<script type=\"math/tex\">w</script>的叶结点的路径（包括根结点和叶结点）上的结点数。 设<script type=\"math/tex\">n(w, j)</script>为该路径上第<script type=\"math/tex\">j</script>个结点，并设该结点的背景词向量为<script type=\"math/tex\">u_{n(w,j)}</script>。以上图为例，<script type=\"math/tex\">L(w_3) = 4</script> 层序softmax将跳字模型中的条件概率近似表示为：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{o} \\mid w_{c}\\right)=\\prod_{j=1}^{L\\left(w_{o}\\right)-1} \\sigma\\left([[ n\\left(w_{o}, j+1\\right)=\\operatorname{leftChild}\\left(n\\left(w_{o}, j\\right)\\right) ]] \\cdot \\boldsymbol{u}_{n\\left(w_{o}, j\\right)}^{\\top} \\boldsymbol{v}_{c}\\right),</script><p>其中如果判断x为真，[[x]] = 1；反之[[x]] = −1。</p>\n<p>例如让我们计算上图中给定词<script type=\"math/tex\">w_c</script>生成词<script type=\"math/tex\">w_3</script>的条件概率：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(w_{3} \\mid w_{c}\\right)=\\sigma\\left(\\boldsymbol{u}_{n\\left(w_{3}, 1\\right)}^{\\top} \\boldsymbol{v}_{c}\\right) \\cdot \\sigma\\left(-\\boldsymbol{u}_{n\\left(w_{3}, 2\\right)}^{\\top} \\boldsymbol{v}_{c}\\right) \\cdot \\sigma\\left(\\boldsymbol{u}_{n\\left(w_{3}, 3\\right)}^{\\top} \\boldsymbol{v}_{c}\\right)</script><ul>\n<li><strong>推导过程：</strong></li>\n</ul>\n<blockquote>\n<p>设<script type=\"math/tex\">d_{2}^{w}, d_{3}^{w}, \\cdots, d_{l w}^{w} \\in\\{0,1\\}</script>为词<script type=\"math/tex\">w</script>的编码，<script type=\"math/tex\">d_j^w</script>代表<script type=\"math/tex\">w</script>路径上的第<script type=\"math/tex\">j</script>个节点对应的编码（根节点无编码），则：</p>\n<script type=\"math/tex; mode=display\">\nP(w_0|w_c) = \\prod_{j=2}^{L(w)} P\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right)</script><p>其中的每一项都是一个Logistic回归：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right) = \\left\\{\\begin{array}{ll}\n\\sigma\\left(v_c^T u_{n_(w, j)}\\right), & d_{j}^{w}=0 \\\\\n1-\\sigma\\left(v_c^T u_{n_(w, j)}\\right), & d_{j}^{w}=1\n\\end{array}\\right.</script><p>可以将两式合并，并且由于<script type=\"math/tex\">\\sigma(x) + \\sigma(-x) = 1</script>，可以将上式转化为：</p>\n<script type=\"math/tex; mode=display\">\nP\\left(d_{j}^{w} \\mid v_c, u_{n_(w, j)}\\right)=\\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]^{1-d_{j}^{w}} \\cdot\\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]^{d^{w}}</script><p>我们取目标函数的对数：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{L} = \\sum_{w \\in \\mathcal{V}}\\log P(Context(w) | w) = \\sum_{w \\in \\mathcal{V}} \\sum_{j=2}^{L(w)}\\left\\{\\left(1-d_{j}^{w}\\right) \\cdot \\log \\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]+d_{j}^{w} \\cdot \\log \\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]\\right\\}</script><p><strong>要最大化上述目标函数是比较困难的，但是我们可以最大化每一子项（即下式），达到局部最大化的效果</strong>：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{L}(w, j) = \\left\\{\\left(1-d_{j}^{w}\\right) \\cdot \\log \\left[\\sigma\\left(v_c^T u_{n_(w, j)}\\right)\\right]+d_{j}^{w} \\cdot \\log \\left[\\sigma\\left(-v_c^T u_{n_(w, j)}\\right)\\right]\\right\\}</script><p><strong>这样就可以将计算复杂度降为<script type=\"math/tex\">O(log_2 |\\mathcal{V}|)</script>，当词典<script type=\"math/tex\">\\mathcal{V}</script>很大时，层序softmax在训练中每⼀步的梯度计算开销相较未使用近似训练时大幅降低。</strong></p>\n<p>显然，上式的<script type=\"math/tex\">d_j^w</script>是用来<strong>判断下一个节点是否是本节点的左孩子的</strong>，其作用是为了满足给定任意中心词<script type=\"math/tex\">w_c</script>，生成背景词的条件概率和为1：</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{w \\in \\mathcal{V}} P\\left(w \\mid w_{c}\\right)=1</script><p>这样才能使似然函数不至于训练到无穷大（和负采样要添加负样本是一样的原因），并且符合概率和为1</p>\n</blockquote>\n<ul>\n<li><strong>层序Softmax中的二叉树是使用的哈夫曼树，可以使搜索次数达到最小</strong></li>\n</ul>\n<h1 id=\"2-子词嵌入（fastText）\"><a href=\"#2-子词嵌入（fastText）\" class=\"headerlink\" title=\"2 子词嵌入（fastText）\"></a>2 子词嵌入（fastText）</h1><ul>\n<li>英语单词通常有其内部结构和形成⽅式。例如，我们可以从“dog” “dogs”和“dogcatcher”的 字面上推测它们的关系。这些词都有同⼀个词根“dog”，但使⽤不同的后缀来改变词的含义。而 且，这个关联可以推⼴⾄其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关 系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。这方面的研究称为构词学</li>\n<li><p>在word2vec中，我们并没有直接利⽤构词学中的信息。⽆论是在跳字模型还是连续词袋模型中， 我们都将形态不同的单词⽤不同的向量来表示。例如，“dog”和“dogs”分别⽤两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌⼊（subword embedding）的⽅法，从而试图将构词信息引⼊word2vec中的跳字模型</p>\n</li>\n<li><p>在fastText中，每个中⼼词被表⽰成子词的集合。举个例子，例如单词”where”，⾸先，我们在单词的<strong>⾸尾分别添加特殊字符“&lt;” 和  “&gt;” 以区分作为前后缀的子词</strong>。然后，<strong>将单词当成⼀个由字符构成的序列来提取n元语法</strong>。例如，当n = 3时，我们得到所有⻓度为3的子词：”<wh\"、\"whe\"、\"her\"、\"ere\"、\"re>“以及特殊子词”where”。</p>\n</li>\n<li>在fastText中，对于⼀个词w，我们将它所有⻓度在3 ∼ 6的子词和特殊子词的并集记为<script type=\"math/tex\">\\mathcal{G}_{w}</script>。<strong>那么词典则是所有词的子词集合的并集</strong>。假设词典中子词<script type=\"math/tex\">g</script>的向量为<script type=\"math/tex\">z_g</script>，那么跳字模型中词w的作为中⼼词的向量<script type=\"math/tex\">v_w</script>则表⽰成：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nv_{w}=\\sum_{g \\in \\mathcal{G}_{w}} z_{g}</script><ul>\n<li>fastText的其余部分同跳字模型⼀致，不在此重复。可以看到，<strong>与跳字模型相⽐，fastText中词典 规模更大，造成模型参数更多</strong>，同时⼀个词的向量需要对所有子词向量求和，继而导致<strong>计算复杂度更高</strong>。但与此同时，<strong>较生僻的复杂单词，甚⾄是词典中没有的单词，可能会从同它结构类似的 其他词那⾥获取更好的词向量表示。</strong></li>\n</ul>\n<h1 id=\"3-全局向量的词嵌入（GloVe）\"><a href=\"#3-全局向量的词嵌入（GloVe）\" class=\"headerlink\" title=\"3 全局向量的词嵌入（GloVe）\"></a>3 全局向量的词嵌入（GloVe）</h1><ul>\n<li>在词嵌入中，交叉熵损失函数有时并不是一个好的选择。一方面，正如上面所说，<strong>会带来整个词典大小的累加项，带来过大的计算开销</strong>。另一方面，<strong>词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。</strong></li>\n<li>GloVe模型既使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。为了做到这一点GloVe模型引入了<strong>共现矩阵（Cooccurrence Probabilities Matrix）</strong></li>\n</ul>\n<h3 id=\"3-1-共现矩阵\"><a href=\"#3-1-共现矩阵\" class=\"headerlink\" title=\"3.1 共现矩阵\"></a>3.1 共现矩阵</h3><ul>\n<li><p>假设：</p>\n<blockquote>\n<ol>\n<li>共现矩阵为<script type=\"math/tex\">X</script>，<script type=\"math/tex\">X</script>中的元素<script type=\"math/tex\">X_{ij}</script>为语料库中<script type=\"math/tex\">word_i</script>上下文中出现<script type=\"math/tex\">word_j</script>的次数</li>\n<li><script type=\"math/tex\">X_i = \\sum_k X_{ik}</script>是出现在<script type=\"math/tex\">word_i</script>上下文中所有词的总次数</li>\n<li><script type=\"math/tex\">P_{ij} = P(j|i) = \\frac{X_{ij}}{X_i}</script>为<script type=\"math/tex\">word_j</script>出现在<script type=\"math/tex\">word_i</script>上下文的概率</li>\n</ol>\n</blockquote>\n</li>\n<li><p>下面我们来举个例子：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235403111.png\" alt=\"image-20220211235403111\" style=\"zoom:80%;\" /></p>\n<p>设Ratio = <script type=\"math/tex\">\\frac{P_{ik}}{P_{jk}}</script>，，从上面的例子中我们可以总结出：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220211235553030.png\" alt=\"image-20220211235553030\"></p>\n<p><strong>所以Ratio可以反应word之间的相关性，而GloVe就是利用了这个值</strong></p>\n<p>假设<script type=\"math/tex\">i, j, k</script>三者的词向量都已经得到<script type=\"math/tex\">w_i, w_j, w_k</script>，那么我们现在就要找一个函数F，使得：</p>\n<script type=\"math/tex; mode=display\">\nF\\left(w_{i}, w_{j}, w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}</script><h3 id=\"3-2-损失函数推导\"><a href=\"#3-2-损失函数推导\" class=\"headerlink\" title=\"3.2 损失函数推导\"></a>3.2 损失函数推导</h3><ul>\n<li><p><strong>上述等式的右边是通过统计得到的已知量，左侧的三个词向量<script type=\"math/tex\">w_i, w_j, w_k</script>是模型要求的量，F是未知的，如果能够将F的形式确定下来， 那么我们就可以通过优化算法求解词向量了，下面是求解过程</strong></p>\n</li>\n<li><p><script type=\"math/tex\">\\frac{P_{i k}}{P_{j k}}</script>考察了<script type=\"math/tex\">i,j,k</script>三个词的相关性，不妨先只考虑<script type=\"math/tex\">i,j</script>两个词的词向量<script type=\"math/tex\">w_i, w_j</script>的相关性，向量相关关系的度量可以用两个向量的差，所以上述等式可以先改为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF\\left((w_{i}-w_{j}), w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}</script><ul>\n<li>由于右边是标量，左边是向量，所以可以联想到使用向量的内积转化为标量：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF\\left(\\left(w_{i}-w_{j}\\right)^{T} w_{k}\\right)=F\\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\\right)=\\frac{P_{i k}}{P_{j k}}</script><ul>\n<li>因为左边是差，右边是商，所以将F设为exp运算，将差转化为商：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\exp \\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\\right)=\\frac{\\exp \\left(w_{i}^{T} w_{k}\\right)}{\\exp \\left(w_{j}^{T} w_{k}\\right)}=\\frac{P_{i k}}{P_{j k}}</script><ul>\n<li>现在只需让分子分母分别相等就能成立：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\exp \\left(w_{i}^{T} w_{k}\\right)= \\alpha P_{i k}, \\exp \\left(w_{j}^{T} w_{k}\\right)= \\alpha P_{j k}，\\quad  \\alpha为常数</script><ul>\n<li>现在只需要在整个词料库中考察<script type=\"math/tex\">\\exp \\left(w_{i}^{T} w_{k}\\right)= \\alpha P_{i k}= \\alpha \\frac{X_{i k}}{X_{i}}</script>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nw_{i}^{T} w_{k}=\\log \\left(\\alpha \\frac{X_{i k}}{X_{i}}\\right)=\\log X_{i k}-\\log X_{i} + log\\alpha</script><ul>\n<li>所以我们需要设置一个偏差项<script type=\"math/tex\">b_i</script>来拟合<script type=\"math/tex\">log X_i - log \\alpha</script>，由于如果上式<script type=\"math/tex\">i,k</script>位置交换，左式的值不变，但是右式的值会变，所以我们还要设置一个偏差项<script type=\"math/tex\">b_k</script>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nw_{i}^{T} w_{k}  + b_i + b_k=log X_{ik}</script><ul>\n<li>上面公式只是理想状态下，实际中只能要求两者接近    从而就有了代价函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ=\\sum_{i, k}\\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\\log X_{i k}\\right)^{2}</script><ul>\n<li>如果两个词共同出现的次数越多，那么其在代价函数中的影响就越大，所以要设计一个权重函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ=\\sum_{i, k} f\\left(X_{i k}\\right)\\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\\log X_{i k}\\right)^{2}</script><blockquote>\n<p>对于函数f要满足一下条件：</p>\n<ol>\n<li>如果两个词没有共同出现过，那么权重就是0，即f(0) = 0</li>\n<li>两个词共同出现次数越多，那么权重就越大，所以f是一个递增函数</li>\n<li>由一些于高频词（<script type=\"math/tex\">X_{ij}较大</script>，如”the”）的权重很大，但是实际上作用并不是很大，所以f(x)对于较大的x不能取太大的值</li>\n</ol>\n<p>综合上方条件，论文提出了一下函数：</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\left\\{\\begin{array}{r}\n\\left(\\frac{x}{x_{\\max }}\\right)^{\\alpha}, \\text { if } x<x_{\\max } \\\\\n1, \\text { otherwise }\n\\end{array}\\right.</script><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212005820238.png\" alt=\"image-20220212005820238\" style=\"zoom:67%;\" /></p>\n<p>作者认为<script type=\"math/tex\">x_{max} = 100, \\alpha = \\frac{3}{4}</script>比较合适</p>\n</blockquote>\n<ul>\n<li>值得强调的是，如果词<script type=\"math/tex\">w_i</script>出现在词<script type=\"math/tex\">w_j</script>的背景窗口里，那么词<script type=\"math/tex\">w_j</script>也会出现在词<script type=\"math/tex\">w_i</script>的背景窗口⾥。也就是说，<script type=\"math/tex\">x_{ij} = x_{ji}</script>。不同于<strong>word2vec中拟合的是非对称的条件概率<script type=\"math/tex\">p_{ij}</script>，GloVe模型拟合的是对称的<script type=\"math/tex\">\\log X _{ij}</script></strong>。因此，<strong>任意词的中心词向量和背景词向量在GloVe模型中是等价的。但由于初始化值的不同，同⼀个词最终学习到的两组词向量可能不同</strong>。当学习得到所有词向量以后，GloVe模型使⽤<strong>中心词向量与背景词向量之和作为该词的最终词向量，这样做是为了提高鲁棒性（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）</strong></li>\n</ul>\n<h1 id=\"4-词嵌入应用\"><a href=\"#4-词嵌入应用\" class=\"headerlink\" title=\"4 词嵌入应用\"></a>4 词嵌入应用</h1><ul>\n<li>下列应用均为在训练好的预训练模型上进行训练的</li>\n</ul>\n<h3 id=\"4-1-求近义词和类比词\"><a href=\"#4-1-求近义词和类比词\" class=\"headerlink\" title=\"4.1 求近义词和类比词\"></a>4.1 求近义词和类比词</h3><ul>\n<li>求近义词可以运用词向量之间的余弦相似度</li>\n<li>求类比词，例如，“man”（男人）: “woman”（女人）:: “son”（儿子）: “daughter”（女儿）是⼀个类⽐例子。<strong>对于类比关系中的4个词 a : b :: c : d，给定前3个词a、b和c，求d。设词w的词向量为vec(w)。求类比词的思路是vec(a) - vec(b) = vec(c) - vec(d)，来求出vec(d)的近似值，然后寻找和其最相似的几个词向量</strong></li>\n</ul>\n<h3 id=\"4-2-使用循环神经网络进行文本情感分类\"><a href=\"#4-2-使用循环神经网络进行文本情感分类\" class=\"headerlink\" title=\"4.2 使用循环神经网络进行文本情感分类\"></a>4.2 使用循环神经网络进行文本情感分类</h3><ul>\n<li>⽂本分类是⾃然语⾔处理的⼀个常⻅任务，它把⼀段不定⻓的⽂本序列变换为⽂本的类别。本节关 注它的⼀个子问题：使⽤⽂本情感分类来分析⽂本作者的情绪。这个问题也叫<strong>情感分析（sentiment analysis）</strong></li>\n<li>本节我们使用电影评论的情感分析（二分类，positive和negtive的情感），在数据处理的时候，一条评论为一个样本，我们要先对评论进行分词（一般都是<strong>基于空格分词</strong>），并且由于每条评论的长度不一样，无法组成小批量，所以我们需要通过<strong>截断或者补充来使长度定长</strong></li>\n<li>对于模型的设计，先需要一个<strong>嵌入层将文本转化为词向量</strong>，再通过<strong>双向循环网络对特征序列进⼀步编码得到序列信息</strong>，然后将<strong>最初时间步和最终时间步的隐藏状态连结</strong>，再传入全连接层输出</li>\n<li>注意由于是预训练的模型，所以<strong>嵌入层的模型参数是不需要更新的</strong>，直接由训练好的模型参数导入即可。但是<strong>导入的模型的词向量维度要和预先设置的嵌入层词向量维度一致</strong></li>\n</ul>\n<h3 id=\"4-3-使用卷积神经网络（textCNN）进行文本情感分类：\"><a href=\"#4-3-使用卷积神经网络（textCNN）进行文本情感分类：\" class=\"headerlink\" title=\"4.3 使用卷积神经网络（textCNN）进行文本情感分类：\"></a>4.3 使用卷积神经网络（textCNN）进行文本情感分类：</h3><ul>\n<li><p>我们可以将⽂本数据看作只有⼀个维度的时间序列，并很⾃然地使⽤循环神经⽹络来表征这样的数据。其实，我们也可以将⽂本当作⼀维图像，从而可以⽤⼀维卷积神经⽹络来捕捉临近词之间的关联</p>\n</li>\n<li><p><strong>textCNN基于的假设是：一个词和其相邻的词（即卷积核中的词）是相关的</strong></p>\n</li>\n</ul>\n<h5 id=\"4-3-1-一维卷积层\"><a href=\"#4-3-1-一维卷积层\" class=\"headerlink\" title=\"4.3.1 一维卷积层\"></a>4.3.1 一维卷积层</h5><ul>\n<li>和二维卷积层一样，一维卷积层卷积窗口从输⼊数组的最左⽅开始，按从左往右的顺序，依次在输⼊数组上滑动，举个例子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155418239.png\" alt=\"image-20220212155418239\" style=\"zoom:67%;\" /></p>\n<p>输出中的2是由输入的前两个和核逐个相乘得到的：0x1+1x2=2，然后窗口往后滑动一格，以此类推</p>\n<p>对于多通道操作也是和二维卷积层一样的：<br><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212155747478.png\" alt=\"image-20220212155747478\"></p>\n<p>每个通道分别操作，然后各通道结果相加。如果想要输出多通道，则核还要增加一个维度</p>\n<h5 id=\"4-3-2-时序最大池化层\"><a href=\"#4-3-2-时序最大池化层\" class=\"headerlink\" title=\"4.3.2 时序最大池化层\"></a>4.3.2 时序最大池化层</h5><ul>\n<li>假设输⼊包含多个通道，各通道由不同时间步上的数值组成，<strong>各通道的输出即该通道所有时间步中最⼤的数值</strong>。因此，<strong>时序最⼤池化层的输⼊在各个通道上的时间步数可以不同</strong></li>\n<li>为提升计算性能，我们常常将不同⻓度的时序样本组成⼀个小批量，<strong>并通过在较短序列后附加特殊字符（如0）令批量中各时序样本⻓度相同</strong>。这些⼈为添加的特殊字符当然是⽆意义的。由于时序最⼤池化的主要⽬的是抓取时序中最重要的特征，它通常能<strong>使模型不受⼈为添加字符的影响</strong></li>\n</ul>\n<h5 id=\"4-3-3-textCNN模型\"><a href=\"#4-3-3-textCNN模型\" class=\"headerlink\" title=\"4.3.3 textCNN模型\"></a>4.3.3 textCNN模型</h5><ul>\n<li>textCNN模型主要使⽤了⼀维卷积层和时序最⼤池化层。假设输⼊的⽂本序列由n个词组成，每 个词⽤d维的词向量表⽰。那么输⼊样本的宽为n，⾼为1，输⼊通道数为d。textCNN的计算主要分为以下⼏步：</li>\n</ul>\n<blockquote>\n<ol>\n<li>定义多个⼀维卷积核，并使⽤这些卷积核对输⼊分别做卷积计算。宽度不同的卷积核可能 会捕捉到不同个数的相邻词的相关性</li>\n<li>对输出的所有通道分别做时序最⼤池化，再将这些通道的池化输出值连结为向量。</li>\n<li>通过全连接层将连结后的向量变换为有关各类别的输出。这⼀步可以使⽤丢弃层应对过拟合。</li>\n</ol>\n</blockquote>\n<ul>\n<li>举个例子，这⾥的输⼊是⼀个有11个词的句子，每个词⽤6维词向量表⽰。因此输⼊序列的宽为11，输⼊通道数为6。给定2个⼀维卷积核，核宽分别为2和4，输出 通道数分别设为4和5</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220212161751535.png\" alt=\"image-20220212161751535\" style=\"zoom: 67%;\" /></p>\n<p><strong>尽管每个通道的宽不同，我们依然可以对各个通道做时序最⼤池化， 并将9个通道的池化输出连结成⼀个9维向量</strong>。最终，使⽤全连接将9维向量变换为2维输出，即正⾯情感和负⾯情感的预测</p>\n<h1 id=\"5-机器翻译\"><a href=\"#5-机器翻译\" class=\"headerlink\" title=\"5 机器翻译\"></a>5 机器翻译</h1><ul>\n<li><p>我们使用seq2seq模型进行机器翻译，原理具体可看<a href=\"https://zlkqz.top/2021/12/10/RNN/#7-Seq2Seq%E6%A8%A1%E5%9E%8B\">seq2seq</a></p>\n</li>\n<li><p>这里我们只介绍一下如何评价翻译结果。评价机器翻译结果通常使⽤<strong>BLEU（Bilingual Evaluation Understudy）</strong>。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。</p>\n</li>\n<li>具体来说，设词数为n的⼦序列的精度为<script type=\"math/tex\">p_n</script>。它是<strong>预测序列与标签序列匹配词数为n的⼦序列的数量与预测序列中词数为n的⼦序列的数量之⽐</strong>。举个例子，假设标签序列为A、B、C、D、E、F， 预测序列为A、B、B、C、D，那么<script type=\"math/tex\">p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0</script>。设<script type=\"math/tex\">len_{label}</script>和<script type=\"math/tex\">len_{pred}</script>分 别为标签序列和预测序列的词数，那么，BLEU的定义为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\exp \\left(\\min \\left(0,1-\\frac{\\text { len }_{\\text {label }}}{\\text { len }_{\\text {pred }}}\\right)\\right) \\prod_{n=1}^{k} p_{n}^{1 / 2^{n}}</script><p>其中k是我们希望匹配的⼦序列的最⼤词数。可以看到当预测序列和标签序列完全⼀致时， BLEU为1。</p>\n<ul>\n<li>设计思想：</li>\n</ul>\n<blockquote>\n<p>因为匹配较⻓⼦序列⽐匹配较短⼦序列更难，BLEU对匹配较⻓⼦序列的精度赋予了更⼤权重，具体是在上式的<script type=\"math/tex\">p_n^{1/2^n}</script>中，当序列较长，即n较大时，由于<script type=\"math/tex\">p_n \\in [0, 1]</script>，所以<script type=\"math/tex\">1/2^n</script>后会变大，并且n越大越接近1，即给较长序列更大的权重</p>\n<p>并且由于较短序列一般会有比较大的<script type=\"math/tex\">p_n</script>，所以连乘项前面的系数是为了惩罚较短的输出而设置的。举个例子，当<script type=\"math/tex\">len_{pred}</script>较小时，连乘项前面的系数就会小于1，以此来减少较短序列的权重。</p>\n</blockquote>\n"},{"title":"SVM总结","math":true,"date":"2022-04-05T16:00:00.000Z","_content":"\n\n\n- SVM和决策树一样，同样是一种判别式模型， 都是基于条件概率分布进行建模，要在样本空间中找到一个划分超平面，将不同类别的样本分开，如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923114248027.png\" alt=\"image-20220923114248027\" style=\"zoom:67%;\" />\n\n存在多个超平面可将训练样本分开，但是我们是希望找到对于分类结果**最鲁棒的超平面**，也就是图中加粗的那条线，这个超平面对训练样本局部扰动的“容忍性”最好\n\n\n\n# 1 基本概念\n\n- 在样本空间中，一个超平面可以通过以下线性方程来描述：\n\n$$\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b=0\n$$\n\n其中$$w = (w_1, ..., w_d)$$为法向量，确定超平面的方向，b为位移项，确定超平面与原点之间的距离。\n\n- 那么样本空间中任意点$$x$$到超平面$$(w, b)$$的距离为：\n\n$$\nr=\\frac{\\left|\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\\right|}{\\|\\boldsymbol{w}\\|}\n$$\n\n- 若超平面$$(w, b)$$能将样本正确分类，则对于任意$$(x_i, y_i) \\in D$$，若$$y = +1$$则$$w^Tx_i + b > 0$$，若$$y = -1$$，则$$w^Tx_i + b < 0$$，那么令：\n\n$$\n\\left\\{\\begin{array}{ll}\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\geqslant+1, & y_{i}=+1 \\\\\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\leqslant-1, & y_{i}=-1\n\\end{array}\\right.\n$$\n\n另上式等号成立的样本点成为**支持向量**，两个异类支持向量到超平面的距离之和为：\n$$\n\\gamma = \\frac{2}{||w||}\n$$\n称之为**间隔**，如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923120639393.png\" alt=\"image-20220923120639393\" style=\"zoom:75%;\" />\n\n> 可以发现在上式中，$$w^Tx_i + b$$是采用了大于或小于$$\\pm 1$$而不是0或其他数字进行划分。**首先不能使用0，因为若为0，则支持向量会恰好落在划分超平面上。而使用$$\\pm1$$只是因为方便计算（若为其他非0数是一样的效果，因为$w$和$b$可以进行放缩）**\n\n- 欲找到一个具有**最大间隔**的划分超平面，也就是找到能满足约束的参数$w$和$b$，使得$\\gamma$最大，即：\n\n$$\n\\begin{aligned}\n\\max _{\\boldsymbol{w}, b} & \\frac{2}{\\|\\boldsymbol{w}\\|} \\\\\n\\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1, \\quad i=1,2, \\ldots, m .\n\\end{aligned}\n$$\n\n- 因为最大化$$||w||^{-1}$$等价于最小化$$||w||^2$$，所以我们一般写为：\n\n$$\n\\begin{aligned}\n\\min _{\\boldsymbol{w}, b} & \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2} \\\\\n\\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1, \\quad i=1,2, \\ldots, m .\n\\end{aligned}\n$$\n\n这就是SVM的基本型\n\n\n\n\n\n# 2 对偶问题\n\n### 2.1 KKT条件\n\n- 考虑一个有m个等式约束和n个不等式约束的优化问题：\n\n$$\n\\begin{array}{ll}\n\\min _{\\boldsymbol{x}} & f(\\boldsymbol{x}) \\\\\n\\text { s.t. } & h_{i}(\\boldsymbol{x})=0 \\quad(i=1, \\ldots, m) \\\\\n& g_{j}(\\boldsymbol{x}) \\leqslant 0 \\quad(j=1, \\ldots, n)\n\\end{array}\n$$\n\n- 引入拉格朗日乘子$$\\pmb{\\lambda} = (\\lambda_1, ..., \\lambda_m)$$和$$\\pmb{\\mu} = (\\mu_1, ..., \\mu_n)$$，则相应的拉格朗日函数为：\n\n$$\nL(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})=f(\\boldsymbol{x})+\\sum_{i=1}^{m} \\lambda_{i} h_{i}(\\boldsymbol{x})+\\sum_{j=1}^{n} \\mu_{j} g_{j}(\\boldsymbol{x})\n$$\n\n则由不等式约束引入的KKT条件（j = 1, .., n）为：\n$$\n\\left\\{\\begin{array}{l}\ng_{j}(\\boldsymbol{x}) \\leqslant 0 \\\\\n\\mu_{j} \\geqslant 0 \\\\\n\\mu_{j} g_{j}(\\boldsymbol{x})=0\n\\end{array}\\right.\n$$\n\n\n\n### 2.2 原问题转化到对偶问题\n\n- SVM没有使用原问题而使用对偶问题是因为：**对偶函数更易于求解，并且对偶函数是一个光滑的凸函数，可以找到全局最优解，**具体解释可看[例子](https://www.zhihu.com/question/36694952)\n\n- 先写出上述问题的拉格朗日函数，即**原问题**：\n\n$$\n\\mathcal{L}(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{n} \\alpha_{i}\\left(y_{i}\\left(w^{T} x_{i}+b\\right)-1\\right)\n$$\n\n其中的$$\\alpha_i$$为拉格朗日乘子\n\n- 易知：**当有一个约束函数不满足时，L的最大值为$$\\infty$$（只需令其对应的$$\\alpha_i$$为$$\\infty$$即可）；当所有约束条件都满足时，L的最大值为$$\\frac{1}{2}||w||^2$$（只需令所有$$\\alpha_i$$为0）**。所有原问题等价于：\n\n$$\n\\min _{\\boldsymbol{w}, b} \\frac{1}{2}||w||^2 = \\min _{w, b} \\theta(w)=\\min _{w, b} \\max _{\\alpha_{i} \\geq 0} \\mathcal{L}(w, b, \\alpha)=p^{*}\n$$\n\n- 由于这个的求解问题不好做，**因此一般我们将最小和最大的位置交换一下（需满足KKT条件）**：\n\n$$\n\\max _{\\alpha_{i} \\geq 0} \\min _{w, b} \\mathcal{L}(w, b, \\alpha)=d^{*}\n$$\n\n- 接下来就先对w，b求极小，再对$$\\alpha$$求极大：\n\n> 1. 首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：\n>\n> $$\n> \\begin{array}{l}\n> \\frac{\\partial L}{\\partial w}=0 \\Rightarrow w=\\sum_{i=1}^{n} \\alpha_{i} y_{i} x_{i} \\\\\n> \\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\n> \\end{array}\n> $$\n>\n> 2. 将上述结果代入L：\n>\n> $$\n> \\begin{aligned}\n> \\mathcal{L}(w, b, \\alpha) &=\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-\\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-b \\sum_{i=1}^{n} \\alpha_{i} y_{i}+\\sum_{i=1}^{n} \\alpha_{i} \\\\\n> &=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\n> \\end{aligned}\n> $$\n\n- 这样就得到了原问题的**对偶问题**：\n\n$$\n\\begin{array}{ll}\n\\max _{\\alpha} & \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j} \\\\\n\\text { s.t. } & \\alpha_{i} \\geq 0, i=1, \\ldots, n \\\\\n& \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\n\\end{array}\n$$\n\n然后再对$$\\alpha$$求解**（采用SMO算法）**，即可得到模型：\n$$\nf(x) = w^Tx + b \\\\  = \\sum_{i=1}^m{\\alpha_iy_ix_i^Tx + b}\n$$\n\n> 由于满足KKT条件，所有对于任意样本$$(x_i, y_i) \\in D$$，总有$$\\alpha_i=0$$或$$y_if(x_i) = 1$$。若$$\\alpha_i=0$$，则样本将不会出现在上述的模型式子中，就不会对$$f(x)$$产生影响；若$$\\alpha_i > 0$$，则对应样本为支持向量。所以：**最终模型仅与支持向量有关，大部分训练样本都无需保留**\n\n\n\n### 2.3 SMO算法\n\n- 在对偶问题中如果要对$$\\alpha$$求解，这是一个二次规划问题，可使用通用的方法求解，**但是该问题的规模正比于训练样本数，将会有很大的开销**，所以提出了SMO（Sequential Minimal Optimization）等更高效的算法\n- SMO算法之所以高效，是因为其**每次只更新两个参数，而固定其他参数**，具体来说，考虑更新$$\\alpha_i$$和$$\\alpha_j$$，而固定其他参数，由于存在约束$$\\sum_{i=1}^m{\\alpha_iy_i} = 0$$，所以：\n\n$$\n\\alpha_iy_i + \\alpha_jy_j = c, \\alpha_i \\geq 0,\\alpha_j \\geq 0  \\\\ c = -\\sum_{k \\neq i, j}\\alpha_ky_k\n$$\n\nc为一个已知的常数\n\n- 则可以通过上式，消去$\\alpha_j$，从而得到一个关于$\\alpha_i$的单变量二次规划问题，仅有的约束是$\\alpha_i \\geq 0$，这样即可高效地更新$\\alpha_i$，然后通过约束再得到更新后的$\\alpha_j$\n\n- 重复上述过程，每次只更新两个变量，直到收敛。但是每次按一定的规则选择两个变量进行更新：\n\n> 因为选择的$\\alpha_i, \\alpha_j$只要有一个不满足KKT条件（一开始是随机初始化的），目标函数就会在迭代后增大，并且直观上来看，KKT条件违背的程度越大，则更新后获得的收益就越大。所以**SMO先选取先选取一个违背KKT条件程度最大的变量**，第二个变量应该选择使目标函数增长最快的变量，但是找出这个变量过于复杂，所以采用一个启发式：**使选取的两个变量所对应的样本之间的间隔最大**。直观解释为：差别大的两个变量的更新能给目标函数带来更大的增益\n\n- 除了$\\alpha$变量的更新，还需要确定偏移项b。对于任意支持向量$$(x_s, y_s)$$，都有$$y_sf(x_s) = 1$$，即：\n\n$$\ny_{s}\\left(\\sum_{i \\in S} \\alpha_{i} y_{i} \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{s}+b\\right)=1\n$$\n\n其中S为所有支持向量的下标集。理论上，采用任意一个支持向量都可以得到b的值，但是SMO采用更鲁棒的做法：**使用所有支持向量求解的平均值：**\n$$\nb=\\frac{1}{|S|} \\sum_{s \\in S}\\left(y_{s}-\\sum_{i \\in S} \\alpha_{i} y_{i} \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{s}\\right)\n$$\n\n\n\n\n\n# 3 核函数\n\n- 前面的讨论中，我们是假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类。然而在现实任务中，原始样本空间也许并不存在一个能正确划分两类样本的超平面。对于这样的问题，**可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间中线性可分**，如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923160040076.png\" alt=\"image-20220923160040076\" style=\"zoom:80%;\" />\n\n- 令$$\\phi(x)$$表示将x映射后的特征向量，于是在特征空间中的超平面对应的模型为：\n\n$$\nf(x) = w^T\\phi(x) + b\n$$\n\n- 然后像之前一样进行对偶问题的最优化，但是其中有一个内积项$$\\phi(x_i)^T\\phi(x_j)$$，这是样本$x_i$核$x_j$映射到特征空间后的内积。由于特征空间维度可能很高，甚至可能是无穷维，直接计算此内积项通常比较困难，所以提出了**核函数**：\n\n$$\n\\mathcal{k}(x_i, x_j) = <\\phi(x_i), \\phi(x_j)> = \\phi(x_i)^T\\phi(x_j)\n$$\n\n**即$x_i$和$x_j$在特征空间的内积可以通过核函数在原始样本空间中的结果得出**\n\n- 因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，**核函数需要满足以下这个必要条件**：\n\n  ![26.png](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccc468c.png)\n\n  由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：\n\n  ![27.png](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccc541a.png)\n\n\n\n\n\n# 4 软间隔\n\n- 前面讨论的情况是假定样本在样本空间或特征空间线性可分。然而在限时任务中往往很难确定合适的核函数，退一步讲，即使找到了核函数，也无法确定这个线性可分的结果是否是由于过拟合造成的。例如数据中有噪声的情况，噪声数据（outlier）本就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，**当加入这些outlier后导致划分超平面被挤歪了**，如下图所示，对支持向量机的泛化性能造成很大的影响：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccce68e.png\" alt=\"28.png\" style=\"zoom:67%;\" />\n\n可以看到如果不要outlier，能分出一个间隔更大的划分超平面\n\n- 缓解这个问题的一个办法是允许SVM在一些样本上出错。前面所述的SVM要在所有样本上都划分正确，这成为**硬间隔（hard margin）**。而**软间隔则是允许某些样本不满足约束 $$y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1$$，但是不满足该约束的样本要尽可能少**，于是优化目标可以写为：\n\n$$\n\\min _{\\boldsymbol{w}, b} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\ell_{0 / 1}\\left(y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)-1\\right)\n$$\n\n其中C是一个常数，$$\\ell_{0/1}(z)$$是0/1损失函数，即$$z < 0$$的时候取1，其他时候取0。**当C为无穷大的时候，则迫使所有样本满足约束，退化为硬间隔。当C为有限值的时候，允许一些样本不满足约束**\n\n- 由于$$\\ell_{0/1}$$非凸非连续，数学性质不太好，所以可以用以下三种替代损失函数：\n\n> - hinge损失：$$\\ell_{\\text {hinge }}(z)=\\max (0,1-z)$$\n> - 指数损失（exponential loss）：$$\\ell_{\\exp }(z)=\\exp (-z) $$\n> - 对率损失（logistic loss）：$$\\ell_{\\log }(z)=\\log (1+\\exp (-z))$$\n\n- 常用hinge损失进行替代，然后将连加中的每一项换为松弛变量（slack variables）$\\xi_{i} \\ge 0$，则优化目标重写为：\n\n$$\n\\min _{\\boldsymbol{w}, b, \\xi_{i}} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i}\n$$\n\n每个变量都对应一个松弛变量，代表**样本不满足约束的程度**\n\n- 上述问题仍是一个二次规划问题，按照和前面一样的方法进行求解，先写出拉格朗日函数：\n\n$$\n\\begin{aligned}\nL(\\boldsymbol{w}, b, \\boldsymbol{\\alpha}, \\boldsymbol{\\xi}, \\boldsymbol{\\mu})=& \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i} \\\\\n&+\\sum_{i=1}^{m} \\alpha_{i}\\left(1-\\xi_{i}-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\right)-\\sum_{i=1}^{m} \\mu_{i} \\xi_{i}\n\\end{aligned}\n$$\n\n其中$\\alpha_i \\ge 0$、$\\mu_i \\ge 0$是拉格朗日乘子，据此求解即可\n\n\n\n\n\n","source":"_posts/SVM.md","raw":"---\ntitle: SVM总结\nmath: true\ndate: 2022-4-6\n---\n\n\n\n- SVM和决策树一样，同样是一种判别式模型， 都是基于条件概率分布进行建模，要在样本空间中找到一个划分超平面，将不同类别的样本分开，如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923114248027.png\" alt=\"image-20220923114248027\" style=\"zoom:67%;\" />\n\n存在多个超平面可将训练样本分开，但是我们是希望找到对于分类结果**最鲁棒的超平面**，也就是图中加粗的那条线，这个超平面对训练样本局部扰动的“容忍性”最好\n\n\n\n# 1 基本概念\n\n- 在样本空间中，一个超平面可以通过以下线性方程来描述：\n\n$$\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b=0\n$$\n\n其中$$w = (w_1, ..., w_d)$$为法向量，确定超平面的方向，b为位移项，确定超平面与原点之间的距离。\n\n- 那么样本空间中任意点$$x$$到超平面$$(w, b)$$的距离为：\n\n$$\nr=\\frac{\\left|\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\\right|}{\\|\\boldsymbol{w}\\|}\n$$\n\n- 若超平面$$(w, b)$$能将样本正确分类，则对于任意$$(x_i, y_i) \\in D$$，若$$y = +1$$则$$w^Tx_i + b > 0$$，若$$y = -1$$，则$$w^Tx_i + b < 0$$，那么令：\n\n$$\n\\left\\{\\begin{array}{ll}\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\geqslant+1, & y_{i}=+1 \\\\\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\leqslant-1, & y_{i}=-1\n\\end{array}\\right.\n$$\n\n另上式等号成立的样本点成为**支持向量**，两个异类支持向量到超平面的距离之和为：\n$$\n\\gamma = \\frac{2}{||w||}\n$$\n称之为**间隔**，如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923120639393.png\" alt=\"image-20220923120639393\" style=\"zoom:75%;\" />\n\n> 可以发现在上式中，$$w^Tx_i + b$$是采用了大于或小于$$\\pm 1$$而不是0或其他数字进行划分。**首先不能使用0，因为若为0，则支持向量会恰好落在划分超平面上。而使用$$\\pm1$$只是因为方便计算（若为其他非0数是一样的效果，因为$w$和$b$可以进行放缩）**\n\n- 欲找到一个具有**最大间隔**的划分超平面，也就是找到能满足约束的参数$w$和$b$，使得$\\gamma$最大，即：\n\n$$\n\\begin{aligned}\n\\max _{\\boldsymbol{w}, b} & \\frac{2}{\\|\\boldsymbol{w}\\|} \\\\\n\\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1, \\quad i=1,2, \\ldots, m .\n\\end{aligned}\n$$\n\n- 因为最大化$$||w||^{-1}$$等价于最小化$$||w||^2$$，所以我们一般写为：\n\n$$\n\\begin{aligned}\n\\min _{\\boldsymbol{w}, b} & \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2} \\\\\n\\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1, \\quad i=1,2, \\ldots, m .\n\\end{aligned}\n$$\n\n这就是SVM的基本型\n\n\n\n\n\n# 2 对偶问题\n\n### 2.1 KKT条件\n\n- 考虑一个有m个等式约束和n个不等式约束的优化问题：\n\n$$\n\\begin{array}{ll}\n\\min _{\\boldsymbol{x}} & f(\\boldsymbol{x}) \\\\\n\\text { s.t. } & h_{i}(\\boldsymbol{x})=0 \\quad(i=1, \\ldots, m) \\\\\n& g_{j}(\\boldsymbol{x}) \\leqslant 0 \\quad(j=1, \\ldots, n)\n\\end{array}\n$$\n\n- 引入拉格朗日乘子$$\\pmb{\\lambda} = (\\lambda_1, ..., \\lambda_m)$$和$$\\pmb{\\mu} = (\\mu_1, ..., \\mu_n)$$，则相应的拉格朗日函数为：\n\n$$\nL(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})=f(\\boldsymbol{x})+\\sum_{i=1}^{m} \\lambda_{i} h_{i}(\\boldsymbol{x})+\\sum_{j=1}^{n} \\mu_{j} g_{j}(\\boldsymbol{x})\n$$\n\n则由不等式约束引入的KKT条件（j = 1, .., n）为：\n$$\n\\left\\{\\begin{array}{l}\ng_{j}(\\boldsymbol{x}) \\leqslant 0 \\\\\n\\mu_{j} \\geqslant 0 \\\\\n\\mu_{j} g_{j}(\\boldsymbol{x})=0\n\\end{array}\\right.\n$$\n\n\n\n### 2.2 原问题转化到对偶问题\n\n- SVM没有使用原问题而使用对偶问题是因为：**对偶函数更易于求解，并且对偶函数是一个光滑的凸函数，可以找到全局最优解，**具体解释可看[例子](https://www.zhihu.com/question/36694952)\n\n- 先写出上述问题的拉格朗日函数，即**原问题**：\n\n$$\n\\mathcal{L}(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{n} \\alpha_{i}\\left(y_{i}\\left(w^{T} x_{i}+b\\right)-1\\right)\n$$\n\n其中的$$\\alpha_i$$为拉格朗日乘子\n\n- 易知：**当有一个约束函数不满足时，L的最大值为$$\\infty$$（只需令其对应的$$\\alpha_i$$为$$\\infty$$即可）；当所有约束条件都满足时，L的最大值为$$\\frac{1}{2}||w||^2$$（只需令所有$$\\alpha_i$$为0）**。所有原问题等价于：\n\n$$\n\\min _{\\boldsymbol{w}, b} \\frac{1}{2}||w||^2 = \\min _{w, b} \\theta(w)=\\min _{w, b} \\max _{\\alpha_{i} \\geq 0} \\mathcal{L}(w, b, \\alpha)=p^{*}\n$$\n\n- 由于这个的求解问题不好做，**因此一般我们将最小和最大的位置交换一下（需满足KKT条件）**：\n\n$$\n\\max _{\\alpha_{i} \\geq 0} \\min _{w, b} \\mathcal{L}(w, b, \\alpha)=d^{*}\n$$\n\n- 接下来就先对w，b求极小，再对$$\\alpha$$求极大：\n\n> 1. 首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：\n>\n> $$\n> \\begin{array}{l}\n> \\frac{\\partial L}{\\partial w}=0 \\Rightarrow w=\\sum_{i=1}^{n} \\alpha_{i} y_{i} x_{i} \\\\\n> \\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\n> \\end{array}\n> $$\n>\n> 2. 将上述结果代入L：\n>\n> $$\n> \\begin{aligned}\n> \\mathcal{L}(w, b, \\alpha) &=\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-\\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-b \\sum_{i=1}^{n} \\alpha_{i} y_{i}+\\sum_{i=1}^{n} \\alpha_{i} \\\\\n> &=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\n> \\end{aligned}\n> $$\n\n- 这样就得到了原问题的**对偶问题**：\n\n$$\n\\begin{array}{ll}\n\\max _{\\alpha} & \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j} \\\\\n\\text { s.t. } & \\alpha_{i} \\geq 0, i=1, \\ldots, n \\\\\n& \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\n\\end{array}\n$$\n\n然后再对$$\\alpha$$求解**（采用SMO算法）**，即可得到模型：\n$$\nf(x) = w^Tx + b \\\\  = \\sum_{i=1}^m{\\alpha_iy_ix_i^Tx + b}\n$$\n\n> 由于满足KKT条件，所有对于任意样本$$(x_i, y_i) \\in D$$，总有$$\\alpha_i=0$$或$$y_if(x_i) = 1$$。若$$\\alpha_i=0$$，则样本将不会出现在上述的模型式子中，就不会对$$f(x)$$产生影响；若$$\\alpha_i > 0$$，则对应样本为支持向量。所以：**最终模型仅与支持向量有关，大部分训练样本都无需保留**\n\n\n\n### 2.3 SMO算法\n\n- 在对偶问题中如果要对$$\\alpha$$求解，这是一个二次规划问题，可使用通用的方法求解，**但是该问题的规模正比于训练样本数，将会有很大的开销**，所以提出了SMO（Sequential Minimal Optimization）等更高效的算法\n- SMO算法之所以高效，是因为其**每次只更新两个参数，而固定其他参数**，具体来说，考虑更新$$\\alpha_i$$和$$\\alpha_j$$，而固定其他参数，由于存在约束$$\\sum_{i=1}^m{\\alpha_iy_i} = 0$$，所以：\n\n$$\n\\alpha_iy_i + \\alpha_jy_j = c, \\alpha_i \\geq 0,\\alpha_j \\geq 0  \\\\ c = -\\sum_{k \\neq i, j}\\alpha_ky_k\n$$\n\nc为一个已知的常数\n\n- 则可以通过上式，消去$\\alpha_j$，从而得到一个关于$\\alpha_i$的单变量二次规划问题，仅有的约束是$\\alpha_i \\geq 0$，这样即可高效地更新$\\alpha_i$，然后通过约束再得到更新后的$\\alpha_j$\n\n- 重复上述过程，每次只更新两个变量，直到收敛。但是每次按一定的规则选择两个变量进行更新：\n\n> 因为选择的$\\alpha_i, \\alpha_j$只要有一个不满足KKT条件（一开始是随机初始化的），目标函数就会在迭代后增大，并且直观上来看，KKT条件违背的程度越大，则更新后获得的收益就越大。所以**SMO先选取先选取一个违背KKT条件程度最大的变量**，第二个变量应该选择使目标函数增长最快的变量，但是找出这个变量过于复杂，所以采用一个启发式：**使选取的两个变量所对应的样本之间的间隔最大**。直观解释为：差别大的两个变量的更新能给目标函数带来更大的增益\n\n- 除了$\\alpha$变量的更新，还需要确定偏移项b。对于任意支持向量$$(x_s, y_s)$$，都有$$y_sf(x_s) = 1$$，即：\n\n$$\ny_{s}\\left(\\sum_{i \\in S} \\alpha_{i} y_{i} \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{s}+b\\right)=1\n$$\n\n其中S为所有支持向量的下标集。理论上，采用任意一个支持向量都可以得到b的值，但是SMO采用更鲁棒的做法：**使用所有支持向量求解的平均值：**\n$$\nb=\\frac{1}{|S|} \\sum_{s \\in S}\\left(y_{s}-\\sum_{i \\in S} \\alpha_{i} y_{i} \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{s}\\right)\n$$\n\n\n\n\n\n# 3 核函数\n\n- 前面的讨论中，我们是假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类。然而在现实任务中，原始样本空间也许并不存在一个能正确划分两类样本的超平面。对于这样的问题，**可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间中线性可分**，如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923160040076.png\" alt=\"image-20220923160040076\" style=\"zoom:80%;\" />\n\n- 令$$\\phi(x)$$表示将x映射后的特征向量，于是在特征空间中的超平面对应的模型为：\n\n$$\nf(x) = w^T\\phi(x) + b\n$$\n\n- 然后像之前一样进行对偶问题的最优化，但是其中有一个内积项$$\\phi(x_i)^T\\phi(x_j)$$，这是样本$x_i$核$x_j$映射到特征空间后的内积。由于特征空间维度可能很高，甚至可能是无穷维，直接计算此内积项通常比较困难，所以提出了**核函数**：\n\n$$\n\\mathcal{k}(x_i, x_j) = <\\phi(x_i), \\phi(x_j)> = \\phi(x_i)^T\\phi(x_j)\n$$\n\n**即$x_i$和$x_j$在特征空间的内积可以通过核函数在原始样本空间中的结果得出**\n\n- 因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，**核函数需要满足以下这个必要条件**：\n\n  ![26.png](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccc468c.png)\n\n  由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：\n\n  ![27.png](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccc541a.png)\n\n\n\n\n\n# 4 软间隔\n\n- 前面讨论的情况是假定样本在样本空间或特征空间线性可分。然而在限时任务中往往很难确定合适的核函数，退一步讲，即使找到了核函数，也无法确定这个线性可分的结果是否是由于过拟合造成的。例如数据中有噪声的情况，噪声数据（outlier）本就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，**当加入这些outlier后导致划分超平面被挤歪了**，如下图所示，对支持向量机的泛化性能造成很大的影响：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccce68e.png\" alt=\"28.png\" style=\"zoom:67%;\" />\n\n可以看到如果不要outlier，能分出一个间隔更大的划分超平面\n\n- 缓解这个问题的一个办法是允许SVM在一些样本上出错。前面所述的SVM要在所有样本上都划分正确，这成为**硬间隔（hard margin）**。而**软间隔则是允许某些样本不满足约束 $$y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1$$，但是不满足该约束的样本要尽可能少**，于是优化目标可以写为：\n\n$$\n\\min _{\\boldsymbol{w}, b} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\ell_{0 / 1}\\left(y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)-1\\right)\n$$\n\n其中C是一个常数，$$\\ell_{0/1}(z)$$是0/1损失函数，即$$z < 0$$的时候取1，其他时候取0。**当C为无穷大的时候，则迫使所有样本满足约束，退化为硬间隔。当C为有限值的时候，允许一些样本不满足约束**\n\n- 由于$$\\ell_{0/1}$$非凸非连续，数学性质不太好，所以可以用以下三种替代损失函数：\n\n> - hinge损失：$$\\ell_{\\text {hinge }}(z)=\\max (0,1-z)$$\n> - 指数损失（exponential loss）：$$\\ell_{\\exp }(z)=\\exp (-z) $$\n> - 对率损失（logistic loss）：$$\\ell_{\\log }(z)=\\log (1+\\exp (-z))$$\n\n- 常用hinge损失进行替代，然后将连加中的每一项换为松弛变量（slack variables）$\\xi_{i} \\ge 0$，则优化目标重写为：\n\n$$\n\\min _{\\boldsymbol{w}, b, \\xi_{i}} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i}\n$$\n\n每个变量都对应一个松弛变量，代表**样本不满足约束的程度**\n\n- 上述问题仍是一个二次规划问题，按照和前面一样的方法进行求解，先写出拉格朗日函数：\n\n$$\n\\begin{aligned}\nL(\\boldsymbol{w}, b, \\boldsymbol{\\alpha}, \\boldsymbol{\\xi}, \\boldsymbol{\\mu})=& \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i} \\\\\n&+\\sum_{i=1}^{m} \\alpha_{i}\\left(1-\\xi_{i}-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\right)-\\sum_{i=1}^{m} \\mu_{i} \\xi_{i}\n\\end{aligned}\n$$\n\n其中$\\alpha_i \\ge 0$、$\\mu_i \\ge 0$是拉格朗日乘子，据此求解即可\n\n\n\n\n\n","slug":"SVM","published":1,"updated":"2022-12-20T06:20:04.493Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1g000b7cszb78xdz1k","content":"<ul>\n<li>SVM和决策树一样，同样是一种判别式模型， 都是基于条件概率分布进行建模，要在样本空间中找到一个划分超平面，将不同类别的样本分开，如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923114248027.png\" alt=\"image-20220923114248027\" style=\"zoom:67%;\" /></p>\n<p>存在多个超平面可将训练样本分开，但是我们是希望找到对于分类结果<strong>最鲁棒的超平面</strong>，也就是图中加粗的那条线，这个超平面对训练样本局部扰动的“容忍性”最好</p>\n<h1 id=\"1-基本概念\"><a href=\"#1-基本概念\" class=\"headerlink\" title=\"1 基本概念\"></a>1 基本概念</h1><ul>\n<li>在样本空间中，一个超平面可以通过以下线性方程来描述：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b=0</script><p>其中<script type=\"math/tex\">w = (w_1, ..., w_d)</script>为法向量，确定超平面的方向，b为位移项，确定超平面与原点之间的距离。</p>\n<ul>\n<li>那么样本空间中任意点<script type=\"math/tex\">x</script>到超平面<script type=\"math/tex\">(w, b)</script>的距离为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nr=\\frac{\\left|\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\\right|}{\\|\\boldsymbol{w}\\|}</script><ul>\n<li>若超平面<script type=\"math/tex\">(w, b)</script>能将样本正确分类，则对于任意<script type=\"math/tex\">(x_i, y_i) \\in D</script>，若<script type=\"math/tex\">y = +1</script>则<script type=\"math/tex\">w^Tx_i + b > 0</script>，若<script type=\"math/tex\">y = -1</script>，则<script type=\"math/tex\">w^Tx_i + b < 0</script>，那么令：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{array}{ll}\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\geqslant+1, & y_{i}=+1 \\\\\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\leqslant-1, & y_{i}=-1\n\\end{array}\\right.</script><p>另上式等号成立的样本点成为<strong>支持向量</strong>，两个异类支持向量到超平面的距离之和为：</p>\n<script type=\"math/tex; mode=display\">\n\\gamma = \\frac{2}{||w||}</script><p>称之为<strong>间隔</strong>，如下图：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923120639393.png\" alt=\"image-20220923120639393\" style=\"zoom:75%;\" /></p>\n<blockquote>\n<p>可以发现在上式中，<script type=\"math/tex\">w^Tx_i + b</script>是采用了大于或小于<script type=\"math/tex\">\\pm 1</script>而不是0或其他数字进行划分。<strong>首先不能使用0，因为若为0，则支持向量会恰好落在划分超平面上。而使用<script type=\"math/tex\">\\pm1</script>只是因为方便计算（若为其他非0数是一样的效果，因为$w$和$b$可以进行放缩）</strong></p>\n</blockquote>\n<ul>\n<li>欲找到一个具有<strong>最大间隔</strong>的划分超平面，也就是找到能满足约束的参数$w$和$b$，使得$\\gamma$最大，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\max _{\\boldsymbol{w}, b} & \\frac{2}{\\|\\boldsymbol{w}\\|} \\\\\n\\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1, \\quad i=1,2, \\ldots, m .\n\\end{aligned}</script><ul>\n<li>因为最大化<script type=\"math/tex\">||w||^{-1}</script>等价于最小化<script type=\"math/tex\">||w||^2</script>，所以我们一般写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\min _{\\boldsymbol{w}, b} & \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2} \\\\\n\\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1, \\quad i=1,2, \\ldots, m .\n\\end{aligned}</script><p>这就是SVM的基本型</p>\n<h1 id=\"2-对偶问题\"><a href=\"#2-对偶问题\" class=\"headerlink\" title=\"2 对偶问题\"></a>2 对偶问题</h1><h3 id=\"2-1-KKT条件\"><a href=\"#2-1-KKT条件\" class=\"headerlink\" title=\"2.1 KKT条件\"></a>2.1 KKT条件</h3><ul>\n<li>考虑一个有m个等式约束和n个不等式约束的优化问题：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{ll}\n\\min _{\\boldsymbol{x}} & f(\\boldsymbol{x}) \\\\\n\\text { s.t. } & h_{i}(\\boldsymbol{x})=0 \\quad(i=1, \\ldots, m) \\\\\n& g_{j}(\\boldsymbol{x}) \\leqslant 0 \\quad(j=1, \\ldots, n)\n\\end{array}</script><ul>\n<li>引入拉格朗日乘子<script type=\"math/tex\">\\pmb{\\lambda} = (\\lambda_1, ..., \\lambda_m)</script>和<script type=\"math/tex\">\\pmb{\\mu} = (\\mu_1, ..., \\mu_n)</script>，则相应的拉格朗日函数为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})=f(\\boldsymbol{x})+\\sum_{i=1}^{m} \\lambda_{i} h_{i}(\\boldsymbol{x})+\\sum_{j=1}^{n} \\mu_{j} g_{j}(\\boldsymbol{x})</script><p>则由不等式约束引入的KKT条件（j = 1, .., n）为：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{array}{l}\ng_{j}(\\boldsymbol{x}) \\leqslant 0 \\\\\n\\mu_{j} \\geqslant 0 \\\\\n\\mu_{j} g_{j}(\\boldsymbol{x})=0\n\\end{array}\\right.</script><h3 id=\"2-2-原问题转化到对偶问题\"><a href=\"#2-2-原问题转化到对偶问题\" class=\"headerlink\" title=\"2.2 原问题转化到对偶问题\"></a>2.2 原问题转化到对偶问题</h3><ul>\n<li><p>SVM没有使用原问题而使用对偶问题是因为：<strong>对偶函数更易于求解，并且对偶函数是一个光滑的凸函数，可以找到全局最优解，</strong>具体解释可看<a href=\"https://www.zhihu.com/question/36694952\">例子</a></p>\n</li>\n<li><p>先写出上述问题的拉格朗日函数，即<strong>原问题</strong>：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\mathcal{L}(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{n} \\alpha_{i}\\left(y_{i}\\left(w^{T} x_{i}+b\\right)-1\\right)</script><p>其中的<script type=\"math/tex\">\\alpha_i</script>为拉格朗日乘子</p>\n<ul>\n<li>易知：<strong>当有一个约束函数不满足时，L的最大值为<script type=\"math/tex\">\\infty</script>（只需令其对应的<script type=\"math/tex\">\\alpha_i</script>为<script type=\"math/tex\">\\infty</script>即可）；当所有约束条件都满足时，L的最大值为<script type=\"math/tex\">\\frac{1}{2}||w||^2</script>（只需令所有<script type=\"math/tex\">\\alpha_i</script>为0）</strong>。所有原问题等价于：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{\\boldsymbol{w}, b} \\frac{1}{2}||w||^2 = \\min _{w, b} \\theta(w)=\\min _{w, b} \\max _{\\alpha_{i} \\geq 0} \\mathcal{L}(w, b, \\alpha)=p^{*}</script><ul>\n<li>由于这个的求解问题不好做，<strong>因此一般我们将最小和最大的位置交换一下（需满足KKT条件）</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\max _{\\alpha_{i} \\geq 0} \\min _{w, b} \\mathcal{L}(w, b, \\alpha)=d^{*}</script><ul>\n<li>接下来就先对w，b求极小，再对<script type=\"math/tex\">\\alpha</script>求极大：</li>\n</ul>\n<blockquote>\n<ol>\n<li>首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w=\\sum_{i=1}^{n} \\alpha_{i} y_{i} x_{i} \\\\\n\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\n\\end{array}</script><ol>\n<li>将上述结果代入L：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\mathcal{L}(w, b, \\alpha) &=\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-\\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-b \\sum_{i=1}^{n} \\alpha_{i} y_{i}+\\sum_{i=1}^{n} \\alpha_{i} \\\\\n&=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\n\\end{aligned}</script></blockquote>\n<ul>\n<li>这样就得到了原问题的<strong>对偶问题</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{ll}\n\\max _{\\alpha} & \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j} \\\\\n\\text { s.t. } & \\alpha_{i} \\geq 0, i=1, \\ldots, n \\\\\n& \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\n\\end{array}</script><p>然后再对<script type=\"math/tex\">\\alpha</script>求解<strong>（采用SMO算法）</strong>，即可得到模型：</p>\n<script type=\"math/tex; mode=display\">\nf(x) = w^Tx + b \\\\  = \\sum_{i=1}^m{\\alpha_iy_ix_i^Tx + b}</script><blockquote>\n<p>由于满足KKT条件，所有对于任意样本<script type=\"math/tex\">(x_i, y_i) \\in D</script>，总有<script type=\"math/tex\">\\alpha_i=0</script>或<script type=\"math/tex\">y_if(x_i) = 1</script>。若<script type=\"math/tex\">\\alpha_i=0</script>，则样本将不会出现在上述的模型式子中，就不会对<script type=\"math/tex\">f(x)</script>产生影响；若<script type=\"math/tex\">\\alpha_i > 0</script>，则对应样本为支持向量。所以：<strong>最终模型仅与支持向量有关，大部分训练样本都无需保留</strong></p>\n</blockquote>\n<h3 id=\"2-3-SMO算法\"><a href=\"#2-3-SMO算法\" class=\"headerlink\" title=\"2.3 SMO算法\"></a>2.3 SMO算法</h3><ul>\n<li>在对偶问题中如果要对<script type=\"math/tex\">\\alpha</script>求解，这是一个二次规划问题，可使用通用的方法求解，<strong>但是该问题的规模正比于训练样本数，将会有很大的开销</strong>，所以提出了SMO（Sequential Minimal Optimization）等更高效的算法</li>\n<li>SMO算法之所以高效，是因为其<strong>每次只更新两个参数，而固定其他参数</strong>，具体来说，考虑更新<script type=\"math/tex\">\\alpha_i</script>和<script type=\"math/tex\">\\alpha_j</script>，而固定其他参数，由于存在约束<script type=\"math/tex\">\\sum_{i=1}^m{\\alpha_iy_i} = 0</script>，所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\alpha_iy_i + \\alpha_jy_j = c, \\alpha_i \\geq 0,\\alpha_j \\geq 0  \\\\ c = -\\sum_{k \\neq i, j}\\alpha_ky_k</script><p>c为一个已知的常数</p>\n<ul>\n<li><p>则可以通过上式，消去$\\alpha_j$，从而得到一个关于$\\alpha_i$的单变量二次规划问题，仅有的约束是$\\alpha_i \\geq 0$，这样即可高效地更新$\\alpha_i$，然后通过约束再得到更新后的$\\alpha_j$</p>\n</li>\n<li><p>重复上述过程，每次只更新两个变量，直到收敛。但是每次按一定的规则选择两个变量进行更新：</p>\n</li>\n</ul>\n<blockquote>\n<p>因为选择的$\\alpha_i, \\alpha_j$只要有一个不满足KKT条件（一开始是随机初始化的），目标函数就会在迭代后增大，并且直观上来看，KKT条件违背的程度越大，则更新后获得的收益就越大。所以<strong>SMO先选取先选取一个违背KKT条件程度最大的变量</strong>，第二个变量应该选择使目标函数增长最快的变量，但是找出这个变量过于复杂，所以采用一个启发式：<strong>使选取的两个变量所对应的样本之间的间隔最大</strong>。直观解释为：差别大的两个变量的更新能给目标函数带来更大的增益</p>\n</blockquote>\n<ul>\n<li>除了$\\alpha$变量的更新，还需要确定偏移项b。对于任意支持向量<script type=\"math/tex\">(x_s, y_s)</script>，都有<script type=\"math/tex\">y_sf(x_s) = 1</script>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ny_{s}\\left(\\sum_{i \\in S} \\alpha_{i} y_{i} \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{s}+b\\right)=1</script><p>其中S为所有支持向量的下标集。理论上，采用任意一个支持向量都可以得到b的值，但是SMO采用更鲁棒的做法：<strong>使用所有支持向量求解的平均值：</strong></p>\n<script type=\"math/tex; mode=display\">\nb=\\frac{1}{|S|} \\sum_{s \\in S}\\left(y_{s}-\\sum_{i \\in S} \\alpha_{i} y_{i} \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{s}\\right)</script><h1 id=\"3-核函数\"><a href=\"#3-核函数\" class=\"headerlink\" title=\"3 核函数\"></a>3 核函数</h1><ul>\n<li>前面的讨论中，我们是假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类。然而在现实任务中，原始样本空间也许并不存在一个能正确划分两类样本的超平面。对于这样的问题，<strong>可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间中线性可分</strong>，如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923160040076.png\" alt=\"image-20220923160040076\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>令<script type=\"math/tex\">\\phi(x)</script>表示将x映射后的特征向量，于是在特征空间中的超平面对应的模型为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x) = w^T\\phi(x) + b</script><ul>\n<li>然后像之前一样进行对偶问题的最优化，但是其中有一个内积项<script type=\"math/tex\">\\phi(x_i)^T\\phi(x_j)</script>，这是样本$x_i$核$x_j$映射到特征空间后的内积。由于特征空间维度可能很高，甚至可能是无穷维，直接计算此内积项通常比较困难，所以提出了<strong>核函数</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\mathcal{k}(x_i, x_j) = <\\phi(x_i), \\phi(x_j)> = \\phi(x_i)^T\\phi(x_j)</script><p><strong>即$x_i$和$x_j$在特征空间的内积可以通过核函数在原始样本空间中的结果得出</strong></p>\n<ul>\n<li><p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，<strong>核函数需要满足以下这个必要条件</strong>：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccc468c.png\" alt=\"26.png\"></p>\n<p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccc541a.png\" alt=\"27.png\"></p>\n</li>\n</ul>\n<h1 id=\"4-软间隔\"><a href=\"#4-软间隔\" class=\"headerlink\" title=\"4 软间隔\"></a>4 软间隔</h1><ul>\n<li>前面讨论的情况是假定样本在样本空间或特征空间线性可分。然而在限时任务中往往很难确定合适的核函数，退一步讲，即使找到了核函数，也无法确定这个线性可分的结果是否是由于过拟合造成的。例如数据中有噪声的情况，噪声数据（outlier）本就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，<strong>当加入这些outlier后导致划分超平面被挤歪了</strong>，如下图所示，对支持向量机的泛化性能造成很大的影响：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccce68e.png\" alt=\"28.png\" style=\"zoom:67%;\" /></p>\n<p>可以看到如果不要outlier，能分出一个间隔更大的划分超平面</p>\n<ul>\n<li>缓解这个问题的一个办法是允许SVM在一些样本上出错。前面所述的SVM要在所有样本上都划分正确，这成为<strong>硬间隔（hard margin）</strong>。而<strong>软间隔则是允许某些样本不满足约束 <script type=\"math/tex\">y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1</script>，但是不满足该约束的样本要尽可能少</strong>，于是优化目标可以写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{\\boldsymbol{w}, b} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\ell_{0 / 1}\\left(y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)-1\\right)</script><p>其中C是一个常数，<script type=\"math/tex\">\\ell_{0/1}(z)</script>是0/1损失函数，即<script type=\"math/tex\">z < 0</script>的时候取1，其他时候取0。<strong>当C为无穷大的时候，则迫使所有样本满足约束，退化为硬间隔。当C为有限值的时候，允许一些样本不满足约束</strong></p>\n<ul>\n<li>由于<script type=\"math/tex\">\\ell_{0/1}</script>非凸非连续，数学性质不太好，所以可以用以下三种替代损失函数：</li>\n</ul>\n<blockquote>\n<ul>\n<li>hinge损失：<script type=\"math/tex\">\\ell_{\\text {hinge }}(z)=\\max (0,1-z)</script></li>\n<li>指数损失（exponential loss）：<script type=\"math/tex\">\\ell_{\\exp }(z)=\\exp (-z)</script></li>\n<li>对率损失（logistic loss）：<script type=\"math/tex\">\\ell_{\\log }(z)=\\log (1+\\exp (-z))</script></li>\n</ul>\n</blockquote>\n<ul>\n<li>常用hinge损失进行替代，然后将连加中的每一项换为松弛变量（slack variables）$\\xi_{i} \\ge 0$，则优化目标重写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{\\boldsymbol{w}, b, \\xi_{i}} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i}</script><p>每个变量都对应一个松弛变量，代表<strong>样本不满足约束的程度</strong></p>\n<ul>\n<li>上述问题仍是一个二次规划问题，按照和前面一样的方法进行求解，先写出拉格朗日函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nL(\\boldsymbol{w}, b, \\boldsymbol{\\alpha}, \\boldsymbol{\\xi}, \\boldsymbol{\\mu})=& \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i} \\\\\n&+\\sum_{i=1}^{m} \\alpha_{i}\\left(1-\\xi_{i}-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\right)-\\sum_{i=1}^{m} \\mu_{i} \\xi_{i}\n\\end{aligned}</script><p>其中$\\alpha_i \\ge 0$、$\\mu_i \\ge 0$是拉格朗日乘子，据此求解即可</p>\n","site":{"data":{}},"wordcount":6571,"excerpt":"","more":"<ul>\n<li>SVM和决策树一样，同样是一种判别式模型， 都是基于条件概率分布进行建模，要在样本空间中找到一个划分超平面，将不同类别的样本分开，如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923114248027.png\" alt=\"image-20220923114248027\" style=\"zoom:67%;\" /></p>\n<p>存在多个超平面可将训练样本分开，但是我们是希望找到对于分类结果<strong>最鲁棒的超平面</strong>，也就是图中加粗的那条线，这个超平面对训练样本局部扰动的“容忍性”最好</p>\n<h1 id=\"1-基本概念\"><a href=\"#1-基本概念\" class=\"headerlink\" title=\"1 基本概念\"></a>1 基本概念</h1><ul>\n<li>在样本空间中，一个超平面可以通过以下线性方程来描述：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b=0</script><p>其中<script type=\"math/tex\">w = (w_1, ..., w_d)</script>为法向量，确定超平面的方向，b为位移项，确定超平面与原点之间的距离。</p>\n<ul>\n<li>那么样本空间中任意点<script type=\"math/tex\">x</script>到超平面<script type=\"math/tex\">(w, b)</script>的距离为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nr=\\frac{\\left|\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\\right|}{\\|\\boldsymbol{w}\\|}</script><ul>\n<li>若超平面<script type=\"math/tex\">(w, b)</script>能将样本正确分类，则对于任意<script type=\"math/tex\">(x_i, y_i) \\in D</script>，若<script type=\"math/tex\">y = +1</script>则<script type=\"math/tex\">w^Tx_i + b > 0</script>，若<script type=\"math/tex\">y = -1</script>，则<script type=\"math/tex\">w^Tx_i + b < 0</script>，那么令：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{array}{ll}\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\geqslant+1, & y_{i}=+1 \\\\\n\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\leqslant-1, & y_{i}=-1\n\\end{array}\\right.</script><p>另上式等号成立的样本点成为<strong>支持向量</strong>，两个异类支持向量到超平面的距离之和为：</p>\n<script type=\"math/tex; mode=display\">\n\\gamma = \\frac{2}{||w||}</script><p>称之为<strong>间隔</strong>，如下图：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923120639393.png\" alt=\"image-20220923120639393\" style=\"zoom:75%;\" /></p>\n<blockquote>\n<p>可以发现在上式中，<script type=\"math/tex\">w^Tx_i + b</script>是采用了大于或小于<script type=\"math/tex\">\\pm 1</script>而不是0或其他数字进行划分。<strong>首先不能使用0，因为若为0，则支持向量会恰好落在划分超平面上。而使用<script type=\"math/tex\">\\pm1</script>只是因为方便计算（若为其他非0数是一样的效果，因为$w$和$b$可以进行放缩）</strong></p>\n</blockquote>\n<ul>\n<li>欲找到一个具有<strong>最大间隔</strong>的划分超平面，也就是找到能满足约束的参数$w$和$b$，使得$\\gamma$最大，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\max _{\\boldsymbol{w}, b} & \\frac{2}{\\|\\boldsymbol{w}\\|} \\\\\n\\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1, \\quad i=1,2, \\ldots, m .\n\\end{aligned}</script><ul>\n<li>因为最大化<script type=\"math/tex\">||w||^{-1}</script>等价于最小化<script type=\"math/tex\">||w||^2</script>，所以我们一般写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\min _{\\boldsymbol{w}, b} & \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2} \\\\\n\\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1, \\quad i=1,2, \\ldots, m .\n\\end{aligned}</script><p>这就是SVM的基本型</p>\n<h1 id=\"2-对偶问题\"><a href=\"#2-对偶问题\" class=\"headerlink\" title=\"2 对偶问题\"></a>2 对偶问题</h1><h3 id=\"2-1-KKT条件\"><a href=\"#2-1-KKT条件\" class=\"headerlink\" title=\"2.1 KKT条件\"></a>2.1 KKT条件</h3><ul>\n<li>考虑一个有m个等式约束和n个不等式约束的优化问题：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{ll}\n\\min _{\\boldsymbol{x}} & f(\\boldsymbol{x}) \\\\\n\\text { s.t. } & h_{i}(\\boldsymbol{x})=0 \\quad(i=1, \\ldots, m) \\\\\n& g_{j}(\\boldsymbol{x}) \\leqslant 0 \\quad(j=1, \\ldots, n)\n\\end{array}</script><ul>\n<li>引入拉格朗日乘子<script type=\"math/tex\">\\pmb{\\lambda} = (\\lambda_1, ..., \\lambda_m)</script>和<script type=\"math/tex\">\\pmb{\\mu} = (\\mu_1, ..., \\mu_n)</script>，则相应的拉格朗日函数为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})=f(\\boldsymbol{x})+\\sum_{i=1}^{m} \\lambda_{i} h_{i}(\\boldsymbol{x})+\\sum_{j=1}^{n} \\mu_{j} g_{j}(\\boldsymbol{x})</script><p>则由不等式约束引入的KKT条件（j = 1, .., n）为：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{array}{l}\ng_{j}(\\boldsymbol{x}) \\leqslant 0 \\\\\n\\mu_{j} \\geqslant 0 \\\\\n\\mu_{j} g_{j}(\\boldsymbol{x})=0\n\\end{array}\\right.</script><h3 id=\"2-2-原问题转化到对偶问题\"><a href=\"#2-2-原问题转化到对偶问题\" class=\"headerlink\" title=\"2.2 原问题转化到对偶问题\"></a>2.2 原问题转化到对偶问题</h3><ul>\n<li><p>SVM没有使用原问题而使用对偶问题是因为：<strong>对偶函数更易于求解，并且对偶函数是一个光滑的凸函数，可以找到全局最优解，</strong>具体解释可看<a href=\"https://www.zhihu.com/question/36694952\">例子</a></p>\n</li>\n<li><p>先写出上述问题的拉格朗日函数，即<strong>原问题</strong>：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\mathcal{L}(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{n} \\alpha_{i}\\left(y_{i}\\left(w^{T} x_{i}+b\\right)-1\\right)</script><p>其中的<script type=\"math/tex\">\\alpha_i</script>为拉格朗日乘子</p>\n<ul>\n<li>易知：<strong>当有一个约束函数不满足时，L的最大值为<script type=\"math/tex\">\\infty</script>（只需令其对应的<script type=\"math/tex\">\\alpha_i</script>为<script type=\"math/tex\">\\infty</script>即可）；当所有约束条件都满足时，L的最大值为<script type=\"math/tex\">\\frac{1}{2}||w||^2</script>（只需令所有<script type=\"math/tex\">\\alpha_i</script>为0）</strong>。所有原问题等价于：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{\\boldsymbol{w}, b} \\frac{1}{2}||w||^2 = \\min _{w, b} \\theta(w)=\\min _{w, b} \\max _{\\alpha_{i} \\geq 0} \\mathcal{L}(w, b, \\alpha)=p^{*}</script><ul>\n<li>由于这个的求解问题不好做，<strong>因此一般我们将最小和最大的位置交换一下（需满足KKT条件）</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\max _{\\alpha_{i} \\geq 0} \\min _{w, b} \\mathcal{L}(w, b, \\alpha)=d^{*}</script><ul>\n<li>接下来就先对w，b求极小，再对<script type=\"math/tex\">\\alpha</script>求极大：</li>\n</ul>\n<blockquote>\n<ol>\n<li>首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w=\\sum_{i=1}^{n} \\alpha_{i} y_{i} x_{i} \\\\\n\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\n\\end{array}</script><ol>\n<li>将上述结果代入L：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\mathcal{L}(w, b, \\alpha) &=\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-\\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-b \\sum_{i=1}^{n} \\alpha_{i} y_{i}+\\sum_{i=1}^{n} \\alpha_{i} \\\\\n&=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\n\\end{aligned}</script></blockquote>\n<ul>\n<li>这样就得到了原问题的<strong>对偶问题</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{ll}\n\\max _{\\alpha} & \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j} \\\\\n\\text { s.t. } & \\alpha_{i} \\geq 0, i=1, \\ldots, n \\\\\n& \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\n\\end{array}</script><p>然后再对<script type=\"math/tex\">\\alpha</script>求解<strong>（采用SMO算法）</strong>，即可得到模型：</p>\n<script type=\"math/tex; mode=display\">\nf(x) = w^Tx + b \\\\  = \\sum_{i=1}^m{\\alpha_iy_ix_i^Tx + b}</script><blockquote>\n<p>由于满足KKT条件，所有对于任意样本<script type=\"math/tex\">(x_i, y_i) \\in D</script>，总有<script type=\"math/tex\">\\alpha_i=0</script>或<script type=\"math/tex\">y_if(x_i) = 1</script>。若<script type=\"math/tex\">\\alpha_i=0</script>，则样本将不会出现在上述的模型式子中，就不会对<script type=\"math/tex\">f(x)</script>产生影响；若<script type=\"math/tex\">\\alpha_i > 0</script>，则对应样本为支持向量。所以：<strong>最终模型仅与支持向量有关，大部分训练样本都无需保留</strong></p>\n</blockquote>\n<h3 id=\"2-3-SMO算法\"><a href=\"#2-3-SMO算法\" class=\"headerlink\" title=\"2.3 SMO算法\"></a>2.3 SMO算法</h3><ul>\n<li>在对偶问题中如果要对<script type=\"math/tex\">\\alpha</script>求解，这是一个二次规划问题，可使用通用的方法求解，<strong>但是该问题的规模正比于训练样本数，将会有很大的开销</strong>，所以提出了SMO（Sequential Minimal Optimization）等更高效的算法</li>\n<li>SMO算法之所以高效，是因为其<strong>每次只更新两个参数，而固定其他参数</strong>，具体来说，考虑更新<script type=\"math/tex\">\\alpha_i</script>和<script type=\"math/tex\">\\alpha_j</script>，而固定其他参数，由于存在约束<script type=\"math/tex\">\\sum_{i=1}^m{\\alpha_iy_i} = 0</script>，所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\alpha_iy_i + \\alpha_jy_j = c, \\alpha_i \\geq 0,\\alpha_j \\geq 0  \\\\ c = -\\sum_{k \\neq i, j}\\alpha_ky_k</script><p>c为一个已知的常数</p>\n<ul>\n<li><p>则可以通过上式，消去$\\alpha_j$，从而得到一个关于$\\alpha_i$的单变量二次规划问题，仅有的约束是$\\alpha_i \\geq 0$，这样即可高效地更新$\\alpha_i$，然后通过约束再得到更新后的$\\alpha_j$</p>\n</li>\n<li><p>重复上述过程，每次只更新两个变量，直到收敛。但是每次按一定的规则选择两个变量进行更新：</p>\n</li>\n</ul>\n<blockquote>\n<p>因为选择的$\\alpha_i, \\alpha_j$只要有一个不满足KKT条件（一开始是随机初始化的），目标函数就会在迭代后增大，并且直观上来看，KKT条件违背的程度越大，则更新后获得的收益就越大。所以<strong>SMO先选取先选取一个违背KKT条件程度最大的变量</strong>，第二个变量应该选择使目标函数增长最快的变量，但是找出这个变量过于复杂，所以采用一个启发式：<strong>使选取的两个变量所对应的样本之间的间隔最大</strong>。直观解释为：差别大的两个变量的更新能给目标函数带来更大的增益</p>\n</blockquote>\n<ul>\n<li>除了$\\alpha$变量的更新，还需要确定偏移项b。对于任意支持向量<script type=\"math/tex\">(x_s, y_s)</script>，都有<script type=\"math/tex\">y_sf(x_s) = 1</script>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ny_{s}\\left(\\sum_{i \\in S} \\alpha_{i} y_{i} \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{s}+b\\right)=1</script><p>其中S为所有支持向量的下标集。理论上，采用任意一个支持向量都可以得到b的值，但是SMO采用更鲁棒的做法：<strong>使用所有支持向量求解的平均值：</strong></p>\n<script type=\"math/tex; mode=display\">\nb=\\frac{1}{|S|} \\sum_{s \\in S}\\left(y_{s}-\\sum_{i \\in S} \\alpha_{i} y_{i} \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{s}\\right)</script><h1 id=\"3-核函数\"><a href=\"#3-核函数\" class=\"headerlink\" title=\"3 核函数\"></a>3 核函数</h1><ul>\n<li>前面的讨论中，我们是假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类。然而在现实任务中，原始样本空间也许并不存在一个能正确划分两类样本的超平面。对于这样的问题，<strong>可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间中线性可分</strong>，如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220923160040076.png\" alt=\"image-20220923160040076\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>令<script type=\"math/tex\">\\phi(x)</script>表示将x映射后的特征向量，于是在特征空间中的超平面对应的模型为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x) = w^T\\phi(x) + b</script><ul>\n<li>然后像之前一样进行对偶问题的最优化，但是其中有一个内积项<script type=\"math/tex\">\\phi(x_i)^T\\phi(x_j)</script>，这是样本$x_i$核$x_j$映射到特征空间后的内积。由于特征空间维度可能很高，甚至可能是无穷维，直接计算此内积项通常比较困难，所以提出了<strong>核函数</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\mathcal{k}(x_i, x_j) = <\\phi(x_i), \\phi(x_j)> = \\phi(x_i)^T\\phi(x_j)</script><p><strong>即$x_i$和$x_j$在特征空间的内积可以通过核函数在原始样本空间中的结果得出</strong></p>\n<ul>\n<li><p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，<strong>核函数需要满足以下这个必要条件</strong>：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccc468c.png\" alt=\"26.png\"></p>\n<p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccc541a.png\" alt=\"27.png\"></p>\n</li>\n</ul>\n<h1 id=\"4-软间隔\"><a href=\"#4-软间隔\" class=\"headerlink\" title=\"4 软间隔\"></a>4 软间隔</h1><ul>\n<li>前面讨论的情况是假定样本在样本空间或特征空间线性可分。然而在限时任务中往往很难确定合适的核函数，退一步讲，即使找到了核函数，也无法确定这个线性可分的结果是否是由于过拟合造成的。例如数据中有噪声的情况，噪声数据（outlier）本就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，<strong>当加入这些outlier后导致划分超平面被挤歪了</strong>，如下图所示，对支持向量机的泛化性能造成很大的影响：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc730ccce68e.png\" alt=\"28.png\" style=\"zoom:67%;\" /></p>\n<p>可以看到如果不要outlier，能分出一个间隔更大的划分超平面</p>\n<ul>\n<li>缓解这个问题的一个办法是允许SVM在一些样本上出错。前面所述的SVM要在所有样本上都划分正确，这成为<strong>硬间隔（hard margin）</strong>。而<strong>软间隔则是允许某些样本不满足约束 <script type=\"math/tex\">y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1</script>，但是不满足该约束的样本要尽可能少</strong>，于是优化目标可以写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{\\boldsymbol{w}, b} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\ell_{0 / 1}\\left(y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)-1\\right)</script><p>其中C是一个常数，<script type=\"math/tex\">\\ell_{0/1}(z)</script>是0/1损失函数，即<script type=\"math/tex\">z < 0</script>的时候取1，其他时候取0。<strong>当C为无穷大的时候，则迫使所有样本满足约束，退化为硬间隔。当C为有限值的时候，允许一些样本不满足约束</strong></p>\n<ul>\n<li>由于<script type=\"math/tex\">\\ell_{0/1}</script>非凸非连续，数学性质不太好，所以可以用以下三种替代损失函数：</li>\n</ul>\n<blockquote>\n<ul>\n<li>hinge损失：<script type=\"math/tex\">\\ell_{\\text {hinge }}(z)=\\max (0,1-z)</script></li>\n<li>指数损失（exponential loss）：<script type=\"math/tex\">\\ell_{\\exp }(z)=\\exp (-z)</script></li>\n<li>对率损失（logistic loss）：<script type=\"math/tex\">\\ell_{\\log }(z)=\\log (1+\\exp (-z))</script></li>\n</ul>\n</blockquote>\n<ul>\n<li>常用hinge损失进行替代，然后将连加中的每一项换为松弛变量（slack variables）$\\xi_{i} \\ge 0$，则优化目标重写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{\\boldsymbol{w}, b, \\xi_{i}} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i}</script><p>每个变量都对应一个松弛变量，代表<strong>样本不满足约束的程度</strong></p>\n<ul>\n<li>上述问题仍是一个二次规划问题，按照和前面一样的方法进行求解，先写出拉格朗日函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nL(\\boldsymbol{w}, b, \\boldsymbol{\\alpha}, \\boldsymbol{\\xi}, \\boldsymbol{\\mu})=& \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i} \\\\\n&+\\sum_{i=1}^{m} \\alpha_{i}\\left(1-\\xi_{i}-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\right)-\\sum_{i=1}^{m} \\mu_{i} \\xi_{i}\n\\end{aligned}</script><p>其中$\\alpha_i \\ge 0$、$\\mu_i \\ge 0$是拉格朗日乘子，据此求解即可</p>\n"},{"title":"SimCSE总结","math":true,"date":"2022-10-20T16:00:00.000Z","_content":"\n\n\n\n\n- 作者提出了一种对比学习的方法，分为**有监督和无监督两种**。其中**只用dropout作为噪音，也可以当作一种数据增强，可以改善语义空间，提升其同向异性，使向量空间更为均匀，并且在有监督方法时，还能对齐正样本**\n\n\n\n# 1 评估标准\n\n- 本篇文章的目的是改善embedding，并且作者在后面的实验发现，进行了SimCSE后有部分下游任务上的表现甚至出现了下降，但是这并不影响SimCSE的作用。**句子嵌入的主要目标是对语义相似的句子进行聚类，所以为了更加综合的评估实验结果，肯定不能使用某个下游任务的实验结果**，作者采用了另一篇论文中一种评估embedding质量的方法：**采用语义相关的正样本之间的对齐（alignment）和整个表示空间的一致性（uniformity）来衡量学习嵌入的质量**\n\n> We takes **alignment** between semantically-related positive pairs and **uniformity** of the whole representation space to measure the quality of learned embeddings.  \n\n- 总的来说，对比学习所做的任务就是：**拉近正样本的距离，剩余的随机样本应该均匀分布在一个超平面上（也就是减少其各向异性）**，所以对比学习的任务就变为了降低以下两个指标：\n\n$$\n\\ell_{\\text {align }} \\triangleq \\underset{\\left(x, x^{+}\\right) \\sim p_{\\text {pos }}}{\\mathbb{E}}\\left\\|f(x)-f\\left(x^{+}\\right)\\right\\|^{2}, \\\\\n\\ell_{\\text {uniform }} \\triangleq log \\underset{\\left(x, y\\right) \\sim p_{\\text {data }}}{\\mathbb{E}}e^{-2\\left\\|f(x)-f\\left(y\\right)\\right\\|^{2}}\n$$\n\n其中$$p_{pos}$$为正样本对，$$p_{data}$$为所有数据对，$$f(x)$$为输入$$x$$经过encoder的输出\n\n- 并且作者还发现无监督的SimCSE能够向量空间的均匀性，并且并不会降低正样本之间的对齐。然后对于有监督，作者指出NLI任务最为适合训练出好的sentence embedding，并且有监督能够进一步提升正样本之间的对齐\n\n- 本文还多次使用了STS-B数据集，这是一个五分类任务的数据集，旨在判定两个句子的相关程度，分为了5个等级，并且得分采用斯皮尔曼等级相关系数\n\n\n\n\n\n# 2 无监督SimCSE\n\n### 2.1 基本方法\n\n- 方法非常简单，就是将同一个输入，分别经过两次encoder，encoder中的dropout**（dropout率仍为默认的0.1）**作为一种微小的数据增强，会使得两次的输出有些许不同。这两次的输出，就作为一对正样本，然后使用以下loss：\n\n$$\n\\ell_{i}=-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}^{z_{i}}, \\mathbf{h}_{i}^{z_{i}^{\\prime}}\\right) / \\tau}}{\\sum_{j=1}^{N} e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}^{z_{i}}, \\mathbf{h}_{j}^{z_{j}^{\\prime}}\\right) / \\tau}},\n$$\n\n其中$$h_i^z=f_{\\theta}(x_i, z)$$为输入$$x_i$$经过$$\\theta$$的encoder进行编码得到的结果，其中的z代表不同的dropout mask，每次的dropout mask都不同。N为batch size，所以该loss是每个batch内的交叉熵。$$\\tau$$为温度超参。sim()使用的是余弦距离\n\n- **并且在微调时选择更新所有参数**\n\n\n\n### 2.2 Dropout和其他数据增强方式的对比\n\n- 本文是将dropout作为一种微小的数据增强方式，所以作者也将其他数据增强方式同其对比了一下，本实验采用lr=3e-5，N=64，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810133558725.png\" alt=\"image-20220810133558725\" style=\"zoom:75%;\" />\n\n- **发现其他数据增强方式都没有SimCSE效果好（可能是其他方法噪音太大了）**\n\n\n\n### 2.3 采用一个OR两个Encoder\n\n- 由于之前有些论文是使用的两个不同的encoder，所以作者也就采用一个还是两个encoder的问题进行了对比试验，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810140539627.png\" alt=\"image-20220810140539627\" style=\"zoom:80%;\" />\n\n图中的next sentence为输入原句子和该句子的下一句。Delete on word同2.2中图一样，输入原句子和删除一个词的原句\n\n- 通过实验发现，**只用一个encoder比两个要好**\n\n\n\n### 2.4 采用多少Dropout率\n\n- dropout是SimCSE中重要的一环，所以作者对该超参进行了实验，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810141017109.png\" alt=\"image-20220810141017109\" style=\"zoom:80%;\" />\n\n图中的fixed 0.1为0.1的dropout率，但是对正样本对中的两个样本使用相同的dropout mask，就是两个输出都长一样（有用才有怪了）\n\n- **通过实验发现，还是原先默认的0.1最好用**\n\n\n\n### 2.5 alignment and uniformity\n\n- 前面说过，最综合的评估标准是检测结果向量空间的alignment和uniformity，作者对几种方法进行了评估，并给出了可视化的结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810142955971.png\" alt=\"image-20220810142955971\" style=\"zoom:80%;\" />\n\n其中箭头所指方向是训练进行的方向，横轴和竖轴都是越小越好\n\n- 通过实验发现，**所有的方法都能有效的提升uniformity**，但是前两种方法会降低正样本之间的alignment，而**无监督SimCSE的alignment则稳定不变**，delete one word可以稍微增加alignment，但是总体表现还是低于无监督SimCSE\n\n\n\n\n\n# 3 有监督SimCSE\n\n- 无监督的SimCSE可以提升uniformity，但是alignment不会有改善。而之后作者引入了有监督的数据，**利用其提供更好的训练信号，以提升alignment**\n\n\n\n### 3.1 使用哪种有监督数据\n\n- 先简要介绍一下SNLI和MNLI数据集，都是NLI任务下的数据集，是一个三分类，每次输入两个文本，模型预测两者的相似度，然后进行分类：**entailment（相关）、neutral（无关）、contradiction（矛盾）**，举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810152937757.png\" alt=\"image-20220810152937757\" style=\"zoom:67%;\" />\n\n- 作者探究了使用哪种有监督的数据集，能更有效地提升SimCSE的性能，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810152252447.png\" alt=\"image-20220810152252447\" style=\"zoom:67%;\" />\n\n图中sample指在数据集中采样了134k的正样本对，full指使用整个数据集。最后两行是使用NLI任务中的entailment对做正样本，contradiction对做负样本（把neutral对丢了）\n\n- 作者发现**使用NLI任务的数据集效果最显著，并且加上hard negative能进一步提升表现**\n- 并且作者还又尝试使用两个encoder，但是表现下降了\n\n\n\n### 3.2 基本方法\n\n- 相比于无监督，有监督将每个样本对$$(x_i, x_i^+)$$拓展为了三元组$$(x_i,x_i^+,x_i^-)$$，其中$$x_i^+$$和$$x_i^-$$分别为$$x_i$$的entailment样本和contradiction样本，然后采用以下loss：\n\n$$\n-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{+}\\right) / \\tau}+e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{-}\\right) / \\tau}\\right)}\n$$\n\n- 但是从**直观上**来讲，区分难负例（矛盾文本）和Batch内其他负例可能是有益的，所以将有监督学习SimCSE的训练目标变成：\n\n$$\n-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{+}\\right) / \\tau}+\\alpha^{\\mathbb{1}_{i}^{j}} e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{-}\\right) / \\tau}\\right)}\n$$\n\n其中$$1_i^j \\in \\{0, 1\\}$$仅当$$i=j$$时为1\n\n- 作者对不同的$$\\alpha$$进行了实验，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810165606726.png\" alt=\"image-20220810165606726\" style=\"zoom:80%;\" />\n\n其中N/A为不使用hard negative\n\n- **由上表可以得到$$\\alpha=1$$最合适（其实就是又退化回去了，没啥用......），并且将Neural的样本一起作为负例并不能提升表现**\n\n\n\n\n\n# 4 各向异性问题\n\n- 最近的研究发现了语言表征中的各向异性问题，**即训练后的embeddings仅占据在向量空间中狭窄的部分，严重限制了向量的表现力**。缓解这个问题的一个简单方法是**后处理**，可以**消除主要的主成分或将embeddings映射到各向同性分布**。另一种常见的解决方案是在**训练过程中添加正则项**。 而对比学习的优化目标可以改善缓解各向异性问题，当负例数趋近于无穷大时，对比学习目标的渐近表示为:\n\n$$\n-\\frac{1}{\\tau} \\underset{\\left(x_{i}, x_{i}^{+}\\right) \\sim p_{p o s}}{E}\\left[f(x)^{T} f\\left(x^{+}\\right)\\right]+\\underset{x \\sim p_{\\text {data }}}{E}\\left[\\log \\underset{x^{-} \\sim p_{\\text {data }}}{E}\\left[e^{f(x)^{T} f\\left(x^{-}\\right) / \\tau}\\right]\\right]\n$$\n\n其中，**第一项使正例之间更相似，第二项使将负例之间分开。**而第二项**在优化过程中，会压平向量空间的奇异谱，因此对比学习有望缓解表征退化问题，提高句向量表征的均匀性**\n\n- 并且作者还针对不同的模型、不同的后处理方法、不同的数据扩充方法等，通过alignment和uniformity进行了实验：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810192852943.png\" alt=\"image-20220810192852943\" style=\"zoom:80%;\" />\n\n图中括号中的是再STS任务上的得分\n\n- 通过上图，可以得出以下结论：\n\n> 1. 虽然预训练embeddings具有良好的对齐性，但其均匀性较差\n> 2. 后处理方法，如BERT-flow和BERT-whitening，大大改善均匀性，但也使其对齐性变差\n> 3. 无监督SimCSE有效地提高了预训练embeddings的均匀性，同时保持了良好的对齐性\n> 4. 有监督SimCSE，可以进一步提高对齐性\n\n\n\n\n\n# 5 对比试验\n\n### 5.1 STS任务上的对比\n\n- 作者先在7个STS任务上进行了对比实验，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810170503226.png\" alt=\"image-20220810170503226\" style=\"zoom:70%;\" />\n\n- 可以发现，无监督和有监督的SimCSE均取得了SOTA的效果，并且同时适用于BERT和RoBERTa\n\n\n\n### 5.2 Pooling方式\n\n- 在实验中，是采用[CLS]的表征进行分类的，但是有其他文章表示使用embedding的平均能提升表现。并且如果采用[CLS]，原始的BERT在其之后添加了一个额外的MLP层，本文对MLP同样有三种pooling方式：(1)、保留MLP层；(2)、丢弃MLP层；(3)、训练时采用MLP层，测试时丢弃。实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810172926336.png\" alt=\"image-20220810172926336\" style=\"zoom:80%;\" />\n\n- 从结果中得知：**无监督在train中使用MLP，test中丢弃MLP表现最好；有监督不同的pooling方法不是差别不是很大**\n- **作者选择在无监督中使用MLP(train)，而在有监督中使用with MLP**\n\n\n\n### 5.3 召回任务的结果\n\n- 作者还使用$$SBERT_{base}$$和$$SimCSE-BERT_{base}$$进行了一个小规模的召回实验，给定query，找出相似的句子（基于余弦相似度），结果如下：\n\n![image-20220810185441782](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810185441782.png)\n\n- 结果是，SimCSE找出的句子质量更高\n\n\n\n### 5.4 温度超参和相似度函数的选择\n\n- 作者尝试使用了不同的$$\\tau$$超参，并且尝试用点积代替余弦相似度，结果如下：\n\n![image-20220810194557958](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810194557958.png)\n\nN\\A表示点积代替余弦相似度\n\n- 发现**使用余弦相似度更合适，并且$$\\tau=0.05$$表现最好**\n\n\n\n\n\n# 6 下游任务上的表现\n\n- 作者还在各种下游任务上进行了对比，并且加上了MLM任务（BERT中的MLM任务），**避免模型彻底的忘记token-level的知识，并发现加上MLM后可以在除STS任务外的其他下游任务上取得提升**，加上MLM后，训练目标由原本的$$\\ell$$变成了$$\\ell + \\lambda \\cdot \\ell ^{MLM}$$\n\n\n\n### 6.1 MLM的对比\n\n- 作者对比了在STS任务和其他下游任务上，加与不加MLM的结果对比，以及$$\\lambda$$超参的选择：\n\n![image-20220810195101507](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810195101507.png)\n\n- 结果说明，**添加token-level对于其他大多数下游任务都有提升，并且$$\\lambda=0.1$$最为合适，但是这会带来STS任务表现的下降**\n\n\n\n### 6.2 下游任务的对比\n\n- 最后作者给出了在各种模型、训练策略、处理方式等因素不同时，在各种下游任务上的表现：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810195709648.png\" alt=\"image-20220810195709648\" style=\"zoom:80%;\" />\n\n- 可以发现在迁移任务上该方法并没有做到最好，不过这也证明了作者的说法，句子级别的目标可能并不会有益于下游任务的训练，训练好的句子向量表示模型也并不是为了更好的适应下游任务，但是SimCSE也在许多任务上做到了SOTA，特别是带MLM的时候\n\n","source":"_posts/SimCSE总结.md","raw":"---\ntitle: SimCSE总结\nmath: true\ndate: 2022-10-21\n---\n\n\n\n\n\n- 作者提出了一种对比学习的方法，分为**有监督和无监督两种**。其中**只用dropout作为噪音，也可以当作一种数据增强，可以改善语义空间，提升其同向异性，使向量空间更为均匀，并且在有监督方法时，还能对齐正样本**\n\n\n\n# 1 评估标准\n\n- 本篇文章的目的是改善embedding，并且作者在后面的实验发现，进行了SimCSE后有部分下游任务上的表现甚至出现了下降，但是这并不影响SimCSE的作用。**句子嵌入的主要目标是对语义相似的句子进行聚类，所以为了更加综合的评估实验结果，肯定不能使用某个下游任务的实验结果**，作者采用了另一篇论文中一种评估embedding质量的方法：**采用语义相关的正样本之间的对齐（alignment）和整个表示空间的一致性（uniformity）来衡量学习嵌入的质量**\n\n> We takes **alignment** between semantically-related positive pairs and **uniformity** of the whole representation space to measure the quality of learned embeddings.  \n\n- 总的来说，对比学习所做的任务就是：**拉近正样本的距离，剩余的随机样本应该均匀分布在一个超平面上（也就是减少其各向异性）**，所以对比学习的任务就变为了降低以下两个指标：\n\n$$\n\\ell_{\\text {align }} \\triangleq \\underset{\\left(x, x^{+}\\right) \\sim p_{\\text {pos }}}{\\mathbb{E}}\\left\\|f(x)-f\\left(x^{+}\\right)\\right\\|^{2}, \\\\\n\\ell_{\\text {uniform }} \\triangleq log \\underset{\\left(x, y\\right) \\sim p_{\\text {data }}}{\\mathbb{E}}e^{-2\\left\\|f(x)-f\\left(y\\right)\\right\\|^{2}}\n$$\n\n其中$$p_{pos}$$为正样本对，$$p_{data}$$为所有数据对，$$f(x)$$为输入$$x$$经过encoder的输出\n\n- 并且作者还发现无监督的SimCSE能够向量空间的均匀性，并且并不会降低正样本之间的对齐。然后对于有监督，作者指出NLI任务最为适合训练出好的sentence embedding，并且有监督能够进一步提升正样本之间的对齐\n\n- 本文还多次使用了STS-B数据集，这是一个五分类任务的数据集，旨在判定两个句子的相关程度，分为了5个等级，并且得分采用斯皮尔曼等级相关系数\n\n\n\n\n\n# 2 无监督SimCSE\n\n### 2.1 基本方法\n\n- 方法非常简单，就是将同一个输入，分别经过两次encoder，encoder中的dropout**（dropout率仍为默认的0.1）**作为一种微小的数据增强，会使得两次的输出有些许不同。这两次的输出，就作为一对正样本，然后使用以下loss：\n\n$$\n\\ell_{i}=-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}^{z_{i}}, \\mathbf{h}_{i}^{z_{i}^{\\prime}}\\right) / \\tau}}{\\sum_{j=1}^{N} e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}^{z_{i}}, \\mathbf{h}_{j}^{z_{j}^{\\prime}}\\right) / \\tau}},\n$$\n\n其中$$h_i^z=f_{\\theta}(x_i, z)$$为输入$$x_i$$经过$$\\theta$$的encoder进行编码得到的结果，其中的z代表不同的dropout mask，每次的dropout mask都不同。N为batch size，所以该loss是每个batch内的交叉熵。$$\\tau$$为温度超参。sim()使用的是余弦距离\n\n- **并且在微调时选择更新所有参数**\n\n\n\n### 2.2 Dropout和其他数据增强方式的对比\n\n- 本文是将dropout作为一种微小的数据增强方式，所以作者也将其他数据增强方式同其对比了一下，本实验采用lr=3e-5，N=64，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810133558725.png\" alt=\"image-20220810133558725\" style=\"zoom:75%;\" />\n\n- **发现其他数据增强方式都没有SimCSE效果好（可能是其他方法噪音太大了）**\n\n\n\n### 2.3 采用一个OR两个Encoder\n\n- 由于之前有些论文是使用的两个不同的encoder，所以作者也就采用一个还是两个encoder的问题进行了对比试验，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810140539627.png\" alt=\"image-20220810140539627\" style=\"zoom:80%;\" />\n\n图中的next sentence为输入原句子和该句子的下一句。Delete on word同2.2中图一样，输入原句子和删除一个词的原句\n\n- 通过实验发现，**只用一个encoder比两个要好**\n\n\n\n### 2.4 采用多少Dropout率\n\n- dropout是SimCSE中重要的一环，所以作者对该超参进行了实验，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810141017109.png\" alt=\"image-20220810141017109\" style=\"zoom:80%;\" />\n\n图中的fixed 0.1为0.1的dropout率，但是对正样本对中的两个样本使用相同的dropout mask，就是两个输出都长一样（有用才有怪了）\n\n- **通过实验发现，还是原先默认的0.1最好用**\n\n\n\n### 2.5 alignment and uniformity\n\n- 前面说过，最综合的评估标准是检测结果向量空间的alignment和uniformity，作者对几种方法进行了评估，并给出了可视化的结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810142955971.png\" alt=\"image-20220810142955971\" style=\"zoom:80%;\" />\n\n其中箭头所指方向是训练进行的方向，横轴和竖轴都是越小越好\n\n- 通过实验发现，**所有的方法都能有效的提升uniformity**，但是前两种方法会降低正样本之间的alignment，而**无监督SimCSE的alignment则稳定不变**，delete one word可以稍微增加alignment，但是总体表现还是低于无监督SimCSE\n\n\n\n\n\n# 3 有监督SimCSE\n\n- 无监督的SimCSE可以提升uniformity，但是alignment不会有改善。而之后作者引入了有监督的数据，**利用其提供更好的训练信号，以提升alignment**\n\n\n\n### 3.1 使用哪种有监督数据\n\n- 先简要介绍一下SNLI和MNLI数据集，都是NLI任务下的数据集，是一个三分类，每次输入两个文本，模型预测两者的相似度，然后进行分类：**entailment（相关）、neutral（无关）、contradiction（矛盾）**，举个栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810152937757.png\" alt=\"image-20220810152937757\" style=\"zoom:67%;\" />\n\n- 作者探究了使用哪种有监督的数据集，能更有效地提升SimCSE的性能，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810152252447.png\" alt=\"image-20220810152252447\" style=\"zoom:67%;\" />\n\n图中sample指在数据集中采样了134k的正样本对，full指使用整个数据集。最后两行是使用NLI任务中的entailment对做正样本，contradiction对做负样本（把neutral对丢了）\n\n- 作者发现**使用NLI任务的数据集效果最显著，并且加上hard negative能进一步提升表现**\n- 并且作者还又尝试使用两个encoder，但是表现下降了\n\n\n\n### 3.2 基本方法\n\n- 相比于无监督，有监督将每个样本对$$(x_i, x_i^+)$$拓展为了三元组$$(x_i,x_i^+,x_i^-)$$，其中$$x_i^+$$和$$x_i^-$$分别为$$x_i$$的entailment样本和contradiction样本，然后采用以下loss：\n\n$$\n-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{+}\\right) / \\tau}+e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{-}\\right) / \\tau}\\right)}\n$$\n\n- 但是从**直观上**来讲，区分难负例（矛盾文本）和Batch内其他负例可能是有益的，所以将有监督学习SimCSE的训练目标变成：\n\n$$\n-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{+}\\right) / \\tau}+\\alpha^{\\mathbb{1}_{i}^{j}} e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{-}\\right) / \\tau}\\right)}\n$$\n\n其中$$1_i^j \\in \\{0, 1\\}$$仅当$$i=j$$时为1\n\n- 作者对不同的$$\\alpha$$进行了实验，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810165606726.png\" alt=\"image-20220810165606726\" style=\"zoom:80%;\" />\n\n其中N/A为不使用hard negative\n\n- **由上表可以得到$$\\alpha=1$$最合适（其实就是又退化回去了，没啥用......），并且将Neural的样本一起作为负例并不能提升表现**\n\n\n\n\n\n# 4 各向异性问题\n\n- 最近的研究发现了语言表征中的各向异性问题，**即训练后的embeddings仅占据在向量空间中狭窄的部分，严重限制了向量的表现力**。缓解这个问题的一个简单方法是**后处理**，可以**消除主要的主成分或将embeddings映射到各向同性分布**。另一种常见的解决方案是在**训练过程中添加正则项**。 而对比学习的优化目标可以改善缓解各向异性问题，当负例数趋近于无穷大时，对比学习目标的渐近表示为:\n\n$$\n-\\frac{1}{\\tau} \\underset{\\left(x_{i}, x_{i}^{+}\\right) \\sim p_{p o s}}{E}\\left[f(x)^{T} f\\left(x^{+}\\right)\\right]+\\underset{x \\sim p_{\\text {data }}}{E}\\left[\\log \\underset{x^{-} \\sim p_{\\text {data }}}{E}\\left[e^{f(x)^{T} f\\left(x^{-}\\right) / \\tau}\\right]\\right]\n$$\n\n其中，**第一项使正例之间更相似，第二项使将负例之间分开。**而第二项**在优化过程中，会压平向量空间的奇异谱，因此对比学习有望缓解表征退化问题，提高句向量表征的均匀性**\n\n- 并且作者还针对不同的模型、不同的后处理方法、不同的数据扩充方法等，通过alignment和uniformity进行了实验：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810192852943.png\" alt=\"image-20220810192852943\" style=\"zoom:80%;\" />\n\n图中括号中的是再STS任务上的得分\n\n- 通过上图，可以得出以下结论：\n\n> 1. 虽然预训练embeddings具有良好的对齐性，但其均匀性较差\n> 2. 后处理方法，如BERT-flow和BERT-whitening，大大改善均匀性，但也使其对齐性变差\n> 3. 无监督SimCSE有效地提高了预训练embeddings的均匀性，同时保持了良好的对齐性\n> 4. 有监督SimCSE，可以进一步提高对齐性\n\n\n\n\n\n# 5 对比试验\n\n### 5.1 STS任务上的对比\n\n- 作者先在7个STS任务上进行了对比实验，结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810170503226.png\" alt=\"image-20220810170503226\" style=\"zoom:70%;\" />\n\n- 可以发现，无监督和有监督的SimCSE均取得了SOTA的效果，并且同时适用于BERT和RoBERTa\n\n\n\n### 5.2 Pooling方式\n\n- 在实验中，是采用[CLS]的表征进行分类的，但是有其他文章表示使用embedding的平均能提升表现。并且如果采用[CLS]，原始的BERT在其之后添加了一个额外的MLP层，本文对MLP同样有三种pooling方式：(1)、保留MLP层；(2)、丢弃MLP层；(3)、训练时采用MLP层，测试时丢弃。实验结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810172926336.png\" alt=\"image-20220810172926336\" style=\"zoom:80%;\" />\n\n- 从结果中得知：**无监督在train中使用MLP，test中丢弃MLP表现最好；有监督不同的pooling方法不是差别不是很大**\n- **作者选择在无监督中使用MLP(train)，而在有监督中使用with MLP**\n\n\n\n### 5.3 召回任务的结果\n\n- 作者还使用$$SBERT_{base}$$和$$SimCSE-BERT_{base}$$进行了一个小规模的召回实验，给定query，找出相似的句子（基于余弦相似度），结果如下：\n\n![image-20220810185441782](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810185441782.png)\n\n- 结果是，SimCSE找出的句子质量更高\n\n\n\n### 5.4 温度超参和相似度函数的选择\n\n- 作者尝试使用了不同的$$\\tau$$超参，并且尝试用点积代替余弦相似度，结果如下：\n\n![image-20220810194557958](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810194557958.png)\n\nN\\A表示点积代替余弦相似度\n\n- 发现**使用余弦相似度更合适，并且$$\\tau=0.05$$表现最好**\n\n\n\n\n\n# 6 下游任务上的表现\n\n- 作者还在各种下游任务上进行了对比，并且加上了MLM任务（BERT中的MLM任务），**避免模型彻底的忘记token-level的知识，并发现加上MLM后可以在除STS任务外的其他下游任务上取得提升**，加上MLM后，训练目标由原本的$$\\ell$$变成了$$\\ell + \\lambda \\cdot \\ell ^{MLM}$$\n\n\n\n### 6.1 MLM的对比\n\n- 作者对比了在STS任务和其他下游任务上，加与不加MLM的结果对比，以及$$\\lambda$$超参的选择：\n\n![image-20220810195101507](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810195101507.png)\n\n- 结果说明，**添加token-level对于其他大多数下游任务都有提升，并且$$\\lambda=0.1$$最为合适，但是这会带来STS任务表现的下降**\n\n\n\n### 6.2 下游任务的对比\n\n- 最后作者给出了在各种模型、训练策略、处理方式等因素不同时，在各种下游任务上的表现：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810195709648.png\" alt=\"image-20220810195709648\" style=\"zoom:80%;\" />\n\n- 可以发现在迁移任务上该方法并没有做到最好，不过这也证明了作者的说法，句子级别的目标可能并不会有益于下游任务的训练，训练好的句子向量表示模型也并不是为了更好的适应下游任务，但是SimCSE也在许多任务上做到了SOTA，特别是带MLM的时候\n\n","slug":"SimCSE总结","published":1,"updated":"2022-12-20T06:25:00.657Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1g000c7cszhz6ch9qw","content":"<ul>\n<li>作者提出了一种对比学习的方法，分为<strong>有监督和无监督两种</strong>。其中<strong>只用dropout作为噪音，也可以当作一种数据增强，可以改善语义空间，提升其同向异性，使向量空间更为均匀，并且在有监督方法时，还能对齐正样本</strong></li>\n</ul>\n<h1 id=\"1-评估标准\"><a href=\"#1-评估标准\" class=\"headerlink\" title=\"1 评估标准\"></a>1 评估标准</h1><ul>\n<li>本篇文章的目的是改善embedding，并且作者在后面的实验发现，进行了SimCSE后有部分下游任务上的表现甚至出现了下降，但是这并不影响SimCSE的作用。<strong>句子嵌入的主要目标是对语义相似的句子进行聚类，所以为了更加综合的评估实验结果，肯定不能使用某个下游任务的实验结果</strong>，作者采用了另一篇论文中一种评估embedding质量的方法：<strong>采用语义相关的正样本之间的对齐（alignment）和整个表示空间的一致性（uniformity）来衡量学习嵌入的质量</strong></li>\n</ul>\n<blockquote>\n<p>We takes <strong>alignment</strong> between semantically-related positive pairs and <strong>uniformity</strong> of the whole representation space to measure the quality of learned embeddings.  </p>\n</blockquote>\n<ul>\n<li>总的来说，对比学习所做的任务就是：<strong>拉近正样本的距离，剩余的随机样本应该均匀分布在一个超平面上（也就是减少其各向异性）</strong>，所以对比学习的任务就变为了降低以下两个指标：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell_{\\text {align }} \\triangleq \\underset{\\left(x, x^{+}\\right) \\sim p_{\\text {pos }}}{\\mathbb{E}}\\left\\|f(x)-f\\left(x^{+}\\right)\\right\\|^{2}, \\\\\n\\ell_{\\text {uniform }} \\triangleq log \\underset{\\left(x, y\\right) \\sim p_{\\text {data }}}{\\mathbb{E}}e^{-2\\left\\|f(x)-f\\left(y\\right)\\right\\|^{2}}</script><p>其中<script type=\"math/tex\">p_{pos}</script>为正样本对，<script type=\"math/tex\">p_{data}</script>为所有数据对，<script type=\"math/tex\">f(x)</script>为输入<script type=\"math/tex\">x</script>经过encoder的输出</p>\n<ul>\n<li><p>并且作者还发现无监督的SimCSE能够向量空间的均匀性，并且并不会降低正样本之间的对齐。然后对于有监督，作者指出NLI任务最为适合训练出好的sentence embedding，并且有监督能够进一步提升正样本之间的对齐</p>\n</li>\n<li><p>本文还多次使用了STS-B数据集，这是一个五分类任务的数据集，旨在判定两个句子的相关程度，分为了5个等级，并且得分采用斯皮尔曼等级相关系数</p>\n</li>\n</ul>\n<h1 id=\"2-无监督SimCSE\"><a href=\"#2-无监督SimCSE\" class=\"headerlink\" title=\"2 无监督SimCSE\"></a>2 无监督SimCSE</h1><h3 id=\"2-1-基本方法\"><a href=\"#2-1-基本方法\" class=\"headerlink\" title=\"2.1 基本方法\"></a>2.1 基本方法</h3><ul>\n<li>方法非常简单，就是将同一个输入，分别经过两次encoder，encoder中的dropout<strong>（dropout率仍为默认的0.1）</strong>作为一种微小的数据增强，会使得两次的输出有些许不同。这两次的输出，就作为一对正样本，然后使用以下loss：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell_{i}=-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}^{z_{i}}, \\mathbf{h}_{i}^{z_{i}^{\\prime}}\\right) / \\tau}}{\\sum_{j=1}^{N} e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}^{z_{i}}, \\mathbf{h}_{j}^{z_{j}^{\\prime}}\\right) / \\tau}},</script><p>其中<script type=\"math/tex\">h_i^z=f_{\\theta}(x_i, z)</script>为输入<script type=\"math/tex\">x_i</script>经过<script type=\"math/tex\">\\theta</script>的encoder进行编码得到的结果，其中的z代表不同的dropout mask，每次的dropout mask都不同。N为batch size，所以该loss是每个batch内的交叉熵。<script type=\"math/tex\">\\tau</script>为温度超参。sim()使用的是余弦距离</p>\n<ul>\n<li><strong>并且在微调时选择更新所有参数</strong></li>\n</ul>\n<h3 id=\"2-2-Dropout和其他数据增强方式的对比\"><a href=\"#2-2-Dropout和其他数据增强方式的对比\" class=\"headerlink\" title=\"2.2 Dropout和其他数据增强方式的对比\"></a>2.2 Dropout和其他数据增强方式的对比</h3><ul>\n<li>本文是将dropout作为一种微小的数据增强方式，所以作者也将其他数据增强方式同其对比了一下，本实验采用lr=3e-5，N=64，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810133558725.png\" alt=\"image-20220810133558725\" style=\"zoom:75%;\" /></p>\n<ul>\n<li><strong>发现其他数据增强方式都没有SimCSE效果好（可能是其他方法噪音太大了）</strong></li>\n</ul>\n<h3 id=\"2-3-采用一个OR两个Encoder\"><a href=\"#2-3-采用一个OR两个Encoder\" class=\"headerlink\" title=\"2.3 采用一个OR两个Encoder\"></a>2.3 采用一个OR两个Encoder</h3><ul>\n<li>由于之前有些论文是使用的两个不同的encoder，所以作者也就采用一个还是两个encoder的问题进行了对比试验，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810140539627.png\" alt=\"image-20220810140539627\" style=\"zoom:80%;\" /></p>\n<p>图中的next sentence为输入原句子和该句子的下一句。Delete on word同2.2中图一样，输入原句子和删除一个词的原句</p>\n<ul>\n<li>通过实验发现，<strong>只用一个encoder比两个要好</strong></li>\n</ul>\n<h3 id=\"2-4-采用多少Dropout率\"><a href=\"#2-4-采用多少Dropout率\" class=\"headerlink\" title=\"2.4 采用多少Dropout率\"></a>2.4 采用多少Dropout率</h3><ul>\n<li>dropout是SimCSE中重要的一环，所以作者对该超参进行了实验，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810141017109.png\" alt=\"image-20220810141017109\" style=\"zoom:80%;\" /></p>\n<p>图中的fixed 0.1为0.1的dropout率，但是对正样本对中的两个样本使用相同的dropout mask，就是两个输出都长一样（有用才有怪了）</p>\n<ul>\n<li><strong>通过实验发现，还是原先默认的0.1最好用</strong></li>\n</ul>\n<h3 id=\"2-5-alignment-and-uniformity\"><a href=\"#2-5-alignment-and-uniformity\" class=\"headerlink\" title=\"2.5 alignment and uniformity\"></a>2.5 alignment and uniformity</h3><ul>\n<li>前面说过，最综合的评估标准是检测结果向量空间的alignment和uniformity，作者对几种方法进行了评估，并给出了可视化的结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810142955971.png\" alt=\"image-20220810142955971\" style=\"zoom:80%;\" /></p>\n<p>其中箭头所指方向是训练进行的方向，横轴和竖轴都是越小越好</p>\n<ul>\n<li>通过实验发现，<strong>所有的方法都能有效的提升uniformity</strong>，但是前两种方法会降低正样本之间的alignment，而<strong>无监督SimCSE的alignment则稳定不变</strong>，delete one word可以稍微增加alignment，但是总体表现还是低于无监督SimCSE</li>\n</ul>\n<h1 id=\"3-有监督SimCSE\"><a href=\"#3-有监督SimCSE\" class=\"headerlink\" title=\"3 有监督SimCSE\"></a>3 有监督SimCSE</h1><ul>\n<li>无监督的SimCSE可以提升uniformity，但是alignment不会有改善。而之后作者引入了有监督的数据，<strong>利用其提供更好的训练信号，以提升alignment</strong></li>\n</ul>\n<h3 id=\"3-1-使用哪种有监督数据\"><a href=\"#3-1-使用哪种有监督数据\" class=\"headerlink\" title=\"3.1 使用哪种有监督数据\"></a>3.1 使用哪种有监督数据</h3><ul>\n<li>先简要介绍一下SNLI和MNLI数据集，都是NLI任务下的数据集，是一个三分类，每次输入两个文本，模型预测两者的相似度，然后进行分类：<strong>entailment（相关）、neutral（无关）、contradiction（矛盾）</strong>，举个栗子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810152937757.png\" alt=\"image-20220810152937757\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>作者探究了使用哪种有监督的数据集，能更有效地提升SimCSE的性能，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810152252447.png\" alt=\"image-20220810152252447\" style=\"zoom:67%;\" /></p>\n<p>图中sample指在数据集中采样了134k的正样本对，full指使用整个数据集。最后两行是使用NLI任务中的entailment对做正样本，contradiction对做负样本（把neutral对丢了）</p>\n<ul>\n<li>作者发现<strong>使用NLI任务的数据集效果最显著，并且加上hard negative能进一步提升表现</strong></li>\n<li>并且作者还又尝试使用两个encoder，但是表现下降了</li>\n</ul>\n<h3 id=\"3-2-基本方法\"><a href=\"#3-2-基本方法\" class=\"headerlink\" title=\"3.2 基本方法\"></a>3.2 基本方法</h3><ul>\n<li>相比于无监督，有监督将每个样本对<script type=\"math/tex\">(x_i, x_i^+)</script>拓展为了三元组<script type=\"math/tex\">(x_i,x_i^+,x_i^-)</script>，其中<script type=\"math/tex\">x_i^+</script>和<script type=\"math/tex\">x_i^-</script>分别为<script type=\"math/tex\">x_i</script>的entailment样本和contradiction样本，然后采用以下loss：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{+}\\right) / \\tau}+e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{-}\\right) / \\tau}\\right)}</script><ul>\n<li>但是从<strong>直观上</strong>来讲，区分难负例（矛盾文本）和Batch内其他负例可能是有益的，所以将有监督学习SimCSE的训练目标变成：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{+}\\right) / \\tau}+\\alpha^{\\mathbb{1}_{i}^{j}} e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{-}\\right) / \\tau}\\right)}</script><p>其中<script type=\"math/tex\">1_i^j \\in \\{0, 1\\}</script>仅当<script type=\"math/tex\">i=j</script>时为1</p>\n<ul>\n<li>作者对不同的<script type=\"math/tex\">\\alpha</script>进行了实验，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810165606726.png\" alt=\"image-20220810165606726\" style=\"zoom:80%;\" /></p>\n<p>其中N/A为不使用hard negative</p>\n<ul>\n<li><strong>由上表可以得到<script type=\"math/tex\">\\alpha=1</script>最合适（其实就是又退化回去了，没啥用……），并且将Neural的样本一起作为负例并不能提升表现</strong></li>\n</ul>\n<h1 id=\"4-各向异性问题\"><a href=\"#4-各向异性问题\" class=\"headerlink\" title=\"4 各向异性问题\"></a>4 各向异性问题</h1><ul>\n<li>最近的研究发现了语言表征中的各向异性问题，<strong>即训练后的embeddings仅占据在向量空间中狭窄的部分，严重限制了向量的表现力</strong>。缓解这个问题的一个简单方法是<strong>后处理</strong>，可以<strong>消除主要的主成分或将embeddings映射到各向同性分布</strong>。另一种常见的解决方案是在<strong>训练过程中添加正则项</strong>。 而对比学习的优化目标可以改善缓解各向异性问题，当负例数趋近于无穷大时，对比学习目标的渐近表示为:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\frac{1}{\\tau} \\underset{\\left(x_{i}, x_{i}^{+}\\right) \\sim p_{p o s}}{E}\\left[f(x)^{T} f\\left(x^{+}\\right)\\right]+\\underset{x \\sim p_{\\text {data }}}{E}\\left[\\log \\underset{x^{-} \\sim p_{\\text {data }}}{E}\\left[e^{f(x)^{T} f\\left(x^{-}\\right) / \\tau}\\right]\\right]</script><p>其中，<strong>第一项使正例之间更相似，第二项使将负例之间分开。</strong>而第二项<strong>在优化过程中，会压平向量空间的奇异谱，因此对比学习有望缓解表征退化问题，提高句向量表征的均匀性</strong></p>\n<ul>\n<li>并且作者还针对不同的模型、不同的后处理方法、不同的数据扩充方法等，通过alignment和uniformity进行了实验：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810192852943.png\" alt=\"image-20220810192852943\" style=\"zoom:80%;\" /></p>\n<p>图中括号中的是再STS任务上的得分</p>\n<ul>\n<li>通过上图，可以得出以下结论：</li>\n</ul>\n<blockquote>\n<ol>\n<li>虽然预训练embeddings具有良好的对齐性，但其均匀性较差</li>\n<li>后处理方法，如BERT-flow和BERT-whitening，大大改善均匀性，但也使其对齐性变差</li>\n<li>无监督SimCSE有效地提高了预训练embeddings的均匀性，同时保持了良好的对齐性</li>\n<li>有监督SimCSE，可以进一步提高对齐性</li>\n</ol>\n</blockquote>\n<h1 id=\"5-对比试验\"><a href=\"#5-对比试验\" class=\"headerlink\" title=\"5 对比试验\"></a>5 对比试验</h1><h3 id=\"5-1-STS任务上的对比\"><a href=\"#5-1-STS任务上的对比\" class=\"headerlink\" title=\"5.1 STS任务上的对比\"></a>5.1 STS任务上的对比</h3><ul>\n<li>作者先在7个STS任务上进行了对比实验，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810170503226.png\" alt=\"image-20220810170503226\" style=\"zoom:70%;\" /></p>\n<ul>\n<li>可以发现，无监督和有监督的SimCSE均取得了SOTA的效果，并且同时适用于BERT和RoBERTa</li>\n</ul>\n<h3 id=\"5-2-Pooling方式\"><a href=\"#5-2-Pooling方式\" class=\"headerlink\" title=\"5.2 Pooling方式\"></a>5.2 Pooling方式</h3><ul>\n<li>在实验中，是采用[CLS]的表征进行分类的，但是有其他文章表示使用embedding的平均能提升表现。并且如果采用[CLS]，原始的BERT在其之后添加了一个额外的MLP层，本文对MLP同样有三种pooling方式：(1)、保留MLP层；(2)、丢弃MLP层；(3)、训练时采用MLP层，测试时丢弃。实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810172926336.png\" alt=\"image-20220810172926336\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>从结果中得知：<strong>无监督在train中使用MLP，test中丢弃MLP表现最好；有监督不同的pooling方法不是差别不是很大</strong></li>\n<li><strong>作者选择在无监督中使用MLP(train)，而在有监督中使用with MLP</strong></li>\n</ul>\n<h3 id=\"5-3-召回任务的结果\"><a href=\"#5-3-召回任务的结果\" class=\"headerlink\" title=\"5.3 召回任务的结果\"></a>5.3 召回任务的结果</h3><ul>\n<li>作者还使用<script type=\"math/tex\">SBERT_{base}</script>和<script type=\"math/tex\">SimCSE-BERT_{base}</script>进行了一个小规模的召回实验，给定query，找出相似的句子（基于余弦相似度），结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810185441782.png\" alt=\"image-20220810185441782\"></p>\n<ul>\n<li>结果是，SimCSE找出的句子质量更高</li>\n</ul>\n<h3 id=\"5-4-温度超参和相似度函数的选择\"><a href=\"#5-4-温度超参和相似度函数的选择\" class=\"headerlink\" title=\"5.4 温度超参和相似度函数的选择\"></a>5.4 温度超参和相似度函数的选择</h3><ul>\n<li>作者尝试使用了不同的<script type=\"math/tex\">\\tau</script>超参，并且尝试用点积代替余弦相似度，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810194557958.png\" alt=\"image-20220810194557958\"></p>\n<p>N\\A表示点积代替余弦相似度</p>\n<ul>\n<li>发现<strong>使用余弦相似度更合适，并且<script type=\"math/tex\">\\tau=0.05</script>表现最好</strong></li>\n</ul>\n<h1 id=\"6-下游任务上的表现\"><a href=\"#6-下游任务上的表现\" class=\"headerlink\" title=\"6 下游任务上的表现\"></a>6 下游任务上的表现</h1><ul>\n<li>作者还在各种下游任务上进行了对比，并且加上了MLM任务（BERT中的MLM任务），<strong>避免模型彻底的忘记token-level的知识，并发现加上MLM后可以在除STS任务外的其他下游任务上取得提升</strong>，加上MLM后，训练目标由原本的<script type=\"math/tex\">\\ell</script>变成了<script type=\"math/tex\">\\ell + \\lambda \\cdot \\ell ^{MLM}</script></li>\n</ul>\n<h3 id=\"6-1-MLM的对比\"><a href=\"#6-1-MLM的对比\" class=\"headerlink\" title=\"6.1 MLM的对比\"></a>6.1 MLM的对比</h3><ul>\n<li>作者对比了在STS任务和其他下游任务上，加与不加MLM的结果对比，以及<script type=\"math/tex\">\\lambda</script>超参的选择：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810195101507.png\" alt=\"image-20220810195101507\"></p>\n<ul>\n<li>结果说明，<strong>添加token-level对于其他大多数下游任务都有提升，并且<script type=\"math/tex\">\\lambda=0.1</script>最为合适，但是这会带来STS任务表现的下降</strong></li>\n</ul>\n<h3 id=\"6-2-下游任务的对比\"><a href=\"#6-2-下游任务的对比\" class=\"headerlink\" title=\"6.2 下游任务的对比\"></a>6.2 下游任务的对比</h3><ul>\n<li>最后作者给出了在各种模型、训练策略、处理方式等因素不同时，在各种下游任务上的表现：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810195709648.png\" alt=\"image-20220810195709648\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>可以发现在迁移任务上该方法并没有做到最好，不过这也证明了作者的说法，句子级别的目标可能并不会有益于下游任务的训练，训练好的句子向量表示模型也并不是为了更好的适应下游任务，但是SimCSE也在许多任务上做到了SOTA，特别是带MLM的时候</li>\n</ul>\n","site":{"data":{}},"wordcount":5324,"excerpt":"","more":"<ul>\n<li>作者提出了一种对比学习的方法，分为<strong>有监督和无监督两种</strong>。其中<strong>只用dropout作为噪音，也可以当作一种数据增强，可以改善语义空间，提升其同向异性，使向量空间更为均匀，并且在有监督方法时，还能对齐正样本</strong></li>\n</ul>\n<h1 id=\"1-评估标准\"><a href=\"#1-评估标准\" class=\"headerlink\" title=\"1 评估标准\"></a>1 评估标准</h1><ul>\n<li>本篇文章的目的是改善embedding，并且作者在后面的实验发现，进行了SimCSE后有部分下游任务上的表现甚至出现了下降，但是这并不影响SimCSE的作用。<strong>句子嵌入的主要目标是对语义相似的句子进行聚类，所以为了更加综合的评估实验结果，肯定不能使用某个下游任务的实验结果</strong>，作者采用了另一篇论文中一种评估embedding质量的方法：<strong>采用语义相关的正样本之间的对齐（alignment）和整个表示空间的一致性（uniformity）来衡量学习嵌入的质量</strong></li>\n</ul>\n<blockquote>\n<p>We takes <strong>alignment</strong> between semantically-related positive pairs and <strong>uniformity</strong> of the whole representation space to measure the quality of learned embeddings.  </p>\n</blockquote>\n<ul>\n<li>总的来说，对比学习所做的任务就是：<strong>拉近正样本的距离，剩余的随机样本应该均匀分布在一个超平面上（也就是减少其各向异性）</strong>，所以对比学习的任务就变为了降低以下两个指标：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell_{\\text {align }} \\triangleq \\underset{\\left(x, x^{+}\\right) \\sim p_{\\text {pos }}}{\\mathbb{E}}\\left\\|f(x)-f\\left(x^{+}\\right)\\right\\|^{2}, \\\\\n\\ell_{\\text {uniform }} \\triangleq log \\underset{\\left(x, y\\right) \\sim p_{\\text {data }}}{\\mathbb{E}}e^{-2\\left\\|f(x)-f\\left(y\\right)\\right\\|^{2}}</script><p>其中<script type=\"math/tex\">p_{pos}</script>为正样本对，<script type=\"math/tex\">p_{data}</script>为所有数据对，<script type=\"math/tex\">f(x)</script>为输入<script type=\"math/tex\">x</script>经过encoder的输出</p>\n<ul>\n<li><p>并且作者还发现无监督的SimCSE能够向量空间的均匀性，并且并不会降低正样本之间的对齐。然后对于有监督，作者指出NLI任务最为适合训练出好的sentence embedding，并且有监督能够进一步提升正样本之间的对齐</p>\n</li>\n<li><p>本文还多次使用了STS-B数据集，这是一个五分类任务的数据集，旨在判定两个句子的相关程度，分为了5个等级，并且得分采用斯皮尔曼等级相关系数</p>\n</li>\n</ul>\n<h1 id=\"2-无监督SimCSE\"><a href=\"#2-无监督SimCSE\" class=\"headerlink\" title=\"2 无监督SimCSE\"></a>2 无监督SimCSE</h1><h3 id=\"2-1-基本方法\"><a href=\"#2-1-基本方法\" class=\"headerlink\" title=\"2.1 基本方法\"></a>2.1 基本方法</h3><ul>\n<li>方法非常简单，就是将同一个输入，分别经过两次encoder，encoder中的dropout<strong>（dropout率仍为默认的0.1）</strong>作为一种微小的数据增强，会使得两次的输出有些许不同。这两次的输出，就作为一对正样本，然后使用以下loss：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell_{i}=-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}^{z_{i}}, \\mathbf{h}_{i}^{z_{i}^{\\prime}}\\right) / \\tau}}{\\sum_{j=1}^{N} e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}^{z_{i}}, \\mathbf{h}_{j}^{z_{j}^{\\prime}}\\right) / \\tau}},</script><p>其中<script type=\"math/tex\">h_i^z=f_{\\theta}(x_i, z)</script>为输入<script type=\"math/tex\">x_i</script>经过<script type=\"math/tex\">\\theta</script>的encoder进行编码得到的结果，其中的z代表不同的dropout mask，每次的dropout mask都不同。N为batch size，所以该loss是每个batch内的交叉熵。<script type=\"math/tex\">\\tau</script>为温度超参。sim()使用的是余弦距离</p>\n<ul>\n<li><strong>并且在微调时选择更新所有参数</strong></li>\n</ul>\n<h3 id=\"2-2-Dropout和其他数据增强方式的对比\"><a href=\"#2-2-Dropout和其他数据增强方式的对比\" class=\"headerlink\" title=\"2.2 Dropout和其他数据增强方式的对比\"></a>2.2 Dropout和其他数据增强方式的对比</h3><ul>\n<li>本文是将dropout作为一种微小的数据增强方式，所以作者也将其他数据增强方式同其对比了一下，本实验采用lr=3e-5，N=64，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810133558725.png\" alt=\"image-20220810133558725\" style=\"zoom:75%;\" /></p>\n<ul>\n<li><strong>发现其他数据增强方式都没有SimCSE效果好（可能是其他方法噪音太大了）</strong></li>\n</ul>\n<h3 id=\"2-3-采用一个OR两个Encoder\"><a href=\"#2-3-采用一个OR两个Encoder\" class=\"headerlink\" title=\"2.3 采用一个OR两个Encoder\"></a>2.3 采用一个OR两个Encoder</h3><ul>\n<li>由于之前有些论文是使用的两个不同的encoder，所以作者也就采用一个还是两个encoder的问题进行了对比试验，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810140539627.png\" alt=\"image-20220810140539627\" style=\"zoom:80%;\" /></p>\n<p>图中的next sentence为输入原句子和该句子的下一句。Delete on word同2.2中图一样，输入原句子和删除一个词的原句</p>\n<ul>\n<li>通过实验发现，<strong>只用一个encoder比两个要好</strong></li>\n</ul>\n<h3 id=\"2-4-采用多少Dropout率\"><a href=\"#2-4-采用多少Dropout率\" class=\"headerlink\" title=\"2.4 采用多少Dropout率\"></a>2.4 采用多少Dropout率</h3><ul>\n<li>dropout是SimCSE中重要的一环，所以作者对该超参进行了实验，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810141017109.png\" alt=\"image-20220810141017109\" style=\"zoom:80%;\" /></p>\n<p>图中的fixed 0.1为0.1的dropout率，但是对正样本对中的两个样本使用相同的dropout mask，就是两个输出都长一样（有用才有怪了）</p>\n<ul>\n<li><strong>通过实验发现，还是原先默认的0.1最好用</strong></li>\n</ul>\n<h3 id=\"2-5-alignment-and-uniformity\"><a href=\"#2-5-alignment-and-uniformity\" class=\"headerlink\" title=\"2.5 alignment and uniformity\"></a>2.5 alignment and uniformity</h3><ul>\n<li>前面说过，最综合的评估标准是检测结果向量空间的alignment和uniformity，作者对几种方法进行了评估，并给出了可视化的结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810142955971.png\" alt=\"image-20220810142955971\" style=\"zoom:80%;\" /></p>\n<p>其中箭头所指方向是训练进行的方向，横轴和竖轴都是越小越好</p>\n<ul>\n<li>通过实验发现，<strong>所有的方法都能有效的提升uniformity</strong>，但是前两种方法会降低正样本之间的alignment，而<strong>无监督SimCSE的alignment则稳定不变</strong>，delete one word可以稍微增加alignment，但是总体表现还是低于无监督SimCSE</li>\n</ul>\n<h1 id=\"3-有监督SimCSE\"><a href=\"#3-有监督SimCSE\" class=\"headerlink\" title=\"3 有监督SimCSE\"></a>3 有监督SimCSE</h1><ul>\n<li>无监督的SimCSE可以提升uniformity，但是alignment不会有改善。而之后作者引入了有监督的数据，<strong>利用其提供更好的训练信号，以提升alignment</strong></li>\n</ul>\n<h3 id=\"3-1-使用哪种有监督数据\"><a href=\"#3-1-使用哪种有监督数据\" class=\"headerlink\" title=\"3.1 使用哪种有监督数据\"></a>3.1 使用哪种有监督数据</h3><ul>\n<li>先简要介绍一下SNLI和MNLI数据集，都是NLI任务下的数据集，是一个三分类，每次输入两个文本，模型预测两者的相似度，然后进行分类：<strong>entailment（相关）、neutral（无关）、contradiction（矛盾）</strong>，举个栗子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810152937757.png\" alt=\"image-20220810152937757\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>作者探究了使用哪种有监督的数据集，能更有效地提升SimCSE的性能，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810152252447.png\" alt=\"image-20220810152252447\" style=\"zoom:67%;\" /></p>\n<p>图中sample指在数据集中采样了134k的正样本对，full指使用整个数据集。最后两行是使用NLI任务中的entailment对做正样本，contradiction对做负样本（把neutral对丢了）</p>\n<ul>\n<li>作者发现<strong>使用NLI任务的数据集效果最显著，并且加上hard negative能进一步提升表现</strong></li>\n<li>并且作者还又尝试使用两个encoder，但是表现下降了</li>\n</ul>\n<h3 id=\"3-2-基本方法\"><a href=\"#3-2-基本方法\" class=\"headerlink\" title=\"3.2 基本方法\"></a>3.2 基本方法</h3><ul>\n<li>相比于无监督，有监督将每个样本对<script type=\"math/tex\">(x_i, x_i^+)</script>拓展为了三元组<script type=\"math/tex\">(x_i,x_i^+,x_i^-)</script>，其中<script type=\"math/tex\">x_i^+</script>和<script type=\"math/tex\">x_i^-</script>分别为<script type=\"math/tex\">x_i</script>的entailment样本和contradiction样本，然后采用以下loss：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{+}\\right) / \\tau}+e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{-}\\right) / \\tau}\\right)}</script><ul>\n<li>但是从<strong>直观上</strong>来讲，区分难负例（矛盾文本）和Batch内其他负例可能是有益的，所以将有监督学习SimCSE的训练目标变成：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{+}\\right) / \\tau}+\\alpha^{\\mathbb{1}_{i}^{j}} e^{\\operatorname{sim}\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}^{-}\\right) / \\tau}\\right)}</script><p>其中<script type=\"math/tex\">1_i^j \\in \\{0, 1\\}</script>仅当<script type=\"math/tex\">i=j</script>时为1</p>\n<ul>\n<li>作者对不同的<script type=\"math/tex\">\\alpha</script>进行了实验，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810165606726.png\" alt=\"image-20220810165606726\" style=\"zoom:80%;\" /></p>\n<p>其中N/A为不使用hard negative</p>\n<ul>\n<li><strong>由上表可以得到<script type=\"math/tex\">\\alpha=1</script>最合适（其实就是又退化回去了，没啥用……），并且将Neural的样本一起作为负例并不能提升表现</strong></li>\n</ul>\n<h1 id=\"4-各向异性问题\"><a href=\"#4-各向异性问题\" class=\"headerlink\" title=\"4 各向异性问题\"></a>4 各向异性问题</h1><ul>\n<li>最近的研究发现了语言表征中的各向异性问题，<strong>即训练后的embeddings仅占据在向量空间中狭窄的部分，严重限制了向量的表现力</strong>。缓解这个问题的一个简单方法是<strong>后处理</strong>，可以<strong>消除主要的主成分或将embeddings映射到各向同性分布</strong>。另一种常见的解决方案是在<strong>训练过程中添加正则项</strong>。 而对比学习的优化目标可以改善缓解各向异性问题，当负例数趋近于无穷大时，对比学习目标的渐近表示为:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n-\\frac{1}{\\tau} \\underset{\\left(x_{i}, x_{i}^{+}\\right) \\sim p_{p o s}}{E}\\left[f(x)^{T} f\\left(x^{+}\\right)\\right]+\\underset{x \\sim p_{\\text {data }}}{E}\\left[\\log \\underset{x^{-} \\sim p_{\\text {data }}}{E}\\left[e^{f(x)^{T} f\\left(x^{-}\\right) / \\tau}\\right]\\right]</script><p>其中，<strong>第一项使正例之间更相似，第二项使将负例之间分开。</strong>而第二项<strong>在优化过程中，会压平向量空间的奇异谱，因此对比学习有望缓解表征退化问题，提高句向量表征的均匀性</strong></p>\n<ul>\n<li>并且作者还针对不同的模型、不同的后处理方法、不同的数据扩充方法等，通过alignment和uniformity进行了实验：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810192852943.png\" alt=\"image-20220810192852943\" style=\"zoom:80%;\" /></p>\n<p>图中括号中的是再STS任务上的得分</p>\n<ul>\n<li>通过上图，可以得出以下结论：</li>\n</ul>\n<blockquote>\n<ol>\n<li>虽然预训练embeddings具有良好的对齐性，但其均匀性较差</li>\n<li>后处理方法，如BERT-flow和BERT-whitening，大大改善均匀性，但也使其对齐性变差</li>\n<li>无监督SimCSE有效地提高了预训练embeddings的均匀性，同时保持了良好的对齐性</li>\n<li>有监督SimCSE，可以进一步提高对齐性</li>\n</ol>\n</blockquote>\n<h1 id=\"5-对比试验\"><a href=\"#5-对比试验\" class=\"headerlink\" title=\"5 对比试验\"></a>5 对比试验</h1><h3 id=\"5-1-STS任务上的对比\"><a href=\"#5-1-STS任务上的对比\" class=\"headerlink\" title=\"5.1 STS任务上的对比\"></a>5.1 STS任务上的对比</h3><ul>\n<li>作者先在7个STS任务上进行了对比实验，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810170503226.png\" alt=\"image-20220810170503226\" style=\"zoom:70%;\" /></p>\n<ul>\n<li>可以发现，无监督和有监督的SimCSE均取得了SOTA的效果，并且同时适用于BERT和RoBERTa</li>\n</ul>\n<h3 id=\"5-2-Pooling方式\"><a href=\"#5-2-Pooling方式\" class=\"headerlink\" title=\"5.2 Pooling方式\"></a>5.2 Pooling方式</h3><ul>\n<li>在实验中，是采用[CLS]的表征进行分类的，但是有其他文章表示使用embedding的平均能提升表现。并且如果采用[CLS]，原始的BERT在其之后添加了一个额外的MLP层，本文对MLP同样有三种pooling方式：(1)、保留MLP层；(2)、丢弃MLP层；(3)、训练时采用MLP层，测试时丢弃。实验结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810172926336.png\" alt=\"image-20220810172926336\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>从结果中得知：<strong>无监督在train中使用MLP，test中丢弃MLP表现最好；有监督不同的pooling方法不是差别不是很大</strong></li>\n<li><strong>作者选择在无监督中使用MLP(train)，而在有监督中使用with MLP</strong></li>\n</ul>\n<h3 id=\"5-3-召回任务的结果\"><a href=\"#5-3-召回任务的结果\" class=\"headerlink\" title=\"5.3 召回任务的结果\"></a>5.3 召回任务的结果</h3><ul>\n<li>作者还使用<script type=\"math/tex\">SBERT_{base}</script>和<script type=\"math/tex\">SimCSE-BERT_{base}</script>进行了一个小规模的召回实验，给定query，找出相似的句子（基于余弦相似度），结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810185441782.png\" alt=\"image-20220810185441782\"></p>\n<ul>\n<li>结果是，SimCSE找出的句子质量更高</li>\n</ul>\n<h3 id=\"5-4-温度超参和相似度函数的选择\"><a href=\"#5-4-温度超参和相似度函数的选择\" class=\"headerlink\" title=\"5.4 温度超参和相似度函数的选择\"></a>5.4 温度超参和相似度函数的选择</h3><ul>\n<li>作者尝试使用了不同的<script type=\"math/tex\">\\tau</script>超参，并且尝试用点积代替余弦相似度，结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810194557958.png\" alt=\"image-20220810194557958\"></p>\n<p>N\\A表示点积代替余弦相似度</p>\n<ul>\n<li>发现<strong>使用余弦相似度更合适，并且<script type=\"math/tex\">\\tau=0.05</script>表现最好</strong></li>\n</ul>\n<h1 id=\"6-下游任务上的表现\"><a href=\"#6-下游任务上的表现\" class=\"headerlink\" title=\"6 下游任务上的表现\"></a>6 下游任务上的表现</h1><ul>\n<li>作者还在各种下游任务上进行了对比，并且加上了MLM任务（BERT中的MLM任务），<strong>避免模型彻底的忘记token-level的知识，并发现加上MLM后可以在除STS任务外的其他下游任务上取得提升</strong>，加上MLM后，训练目标由原本的<script type=\"math/tex\">\\ell</script>变成了<script type=\"math/tex\">\\ell + \\lambda \\cdot \\ell ^{MLM}</script></li>\n</ul>\n<h3 id=\"6-1-MLM的对比\"><a href=\"#6-1-MLM的对比\" class=\"headerlink\" title=\"6.1 MLM的对比\"></a>6.1 MLM的对比</h3><ul>\n<li>作者对比了在STS任务和其他下游任务上，加与不加MLM的结果对比，以及<script type=\"math/tex\">\\lambda</script>超参的选择：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810195101507.png\" alt=\"image-20220810195101507\"></p>\n<ul>\n<li>结果说明，<strong>添加token-level对于其他大多数下游任务都有提升，并且<script type=\"math/tex\">\\lambda=0.1</script>最为合适，但是这会带来STS任务表现的下降</strong></li>\n</ul>\n<h3 id=\"6-2-下游任务的对比\"><a href=\"#6-2-下游任务的对比\" class=\"headerlink\" title=\"6.2 下游任务的对比\"></a>6.2 下游任务的对比</h3><ul>\n<li>最后作者给出了在各种模型、训练策略、处理方式等因素不同时，在各种下游任务上的表现：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220810195709648.png\" alt=\"image-20220810195709648\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>可以发现在迁移任务上该方法并没有做到最好，不过这也证明了作者的说法，句子级别的目标可能并不会有益于下游任务的训练，训练好的句子向量表示模型也并不是为了更好的适应下游任务，但是SimCSE也在许多任务上做到了SOTA，特别是带MLM的时候</li>\n</ul>\n"},{"title":"T5 && mT5","math":true,"date":"2022-11-21T16:00:00.000Z","_content":"\n\n\n- T5（Text-to-Text Transfer Transformer）模型采用了一种Text-to-text（文本到文本）的框架，想要把NLP领域的许多常见任务，如文本分类、QA等，都套到这个框架中解决\n- 如机器翻译任务，输入”translate English to German: That is good.”，目标输出是”Das ist gut.”，在输入中” : “前面称为prompt，代指现在需要执行的任务\n- 这样的好处是可以把所有的问题都套进去一个统一的范式，从而可以采用同样的模型架构、同样的训练策略、同样的损失函数、同样的解码手段。\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126211139471.png\" alt=\"image-20221126211139471\" style=\"zoom:80%;\" />\n\n- 这里我们讲的是T51.0，之后再介绍T51.1的不同点\n\n\n\n### 1.1 C4数据集\n\n- C4全称Colossal Clean Crawled Corpus，跟GPT2、GPT3的训练数据来源一样，是从网上爬的文本数据，由于是爬的数据，所以数据量足够大，而且类型丰富，缺点是数据质量差，需要过滤，过滤手段包括\n- 经过一番数据清洗后得到了一个750G的数据集\n\n\n\n### 1.2 Baseline\n\n- 其实T5就是经过很多个实验，对不同的模型结构、训练策略等进行对比，然后挑出效果最好的，所以我们先给出实验中用于对比的baseline\n- **模型结构：**和Transformer一模一样，12层的encoder-decoder架构\n- **训练/预测策略：**训练时采用teacher-forcing，预测时采用贪婪搜索\n- **预训练：**在C4上面训练$$2^{19}$$个steps，batch_size=128，seq_len=512。**预训练并没有覆盖所有C4数据集，即没一个样本会重复训练**。预训练目标稍后介绍\n- **学习率调整：**采用平方根倒数：\n\n$$\nl r=\\frac{1}{\\sqrt{\\max (n, k)}}, k=10^{4}\n$$\n\n- **微调：**对每个下游任务训练$$2^{18}$$个steps\n- **词表：**采用WordPiece，大约有32000个token，有部分非英语词\n\n\n\n### 1.3 无监督预训练目标\n\n- 预训练目标和BERT一样，都是采用随机mask破坏文本，然后通过上下文将这个词训练出来，称为**Denoising**的预训练目标\n- **对输入随机挑选15%的token，然后使用一个哨兵token进行替换，注意挑选出来的token如果时连续的text span，则只用一个哨兵token进行替换。然后target文本变为：每个哨兵token+其对应的值的形式，最后再接一个特殊的哨兵token，表示结束**\n- 举例栗子：\n\n![image-20221126223744785](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126223744785.png)\n\n如上图，将for inviting和last分别替换成了两个不同的哨兵token，然后target变为了分别预测每个哨兵token，然后文本最后预测出另一个哨兵token\\，表示结束\n\n\n\n### 1.4 不同模型结构的对比\n\n- 针对self-atttion，有三种mask方式：\n\n![image-20221126230434585](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126230434585.png)\n\n分别为：\n\n> 1. fully-visible：每个时间步都对其他时间步可见\n> 2. causal：对未来的时间步不可见\n> 3. causal with prefix：前面两者的结合，prefix部分的token能看到prefix所有token的信息，非prefix的token只能看到它的上文信息。那么什么是prefix，如上面提到的英文翻译德文的例子，prefix就是”translate English to German: That is good.”，说白了就是输入部分的时间步是fully-visible，输出部分的时间步是causal\n\n- 针对三种不一样的mask方式，作者对如下三种模型架构进行了比较：\n\n![image-20221126230815635](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126230815635.png)\n\n- 对此，作者提出了五种模型（为了公平，只有两个模型有同样数量的参数或者同样的计算效率才能进行比较）：\n\n> 1. **Encoder-decoder：**编码层和解码层各有L层\n> 2. **Enc-dec, shared：**编码层和解码层各有L层，但它们参数共享，所以参数减半\n> 3. **Enc-dec, 6 layers：**编码层和解码层各有L/2层\n> 4. **LM：**只有L层解码层，采用语言模型的形式\n> 5. **Prefix LM：**只有L层解码层，但采用Prefix语言模型的形式\n\n- 并且还对比使用了两种预训练目标：\n\n> 1. **Denoising：**即baseline中使用的随机mask词然后预测出来\n> 2. **LM：**LM中常用的自回归预测，即每个时间步预测通过上个时间步的输出来进行当前时间步的输出预测\n\n- 最后得到以下结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127105905994.png\" alt=\"image-20221127105905994\" style=\"zoom:67%;\" />\n\n\n\n### 1.5 不同的无监督预训练目标对比\n\n- 首先介绍采用的预训练目标：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127110614966.png\" alt=\"image-20221127110614966\" style=\"zoom: 80%;\" />\n\n> 1. **Prefix LM：**输入前部分文本，预测剩余的文本\n> 2. **BERT-style：**利用和BERT中一样的mask策略，然后预测出原文本\n> 3. **Deshuffling：**随机打乱文本，然后预测出原文本\n> 4. **MASS-style：**和BERT-style的不同在于，mask时直接用[M]替换\n> 5. **noise, replace spans：**前文提到的无监督预训练目标\n> 6. **noise, drop tokens：**和5差不多，但是不用哨兵token替换，直接drop\n> 7. **Random spans：**和5差不多，但是每次选择的是一个长为3的text span\n\n- 作者首先对前三种目标进行对比：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112236904.png\" alt=\"image-20221127112236904\" style=\"zoom:80%;\" />\n\n- 结果发现BERT-style效果最好，然后再使用余下的方法和BERT-style进行比较：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112315271.png\" alt=\"image-20221127112315271\" style=\"zoom:80%;\" />\n\n**其实这三种和BERT-style差不多，但是后两种不需要预测出整个原文本，更快，**\n\n- 此外，作者还对比了**不同的文本corruption率和允许的最长text span的长度**（由于连续的mask掉的token都处理为一个哨兵token，允许最长的text span即指最多只有3个token可以替换成一个哨兵token，超过三个要使用另一个哨兵token）\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112647149.png\" alt=\"image-20221127112647149\" style=\"zoom:80%;\" />\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112657299.png\" alt=\"image-20221127112657299\" style=\"zoom:80%;\" />\n\n- 最后对这部分实验做个总结，作者是逐层递进来进行的对比试验：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112910646.png\" alt=\"image-20221127112910646\" style=\"zoom:80%;\" />\n\n\n\n### 1.6 数据集对比\n\n- 作者还对不同类型和不同大小的预训练数据集进行了对比\n- 最后得出：**用更专的数据来做预训练，对下游任务的提升越明显，或者换句更准确的话来说，预训练的语料跟任务语料domain越接近，效果越好，并且数据越多越好，即使预训练不能覆盖完**\n- 所以个人认为最佳的策略是**在丰富的数据上进行预训练，然后再在领域相关、任务相关的语料上继续预训练，最后再fine-tuning**\n\n\n\n### 1.7 训练策略\n\n- **fine-tuning方法：**作者对三种微调方法进行了对比：\n\n> 1. **All parameters：**微调时更新所有参数\n> 2. **Adapter layers：**adapter layers接在编码器和解码器的每一个block的全连接层后面，在fine-tuning的时候只更新它们。adapter layers有一个内部维度d作为超参\n> 3. **Gradual unfreezing：**一开始离任务层近的参数先更新，其它保持不动，随着训练的进行，逐渐放开其它层的参数。\n\n实验结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127115209871.png\" alt=\"image-20221127115209871\" style=\"zoom:80%;\" />\n\n发现还是更新所有参数效果最好，但是会慢很多\n\n- **多任务学习：**得益于提出的text-to-text范式，我们可以**在预训练的时候把有监督的训练也加进来，一起做预训练（注意：多任务学习预训练中的数据集包括原本的无监督数据集+多个有监督数据集）**。现在问题就变为了**给定多个不同任务的数据集，怎样对数据进行采样**，作者使用了以下三种策略：\n\n> 1. **Examples-proportional mixing：**设第$$i$$个任务的数据集大小为$$e_i$$，那么采样自第$$j$$个数据集的概率为$$r_j=\\min(e_j,K)/∑_i\\min(e_i,K)$$，其中K为提前设置好的超参\n> 2. **Temperature-scaled mixing：**在上面的策略下，再做一些软化，具体来说就是求得$$r_j$$后再开1/T方根，T为提前设置好的超参，T越大，各个任务数据集采样越均衡\n> 3. **Equal mixing：**各数据集均匀采样\n\n实验结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127120459146.png\" alt=\"image-20221127120459146\" style=\"zoom:80%;\" />\n\n实验结果都一般，**不过注意，这里的多任务学习是多个任务一起做训练，相当于把pre-training和fine-tuning两个合并了，而不会对单个任务进行fine-tuning**，所以效果不好也可以理解。\n\n- **多任务学习+fine-tuning：**作者采用了一下集中训练策略进行比较：\n\n> 1. **Unsupervised pre-training + fine-tuning：**baseline中使用的方法，先无监督预训练再在特定的下游任务上微调\n> 2. **Multi-task training：**直接在多任务数据集上训练（注意mutl-task的训练集中有有监督的也有无监督的）\n> 3. **Multi-task pre-training + fine-tuning：**多任务预训练+微调\n> 4. **Leave-one-out multi-task training：**在预训练的时候同样使用多任务，但是要去除和下游任务相关的那个数据集，然后再在下游任务微调\n> 5. **Supervised multi-task pre-training：**在多任务预训练的时候把无监督任务剔除掉，然后再微调\n\n实验结果：\n\n![image-20221128120902619](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221128120902619.png)\n\n通过实验结果得到以下结论：\n\n> 1. 使用Multi-task pre-training + fine-tuning的结果和baseline差不多，表明**在多任务后使用微调可以减轻不同的数据集mixing比例之间的权衡**，即你选用的Mixing方式（比如上文的三种）不一定是最好的，但是微调可以减轻这种错选带来的干扰\n> 2. Leave-one-out的效果只有一点点下降，表明**多任务学习不会导致严重的任务干扰**\n> 3. 使用Supervised multi-task pre-training几乎除了翻译任务都导致了下降，表明**翻译任务从英语预训练中学得的很少，反之其他任务仍然很依赖无监督预训练**\n\n\n\n### 1.8 Scaling\n\n- 此外，作者还对模型规模等进行了测试，得出**使用更多的数据、训练更大的模型、模型融合都能提高性能**\n- 最后提一句，经过一系列实验，T5还是选择了**Multi-task pre-training + fine-tuning**以及预测时采用束搜索，无监督预训练目标采用了**noise, replace spans**\n\n\n\n\n\n# 2 T5 v1.1\n\n- 上文讲的是T5 v1.0，谷歌之后又发布了一个T5 v1.1，只有一些细微差别，改进如下：\n\n> 1. 前馈神经层的激活函数由ReLU改为了GEGLU\n> 2. 在pre-training的时候关闭Dropout，在微调的时候重新开启\n> 3. 预训练的时候只使用C4数据集，而不混入下游数据集\n> 4. Embedding层和最后的分类层没有使用Weight Tying\n> 5. 模型形状有点不同，较大的 d_model 和较小的 num_heads 和 d_ff\n\n\n\n\n\n# 3 mT5\n\n- mT5的预训练目标和策略等等和T5基本相同， 值得注意的是mT5使用的是T5 v1.1\n\n\n\n### 3.1 mC4数据集\n\n- 一个多语言版的C4数据集，但是使用的数据清洗方法和T5不同：\n- 对于多语言模型，一个很重要的部分是如何多多种语言进行采样，**不同语种数据占比不同，有的语言样本少（low-resource languages ），如果不常采样到，模型就会由于样本过少而过拟合；如果样本量太大（high-resource languages ），内容丰富，模型又可能欠拟合，所以不能让模型遍历太多high-resource languages**\n- 要解决上述问题，直观上来说可以使用均匀分布来采样，但是使用均匀分布效果肯定比较差，因为很多high-resource languages 压根用不到\n- 所以采用了：\n\n$$\nP(L) \\propto L^{\\alpha}\n$$\n\n其中L为对应语言的样本数，$$\\alpha \\in [0,1]$$为超参，$$\\alpha$$越小分布越接近均匀分布，**mT5经过实验发现$$\\alpha=0.3$$最合适**。那么这样就可以**适当提升low-resource languages的采样概率而适当减少high-resource languages的采样概率**\n\n- mC4中不同语言的样本数，以及使用不同$$\\alpha$$的采样概率：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129191709077.png\" alt=\"image-20221129191709077\" style=\"zoom:80%;\" />\n\n\n\n### 3.2 微调策略\n\n- 模型在mC4上预训练之后，作者采用了一下三种微调方式进行对比（微调采用lr = 0.001）：\n\n> 1. **zero-shot：**仅在英语训练集上微调\n> 2. **translate-train：**在英语+由英语翻译到所有目标语言的数据集上微调\n> 3. **in-language multitask：**在目标语言的gold data上微调（这里是真实的人工表述的数据，而tanslate-train的目标语言数据是翻译过来的）\n\n结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193118134.png\" alt=\"image-20221129193118134\" style=\"zoom:80%;\" />\n\n- 此外，作者还对比了采用不同的模型参数量对这三种微调方式的提升：\n\n![image-20221129193253828](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193253828.png)\n\n\n\n### 3.3 T5 vs mT5\n\n- 作者还对比了T5和mT5在英语QA任务上的效果差异：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193755886.png\" alt=\"image-20221129193755886\" style=\"zoom:80%;\" />\n\n- 发现**mT5还是略逊色于T5，但是随着模型规模的增大，之间的差异越来越小。也证明了多语言模型同样有能力比肩单语言模型**\n\n\n\n### 3.4 消融实验\n\n- 作者还对训练的各方面进行了一些消融实验，策略如下：\n\n> 1. **Dropout 0.1：**由于使用的是T5 v1.1，所以在预训练时没有使用Dropout，这里为了对照又把Dropout加上了\n> 2. **Sequence length 512：**将最大序列长度减少为512\n> 3. **Span length 10：**将连续token的长度由3变为10\n> 4. **$$\\alpha=0.7,0.2$$：**采样时的超参改一下\n> 5. **No line length filter：**数据清洗时的策略改一下\n> 6. **Add Wikipedia data：**预训练使用mC4+Wikipedia data\n\n\n\n### 3.5 zero-shot微调策略的问题\n\n- 采用zero-shot会造成预测时产生一些非法输出：\n\n> 1. **Normalization：**prediction是合法的，但是unicode characters被替代了，可以通过Unicode NFKC normalization来恢复\n> 2. **Grammatical adjustment：**answer本身就存在语法问题\n> 3. **Accidental translation：**模型直接做了翻译，将目标语言翻译成英文了，以至于生成部分或者完整英文\n>\n> 同时，在一些短语生成的时候，出现正确答案之前可能会先预测出两个英语词\n>\n> 上面最常出现的是Accidental translation\n\n以下是非法输出的一些栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129195942926.png\" alt=\"image-20221129195942926\" style=\"zoom:80%;\" />\n\n- **产生原因：**模型在微调的时候压根没有接触过non-English的target文本，在non-English上做推理时，non-English的likelihood会降低，以至于English变成最可能的输出\n- **解决方法：**在微调时再次使用**少量的mC4数据进行无监督二次预训练**（和微调的样本数比例是1：100，并且包含全部101种语言），并且二次预训练时**删除了target文本中的哨兵token**，因为最后的结果发现在下游任务时就偶尔会预测出哨兵token，然后还将α从0.3降为0.1，**使采样分布十分近似于均匀分布**。结果提升显著：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129201243017.png\" alt=\"image-20221129201243017\" style=\"zoom:80%;\" />","source":"_posts/T5 && mT5.md","raw":"---\ntitle: T5 && mT5\nmath: true\ndate: 2022-11-22\n---\n\n\n\n- T5（Text-to-Text Transfer Transformer）模型采用了一种Text-to-text（文本到文本）的框架，想要把NLP领域的许多常见任务，如文本分类、QA等，都套到这个框架中解决\n- 如机器翻译任务，输入”translate English to German: That is good.”，目标输出是”Das ist gut.”，在输入中” : “前面称为prompt，代指现在需要执行的任务\n- 这样的好处是可以把所有的问题都套进去一个统一的范式，从而可以采用同样的模型架构、同样的训练策略、同样的损失函数、同样的解码手段。\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126211139471.png\" alt=\"image-20221126211139471\" style=\"zoom:80%;\" />\n\n- 这里我们讲的是T51.0，之后再介绍T51.1的不同点\n\n\n\n### 1.1 C4数据集\n\n- C4全称Colossal Clean Crawled Corpus，跟GPT2、GPT3的训练数据来源一样，是从网上爬的文本数据，由于是爬的数据，所以数据量足够大，而且类型丰富，缺点是数据质量差，需要过滤，过滤手段包括\n- 经过一番数据清洗后得到了一个750G的数据集\n\n\n\n### 1.2 Baseline\n\n- 其实T5就是经过很多个实验，对不同的模型结构、训练策略等进行对比，然后挑出效果最好的，所以我们先给出实验中用于对比的baseline\n- **模型结构：**和Transformer一模一样，12层的encoder-decoder架构\n- **训练/预测策略：**训练时采用teacher-forcing，预测时采用贪婪搜索\n- **预训练：**在C4上面训练$$2^{19}$$个steps，batch_size=128，seq_len=512。**预训练并没有覆盖所有C4数据集，即没一个样本会重复训练**。预训练目标稍后介绍\n- **学习率调整：**采用平方根倒数：\n\n$$\nl r=\\frac{1}{\\sqrt{\\max (n, k)}}, k=10^{4}\n$$\n\n- **微调：**对每个下游任务训练$$2^{18}$$个steps\n- **词表：**采用WordPiece，大约有32000个token，有部分非英语词\n\n\n\n### 1.3 无监督预训练目标\n\n- 预训练目标和BERT一样，都是采用随机mask破坏文本，然后通过上下文将这个词训练出来，称为**Denoising**的预训练目标\n- **对输入随机挑选15%的token，然后使用一个哨兵token进行替换，注意挑选出来的token如果时连续的text span，则只用一个哨兵token进行替换。然后target文本变为：每个哨兵token+其对应的值的形式，最后再接一个特殊的哨兵token，表示结束**\n- 举例栗子：\n\n![image-20221126223744785](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126223744785.png)\n\n如上图，将for inviting和last分别替换成了两个不同的哨兵token，然后target变为了分别预测每个哨兵token，然后文本最后预测出另一个哨兵token\\，表示结束\n\n\n\n### 1.4 不同模型结构的对比\n\n- 针对self-atttion，有三种mask方式：\n\n![image-20221126230434585](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126230434585.png)\n\n分别为：\n\n> 1. fully-visible：每个时间步都对其他时间步可见\n> 2. causal：对未来的时间步不可见\n> 3. causal with prefix：前面两者的结合，prefix部分的token能看到prefix所有token的信息，非prefix的token只能看到它的上文信息。那么什么是prefix，如上面提到的英文翻译德文的例子，prefix就是”translate English to German: That is good.”，说白了就是输入部分的时间步是fully-visible，输出部分的时间步是causal\n\n- 针对三种不一样的mask方式，作者对如下三种模型架构进行了比较：\n\n![image-20221126230815635](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126230815635.png)\n\n- 对此，作者提出了五种模型（为了公平，只有两个模型有同样数量的参数或者同样的计算效率才能进行比较）：\n\n> 1. **Encoder-decoder：**编码层和解码层各有L层\n> 2. **Enc-dec, shared：**编码层和解码层各有L层，但它们参数共享，所以参数减半\n> 3. **Enc-dec, 6 layers：**编码层和解码层各有L/2层\n> 4. **LM：**只有L层解码层，采用语言模型的形式\n> 5. **Prefix LM：**只有L层解码层，但采用Prefix语言模型的形式\n\n- 并且还对比使用了两种预训练目标：\n\n> 1. **Denoising：**即baseline中使用的随机mask词然后预测出来\n> 2. **LM：**LM中常用的自回归预测，即每个时间步预测通过上个时间步的输出来进行当前时间步的输出预测\n\n- 最后得到以下结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127105905994.png\" alt=\"image-20221127105905994\" style=\"zoom:67%;\" />\n\n\n\n### 1.5 不同的无监督预训练目标对比\n\n- 首先介绍采用的预训练目标：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127110614966.png\" alt=\"image-20221127110614966\" style=\"zoom: 80%;\" />\n\n> 1. **Prefix LM：**输入前部分文本，预测剩余的文本\n> 2. **BERT-style：**利用和BERT中一样的mask策略，然后预测出原文本\n> 3. **Deshuffling：**随机打乱文本，然后预测出原文本\n> 4. **MASS-style：**和BERT-style的不同在于，mask时直接用[M]替换\n> 5. **noise, replace spans：**前文提到的无监督预训练目标\n> 6. **noise, drop tokens：**和5差不多，但是不用哨兵token替换，直接drop\n> 7. **Random spans：**和5差不多，但是每次选择的是一个长为3的text span\n\n- 作者首先对前三种目标进行对比：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112236904.png\" alt=\"image-20221127112236904\" style=\"zoom:80%;\" />\n\n- 结果发现BERT-style效果最好，然后再使用余下的方法和BERT-style进行比较：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112315271.png\" alt=\"image-20221127112315271\" style=\"zoom:80%;\" />\n\n**其实这三种和BERT-style差不多，但是后两种不需要预测出整个原文本，更快，**\n\n- 此外，作者还对比了**不同的文本corruption率和允许的最长text span的长度**（由于连续的mask掉的token都处理为一个哨兵token，允许最长的text span即指最多只有3个token可以替换成一个哨兵token，超过三个要使用另一个哨兵token）\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112647149.png\" alt=\"image-20221127112647149\" style=\"zoom:80%;\" />\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112657299.png\" alt=\"image-20221127112657299\" style=\"zoom:80%;\" />\n\n- 最后对这部分实验做个总结，作者是逐层递进来进行的对比试验：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112910646.png\" alt=\"image-20221127112910646\" style=\"zoom:80%;\" />\n\n\n\n### 1.6 数据集对比\n\n- 作者还对不同类型和不同大小的预训练数据集进行了对比\n- 最后得出：**用更专的数据来做预训练，对下游任务的提升越明显，或者换句更准确的话来说，预训练的语料跟任务语料domain越接近，效果越好，并且数据越多越好，即使预训练不能覆盖完**\n- 所以个人认为最佳的策略是**在丰富的数据上进行预训练，然后再在领域相关、任务相关的语料上继续预训练，最后再fine-tuning**\n\n\n\n### 1.7 训练策略\n\n- **fine-tuning方法：**作者对三种微调方法进行了对比：\n\n> 1. **All parameters：**微调时更新所有参数\n> 2. **Adapter layers：**adapter layers接在编码器和解码器的每一个block的全连接层后面，在fine-tuning的时候只更新它们。adapter layers有一个内部维度d作为超参\n> 3. **Gradual unfreezing：**一开始离任务层近的参数先更新，其它保持不动，随着训练的进行，逐渐放开其它层的参数。\n\n实验结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127115209871.png\" alt=\"image-20221127115209871\" style=\"zoom:80%;\" />\n\n发现还是更新所有参数效果最好，但是会慢很多\n\n- **多任务学习：**得益于提出的text-to-text范式，我们可以**在预训练的时候把有监督的训练也加进来，一起做预训练（注意：多任务学习预训练中的数据集包括原本的无监督数据集+多个有监督数据集）**。现在问题就变为了**给定多个不同任务的数据集，怎样对数据进行采样**，作者使用了以下三种策略：\n\n> 1. **Examples-proportional mixing：**设第$$i$$个任务的数据集大小为$$e_i$$，那么采样自第$$j$$个数据集的概率为$$r_j=\\min(e_j,K)/∑_i\\min(e_i,K)$$，其中K为提前设置好的超参\n> 2. **Temperature-scaled mixing：**在上面的策略下，再做一些软化，具体来说就是求得$$r_j$$后再开1/T方根，T为提前设置好的超参，T越大，各个任务数据集采样越均衡\n> 3. **Equal mixing：**各数据集均匀采样\n\n实验结果：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127120459146.png\" alt=\"image-20221127120459146\" style=\"zoom:80%;\" />\n\n实验结果都一般，**不过注意，这里的多任务学习是多个任务一起做训练，相当于把pre-training和fine-tuning两个合并了，而不会对单个任务进行fine-tuning**，所以效果不好也可以理解。\n\n- **多任务学习+fine-tuning：**作者采用了一下集中训练策略进行比较：\n\n> 1. **Unsupervised pre-training + fine-tuning：**baseline中使用的方法，先无监督预训练再在特定的下游任务上微调\n> 2. **Multi-task training：**直接在多任务数据集上训练（注意mutl-task的训练集中有有监督的也有无监督的）\n> 3. **Multi-task pre-training + fine-tuning：**多任务预训练+微调\n> 4. **Leave-one-out multi-task training：**在预训练的时候同样使用多任务，但是要去除和下游任务相关的那个数据集，然后再在下游任务微调\n> 5. **Supervised multi-task pre-training：**在多任务预训练的时候把无监督任务剔除掉，然后再微调\n\n实验结果：\n\n![image-20221128120902619](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221128120902619.png)\n\n通过实验结果得到以下结论：\n\n> 1. 使用Multi-task pre-training + fine-tuning的结果和baseline差不多，表明**在多任务后使用微调可以减轻不同的数据集mixing比例之间的权衡**，即你选用的Mixing方式（比如上文的三种）不一定是最好的，但是微调可以减轻这种错选带来的干扰\n> 2. Leave-one-out的效果只有一点点下降，表明**多任务学习不会导致严重的任务干扰**\n> 3. 使用Supervised multi-task pre-training几乎除了翻译任务都导致了下降，表明**翻译任务从英语预训练中学得的很少，反之其他任务仍然很依赖无监督预训练**\n\n\n\n### 1.8 Scaling\n\n- 此外，作者还对模型规模等进行了测试，得出**使用更多的数据、训练更大的模型、模型融合都能提高性能**\n- 最后提一句，经过一系列实验，T5还是选择了**Multi-task pre-training + fine-tuning**以及预测时采用束搜索，无监督预训练目标采用了**noise, replace spans**\n\n\n\n\n\n# 2 T5 v1.1\n\n- 上文讲的是T5 v1.0，谷歌之后又发布了一个T5 v1.1，只有一些细微差别，改进如下：\n\n> 1. 前馈神经层的激活函数由ReLU改为了GEGLU\n> 2. 在pre-training的时候关闭Dropout，在微调的时候重新开启\n> 3. 预训练的时候只使用C4数据集，而不混入下游数据集\n> 4. Embedding层和最后的分类层没有使用Weight Tying\n> 5. 模型形状有点不同，较大的 d_model 和较小的 num_heads 和 d_ff\n\n\n\n\n\n# 3 mT5\n\n- mT5的预训练目标和策略等等和T5基本相同， 值得注意的是mT5使用的是T5 v1.1\n\n\n\n### 3.1 mC4数据集\n\n- 一个多语言版的C4数据集，但是使用的数据清洗方法和T5不同：\n- 对于多语言模型，一个很重要的部分是如何多多种语言进行采样，**不同语种数据占比不同，有的语言样本少（low-resource languages ），如果不常采样到，模型就会由于样本过少而过拟合；如果样本量太大（high-resource languages ），内容丰富，模型又可能欠拟合，所以不能让模型遍历太多high-resource languages**\n- 要解决上述问题，直观上来说可以使用均匀分布来采样，但是使用均匀分布效果肯定比较差，因为很多high-resource languages 压根用不到\n- 所以采用了：\n\n$$\nP(L) \\propto L^{\\alpha}\n$$\n\n其中L为对应语言的样本数，$$\\alpha \\in [0,1]$$为超参，$$\\alpha$$越小分布越接近均匀分布，**mT5经过实验发现$$\\alpha=0.3$$最合适**。那么这样就可以**适当提升low-resource languages的采样概率而适当减少high-resource languages的采样概率**\n\n- mC4中不同语言的样本数，以及使用不同$$\\alpha$$的采样概率：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129191709077.png\" alt=\"image-20221129191709077\" style=\"zoom:80%;\" />\n\n\n\n### 3.2 微调策略\n\n- 模型在mC4上预训练之后，作者采用了一下三种微调方式进行对比（微调采用lr = 0.001）：\n\n> 1. **zero-shot：**仅在英语训练集上微调\n> 2. **translate-train：**在英语+由英语翻译到所有目标语言的数据集上微调\n> 3. **in-language multitask：**在目标语言的gold data上微调（这里是真实的人工表述的数据，而tanslate-train的目标语言数据是翻译过来的）\n\n结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193118134.png\" alt=\"image-20221129193118134\" style=\"zoom:80%;\" />\n\n- 此外，作者还对比了采用不同的模型参数量对这三种微调方式的提升：\n\n![image-20221129193253828](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193253828.png)\n\n\n\n### 3.3 T5 vs mT5\n\n- 作者还对比了T5和mT5在英语QA任务上的效果差异：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193755886.png\" alt=\"image-20221129193755886\" style=\"zoom:80%;\" />\n\n- 发现**mT5还是略逊色于T5，但是随着模型规模的增大，之间的差异越来越小。也证明了多语言模型同样有能力比肩单语言模型**\n\n\n\n### 3.4 消融实验\n\n- 作者还对训练的各方面进行了一些消融实验，策略如下：\n\n> 1. **Dropout 0.1：**由于使用的是T5 v1.1，所以在预训练时没有使用Dropout，这里为了对照又把Dropout加上了\n> 2. **Sequence length 512：**将最大序列长度减少为512\n> 3. **Span length 10：**将连续token的长度由3变为10\n> 4. **$$\\alpha=0.7,0.2$$：**采样时的超参改一下\n> 5. **No line length filter：**数据清洗时的策略改一下\n> 6. **Add Wikipedia data：**预训练使用mC4+Wikipedia data\n\n\n\n### 3.5 zero-shot微调策略的问题\n\n- 采用zero-shot会造成预测时产生一些非法输出：\n\n> 1. **Normalization：**prediction是合法的，但是unicode characters被替代了，可以通过Unicode NFKC normalization来恢复\n> 2. **Grammatical adjustment：**answer本身就存在语法问题\n> 3. **Accidental translation：**模型直接做了翻译，将目标语言翻译成英文了，以至于生成部分或者完整英文\n>\n> 同时，在一些短语生成的时候，出现正确答案之前可能会先预测出两个英语词\n>\n> 上面最常出现的是Accidental translation\n\n以下是非法输出的一些栗子：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129195942926.png\" alt=\"image-20221129195942926\" style=\"zoom:80%;\" />\n\n- **产生原因：**模型在微调的时候压根没有接触过non-English的target文本，在non-English上做推理时，non-English的likelihood会降低，以至于English变成最可能的输出\n- **解决方法：**在微调时再次使用**少量的mC4数据进行无监督二次预训练**（和微调的样本数比例是1：100，并且包含全部101种语言），并且二次预训练时**删除了target文本中的哨兵token**，因为最后的结果发现在下游任务时就偶尔会预测出哨兵token，然后还将α从0.3降为0.1，**使采样分布十分近似于均匀分布**。结果提升显著：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129201243017.png\" alt=\"image-20221129201243017\" style=\"zoom:80%;\" />","slug":"T5 && mT5","published":1,"updated":"2023-02-07T05:51:57.996Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1h000d7csz4eg71rvf","content":"<ul>\n<li>T5（Text-to-Text Transfer Transformer）模型采用了一种Text-to-text（文本到文本）的框架，想要把NLP领域的许多常见任务，如文本分类、QA等，都套到这个框架中解决</li>\n<li>如机器翻译任务，输入”translate English to German: That is good.”，目标输出是”Das ist gut.”，在输入中” : “前面称为prompt，代指现在需要执行的任务</li>\n<li>这样的好处是可以把所有的问题都套进去一个统一的范式，从而可以采用同样的模型架构、同样的训练策略、同样的损失函数、同样的解码手段。</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126211139471.png\" alt=\"image-20221126211139471\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>这里我们讲的是T51.0，之后再介绍T51.1的不同点</li>\n</ul>\n<h3 id=\"1-1-C4数据集\"><a href=\"#1-1-C4数据集\" class=\"headerlink\" title=\"1.1 C4数据集\"></a>1.1 C4数据集</h3><ul>\n<li>C4全称Colossal Clean Crawled Corpus，跟GPT2、GPT3的训练数据来源一样，是从网上爬的文本数据，由于是爬的数据，所以数据量足够大，而且类型丰富，缺点是数据质量差，需要过滤，过滤手段包括</li>\n<li>经过一番数据清洗后得到了一个750G的数据集</li>\n</ul>\n<h3 id=\"1-2-Baseline\"><a href=\"#1-2-Baseline\" class=\"headerlink\" title=\"1.2 Baseline\"></a>1.2 Baseline</h3><ul>\n<li>其实T5就是经过很多个实验，对不同的模型结构、训练策略等进行对比，然后挑出效果最好的，所以我们先给出实验中用于对比的baseline</li>\n<li><strong>模型结构：</strong>和Transformer一模一样，12层的encoder-decoder架构</li>\n<li><strong>训练/预测策略：</strong>训练时采用teacher-forcing，预测时采用贪婪搜索</li>\n<li><strong>预训练：</strong>在C4上面训练<script type=\"math/tex\">2^{19}</script>个steps，batch_size=128，seq_len=512。<strong>预训练并没有覆盖所有C4数据集，即没一个样本会重复训练</strong>。预训练目标稍后介绍</li>\n<li><strong>学习率调整：</strong>采用平方根倒数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nl r=\\frac{1}{\\sqrt{\\max (n, k)}}, k=10^{4}</script><ul>\n<li><strong>微调：</strong>对每个下游任务训练<script type=\"math/tex\">2^{18}</script>个steps</li>\n<li><strong>词表：</strong>采用WordPiece，大约有32000个token，有部分非英语词</li>\n</ul>\n<h3 id=\"1-3-无监督预训练目标\"><a href=\"#1-3-无监督预训练目标\" class=\"headerlink\" title=\"1.3 无监督预训练目标\"></a>1.3 无监督预训练目标</h3><ul>\n<li>预训练目标和BERT一样，都是采用随机mask破坏文本，然后通过上下文将这个词训练出来，称为<strong>Denoising</strong>的预训练目标</li>\n<li><strong>对输入随机挑选15%的token，然后使用一个哨兵token进行替换，注意挑选出来的token如果时连续的text span，则只用一个哨兵token进行替换。然后target文本变为：每个哨兵token+其对应的值的形式，最后再接一个特殊的哨兵token，表示结束</strong></li>\n<li>举例栗子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126223744785.png\" alt=\"image-20221126223744785\"></p>\n<p>如上图，将for inviting和last分别替换成了两个不同的哨兵token，然后target变为了分别预测每个哨兵token，然后文本最后预测出另一个哨兵token\\，表示结束</p>\n<h3 id=\"1-4-不同模型结构的对比\"><a href=\"#1-4-不同模型结构的对比\" class=\"headerlink\" title=\"1.4 不同模型结构的对比\"></a>1.4 不同模型结构的对比</h3><ul>\n<li>针对self-atttion，有三种mask方式：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126230434585.png\" alt=\"image-20221126230434585\"></p>\n<p>分别为：</p>\n<blockquote>\n<ol>\n<li>fully-visible：每个时间步都对其他时间步可见</li>\n<li>causal：对未来的时间步不可见</li>\n<li>causal with prefix：前面两者的结合，prefix部分的token能看到prefix所有token的信息，非prefix的token只能看到它的上文信息。那么什么是prefix，如上面提到的英文翻译德文的例子，prefix就是”translate English to German: That is good.”，说白了就是输入部分的时间步是fully-visible，输出部分的时间步是causal</li>\n</ol>\n</blockquote>\n<ul>\n<li>针对三种不一样的mask方式，作者对如下三种模型架构进行了比较：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126230815635.png\" alt=\"image-20221126230815635\"></p>\n<ul>\n<li>对此，作者提出了五种模型（为了公平，只有两个模型有同样数量的参数或者同样的计算效率才能进行比较）：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Encoder-decoder：</strong>编码层和解码层各有L层</li>\n<li><strong>Enc-dec, shared：</strong>编码层和解码层各有L层，但它们参数共享，所以参数减半</li>\n<li><strong>Enc-dec, 6 layers：</strong>编码层和解码层各有L/2层</li>\n<li><strong>LM：</strong>只有L层解码层，采用语言模型的形式</li>\n<li><strong>Prefix LM：</strong>只有L层解码层，但采用Prefix语言模型的形式</li>\n</ol>\n</blockquote>\n<ul>\n<li>并且还对比使用了两种预训练目标：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Denoising：</strong>即baseline中使用的随机mask词然后预测出来</li>\n<li><strong>LM：</strong>LM中常用的自回归预测，即每个时间步预测通过上个时间步的输出来进行当前时间步的输出预测</li>\n</ol>\n</blockquote>\n<ul>\n<li>最后得到以下结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127105905994.png\" alt=\"image-20221127105905994\" style=\"zoom:67%;\" /></p>\n<h3 id=\"1-5-不同的无监督预训练目标对比\"><a href=\"#1-5-不同的无监督预训练目标对比\" class=\"headerlink\" title=\"1.5 不同的无监督预训练目标对比\"></a>1.5 不同的无监督预训练目标对比</h3><ul>\n<li>首先介绍采用的预训练目标：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127110614966.png\" alt=\"image-20221127110614966\" style=\"zoom: 80%;\" /></p>\n<blockquote>\n<ol>\n<li><strong>Prefix LM：</strong>输入前部分文本，预测剩余的文本</li>\n<li><strong>BERT-style：</strong>利用和BERT中一样的mask策略，然后预测出原文本</li>\n<li><strong>Deshuffling：</strong>随机打乱文本，然后预测出原文本</li>\n<li><strong>MASS-style：</strong>和BERT-style的不同在于，mask时直接用[M]替换</li>\n<li><strong>noise, replace spans：</strong>前文提到的无监督预训练目标</li>\n<li><strong>noise, drop tokens：</strong>和5差不多，但是不用哨兵token替换，直接drop</li>\n<li><strong>Random spans：</strong>和5差不多，但是每次选择的是一个长为3的text span</li>\n</ol>\n</blockquote>\n<ul>\n<li>作者首先对前三种目标进行对比：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112236904.png\" alt=\"image-20221127112236904\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>结果发现BERT-style效果最好，然后再使用余下的方法和BERT-style进行比较：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112315271.png\" alt=\"image-20221127112315271\" style=\"zoom:80%;\" /></p>\n<p><strong>其实这三种和BERT-style差不多，但是后两种不需要预测出整个原文本，更快，</strong></p>\n<ul>\n<li>此外，作者还对比了<strong>不同的文本corruption率和允许的最长text span的长度</strong>（由于连续的mask掉的token都处理为一个哨兵token，允许最长的text span即指最多只有3个token可以替换成一个哨兵token，超过三个要使用另一个哨兵token）</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112647149.png\" alt=\"image-20221127112647149\" style=\"zoom:80%;\" /></p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112657299.png\" alt=\"image-20221127112657299\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>最后对这部分实验做个总结，作者是逐层递进来进行的对比试验：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112910646.png\" alt=\"image-20221127112910646\" style=\"zoom:80%;\" /></p>\n<h3 id=\"1-6-数据集对比\"><a href=\"#1-6-数据集对比\" class=\"headerlink\" title=\"1.6 数据集对比\"></a>1.6 数据集对比</h3><ul>\n<li>作者还对不同类型和不同大小的预训练数据集进行了对比</li>\n<li>最后得出：<strong>用更专的数据来做预训练，对下游任务的提升越明显，或者换句更准确的话来说，预训练的语料跟任务语料domain越接近，效果越好，并且数据越多越好，即使预训练不能覆盖完</strong></li>\n<li>所以个人认为最佳的策略是<strong>在丰富的数据上进行预训练，然后再在领域相关、任务相关的语料上继续预训练，最后再fine-tuning</strong></li>\n</ul>\n<h3 id=\"1-7-训练策略\"><a href=\"#1-7-训练策略\" class=\"headerlink\" title=\"1.7 训练策略\"></a>1.7 训练策略</h3><ul>\n<li><strong>fine-tuning方法：</strong>作者对三种微调方法进行了对比：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>All parameters：</strong>微调时更新所有参数</li>\n<li><strong>Adapter layers：</strong>adapter layers接在编码器和解码器的每一个block的全连接层后面，在fine-tuning的时候只更新它们。adapter layers有一个内部维度d作为超参</li>\n<li><strong>Gradual unfreezing：</strong>一开始离任务层近的参数先更新，其它保持不动，随着训练的进行，逐渐放开其它层的参数。</li>\n</ol>\n</blockquote>\n<p>实验结果：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127115209871.png\" alt=\"image-20221127115209871\" style=\"zoom:80%;\" /></p>\n<p>发现还是更新所有参数效果最好，但是会慢很多</p>\n<ul>\n<li><strong>多任务学习：</strong>得益于提出的text-to-text范式，我们可以<strong>在预训练的时候把有监督的训练也加进来，一起做预训练（注意：多任务学习预训练中的数据集包括原本的无监督数据集+多个有监督数据集）</strong>。现在问题就变为了<strong>给定多个不同任务的数据集，怎样对数据进行采样</strong>，作者使用了以下三种策略：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Examples-proportional mixing：</strong>设第<script type=\"math/tex\">i</script>个任务的数据集大小为<script type=\"math/tex\">e_i</script>，那么采样自第<script type=\"math/tex\">j</script>个数据集的概率为<script type=\"math/tex\">r_j=\\min(e_j,K)/∑_i\\min(e_i,K)</script>，其中K为提前设置好的超参</li>\n<li><strong>Temperature-scaled mixing：</strong>在上面的策略下，再做一些软化，具体来说就是求得<script type=\"math/tex\">r_j</script>后再开1/T方根，T为提前设置好的超参，T越大，各个任务数据集采样越均衡</li>\n<li><strong>Equal mixing：</strong>各数据集均匀采样</li>\n</ol>\n</blockquote>\n<p>实验结果：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127120459146.png\" alt=\"image-20221127120459146\" style=\"zoom:80%;\" /></p>\n<p>实验结果都一般，<strong>不过注意，这里的多任务学习是多个任务一起做训练，相当于把pre-training和fine-tuning两个合并了，而不会对单个任务进行fine-tuning</strong>，所以效果不好也可以理解。</p>\n<ul>\n<li><strong>多任务学习+fine-tuning：</strong>作者采用了一下集中训练策略进行比较：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Unsupervised pre-training + fine-tuning：</strong>baseline中使用的方法，先无监督预训练再在特定的下游任务上微调</li>\n<li><strong>Multi-task training：</strong>直接在多任务数据集上训练（注意mutl-task的训练集中有有监督的也有无监督的）</li>\n<li><strong>Multi-task pre-training + fine-tuning：</strong>多任务预训练+微调</li>\n<li><strong>Leave-one-out multi-task training：</strong>在预训练的时候同样使用多任务，但是要去除和下游任务相关的那个数据集，然后再在下游任务微调</li>\n<li><strong>Supervised multi-task pre-training：</strong>在多任务预训练的时候把无监督任务剔除掉，然后再微调</li>\n</ol>\n</blockquote>\n<p>实验结果：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221128120902619.png\" alt=\"image-20221128120902619\"></p>\n<p>通过实验结果得到以下结论：</p>\n<blockquote>\n<ol>\n<li>使用Multi-task pre-training + fine-tuning的结果和baseline差不多，表明<strong>在多任务后使用微调可以减轻不同的数据集mixing比例之间的权衡</strong>，即你选用的Mixing方式（比如上文的三种）不一定是最好的，但是微调可以减轻这种错选带来的干扰</li>\n<li>Leave-one-out的效果只有一点点下降，表明<strong>多任务学习不会导致严重的任务干扰</strong></li>\n<li>使用Supervised multi-task pre-training几乎除了翻译任务都导致了下降，表明<strong>翻译任务从英语预训练中学得的很少，反之其他任务仍然很依赖无监督预训练</strong></li>\n</ol>\n</blockquote>\n<h3 id=\"1-8-Scaling\"><a href=\"#1-8-Scaling\" class=\"headerlink\" title=\"1.8 Scaling\"></a>1.8 Scaling</h3><ul>\n<li>此外，作者还对模型规模等进行了测试，得出<strong>使用更多的数据、训练更大的模型、模型融合都能提高性能</strong></li>\n<li>最后提一句，经过一系列实验，T5还是选择了<strong>Multi-task pre-training + fine-tuning</strong>以及预测时采用束搜索，无监督预训练目标采用了<strong>noise, replace spans</strong></li>\n</ul>\n<h1 id=\"2-T5-v1-1\"><a href=\"#2-T5-v1-1\" class=\"headerlink\" title=\"2 T5 v1.1\"></a>2 T5 v1.1</h1><ul>\n<li>上文讲的是T5 v1.0，谷歌之后又发布了一个T5 v1.1，只有一些细微差别，改进如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li>前馈神经层的激活函数由ReLU改为了GEGLU</li>\n<li>在pre-training的时候关闭Dropout，在微调的时候重新开启</li>\n<li>预训练的时候只使用C4数据集，而不混入下游数据集</li>\n<li>Embedding层和最后的分类层没有使用Weight Tying</li>\n<li>模型形状有点不同，较大的 d_model 和较小的 num_heads 和 d_ff</li>\n</ol>\n</blockquote>\n<h1 id=\"3-mT5\"><a href=\"#3-mT5\" class=\"headerlink\" title=\"3 mT5\"></a>3 mT5</h1><ul>\n<li>mT5的预训练目标和策略等等和T5基本相同， 值得注意的是mT5使用的是T5 v1.1</li>\n</ul>\n<h3 id=\"3-1-mC4数据集\"><a href=\"#3-1-mC4数据集\" class=\"headerlink\" title=\"3.1 mC4数据集\"></a>3.1 mC4数据集</h3><ul>\n<li>一个多语言版的C4数据集，但是使用的数据清洗方法和T5不同：</li>\n<li>对于多语言模型，一个很重要的部分是如何多多种语言进行采样，<strong>不同语种数据占比不同，有的语言样本少（low-resource languages ），如果不常采样到，模型就会由于样本过少而过拟合；如果样本量太大（high-resource languages ），内容丰富，模型又可能欠拟合，所以不能让模型遍历太多high-resource languages</strong></li>\n<li>要解决上述问题，直观上来说可以使用均匀分布来采样，但是使用均匀分布效果肯定比较差，因为很多high-resource languages 压根用不到</li>\n<li>所以采用了：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(L) \\propto L^{\\alpha}</script><p>其中L为对应语言的样本数，<script type=\"math/tex\">\\alpha \\in [0,1]</script>为超参，<script type=\"math/tex\">\\alpha</script>越小分布越接近均匀分布，<strong>mT5经过实验发现<script type=\"math/tex\">\\alpha=0.3</script>最合适</strong>。那么这样就可以<strong>适当提升low-resource languages的采样概率而适当减少high-resource languages的采样概率</strong></p>\n<ul>\n<li>mC4中不同语言的样本数，以及使用不同<script type=\"math/tex\">\\alpha</script>的采样概率：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129191709077.png\" alt=\"image-20221129191709077\" style=\"zoom:80%;\" /></p>\n<h3 id=\"3-2-微调策略\"><a href=\"#3-2-微调策略\" class=\"headerlink\" title=\"3.2 微调策略\"></a>3.2 微调策略</h3><ul>\n<li>模型在mC4上预训练之后，作者采用了一下三种微调方式进行对比（微调采用lr = 0.001）：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>zero-shot：</strong>仅在英语训练集上微调</li>\n<li><strong>translate-train：</strong>在英语+由英语翻译到所有目标语言的数据集上微调</li>\n<li><strong>in-language multitask：</strong>在目标语言的gold data上微调（这里是真实的人工表述的数据，而tanslate-train的目标语言数据是翻译过来的）</li>\n</ol>\n</blockquote>\n<p>结果如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193118134.png\" alt=\"image-20221129193118134\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>此外，作者还对比了采用不同的模型参数量对这三种微调方式的提升：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193253828.png\" alt=\"image-20221129193253828\"></p>\n<h3 id=\"3-3-T5-vs-mT5\"><a href=\"#3-3-T5-vs-mT5\" class=\"headerlink\" title=\"3.3 T5 vs mT5\"></a>3.3 T5 vs mT5</h3><ul>\n<li>作者还对比了T5和mT5在英语QA任务上的效果差异：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193755886.png\" alt=\"image-20221129193755886\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>发现<strong>mT5还是略逊色于T5，但是随着模型规模的增大，之间的差异越来越小。也证明了多语言模型同样有能力比肩单语言模型</strong></li>\n</ul>\n<h3 id=\"3-4-消融实验\"><a href=\"#3-4-消融实验\" class=\"headerlink\" title=\"3.4 消融实验\"></a>3.4 消融实验</h3><ul>\n<li>作者还对训练的各方面进行了一些消融实验，策略如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Dropout 0.1：</strong>由于使用的是T5 v1.1，所以在预训练时没有使用Dropout，这里为了对照又把Dropout加上了</li>\n<li><strong>Sequence length 512：</strong>将最大序列长度减少为512</li>\n<li><strong>Span length 10：</strong>将连续token的长度由3变为10</li>\n<li><strong><script type=\"math/tex\">\\alpha=0.7,0.2</script>：</strong>采样时的超参改一下</li>\n<li><strong>No line length filter：</strong>数据清洗时的策略改一下</li>\n<li><strong>Add Wikipedia data：</strong>预训练使用mC4+Wikipedia data</li>\n</ol>\n</blockquote>\n<h3 id=\"3-5-zero-shot微调策略的问题\"><a href=\"#3-5-zero-shot微调策略的问题\" class=\"headerlink\" title=\"3.5 zero-shot微调策略的问题\"></a>3.5 zero-shot微调策略的问题</h3><ul>\n<li>采用zero-shot会造成预测时产生一些非法输出：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Normalization：</strong>prediction是合法的，但是unicode characters被替代了，可以通过Unicode NFKC normalization来恢复</li>\n<li><strong>Grammatical adjustment：</strong>answer本身就存在语法问题</li>\n<li><strong>Accidental translation：</strong>模型直接做了翻译，将目标语言翻译成英文了，以至于生成部分或者完整英文</li>\n</ol>\n<p>同时，在一些短语生成的时候，出现正确答案之前可能会先预测出两个英语词</p>\n<p>上面最常出现的是Accidental translation</p>\n</blockquote>\n<p>以下是非法输出的一些栗子：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129195942926.png\" alt=\"image-20221129195942926\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><strong>产生原因：</strong>模型在微调的时候压根没有接触过non-English的target文本，在non-English上做推理时，non-English的likelihood会降低，以至于English变成最可能的输出</li>\n<li><strong>解决方法：</strong>在微调时再次使用<strong>少量的mC4数据进行无监督二次预训练</strong>（和微调的样本数比例是1：100，并且包含全部101种语言），并且二次预训练时<strong>删除了target文本中的哨兵token</strong>，因为最后的结果发现在下游任务时就偶尔会预测出哨兵token，然后还将α从0.3降为0.1，<strong>使采样分布十分近似于均匀分布</strong>。结果提升显著：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129201243017.png\" alt=\"image-20221129201243017\" style=\"zoom:80%;\" /></p>\n","site":{"data":{}},"wordcount":5751,"excerpt":"","more":"<ul>\n<li>T5（Text-to-Text Transfer Transformer）模型采用了一种Text-to-text（文本到文本）的框架，想要把NLP领域的许多常见任务，如文本分类、QA等，都套到这个框架中解决</li>\n<li>如机器翻译任务，输入”translate English to German: That is good.”，目标输出是”Das ist gut.”，在输入中” : “前面称为prompt，代指现在需要执行的任务</li>\n<li>这样的好处是可以把所有的问题都套进去一个统一的范式，从而可以采用同样的模型架构、同样的训练策略、同样的损失函数、同样的解码手段。</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126211139471.png\" alt=\"image-20221126211139471\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>这里我们讲的是T51.0，之后再介绍T51.1的不同点</li>\n</ul>\n<h3 id=\"1-1-C4数据集\"><a href=\"#1-1-C4数据集\" class=\"headerlink\" title=\"1.1 C4数据集\"></a>1.1 C4数据集</h3><ul>\n<li>C4全称Colossal Clean Crawled Corpus，跟GPT2、GPT3的训练数据来源一样，是从网上爬的文本数据，由于是爬的数据，所以数据量足够大，而且类型丰富，缺点是数据质量差，需要过滤，过滤手段包括</li>\n<li>经过一番数据清洗后得到了一个750G的数据集</li>\n</ul>\n<h3 id=\"1-2-Baseline\"><a href=\"#1-2-Baseline\" class=\"headerlink\" title=\"1.2 Baseline\"></a>1.2 Baseline</h3><ul>\n<li>其实T5就是经过很多个实验，对不同的模型结构、训练策略等进行对比，然后挑出效果最好的，所以我们先给出实验中用于对比的baseline</li>\n<li><strong>模型结构：</strong>和Transformer一模一样，12层的encoder-decoder架构</li>\n<li><strong>训练/预测策略：</strong>训练时采用teacher-forcing，预测时采用贪婪搜索</li>\n<li><strong>预训练：</strong>在C4上面训练<script type=\"math/tex\">2^{19}</script>个steps，batch_size=128，seq_len=512。<strong>预训练并没有覆盖所有C4数据集，即没一个样本会重复训练</strong>。预训练目标稍后介绍</li>\n<li><strong>学习率调整：</strong>采用平方根倒数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nl r=\\frac{1}{\\sqrt{\\max (n, k)}}, k=10^{4}</script><ul>\n<li><strong>微调：</strong>对每个下游任务训练<script type=\"math/tex\">2^{18}</script>个steps</li>\n<li><strong>词表：</strong>采用WordPiece，大约有32000个token，有部分非英语词</li>\n</ul>\n<h3 id=\"1-3-无监督预训练目标\"><a href=\"#1-3-无监督预训练目标\" class=\"headerlink\" title=\"1.3 无监督预训练目标\"></a>1.3 无监督预训练目标</h3><ul>\n<li>预训练目标和BERT一样，都是采用随机mask破坏文本，然后通过上下文将这个词训练出来，称为<strong>Denoising</strong>的预训练目标</li>\n<li><strong>对输入随机挑选15%的token，然后使用一个哨兵token进行替换，注意挑选出来的token如果时连续的text span，则只用一个哨兵token进行替换。然后target文本变为：每个哨兵token+其对应的值的形式，最后再接一个特殊的哨兵token，表示结束</strong></li>\n<li>举例栗子：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126223744785.png\" alt=\"image-20221126223744785\"></p>\n<p>如上图，将for inviting和last分别替换成了两个不同的哨兵token，然后target变为了分别预测每个哨兵token，然后文本最后预测出另一个哨兵token\\，表示结束</p>\n<h3 id=\"1-4-不同模型结构的对比\"><a href=\"#1-4-不同模型结构的对比\" class=\"headerlink\" title=\"1.4 不同模型结构的对比\"></a>1.4 不同模型结构的对比</h3><ul>\n<li>针对self-atttion，有三种mask方式：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126230434585.png\" alt=\"image-20221126230434585\"></p>\n<p>分别为：</p>\n<blockquote>\n<ol>\n<li>fully-visible：每个时间步都对其他时间步可见</li>\n<li>causal：对未来的时间步不可见</li>\n<li>causal with prefix：前面两者的结合，prefix部分的token能看到prefix所有token的信息，非prefix的token只能看到它的上文信息。那么什么是prefix，如上面提到的英文翻译德文的例子，prefix就是”translate English to German: That is good.”，说白了就是输入部分的时间步是fully-visible，输出部分的时间步是causal</li>\n</ol>\n</blockquote>\n<ul>\n<li>针对三种不一样的mask方式，作者对如下三种模型架构进行了比较：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221126230815635.png\" alt=\"image-20221126230815635\"></p>\n<ul>\n<li>对此，作者提出了五种模型（为了公平，只有两个模型有同样数量的参数或者同样的计算效率才能进行比较）：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Encoder-decoder：</strong>编码层和解码层各有L层</li>\n<li><strong>Enc-dec, shared：</strong>编码层和解码层各有L层，但它们参数共享，所以参数减半</li>\n<li><strong>Enc-dec, 6 layers：</strong>编码层和解码层各有L/2层</li>\n<li><strong>LM：</strong>只有L层解码层，采用语言模型的形式</li>\n<li><strong>Prefix LM：</strong>只有L层解码层，但采用Prefix语言模型的形式</li>\n</ol>\n</blockquote>\n<ul>\n<li>并且还对比使用了两种预训练目标：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Denoising：</strong>即baseline中使用的随机mask词然后预测出来</li>\n<li><strong>LM：</strong>LM中常用的自回归预测，即每个时间步预测通过上个时间步的输出来进行当前时间步的输出预测</li>\n</ol>\n</blockquote>\n<ul>\n<li>最后得到以下结果：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127105905994.png\" alt=\"image-20221127105905994\" style=\"zoom:67%;\" /></p>\n<h3 id=\"1-5-不同的无监督预训练目标对比\"><a href=\"#1-5-不同的无监督预训练目标对比\" class=\"headerlink\" title=\"1.5 不同的无监督预训练目标对比\"></a>1.5 不同的无监督预训练目标对比</h3><ul>\n<li>首先介绍采用的预训练目标：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127110614966.png\" alt=\"image-20221127110614966\" style=\"zoom: 80%;\" /></p>\n<blockquote>\n<ol>\n<li><strong>Prefix LM：</strong>输入前部分文本，预测剩余的文本</li>\n<li><strong>BERT-style：</strong>利用和BERT中一样的mask策略，然后预测出原文本</li>\n<li><strong>Deshuffling：</strong>随机打乱文本，然后预测出原文本</li>\n<li><strong>MASS-style：</strong>和BERT-style的不同在于，mask时直接用[M]替换</li>\n<li><strong>noise, replace spans：</strong>前文提到的无监督预训练目标</li>\n<li><strong>noise, drop tokens：</strong>和5差不多，但是不用哨兵token替换，直接drop</li>\n<li><strong>Random spans：</strong>和5差不多，但是每次选择的是一个长为3的text span</li>\n</ol>\n</blockquote>\n<ul>\n<li>作者首先对前三种目标进行对比：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112236904.png\" alt=\"image-20221127112236904\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>结果发现BERT-style效果最好，然后再使用余下的方法和BERT-style进行比较：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112315271.png\" alt=\"image-20221127112315271\" style=\"zoom:80%;\" /></p>\n<p><strong>其实这三种和BERT-style差不多，但是后两种不需要预测出整个原文本，更快，</strong></p>\n<ul>\n<li>此外，作者还对比了<strong>不同的文本corruption率和允许的最长text span的长度</strong>（由于连续的mask掉的token都处理为一个哨兵token，允许最长的text span即指最多只有3个token可以替换成一个哨兵token，超过三个要使用另一个哨兵token）</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112647149.png\" alt=\"image-20221127112647149\" style=\"zoom:80%;\" /></p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112657299.png\" alt=\"image-20221127112657299\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>最后对这部分实验做个总结，作者是逐层递进来进行的对比试验：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127112910646.png\" alt=\"image-20221127112910646\" style=\"zoom:80%;\" /></p>\n<h3 id=\"1-6-数据集对比\"><a href=\"#1-6-数据集对比\" class=\"headerlink\" title=\"1.6 数据集对比\"></a>1.6 数据集对比</h3><ul>\n<li>作者还对不同类型和不同大小的预训练数据集进行了对比</li>\n<li>最后得出：<strong>用更专的数据来做预训练，对下游任务的提升越明显，或者换句更准确的话来说，预训练的语料跟任务语料domain越接近，效果越好，并且数据越多越好，即使预训练不能覆盖完</strong></li>\n<li>所以个人认为最佳的策略是<strong>在丰富的数据上进行预训练，然后再在领域相关、任务相关的语料上继续预训练，最后再fine-tuning</strong></li>\n</ul>\n<h3 id=\"1-7-训练策略\"><a href=\"#1-7-训练策略\" class=\"headerlink\" title=\"1.7 训练策略\"></a>1.7 训练策略</h3><ul>\n<li><strong>fine-tuning方法：</strong>作者对三种微调方法进行了对比：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>All parameters：</strong>微调时更新所有参数</li>\n<li><strong>Adapter layers：</strong>adapter layers接在编码器和解码器的每一个block的全连接层后面，在fine-tuning的时候只更新它们。adapter layers有一个内部维度d作为超参</li>\n<li><strong>Gradual unfreezing：</strong>一开始离任务层近的参数先更新，其它保持不动，随着训练的进行，逐渐放开其它层的参数。</li>\n</ol>\n</blockquote>\n<p>实验结果：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127115209871.png\" alt=\"image-20221127115209871\" style=\"zoom:80%;\" /></p>\n<p>发现还是更新所有参数效果最好，但是会慢很多</p>\n<ul>\n<li><strong>多任务学习：</strong>得益于提出的text-to-text范式，我们可以<strong>在预训练的时候把有监督的训练也加进来，一起做预训练（注意：多任务学习预训练中的数据集包括原本的无监督数据集+多个有监督数据集）</strong>。现在问题就变为了<strong>给定多个不同任务的数据集，怎样对数据进行采样</strong>，作者使用了以下三种策略：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Examples-proportional mixing：</strong>设第<script type=\"math/tex\">i</script>个任务的数据集大小为<script type=\"math/tex\">e_i</script>，那么采样自第<script type=\"math/tex\">j</script>个数据集的概率为<script type=\"math/tex\">r_j=\\min(e_j,K)/∑_i\\min(e_i,K)</script>，其中K为提前设置好的超参</li>\n<li><strong>Temperature-scaled mixing：</strong>在上面的策略下，再做一些软化，具体来说就是求得<script type=\"math/tex\">r_j</script>后再开1/T方根，T为提前设置好的超参，T越大，各个任务数据集采样越均衡</li>\n<li><strong>Equal mixing：</strong>各数据集均匀采样</li>\n</ol>\n</blockquote>\n<p>实验结果：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221127120459146.png\" alt=\"image-20221127120459146\" style=\"zoom:80%;\" /></p>\n<p>实验结果都一般，<strong>不过注意，这里的多任务学习是多个任务一起做训练，相当于把pre-training和fine-tuning两个合并了，而不会对单个任务进行fine-tuning</strong>，所以效果不好也可以理解。</p>\n<ul>\n<li><strong>多任务学习+fine-tuning：</strong>作者采用了一下集中训练策略进行比较：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Unsupervised pre-training + fine-tuning：</strong>baseline中使用的方法，先无监督预训练再在特定的下游任务上微调</li>\n<li><strong>Multi-task training：</strong>直接在多任务数据集上训练（注意mutl-task的训练集中有有监督的也有无监督的）</li>\n<li><strong>Multi-task pre-training + fine-tuning：</strong>多任务预训练+微调</li>\n<li><strong>Leave-one-out multi-task training：</strong>在预训练的时候同样使用多任务，但是要去除和下游任务相关的那个数据集，然后再在下游任务微调</li>\n<li><strong>Supervised multi-task pre-training：</strong>在多任务预训练的时候把无监督任务剔除掉，然后再微调</li>\n</ol>\n</blockquote>\n<p>实验结果：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221128120902619.png\" alt=\"image-20221128120902619\"></p>\n<p>通过实验结果得到以下结论：</p>\n<blockquote>\n<ol>\n<li>使用Multi-task pre-training + fine-tuning的结果和baseline差不多，表明<strong>在多任务后使用微调可以减轻不同的数据集mixing比例之间的权衡</strong>，即你选用的Mixing方式（比如上文的三种）不一定是最好的，但是微调可以减轻这种错选带来的干扰</li>\n<li>Leave-one-out的效果只有一点点下降，表明<strong>多任务学习不会导致严重的任务干扰</strong></li>\n<li>使用Supervised multi-task pre-training几乎除了翻译任务都导致了下降，表明<strong>翻译任务从英语预训练中学得的很少，反之其他任务仍然很依赖无监督预训练</strong></li>\n</ol>\n</blockquote>\n<h3 id=\"1-8-Scaling\"><a href=\"#1-8-Scaling\" class=\"headerlink\" title=\"1.8 Scaling\"></a>1.8 Scaling</h3><ul>\n<li>此外，作者还对模型规模等进行了测试，得出<strong>使用更多的数据、训练更大的模型、模型融合都能提高性能</strong></li>\n<li>最后提一句，经过一系列实验，T5还是选择了<strong>Multi-task pre-training + fine-tuning</strong>以及预测时采用束搜索，无监督预训练目标采用了<strong>noise, replace spans</strong></li>\n</ul>\n<h1 id=\"2-T5-v1-1\"><a href=\"#2-T5-v1-1\" class=\"headerlink\" title=\"2 T5 v1.1\"></a>2 T5 v1.1</h1><ul>\n<li>上文讲的是T5 v1.0，谷歌之后又发布了一个T5 v1.1，只有一些细微差别，改进如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li>前馈神经层的激活函数由ReLU改为了GEGLU</li>\n<li>在pre-training的时候关闭Dropout，在微调的时候重新开启</li>\n<li>预训练的时候只使用C4数据集，而不混入下游数据集</li>\n<li>Embedding层和最后的分类层没有使用Weight Tying</li>\n<li>模型形状有点不同，较大的 d_model 和较小的 num_heads 和 d_ff</li>\n</ol>\n</blockquote>\n<h1 id=\"3-mT5\"><a href=\"#3-mT5\" class=\"headerlink\" title=\"3 mT5\"></a>3 mT5</h1><ul>\n<li>mT5的预训练目标和策略等等和T5基本相同， 值得注意的是mT5使用的是T5 v1.1</li>\n</ul>\n<h3 id=\"3-1-mC4数据集\"><a href=\"#3-1-mC4数据集\" class=\"headerlink\" title=\"3.1 mC4数据集\"></a>3.1 mC4数据集</h3><ul>\n<li>一个多语言版的C4数据集，但是使用的数据清洗方法和T5不同：</li>\n<li>对于多语言模型，一个很重要的部分是如何多多种语言进行采样，<strong>不同语种数据占比不同，有的语言样本少（low-resource languages ），如果不常采样到，模型就会由于样本过少而过拟合；如果样本量太大（high-resource languages ），内容丰富，模型又可能欠拟合，所以不能让模型遍历太多high-resource languages</strong></li>\n<li>要解决上述问题，直观上来说可以使用均匀分布来采样，但是使用均匀分布效果肯定比较差，因为很多high-resource languages 压根用不到</li>\n<li>所以采用了：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(L) \\propto L^{\\alpha}</script><p>其中L为对应语言的样本数，<script type=\"math/tex\">\\alpha \\in [0,1]</script>为超参，<script type=\"math/tex\">\\alpha</script>越小分布越接近均匀分布，<strong>mT5经过实验发现<script type=\"math/tex\">\\alpha=0.3</script>最合适</strong>。那么这样就可以<strong>适当提升low-resource languages的采样概率而适当减少high-resource languages的采样概率</strong></p>\n<ul>\n<li>mC4中不同语言的样本数，以及使用不同<script type=\"math/tex\">\\alpha</script>的采样概率：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129191709077.png\" alt=\"image-20221129191709077\" style=\"zoom:80%;\" /></p>\n<h3 id=\"3-2-微调策略\"><a href=\"#3-2-微调策略\" class=\"headerlink\" title=\"3.2 微调策略\"></a>3.2 微调策略</h3><ul>\n<li>模型在mC4上预训练之后，作者采用了一下三种微调方式进行对比（微调采用lr = 0.001）：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>zero-shot：</strong>仅在英语训练集上微调</li>\n<li><strong>translate-train：</strong>在英语+由英语翻译到所有目标语言的数据集上微调</li>\n<li><strong>in-language multitask：</strong>在目标语言的gold data上微调（这里是真实的人工表述的数据，而tanslate-train的目标语言数据是翻译过来的）</li>\n</ol>\n</blockquote>\n<p>结果如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193118134.png\" alt=\"image-20221129193118134\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>此外，作者还对比了采用不同的模型参数量对这三种微调方式的提升：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193253828.png\" alt=\"image-20221129193253828\"></p>\n<h3 id=\"3-3-T5-vs-mT5\"><a href=\"#3-3-T5-vs-mT5\" class=\"headerlink\" title=\"3.3 T5 vs mT5\"></a>3.3 T5 vs mT5</h3><ul>\n<li>作者还对比了T5和mT5在英语QA任务上的效果差异：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129193755886.png\" alt=\"image-20221129193755886\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>发现<strong>mT5还是略逊色于T5，但是随着模型规模的增大，之间的差异越来越小。也证明了多语言模型同样有能力比肩单语言模型</strong></li>\n</ul>\n<h3 id=\"3-4-消融实验\"><a href=\"#3-4-消融实验\" class=\"headerlink\" title=\"3.4 消融实验\"></a>3.4 消融实验</h3><ul>\n<li>作者还对训练的各方面进行了一些消融实验，策略如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Dropout 0.1：</strong>由于使用的是T5 v1.1，所以在预训练时没有使用Dropout，这里为了对照又把Dropout加上了</li>\n<li><strong>Sequence length 512：</strong>将最大序列长度减少为512</li>\n<li><strong>Span length 10：</strong>将连续token的长度由3变为10</li>\n<li><strong><script type=\"math/tex\">\\alpha=0.7,0.2</script>：</strong>采样时的超参改一下</li>\n<li><strong>No line length filter：</strong>数据清洗时的策略改一下</li>\n<li><strong>Add Wikipedia data：</strong>预训练使用mC4+Wikipedia data</li>\n</ol>\n</blockquote>\n<h3 id=\"3-5-zero-shot微调策略的问题\"><a href=\"#3-5-zero-shot微调策略的问题\" class=\"headerlink\" title=\"3.5 zero-shot微调策略的问题\"></a>3.5 zero-shot微调策略的问题</h3><ul>\n<li>采用zero-shot会造成预测时产生一些非法输出：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>Normalization：</strong>prediction是合法的，但是unicode characters被替代了，可以通过Unicode NFKC normalization来恢复</li>\n<li><strong>Grammatical adjustment：</strong>answer本身就存在语法问题</li>\n<li><strong>Accidental translation：</strong>模型直接做了翻译，将目标语言翻译成英文了，以至于生成部分或者完整英文</li>\n</ol>\n<p>同时，在一些短语生成的时候，出现正确答案之前可能会先预测出两个英语词</p>\n<p>上面最常出现的是Accidental translation</p>\n</blockquote>\n<p>以下是非法输出的一些栗子：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129195942926.png\" alt=\"image-20221129195942926\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><strong>产生原因：</strong>模型在微调的时候压根没有接触过non-English的target文本，在non-English上做推理时，non-English的likelihood会降低，以至于English变成最可能的输出</li>\n<li><strong>解决方法：</strong>在微调时再次使用<strong>少量的mC4数据进行无监督二次预训练</strong>（和微调的样本数比例是1：100，并且包含全部101种语言），并且二次预训练时<strong>删除了target文本中的哨兵token</strong>，因为最后的结果发现在下游任务时就偶尔会预测出哨兵token，然后还将α从0.3降为0.1，<strong>使采样分布十分近似于均匀分布</strong>。结果提升显著：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221129201243017.png\" alt=\"image-20221129201243017\" style=\"zoom:80%;\" /></p>\n"},{"title":"不同batch size梯度下降的影响","math":true,"date":"2021-11-22T16:00:00.000Z","_content":"\n\n\n- 梯度下降法作为机器学习中较常使用的优化算法，针对不同的batch size，有着3种不同的形式：**批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）、小批量梯度下降（Mini-Batch Gradient Descent）**\n\n\n\n# 1 批量梯度下降（BGD）\n\n使用整个训练集的优化算法被称为**批量**(batch)或**确定性**(deterministic)梯度算法，因为它们会**在一个大批量中同时处理所有样本**\n\n**批量梯度下降法**是最原始的形式，它是指在**每一次迭代时**使用**所有样本**来进行梯度的更新\n\n- **优点：**\n\n1. 在训练过程中，使用固定的学习率\n2. 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向\n3. 一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行\n\n- **缺点：**\n\n1. **尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了**\n2. 不能投入新数据实时更新模型\n3. 对非凸函数可能只能收敛到局部最小点，而非全局最小点\n\n![BGD](https://i.loli.net/2021/11/11/mCkfeDIPyh3a5iY.png)\n\n\n\n\n\n# 2 随机梯度下降（SGD）\n\n**随机梯度下降法**不同于批量梯度下降，随机梯度下降是在**每次迭代时**使用**一个样本**来对参数进行更新（mini-batch size =1）\n\n- **优点：**\n\n1. 在学习过程中加入了噪声，提高了泛化误差\n2. **噪声造成的扰动可能可以使其脱离局部最小点**\n3. SGD一次只遍历一个样本就可以进行更新 ，所以收敛很快，并且可以新增样本\n\n- **缺点：**\n\n1. **不收敛，在最小值附近波动**\n2. **不能在一个样本中使用并行化计算，学习过程变得很慢**\n3. 单个样本并不能代表全体样本的趋势，所以学习过程可能变得特别的曲折，**虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的**，如下图\n\n<img src=\"https://i.loli.net/2021/11/11/GLBkox67lvnmsOA.png\" alt=\"image-20211111201918007\" style=\"zoom: 67%;\" />\n\n- **对上面期望的证明，SGD的梯度是BGD梯度的无偏估计：**\n\n$$\nE(\\nabla f_i(x)) = \\frac{1}{n}\\sum_{i = 1}^n\\nabla f_i(x) = \\nabla f(x)\n$$\n\n**这说明了SGD的总体优化方向仍然是对的**\n\n\n\n\n\n# 3 Mini-batch梯度下降\n\n大多数用于深度学习的梯度下降算法介于以上两者之间，**使用一个以上而又不是全部的训练样本**\n\n- 在一次取样本的时候我们需要在所有样本中**随机**取batch-size个样本\n\n- **优点：**\n\n1. 收敛速度比BGD快，因为只遍历部分样例就可执行更新\n2. 随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例\n3. 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果\n4. 因为有噪音（但是噪音比SGD小），所以**可能脱离局部最小点**\n\n- **缺点：**\n\n1. 在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛\n2. 学习过程会有更多的振荡，为更接近最小值，需要增加**学习率衰减项**，以降低学习率，避免过度振荡\n\n- **小批量大小**（mini-batch size）的选择：\n\n1. 更大的批量会计算更精确的梯度，但是回报却是小于线性的\n2. 极小的批量通常难以充分利用多核结构。当批量低于某个数值时，计算时间不会减少\n3. 批量处理中的所有样本可以并行处理，**内存消耗和批量大小会成正比**。对于很多硬件设备，这是批量大小的限制因素\n4. 在使用**GPU**时，通常使用**2的幂数作为批量大小**可以获得更少的运行时间。一般，2的幂数取值范围是**32~256**。16有时在尝试大模型时使用\n\n\n\n\n\n使用三种梯度下降的收敛过程：\n\n<img src=\"https://i.loli.net/2021/11/11/paxvbFh9fRiDKNg.png\" alt=\"image-20211111204100788\" style=\"zoom: 80%;\" />\n\n- **Mini-batch梯度下降和SGD一样，其梯度也是BGD梯度的无偏估计**\n\n- **从无偏估计可以看出，Mini-batch和SGD其实就是在用部分样本梯度来代替总体梯度**","source":"_posts/不同batch size梯度下降的影响.md","raw":"---\ntitle: 不同batch size梯度下降的影响\nmath: true\ndate: 2021-11-23\n---\n\n\n\n- 梯度下降法作为机器学习中较常使用的优化算法，针对不同的batch size，有着3种不同的形式：**批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）、小批量梯度下降（Mini-Batch Gradient Descent）**\n\n\n\n# 1 批量梯度下降（BGD）\n\n使用整个训练集的优化算法被称为**批量**(batch)或**确定性**(deterministic)梯度算法，因为它们会**在一个大批量中同时处理所有样本**\n\n**批量梯度下降法**是最原始的形式，它是指在**每一次迭代时**使用**所有样本**来进行梯度的更新\n\n- **优点：**\n\n1. 在训练过程中，使用固定的学习率\n2. 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向\n3. 一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行\n\n- **缺点：**\n\n1. **尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了**\n2. 不能投入新数据实时更新模型\n3. 对非凸函数可能只能收敛到局部最小点，而非全局最小点\n\n![BGD](https://i.loli.net/2021/11/11/mCkfeDIPyh3a5iY.png)\n\n\n\n\n\n# 2 随机梯度下降（SGD）\n\n**随机梯度下降法**不同于批量梯度下降，随机梯度下降是在**每次迭代时**使用**一个样本**来对参数进行更新（mini-batch size =1）\n\n- **优点：**\n\n1. 在学习过程中加入了噪声，提高了泛化误差\n2. **噪声造成的扰动可能可以使其脱离局部最小点**\n3. SGD一次只遍历一个样本就可以进行更新 ，所以收敛很快，并且可以新增样本\n\n- **缺点：**\n\n1. **不收敛，在最小值附近波动**\n2. **不能在一个样本中使用并行化计算，学习过程变得很慢**\n3. 单个样本并不能代表全体样本的趋势，所以学习过程可能变得特别的曲折，**虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的**，如下图\n\n<img src=\"https://i.loli.net/2021/11/11/GLBkox67lvnmsOA.png\" alt=\"image-20211111201918007\" style=\"zoom: 67%;\" />\n\n- **对上面期望的证明，SGD的梯度是BGD梯度的无偏估计：**\n\n$$\nE(\\nabla f_i(x)) = \\frac{1}{n}\\sum_{i = 1}^n\\nabla f_i(x) = \\nabla f(x)\n$$\n\n**这说明了SGD的总体优化方向仍然是对的**\n\n\n\n\n\n# 3 Mini-batch梯度下降\n\n大多数用于深度学习的梯度下降算法介于以上两者之间，**使用一个以上而又不是全部的训练样本**\n\n- 在一次取样本的时候我们需要在所有样本中**随机**取batch-size个样本\n\n- **优点：**\n\n1. 收敛速度比BGD快，因为只遍历部分样例就可执行更新\n2. 随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例\n3. 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果\n4. 因为有噪音（但是噪音比SGD小），所以**可能脱离局部最小点**\n\n- **缺点：**\n\n1. 在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛\n2. 学习过程会有更多的振荡，为更接近最小值，需要增加**学习率衰减项**，以降低学习率，避免过度振荡\n\n- **小批量大小**（mini-batch size）的选择：\n\n1. 更大的批量会计算更精确的梯度，但是回报却是小于线性的\n2. 极小的批量通常难以充分利用多核结构。当批量低于某个数值时，计算时间不会减少\n3. 批量处理中的所有样本可以并行处理，**内存消耗和批量大小会成正比**。对于很多硬件设备，这是批量大小的限制因素\n4. 在使用**GPU**时，通常使用**2的幂数作为批量大小**可以获得更少的运行时间。一般，2的幂数取值范围是**32~256**。16有时在尝试大模型时使用\n\n\n\n\n\n使用三种梯度下降的收敛过程：\n\n<img src=\"https://i.loli.net/2021/11/11/paxvbFh9fRiDKNg.png\" alt=\"image-20211111204100788\" style=\"zoom: 80%;\" />\n\n- **Mini-batch梯度下降和SGD一样，其梯度也是BGD梯度的无偏估计**\n\n- **从无偏估计可以看出，Mini-batch和SGD其实就是在用部分样本梯度来代替总体梯度**","slug":"不同batch size梯度下降的影响","published":1,"updated":"2022-12-20T06:15:57.186Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1i000e7csz889m189x","content":"<ul>\n<li>梯度下降法作为机器学习中较常使用的优化算法，针对不同的batch size，有着3种不同的形式：<strong>批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）、小批量梯度下降（Mini-Batch Gradient Descent）</strong></li>\n</ul>\n<h1 id=\"1-批量梯度下降（BGD）\"><a href=\"#1-批量梯度下降（BGD）\" class=\"headerlink\" title=\"1 批量梯度下降（BGD）\"></a>1 批量梯度下降（BGD）</h1><p>使用整个训练集的优化算法被称为<strong>批量</strong>(batch)或<strong>确定性</strong>(deterministic)梯度算法，因为它们会<strong>在一个大批量中同时处理所有样本</strong></p>\n<p><strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新</p>\n<ul>\n<li><strong>优点：</strong></li>\n</ul>\n<ol>\n<li>在训练过程中，使用固定的学习率</li>\n<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向</li>\n<li>一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行</li>\n</ol>\n<ul>\n<li><strong>缺点：</strong></li>\n</ul>\n<ol>\n<li><strong>尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了</strong></li>\n<li>不能投入新数据实时更新模型</li>\n<li>对非凸函数可能只能收敛到局部最小点，而非全局最小点</li>\n</ol>\n<p><img src=\"https://i.loli.net/2021/11/11/mCkfeDIPyh3a5iY.png\" alt=\"BGD\"></p>\n<h1 id=\"2-随机梯度下降（SGD）\"><a href=\"#2-随机梯度下降（SGD）\" class=\"headerlink\" title=\"2 随机梯度下降（SGD）\"></a>2 随机梯度下降（SGD）</h1><p><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是在<strong>每次迭代时</strong>使用<strong>一个样本</strong>来对参数进行更新（mini-batch size =1）</p>\n<ul>\n<li><strong>优点：</strong></li>\n</ul>\n<ol>\n<li>在学习过程中加入了噪声，提高了泛化误差</li>\n<li><strong>噪声造成的扰动可能可以使其脱离局部最小点</strong></li>\n<li>SGD一次只遍历一个样本就可以进行更新 ，所以收敛很快，并且可以新增样本</li>\n</ol>\n<ul>\n<li><strong>缺点：</strong></li>\n</ul>\n<ol>\n<li><strong>不收敛，在最小值附近波动</strong></li>\n<li><strong>不能在一个样本中使用并行化计算，学习过程变得很慢</strong></li>\n<li>单个样本并不能代表全体样本的趋势，所以学习过程可能变得特别的曲折，<strong>虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的</strong>，如下图</li>\n</ol>\n<p><img src=\"https://i.loli.net/2021/11/11/GLBkox67lvnmsOA.png\" alt=\"image-20211111201918007\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li><strong>对上面期望的证明，SGD的梯度是BGD梯度的无偏估计：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE(\\nabla f_i(x)) = \\frac{1}{n}\\sum_{i = 1}^n\\nabla f_i(x) = \\nabla f(x)</script><p><strong>这说明了SGD的总体优化方向仍然是对的</strong></p>\n<h1 id=\"3-Mini-batch梯度下降\"><a href=\"#3-Mini-batch梯度下降\" class=\"headerlink\" title=\"3 Mini-batch梯度下降\"></a>3 Mini-batch梯度下降</h1><p>大多数用于深度学习的梯度下降算法介于以上两者之间，<strong>使用一个以上而又不是全部的训练样本</strong></p>\n<ul>\n<li><p>在一次取样本的时候我们需要在所有样本中<strong>随机</strong>取batch-size个样本</p>\n</li>\n<li><p><strong>优点：</strong></p>\n</li>\n</ul>\n<ol>\n<li>收敛速度比BGD快，因为只遍历部分样例就可执行更新</li>\n<li>随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例</li>\n<li>每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果</li>\n<li>因为有噪音（但是噪音比SGD小），所以<strong>可能脱离局部最小点</strong></li>\n</ol>\n<ul>\n<li><strong>缺点：</strong></li>\n</ul>\n<ol>\n<li>在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛</li>\n<li>学习过程会有更多的振荡，为更接近最小值，需要增加<strong>学习率衰减项</strong>，以降低学习率，避免过度振荡</li>\n</ol>\n<ul>\n<li><strong>小批量大小</strong>（mini-batch size）的选择：</li>\n</ul>\n<ol>\n<li>更大的批量会计算更精确的梯度，但是回报却是小于线性的</li>\n<li>极小的批量通常难以充分利用多核结构。当批量低于某个数值时，计算时间不会减少</li>\n<li>批量处理中的所有样本可以并行处理，<strong>内存消耗和批量大小会成正比</strong>。对于很多硬件设备，这是批量大小的限制因素</li>\n<li>在使用<strong>GPU</strong>时，通常使用<strong>2的幂数作为批量大小</strong>可以获得更少的运行时间。一般，2的幂数取值范围是<strong>32~256</strong>。16有时在尝试大模型时使用</li>\n</ol>\n<p>使用三种梯度下降的收敛过程：</p>\n<p><img src=\"https://i.loli.net/2021/11/11/paxvbFh9fRiDKNg.png\" alt=\"image-20211111204100788\" style=\"zoom: 80%;\" /></p>\n<ul>\n<li><p><strong>Mini-batch梯度下降和SGD一样，其梯度也是BGD梯度的无偏估计</strong></p>\n</li>\n<li><p><strong>从无偏估计可以看出，Mini-batch和SGD其实就是在用部分样本梯度来代替总体梯度</strong></p>\n</li>\n</ul>\n","site":{"data":{}},"wordcount":1436,"excerpt":"","more":"<ul>\n<li>梯度下降法作为机器学习中较常使用的优化算法，针对不同的batch size，有着3种不同的形式：<strong>批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）、小批量梯度下降（Mini-Batch Gradient Descent）</strong></li>\n</ul>\n<h1 id=\"1-批量梯度下降（BGD）\"><a href=\"#1-批量梯度下降（BGD）\" class=\"headerlink\" title=\"1 批量梯度下降（BGD）\"></a>1 批量梯度下降（BGD）</h1><p>使用整个训练集的优化算法被称为<strong>批量</strong>(batch)或<strong>确定性</strong>(deterministic)梯度算法，因为它们会<strong>在一个大批量中同时处理所有样本</strong></p>\n<p><strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新</p>\n<ul>\n<li><strong>优点：</strong></li>\n</ul>\n<ol>\n<li>在训练过程中，使用固定的学习率</li>\n<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向</li>\n<li>一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行</li>\n</ol>\n<ul>\n<li><strong>缺点：</strong></li>\n</ul>\n<ol>\n<li><strong>尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了</strong></li>\n<li>不能投入新数据实时更新模型</li>\n<li>对非凸函数可能只能收敛到局部最小点，而非全局最小点</li>\n</ol>\n<p><img src=\"https://i.loli.net/2021/11/11/mCkfeDIPyh3a5iY.png\" alt=\"BGD\"></p>\n<h1 id=\"2-随机梯度下降（SGD）\"><a href=\"#2-随机梯度下降（SGD）\" class=\"headerlink\" title=\"2 随机梯度下降（SGD）\"></a>2 随机梯度下降（SGD）</h1><p><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是在<strong>每次迭代时</strong>使用<strong>一个样本</strong>来对参数进行更新（mini-batch size =1）</p>\n<ul>\n<li><strong>优点：</strong></li>\n</ul>\n<ol>\n<li>在学习过程中加入了噪声，提高了泛化误差</li>\n<li><strong>噪声造成的扰动可能可以使其脱离局部最小点</strong></li>\n<li>SGD一次只遍历一个样本就可以进行更新 ，所以收敛很快，并且可以新增样本</li>\n</ol>\n<ul>\n<li><strong>缺点：</strong></li>\n</ul>\n<ol>\n<li><strong>不收敛，在最小值附近波动</strong></li>\n<li><strong>不能在一个样本中使用并行化计算，学习过程变得很慢</strong></li>\n<li>单个样本并不能代表全体样本的趋势，所以学习过程可能变得特别的曲折，<strong>虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的</strong>，如下图</li>\n</ol>\n<p><img src=\"https://i.loli.net/2021/11/11/GLBkox67lvnmsOA.png\" alt=\"image-20211111201918007\" style=\"zoom: 67%;\" /></p>\n<ul>\n<li><strong>对上面期望的证明，SGD的梯度是BGD梯度的无偏估计：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE(\\nabla f_i(x)) = \\frac{1}{n}\\sum_{i = 1}^n\\nabla f_i(x) = \\nabla f(x)</script><p><strong>这说明了SGD的总体优化方向仍然是对的</strong></p>\n<h1 id=\"3-Mini-batch梯度下降\"><a href=\"#3-Mini-batch梯度下降\" class=\"headerlink\" title=\"3 Mini-batch梯度下降\"></a>3 Mini-batch梯度下降</h1><p>大多数用于深度学习的梯度下降算法介于以上两者之间，<strong>使用一个以上而又不是全部的训练样本</strong></p>\n<ul>\n<li><p>在一次取样本的时候我们需要在所有样本中<strong>随机</strong>取batch-size个样本</p>\n</li>\n<li><p><strong>优点：</strong></p>\n</li>\n</ul>\n<ol>\n<li>收敛速度比BGD快，因为只遍历部分样例就可执行更新</li>\n<li>随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例</li>\n<li>每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果</li>\n<li>因为有噪音（但是噪音比SGD小），所以<strong>可能脱离局部最小点</strong></li>\n</ol>\n<ul>\n<li><strong>缺点：</strong></li>\n</ul>\n<ol>\n<li>在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛</li>\n<li>学习过程会有更多的振荡，为更接近最小值，需要增加<strong>学习率衰减项</strong>，以降低学习率，避免过度振荡</li>\n</ol>\n<ul>\n<li><strong>小批量大小</strong>（mini-batch size）的选择：</li>\n</ul>\n<ol>\n<li>更大的批量会计算更精确的梯度，但是回报却是小于线性的</li>\n<li>极小的批量通常难以充分利用多核结构。当批量低于某个数值时，计算时间不会减少</li>\n<li>批量处理中的所有样本可以并行处理，<strong>内存消耗和批量大小会成正比</strong>。对于很多硬件设备，这是批量大小的限制因素</li>\n<li>在使用<strong>GPU</strong>时，通常使用<strong>2的幂数作为批量大小</strong>可以获得更少的运行时间。一般，2的幂数取值范围是<strong>32~256</strong>。16有时在尝试大模型时使用</li>\n</ol>\n<p>使用三种梯度下降的收敛过程：</p>\n<p><img src=\"https://i.loli.net/2021/11/11/paxvbFh9fRiDKNg.png\" alt=\"image-20211111204100788\" style=\"zoom: 80%;\" /></p>\n<ul>\n<li><p><strong>Mini-batch梯度下降和SGD一样，其梯度也是BGD梯度的无偏估计</strong></p>\n</li>\n<li><p><strong>从无偏估计可以看出，Mini-batch和SGD其实就是在用部分样本梯度来代替总体梯度</strong></p>\n</li>\n</ul>\n"},{"title":"Transformer总结","math":true,"date":"2022-01-24T16:00:00.000Z","_content":"\n- Transformer摒弃了传统的CNN/RNN模型，而是用纯注意力机制， **相对于RNN，实现了并行化，并且消除了memory对于距离的依赖性（无法捕捉长距离依赖）。**\n\n\n\n\n\n# 1 注意力机制\n\n- **注意力机制中分别有key、query、value（一般key=value），通过key、query之间的相似度，计算得到每个value对应的权值，再对所有value加权求和，得到一整个序列的表征。其中对于自己本身的注意力机制称为self-attention（自注意力机制），即key=value=query**\n\n\n\n### 1.1 Scaled Dot-Product Attention(点积)\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112222285.png\" alt=\"image-20220512112222285\" style=\"zoom: 37%;\" />\n\n- 在计算时，我们是将query、key、value（分别为$$d_k, d_k, d_v$$维）打包成Q，K$$\\in R^{N \\times d_k}$$，V$$\\in R^{N \\times d_v}$$，具体做法是：\n\n> 将送进来的输入input$$\\in R^{N \\times d_{model} }$$（其中$$d_{model}$$为embedding的维度，且q、k、v三者的input可能各自不同），input分别乘$$W^Q、W^K \\in R^{d_{model} \\times d_k}$$，$$W^V \\in R^{ {d_{model} \\times d_v} }$$即可得到Q、K、V\n\n- 在计算权值时，将Q、K相乘，再除以$$\\sqrt{d_k}$$，再softmax得到权值。**除以$$\\sqrt{d_k}$$的原因**：\n\n> **维度过大会使Q、K相乘的结果过大，容易把softmax的区域推向梯度极小的区域。并且实验证明在$$d_k$$较小时，其实除不除效果差不多**\n\n- 得到权重后再和V相乘，总过程为：\n\n$$\n\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T} }{\\sqrt{d_{k} }}\\right) V\n$$\n\n- 还有一种较常用的注意力机制叫Additive attention， 是使用一个单隐藏层的全连接网络计算权重，两者效果差不多，**但是dot-product会快得多**\n\n\n\n### 1.2 Multi-Head Attention(多头注意力机制)\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112235945.png\" alt=\"image-20220512112235945\" style=\"zoom:35%;\" />\n\n- 多头注意力机制本质上就是**做多次Scaled Dot-Product Attention**\n- 具体做法是：\n\n> **重复做h次Scaled Dot-Product Attention（每次的W权重矩阵分别独立），将每次得到的结果$$Z \\in R^{N \\times d_v}$$在第二维连结，形状变为$$R^{N \\times hd_v}$$，再乘一个$$W^O \\in R^{hd_v \\times d_{model} }$$，即可得到形状为$$R^{N \\times d_{model} }$$的最终结果**\n\n- 总过程为：\n\n$$\n\\begin{aligned}\n\\operatorname{MultiHead}(Q, K, V) &=\\operatorname{Concat}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{\\mathrm{h} }\\right) W^{O} \\\\\n\\text { where head } &=\\operatorname{Attention}\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)\n\\end{aligned}\n$$\n\n- 在base模型中取的$$h = 8$$，且$$d_k = d_v = d_{model}/h = 64$$\n\n- **多头注意力的好处：类似于CNN中的通道，能提取到不同子空间下的特征。多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。**如果单纯使用单注意力头+平均化，会抑制这一点\n\n> Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  \n\n\n\n### 1.3 自注意力机制的好处\n\n- 自注意力机制最大的好处肯定是实现了**并行化，加快了训练速度**。并且得到的结果相比于其他方法（如全局平均池化），**更具有解释性**，self-attention是可以退化成平均的，所以结果肯定好于平均。\n\n- 论文从每层的总计算复杂度、可并行化的计算数量（用顺序操作的最小量来衡量）、长距离依赖的距离三个方面进行了对比：\n\n![image-20220512114401561](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512114401561.png)\n\n- **并且单个注意力头不仅清楚地学习执行不同的任务，而且许多似乎表现出与句子的句法和语义结构相关的行为**\n\n> Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences  \n\n\n\n\n\n# 2 模型结构\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512121448150.png\" alt=\"image-20220512121448150\" style=\"zoom:28%;\" />\n\n- 图中多头注意力层的输入从左到右依次为V、K、Q\n\n### 2.1 Encoder和Decoder\n\n- Encoder由N=6个相同的块组成，每个块有两个子层，一个多头注意力层，一个全连接层，子层输入输出都是都是$$R^{N \\times d_{model} }$$，其中N为时间步个数，也就是词数。**并且在每个子层都有一个残差结构+LayerNorm**，先残差后LayerNorm：\n\n$$\nLayerNorm(x + Sublayer(x))\n$$\n\n- Decoder同样是由N=6个相同的块组成，每个块有3个子层，有两个和Encoder中一模一样。增加了一个带Mask的多头注意力层。**Decoder最开始的输入在训练时和预测时不一样**，在**训练时是把所有的翻译结果一次性输入**，并行化提高速度。而**预测时是类似于RNN一样的串行方式**，第一次给Decoder输入句子的开始符号，然后得到第一个翻译结果，再将第一个翻译结果当作输入送入Decoder。总结来说就是：**每次Decoder的输入为之前所有时间步的结果**。而在训练时，是一次导入所有结果，所以需要**Mask掉未来时间步的翻译结果**。\n\n\n\n### 2.2 多头注意力层\n\n- 进行的操作其实就是上文提到的多头注意力机制：**将输入分别乘一个矩阵W，转换成Q、K、V，再计算权重并加权平均，得到Z。将上述过程进行h次，每次使用的是相互独立的W，再将Z连结，再乘一个权重矩阵，得到最终结果。**\n\n- 需要注意的是Decoder中的Masked Multi-Head Attention。我们在**预测时，肯定是无法知道未来的信息的（也就是之后时间步的输出），但是在训练时我们是将翻译结果一次性使用一个矩阵导入的**。所以为了保持一致性，我们需要在**训练时屏蔽掉未来的信息，即当前时间步t的输出只取决于t-1及其之前的时间步。**\n- 下方为一个Attention Map，每个单元代表该行对四个列对应的权值。如第一行代表\"I\"分别对\"I\"、\"have\"、\"a\"、\"dream\"的权值。\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/64.jpg\" alt=\"64\" style=\"zoom:50%;\" />\n\n显然在通过\"I\"预测\"have\"时，是不知道后面的\"have\"、\"a\"、\"dream\"的，所以需要通过Mask屏蔽掉未来的信息，其他时间步的时候类似：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/56.jpg\" alt=\"56\" style=\"zoom:50%;\" />\n\n上图就是经过Mask后的Attention Map，将每个时间步未来的信息进行了屏蔽，具体的做法是：**在计算V的权重时，softmax之前将对应的值设为$$-\\infty$$**\n\n\n\n### 2.3 全连接层\n\n- 每个全连接子层有两个层，进行的运算为：\n\n$$\n\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n$$\n\n第二层是没有激活函数的\n\n- 层的输入为$$d_{model} = 512$$，经过第一层变为$$d_{ff} = 2048$$，经过第二层又变为512维。\n\n\n\n### 2.4 Layer Normalization\n\n- BN是对一个batch-size样本内的每个特征做归一化，LN是对每个样本的每个时间步中所有特征做归一化\n\n- **使用LN，假设输入X，X.shape = (batch_size,time_step,embedding_dim) 那么mean.shape = std.shape = (batch_size,time_step,1)。即对embedding维做归一化，另外LN中同样是有放缩的参数的**\n- 而使用BN，那么mean.shape = std.shape = (1,time_step,embedding_dim)，即对batch维做归一化\n\n- **选用LN而弃用BN的原因：**BN需要较大的batch_size来保证对于期望、方差的统计的可靠性，对于CNN、Dense层来说则好统计。但是在天然变长的NLP任务中，如果选用BN需要**对每个时间步的状态进行统计**，这会导致在偏长的序列中，**靠后的时间步的统计量不足**。相比之下使用LN则不会有这种限制\n\n- **而对embedding层进行归一化也更具有解释性，因为embedding层的每个值都是直接相关的**\n\n\n\n### 2.5 词嵌入\n\n- Transformer中的embedding是训练出来的，所以总的结构类似于跳字模型或者连续词袋模型，具体可看[跳字模型](https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#1-2-%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B%EF%BC%88skip-gram%EF%BC%89)中的具体实现，简单来说就是：一个单隐藏层的全连接网络，输入one-hot向量，乘一个V矩阵，得到隐藏层值，再乘一个U矩阵，得到输入层值，再softmax计算概率最后梯度下降。**而Decoder的前后就是分别为乘V和乘U两个操作，分别称为embedding转换和pre-softmax linear transformation**\n- 在一般的词嵌入模型当中，U、V矩阵一般是两个不同的矩阵，而Transformer中使用了**Weight Tying**，即U、V使用同一矩阵**（注意只是共用权重矩阵，偏差还是相互独立的）**\n- one-hot向量和对U的操作是“指定抽取”，即取出某个单词的向量行；pre-softmax对V的操作是“逐个点积”，对隐层的输出，依次计算其和每个单词向量行的变换结果。虽然具体的操作不同，**但在本质上，U和V都是对任一的单词进行向量化表示，然后按词表序stack起来。因此，两个权重矩阵在语义上是相通的**。\n- 也是由于上面两种操作方式的不同，且V的更新在靠近输出层，所以**U在反向传播中不如V训练得充分**，将两者绑定在一起缓和了这一问题，可以训练得到质量更高的新矩阵。并且**Weight Tying 可以显著减小模型的参数量**。\n- 在embdding层中，**为了让embedding层的权重值不至于过小，乘以$$\\sqrt{d_{model} }$$后与位置编码的值差不多，可以保护原有向量空间不被破坏**。\n\n\n\n### 2.6 Positional Encode\n\n- 由于模型摒弃了RNN结构，所以**无法获得序列的位置信息**，而为了获得这种位置信息我们需要引入Positional Embedding来表示位置信息。\n- Positional Embedding的维度同样是$$d_{model}$$，并且在一开始的时候和Embedding进行相加，具体表示为：\n\n$$\n\\begin{aligned}\nP E_{(p o s, 2 i)} &=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model } }}\\right) \\\\\nP E_{(p o s, 2 i+1)} &=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model } }}\\right)\n\\end{aligned}\n$$\n\n其中pos代表第几个序列位置（最大值为规定的最长序列长度），i代表第几个维度（最大值为$$d_{mdoel} / 2$$）\n\n- 以上公式不仅能很好的表示单词的绝对位置，还能表示出相对位置：**相隔 k 个词的两个位置 pos 和 pos+k 的位置编码是由 k 的位置编码定义的一个线性变换**：\n\n$$\n\\begin{array}{c}\nP E(p o s+k, 2 i)=P E(p o s, 2 i) P E(k, 2 i+1)+P E(p o s, 2 i+1) P E(k, 2 i) \\\\\nP E(p o s+k, 2 i+1)=P E(p o s, 2 i+1) P E(k, 2 i+1)-P E(p o s, 2 i) P E(k, 2 i)\n\\end{array}\n$$\n\n- 采用正弦方式和学习方式position embedding结果几乎一样。但采用正弦，因为**能让模型推断出比训练期间遇到的序列长度更长的序列长度**\n\n\n\n\n\n# 3 模型训练\n\n### 3.1 Optimizer && learning rate\n\n- 采用Adam优化器，参数都是模型参数：$$\\beta_1=0.9,\\beta_2=0.98,\\epsilon=10^{-9}$$\n\n- Transformer 的学习率更新公式叫作“**noam**”，它将 warmup 和 decay 两个部分组合在一起，总体趋势是**先增加后减小**，具体公式为：\n\n$$\n\\text { lrate }=d_{\\text {model } }^{-0.5} \\cdot \\min \\left(\\text { step }_{-} \\text {num }^{-0.5}, \\text { step_ }_{-} \\text {num } \\cdot \\text { warmup_steps }^{-1.5}\\right)\n$$\n\n- 公式实际上是一个以warmup_steps为分界点的分段函数。该点之前是warmup部分，采用线性函数的形式，且warmup_steps越大，斜率越小。该点之后是decay部分，采用负幂的衰减形式，衰减速度先快后慢：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-7c98b2c7ca4467ab770da064bb2b58ba_720w.jpg\" alt=\"v2-7c98b2c7ca4467ab770da064bb2b58ba_720w\"  />\n\n- **设置warmup的原因：**在CV领域中常常这样做，在《Deep Residual Learning for Image Recognition》中，作者训练110层的超深网络是就用过类似策略：\n\n> In this case, we find that **the initial learning rate of 0.1 is slightly too large to start converging**. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.\n\n对于 Transformer 这样的大型网络，**在训练初始阶段，模型尚不稳定，较大的学习率会增加收敛难度**。因此，使用较小的学习率进行 warmup，等 loss 下降到一定程度后，再恢复回常规学习率。\n\n\n\n### 3.2 Dropout\n\n- 在每个子块中，输出结果加入到残差结构和layer normalization之前，进行Dropout\n- 并且还在Encoder和Decoder最开始的两种embedding相加的时候，使用了Dropout\n- Dropout的概率均为0.1\n\n\n\n### 3.3 Label Smoothing\n\n- 为了不要对正确类别\"too confident\"（防止过拟合），Transformer中还使用了Label Smoothing。这种方法**会增大困惑度（perplexity），但是可以提高accuracy和BLEU**。\n- 假设目标类别为y，任意类别为k，ground-truth 分布为q(k)，模型预测分布为p(k)。 显然，当k=y时，q(k)=1。当k$$\\neq$$y时，q(k)=0。**LSR（Label Smoothing Regularization）为了让模型的输出不要过于贴合单点分布，选择在gound-truth中加入噪声**。即削弱y的概率，并整体叠加一个独立于训练样例的均匀分布u(k)：\n\n$$\nq^{\\prime}(k)=(1-\\epsilon) q(k)+\\epsilon u(k)=(1-\\epsilon) q(k)+\\epsilon / K\n$$\n\n其中K为softmax的类别数，拆开来看就是：\n$$\n\\begin{array}{ll}\nq^{\\prime}(k)=1-\\epsilon+\\epsilon / K, & k=y \\\\\nq^{\\prime}(k)=\\epsilon / K, & k \\neq y\n\\end{array}\n$$\n所有类别的概率和仍然是归一的。说白了就是把最高点砍掉一点，多出来的概率平均分给所有人。\n\n\n","source":"_posts/Transformer总结.md","raw":"---\ntitle: Transformer总结\nmath: true\ndate: 2022-1-25\n---\n\n- Transformer摒弃了传统的CNN/RNN模型，而是用纯注意力机制， **相对于RNN，实现了并行化，并且消除了memory对于距离的依赖性（无法捕捉长距离依赖）。**\n\n\n\n\n\n# 1 注意力机制\n\n- **注意力机制中分别有key、query、value（一般key=value），通过key、query之间的相似度，计算得到每个value对应的权值，再对所有value加权求和，得到一整个序列的表征。其中对于自己本身的注意力机制称为self-attention（自注意力机制），即key=value=query**\n\n\n\n### 1.1 Scaled Dot-Product Attention(点积)\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112222285.png\" alt=\"image-20220512112222285\" style=\"zoom: 37%;\" />\n\n- 在计算时，我们是将query、key、value（分别为$$d_k, d_k, d_v$$维）打包成Q，K$$\\in R^{N \\times d_k}$$，V$$\\in R^{N \\times d_v}$$，具体做法是：\n\n> 将送进来的输入input$$\\in R^{N \\times d_{model} }$$（其中$$d_{model}$$为embedding的维度，且q、k、v三者的input可能各自不同），input分别乘$$W^Q、W^K \\in R^{d_{model} \\times d_k}$$，$$W^V \\in R^{ {d_{model} \\times d_v} }$$即可得到Q、K、V\n\n- 在计算权值时，将Q、K相乘，再除以$$\\sqrt{d_k}$$，再softmax得到权值。**除以$$\\sqrt{d_k}$$的原因**：\n\n> **维度过大会使Q、K相乘的结果过大，容易把softmax的区域推向梯度极小的区域。并且实验证明在$$d_k$$较小时，其实除不除效果差不多**\n\n- 得到权重后再和V相乘，总过程为：\n\n$$\n\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T} }{\\sqrt{d_{k} }}\\right) V\n$$\n\n- 还有一种较常用的注意力机制叫Additive attention， 是使用一个单隐藏层的全连接网络计算权重，两者效果差不多，**但是dot-product会快得多**\n\n\n\n### 1.2 Multi-Head Attention(多头注意力机制)\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112235945.png\" alt=\"image-20220512112235945\" style=\"zoom:35%;\" />\n\n- 多头注意力机制本质上就是**做多次Scaled Dot-Product Attention**\n- 具体做法是：\n\n> **重复做h次Scaled Dot-Product Attention（每次的W权重矩阵分别独立），将每次得到的结果$$Z \\in R^{N \\times d_v}$$在第二维连结，形状变为$$R^{N \\times hd_v}$$，再乘一个$$W^O \\in R^{hd_v \\times d_{model} }$$，即可得到形状为$$R^{N \\times d_{model} }$$的最终结果**\n\n- 总过程为：\n\n$$\n\\begin{aligned}\n\\operatorname{MultiHead}(Q, K, V) &=\\operatorname{Concat}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{\\mathrm{h} }\\right) W^{O} \\\\\n\\text { where head } &=\\operatorname{Attention}\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)\n\\end{aligned}\n$$\n\n- 在base模型中取的$$h = 8$$，且$$d_k = d_v = d_{model}/h = 64$$\n\n- **多头注意力的好处：类似于CNN中的通道，能提取到不同子空间下的特征。多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。**如果单纯使用单注意力头+平均化，会抑制这一点\n\n> Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  \n\n\n\n### 1.3 自注意力机制的好处\n\n- 自注意力机制最大的好处肯定是实现了**并行化，加快了训练速度**。并且得到的结果相比于其他方法（如全局平均池化），**更具有解释性**，self-attention是可以退化成平均的，所以结果肯定好于平均。\n\n- 论文从每层的总计算复杂度、可并行化的计算数量（用顺序操作的最小量来衡量）、长距离依赖的距离三个方面进行了对比：\n\n![image-20220512114401561](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512114401561.png)\n\n- **并且单个注意力头不仅清楚地学习执行不同的任务，而且许多似乎表现出与句子的句法和语义结构相关的行为**\n\n> Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences  \n\n\n\n\n\n# 2 模型结构\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512121448150.png\" alt=\"image-20220512121448150\" style=\"zoom:28%;\" />\n\n- 图中多头注意力层的输入从左到右依次为V、K、Q\n\n### 2.1 Encoder和Decoder\n\n- Encoder由N=6个相同的块组成，每个块有两个子层，一个多头注意力层，一个全连接层，子层输入输出都是都是$$R^{N \\times d_{model} }$$，其中N为时间步个数，也就是词数。**并且在每个子层都有一个残差结构+LayerNorm**，先残差后LayerNorm：\n\n$$\nLayerNorm(x + Sublayer(x))\n$$\n\n- Decoder同样是由N=6个相同的块组成，每个块有3个子层，有两个和Encoder中一模一样。增加了一个带Mask的多头注意力层。**Decoder最开始的输入在训练时和预测时不一样**，在**训练时是把所有的翻译结果一次性输入**，并行化提高速度。而**预测时是类似于RNN一样的串行方式**，第一次给Decoder输入句子的开始符号，然后得到第一个翻译结果，再将第一个翻译结果当作输入送入Decoder。总结来说就是：**每次Decoder的输入为之前所有时间步的结果**。而在训练时，是一次导入所有结果，所以需要**Mask掉未来时间步的翻译结果**。\n\n\n\n### 2.2 多头注意力层\n\n- 进行的操作其实就是上文提到的多头注意力机制：**将输入分别乘一个矩阵W，转换成Q、K、V，再计算权重并加权平均，得到Z。将上述过程进行h次，每次使用的是相互独立的W，再将Z连结，再乘一个权重矩阵，得到最终结果。**\n\n- 需要注意的是Decoder中的Masked Multi-Head Attention。我们在**预测时，肯定是无法知道未来的信息的（也就是之后时间步的输出），但是在训练时我们是将翻译结果一次性使用一个矩阵导入的**。所以为了保持一致性，我们需要在**训练时屏蔽掉未来的信息，即当前时间步t的输出只取决于t-1及其之前的时间步。**\n- 下方为一个Attention Map，每个单元代表该行对四个列对应的权值。如第一行代表\"I\"分别对\"I\"、\"have\"、\"a\"、\"dream\"的权值。\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/64.jpg\" alt=\"64\" style=\"zoom:50%;\" />\n\n显然在通过\"I\"预测\"have\"时，是不知道后面的\"have\"、\"a\"、\"dream\"的，所以需要通过Mask屏蔽掉未来的信息，其他时间步的时候类似：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/56.jpg\" alt=\"56\" style=\"zoom:50%;\" />\n\n上图就是经过Mask后的Attention Map，将每个时间步未来的信息进行了屏蔽，具体的做法是：**在计算V的权重时，softmax之前将对应的值设为$$-\\infty$$**\n\n\n\n### 2.3 全连接层\n\n- 每个全连接子层有两个层，进行的运算为：\n\n$$\n\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n$$\n\n第二层是没有激活函数的\n\n- 层的输入为$$d_{model} = 512$$，经过第一层变为$$d_{ff} = 2048$$，经过第二层又变为512维。\n\n\n\n### 2.4 Layer Normalization\n\n- BN是对一个batch-size样本内的每个特征做归一化，LN是对每个样本的每个时间步中所有特征做归一化\n\n- **使用LN，假设输入X，X.shape = (batch_size,time_step,embedding_dim) 那么mean.shape = std.shape = (batch_size,time_step,1)。即对embedding维做归一化，另外LN中同样是有放缩的参数的**\n- 而使用BN，那么mean.shape = std.shape = (1,time_step,embedding_dim)，即对batch维做归一化\n\n- **选用LN而弃用BN的原因：**BN需要较大的batch_size来保证对于期望、方差的统计的可靠性，对于CNN、Dense层来说则好统计。但是在天然变长的NLP任务中，如果选用BN需要**对每个时间步的状态进行统计**，这会导致在偏长的序列中，**靠后的时间步的统计量不足**。相比之下使用LN则不会有这种限制\n\n- **而对embedding层进行归一化也更具有解释性，因为embedding层的每个值都是直接相关的**\n\n\n\n### 2.5 词嵌入\n\n- Transformer中的embedding是训练出来的，所以总的结构类似于跳字模型或者连续词袋模型，具体可看[跳字模型](https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#1-2-%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B%EF%BC%88skip-gram%EF%BC%89)中的具体实现，简单来说就是：一个单隐藏层的全连接网络，输入one-hot向量，乘一个V矩阵，得到隐藏层值，再乘一个U矩阵，得到输入层值，再softmax计算概率最后梯度下降。**而Decoder的前后就是分别为乘V和乘U两个操作，分别称为embedding转换和pre-softmax linear transformation**\n- 在一般的词嵌入模型当中，U、V矩阵一般是两个不同的矩阵，而Transformer中使用了**Weight Tying**，即U、V使用同一矩阵**（注意只是共用权重矩阵，偏差还是相互独立的）**\n- one-hot向量和对U的操作是“指定抽取”，即取出某个单词的向量行；pre-softmax对V的操作是“逐个点积”，对隐层的输出，依次计算其和每个单词向量行的变换结果。虽然具体的操作不同，**但在本质上，U和V都是对任一的单词进行向量化表示，然后按词表序stack起来。因此，两个权重矩阵在语义上是相通的**。\n- 也是由于上面两种操作方式的不同，且V的更新在靠近输出层，所以**U在反向传播中不如V训练得充分**，将两者绑定在一起缓和了这一问题，可以训练得到质量更高的新矩阵。并且**Weight Tying 可以显著减小模型的参数量**。\n- 在embdding层中，**为了让embedding层的权重值不至于过小，乘以$$\\sqrt{d_{model} }$$后与位置编码的值差不多，可以保护原有向量空间不被破坏**。\n\n\n\n### 2.6 Positional Encode\n\n- 由于模型摒弃了RNN结构，所以**无法获得序列的位置信息**，而为了获得这种位置信息我们需要引入Positional Embedding来表示位置信息。\n- Positional Embedding的维度同样是$$d_{model}$$，并且在一开始的时候和Embedding进行相加，具体表示为：\n\n$$\n\\begin{aligned}\nP E_{(p o s, 2 i)} &=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model } }}\\right) \\\\\nP E_{(p o s, 2 i+1)} &=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model } }}\\right)\n\\end{aligned}\n$$\n\n其中pos代表第几个序列位置（最大值为规定的最长序列长度），i代表第几个维度（最大值为$$d_{mdoel} / 2$$）\n\n- 以上公式不仅能很好的表示单词的绝对位置，还能表示出相对位置：**相隔 k 个词的两个位置 pos 和 pos+k 的位置编码是由 k 的位置编码定义的一个线性变换**：\n\n$$\n\\begin{array}{c}\nP E(p o s+k, 2 i)=P E(p o s, 2 i) P E(k, 2 i+1)+P E(p o s, 2 i+1) P E(k, 2 i) \\\\\nP E(p o s+k, 2 i+1)=P E(p o s, 2 i+1) P E(k, 2 i+1)-P E(p o s, 2 i) P E(k, 2 i)\n\\end{array}\n$$\n\n- 采用正弦方式和学习方式position embedding结果几乎一样。但采用正弦，因为**能让模型推断出比训练期间遇到的序列长度更长的序列长度**\n\n\n\n\n\n# 3 模型训练\n\n### 3.1 Optimizer && learning rate\n\n- 采用Adam优化器，参数都是模型参数：$$\\beta_1=0.9,\\beta_2=0.98,\\epsilon=10^{-9}$$\n\n- Transformer 的学习率更新公式叫作“**noam**”，它将 warmup 和 decay 两个部分组合在一起，总体趋势是**先增加后减小**，具体公式为：\n\n$$\n\\text { lrate }=d_{\\text {model } }^{-0.5} \\cdot \\min \\left(\\text { step }_{-} \\text {num }^{-0.5}, \\text { step_ }_{-} \\text {num } \\cdot \\text { warmup_steps }^{-1.5}\\right)\n$$\n\n- 公式实际上是一个以warmup_steps为分界点的分段函数。该点之前是warmup部分，采用线性函数的形式，且warmup_steps越大，斜率越小。该点之后是decay部分，采用负幂的衰减形式，衰减速度先快后慢：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-7c98b2c7ca4467ab770da064bb2b58ba_720w.jpg\" alt=\"v2-7c98b2c7ca4467ab770da064bb2b58ba_720w\"  />\n\n- **设置warmup的原因：**在CV领域中常常这样做，在《Deep Residual Learning for Image Recognition》中，作者训练110层的超深网络是就用过类似策略：\n\n> In this case, we find that **the initial learning rate of 0.1 is slightly too large to start converging**. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.\n\n对于 Transformer 这样的大型网络，**在训练初始阶段，模型尚不稳定，较大的学习率会增加收敛难度**。因此，使用较小的学习率进行 warmup，等 loss 下降到一定程度后，再恢复回常规学习率。\n\n\n\n### 3.2 Dropout\n\n- 在每个子块中，输出结果加入到残差结构和layer normalization之前，进行Dropout\n- 并且还在Encoder和Decoder最开始的两种embedding相加的时候，使用了Dropout\n- Dropout的概率均为0.1\n\n\n\n### 3.3 Label Smoothing\n\n- 为了不要对正确类别\"too confident\"（防止过拟合），Transformer中还使用了Label Smoothing。这种方法**会增大困惑度（perplexity），但是可以提高accuracy和BLEU**。\n- 假设目标类别为y，任意类别为k，ground-truth 分布为q(k)，模型预测分布为p(k)。 显然，当k=y时，q(k)=1。当k$$\\neq$$y时，q(k)=0。**LSR（Label Smoothing Regularization）为了让模型的输出不要过于贴合单点分布，选择在gound-truth中加入噪声**。即削弱y的概率，并整体叠加一个独立于训练样例的均匀分布u(k)：\n\n$$\nq^{\\prime}(k)=(1-\\epsilon) q(k)+\\epsilon u(k)=(1-\\epsilon) q(k)+\\epsilon / K\n$$\n\n其中K为softmax的类别数，拆开来看就是：\n$$\n\\begin{array}{ll}\nq^{\\prime}(k)=1-\\epsilon+\\epsilon / K, & k=y \\\\\nq^{\\prime}(k)=\\epsilon / K, & k \\neq y\n\\end{array}\n$$\n所有类别的概率和仍然是归一的。说白了就是把最高点砍掉一点，多出来的概率平均分给所有人。\n\n\n","slug":"Transformer总结","published":1,"updated":"2022-12-20T06:17:38.438Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1i000f7csz6hxz4bdx","content":"<ul>\n<li>Transformer摒弃了传统的CNN/RNN模型，而是用纯注意力机制， <strong>相对于RNN，实现了并行化，并且消除了memory对于距离的依赖性（无法捕捉长距离依赖）。</strong></li>\n</ul>\n<h1 id=\"1-注意力机制\"><a href=\"#1-注意力机制\" class=\"headerlink\" title=\"1 注意力机制\"></a>1 注意力机制</h1><ul>\n<li><strong>注意力机制中分别有key、query、value（一般key=value），通过key、query之间的相似度，计算得到每个value对应的权值，再对所有value加权求和，得到一整个序列的表征。其中对于自己本身的注意力机制称为self-attention（自注意力机制），即key=value=query</strong></li>\n</ul>\n<h3 id=\"1-1-Scaled-Dot-Product-Attention-点积\"><a href=\"#1-1-Scaled-Dot-Product-Attention-点积\" class=\"headerlink\" title=\"1.1 Scaled Dot-Product Attention(点积)\"></a>1.1 Scaled Dot-Product Attention(点积)</h3><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112222285.png\" alt=\"image-20220512112222285\" style=\"zoom: 37%;\" /></p>\n<ul>\n<li>在计算时，我们是将query、key、value（分别为<script type=\"math/tex\">d_k, d_k, d_v</script>维）打包成Q，K<script type=\"math/tex\">\\in R^{N \\times d_k}</script>，V<script type=\"math/tex\">\\in R^{N \\times d_v}</script>，具体做法是：</li>\n</ul>\n<blockquote>\n<p>将送进来的输入input<script type=\"math/tex\">\\in R^{N \\times d_{model} }</script>（其中<script type=\"math/tex\">d_{model}</script>为embedding的维度，且q、k、v三者的input可能各自不同），input分别乘<script type=\"math/tex\">W^Q、W^K \\in R^{d_{model} \\times d_k}</script>，<script type=\"math/tex\">W^V \\in R^{ {d_{model} \\times d_v} }</script>即可得到Q、K、V</p>\n</blockquote>\n<ul>\n<li>在计算权值时，将Q、K相乘，再除以<script type=\"math/tex\">\\sqrt{d_k}</script>，再softmax得到权值。<strong>除以<script type=\"math/tex\">\\sqrt{d_k}</script>的原因</strong>：</li>\n</ul>\n<blockquote>\n<p><strong>维度过大会使Q、K相乘的结果过大，容易把softmax的区域推向梯度极小的区域。并且实验证明在<script type=\"math/tex\">d_k</script>较小时，其实除不除效果差不多</strong></p>\n</blockquote>\n<ul>\n<li>得到权重后再和V相乘，总过程为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T} }{\\sqrt{d_{k} }}\\right) V</script><ul>\n<li>还有一种较常用的注意力机制叫Additive attention， 是使用一个单隐藏层的全连接网络计算权重，两者效果差不多，<strong>但是dot-product会快得多</strong></li>\n</ul>\n<h3 id=\"1-2-Multi-Head-Attention-多头注意力机制\"><a href=\"#1-2-Multi-Head-Attention-多头注意力机制\" class=\"headerlink\" title=\"1.2 Multi-Head Attention(多头注意力机制)\"></a>1.2 Multi-Head Attention(多头注意力机制)</h3><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112235945.png\" alt=\"image-20220512112235945\" style=\"zoom:35%;\" /></p>\n<ul>\n<li>多头注意力机制本质上就是<strong>做多次Scaled Dot-Product Attention</strong></li>\n<li>具体做法是：</li>\n</ul>\n<blockquote>\n<p><strong>重复做h次Scaled Dot-Product Attention（每次的W权重矩阵分别独立），将每次得到的结果<script type=\"math/tex\">Z \\in R^{N \\times d_v}</script>在第二维连结，形状变为<script type=\"math/tex\">R^{N \\times hd_v}</script>，再乘一个<script type=\"math/tex\">W^O \\in R^{hd_v \\times d_{model} }</script>，即可得到形状为<script type=\"math/tex\">R^{N \\times d_{model} }</script>的最终结果</strong></p>\n</blockquote>\n<ul>\n<li>总过程为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{MultiHead}(Q, K, V) &=\\operatorname{Concat}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{\\mathrm{h} }\\right) W^{O} \\\\\n\\text { where head } &=\\operatorname{Attention}\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)\n\\end{aligned}</script><ul>\n<li><p>在base模型中取的<script type=\"math/tex\">h = 8</script>，且<script type=\"math/tex\">d_k = d_v = d_{model}/h = 64</script></p>\n</li>\n<li><p><strong>多头注意力的好处：类似于CNN中的通道，能提取到不同子空间下的特征。多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。</strong>如果单纯使用单注意力头+平均化，会抑制这一点</p>\n</li>\n</ul>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  </p>\n</blockquote>\n<h3 id=\"1-3-自注意力机制的好处\"><a href=\"#1-3-自注意力机制的好处\" class=\"headerlink\" title=\"1.3 自注意力机制的好处\"></a>1.3 自注意力机制的好处</h3><ul>\n<li><p>自注意力机制最大的好处肯定是实现了<strong>并行化，加快了训练速度</strong>。并且得到的结果相比于其他方法（如全局平均池化），<strong>更具有解释性</strong>，self-attention是可以退化成平均的，所以结果肯定好于平均。</p>\n</li>\n<li><p>论文从每层的总计算复杂度、可并行化的计算数量（用顺序操作的最小量来衡量）、长距离依赖的距离三个方面进行了对比：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512114401561.png\" alt=\"image-20220512114401561\"></p>\n<ul>\n<li><strong>并且单个注意力头不仅清楚地学习执行不同的任务，而且许多似乎表现出与句子的句法和语义结构相关的行为</strong></li>\n</ul>\n<blockquote>\n<p>Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences  </p>\n</blockquote>\n<h1 id=\"2-模型结构\"><a href=\"#2-模型结构\" class=\"headerlink\" title=\"2 模型结构\"></a>2 模型结构</h1><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512121448150.png\" alt=\"image-20220512121448150\" style=\"zoom:28%;\" /></p>\n<ul>\n<li>图中多头注意力层的输入从左到右依次为V、K、Q</li>\n</ul>\n<h3 id=\"2-1-Encoder和Decoder\"><a href=\"#2-1-Encoder和Decoder\" class=\"headerlink\" title=\"2.1 Encoder和Decoder\"></a>2.1 Encoder和Decoder</h3><ul>\n<li>Encoder由N=6个相同的块组成，每个块有两个子层，一个多头注意力层，一个全连接层，子层输入输出都是都是<script type=\"math/tex\">R^{N \\times d_{model} }</script>，其中N为时间步个数，也就是词数。<strong>并且在每个子层都有一个残差结构+LayerNorm</strong>，先残差后LayerNorm：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nLayerNorm(x + Sublayer(x))</script><ul>\n<li>Decoder同样是由N=6个相同的块组成，每个块有3个子层，有两个和Encoder中一模一样。增加了一个带Mask的多头注意力层。<strong>Decoder最开始的输入在训练时和预测时不一样</strong>，在<strong>训练时是把所有的翻译结果一次性输入</strong>，并行化提高速度。而<strong>预测时是类似于RNN一样的串行方式</strong>，第一次给Decoder输入句子的开始符号，然后得到第一个翻译结果，再将第一个翻译结果当作输入送入Decoder。总结来说就是：<strong>每次Decoder的输入为之前所有时间步的结果</strong>。而在训练时，是一次导入所有结果，所以需要<strong>Mask掉未来时间步的翻译结果</strong>。</li>\n</ul>\n<h3 id=\"2-2-多头注意力层\"><a href=\"#2-2-多头注意力层\" class=\"headerlink\" title=\"2.2 多头注意力层\"></a>2.2 多头注意力层</h3><ul>\n<li><p>进行的操作其实就是上文提到的多头注意力机制：<strong>将输入分别乘一个矩阵W，转换成Q、K、V，再计算权重并加权平均，得到Z。将上述过程进行h次，每次使用的是相互独立的W，再将Z连结，再乘一个权重矩阵，得到最终结果。</strong></p>\n</li>\n<li><p>需要注意的是Decoder中的Masked Multi-Head Attention。我们在<strong>预测时，肯定是无法知道未来的信息的（也就是之后时间步的输出），但是在训练时我们是将翻译结果一次性使用一个矩阵导入的</strong>。所以为了保持一致性，我们需要在<strong>训练时屏蔽掉未来的信息，即当前时间步t的输出只取决于t-1及其之前的时间步。</strong></p>\n</li>\n<li>下方为一个Attention Map，每个单元代表该行对四个列对应的权值。如第一行代表”I”分别对”I”、”have”、”a”、”dream”的权值。</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/64.jpg\" alt=\"64\" style=\"zoom:50%;\" /></p>\n<p>显然在通过”I”预测”have”时，是不知道后面的”have”、”a”、”dream”的，所以需要通过Mask屏蔽掉未来的信息，其他时间步的时候类似：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/56.jpg\" alt=\"56\" style=\"zoom:50%;\" /></p>\n<p>上图就是经过Mask后的Attention Map，将每个时间步未来的信息进行了屏蔽，具体的做法是：<strong>在计算V的权重时，softmax之前将对应的值设为<script type=\"math/tex\">-\\infty</script></strong></p>\n<h3 id=\"2-3-全连接层\"><a href=\"#2-3-全连接层\" class=\"headerlink\" title=\"2.3 全连接层\"></a>2.3 全连接层</h3><ul>\n<li>每个全连接子层有两个层，进行的运算为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}</script><p>第二层是没有激活函数的</p>\n<ul>\n<li>层的输入为<script type=\"math/tex\">d_{model} = 512</script>，经过第一层变为<script type=\"math/tex\">d_{ff} = 2048</script>，经过第二层又变为512维。</li>\n</ul>\n<h3 id=\"2-4-Layer-Normalization\"><a href=\"#2-4-Layer-Normalization\" class=\"headerlink\" title=\"2.4 Layer Normalization\"></a>2.4 Layer Normalization</h3><ul>\n<li><p>BN是对一个batch-size样本内的每个特征做归一化，LN是对每个样本的每个时间步中所有特征做归一化</p>\n</li>\n<li><p><strong>使用LN，假设输入X，X.shape = (batch_size,time_step,embedding_dim) 那么mean.shape = std.shape = (batch_size,time_step,1)。即对embedding维做归一化，另外LN中同样是有放缩的参数的</strong></p>\n</li>\n<li><p>而使用BN，那么mean.shape = std.shape = (1,time_step,embedding_dim)，即对batch维做归一化</p>\n</li>\n<li><p><strong>选用LN而弃用BN的原因：</strong>BN需要较大的batch_size来保证对于期望、方差的统计的可靠性，对于CNN、Dense层来说则好统计。但是在天然变长的NLP任务中，如果选用BN需要<strong>对每个时间步的状态进行统计</strong>，这会导致在偏长的序列中，<strong>靠后的时间步的统计量不足</strong>。相比之下使用LN则不会有这种限制</p>\n</li>\n<li><p><strong>而对embedding层进行归一化也更具有解释性，因为embedding层的每个值都是直接相关的</strong></p>\n</li>\n</ul>\n<h3 id=\"2-5-词嵌入\"><a href=\"#2-5-词嵌入\" class=\"headerlink\" title=\"2.5 词嵌入\"></a>2.5 词嵌入</h3><ul>\n<li>Transformer中的embedding是训练出来的，所以总的结构类似于跳字模型或者连续词袋模型，具体可看<a href=\"https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#1-2-%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B%EF%BC%88skip-gram%EF%BC%89\">跳字模型</a>中的具体实现，简单来说就是：一个单隐藏层的全连接网络，输入one-hot向量，乘一个V矩阵，得到隐藏层值，再乘一个U矩阵，得到输入层值，再softmax计算概率最后梯度下降。<strong>而Decoder的前后就是分别为乘V和乘U两个操作，分别称为embedding转换和pre-softmax linear transformation</strong></li>\n<li>在一般的词嵌入模型当中，U、V矩阵一般是两个不同的矩阵，而Transformer中使用了<strong>Weight Tying</strong>，即U、V使用同一矩阵<strong>（注意只是共用权重矩阵，偏差还是相互独立的）</strong></li>\n<li>one-hot向量和对U的操作是“指定抽取”，即取出某个单词的向量行；pre-softmax对V的操作是“逐个点积”，对隐层的输出，依次计算其和每个单词向量行的变换结果。虽然具体的操作不同，<strong>但在本质上，U和V都是对任一的单词进行向量化表示，然后按词表序stack起来。因此，两个权重矩阵在语义上是相通的</strong>。</li>\n<li>也是由于上面两种操作方式的不同，且V的更新在靠近输出层，所以<strong>U在反向传播中不如V训练得充分</strong>，将两者绑定在一起缓和了这一问题，可以训练得到质量更高的新矩阵。并且<strong>Weight Tying 可以显著减小模型的参数量</strong>。</li>\n<li>在embdding层中，<strong>为了让embedding层的权重值不至于过小，乘以<script type=\"math/tex\">\\sqrt{d_{model} }</script>后与位置编码的值差不多，可以保护原有向量空间不被破坏</strong>。</li>\n</ul>\n<h3 id=\"2-6-Positional-Encode\"><a href=\"#2-6-Positional-Encode\" class=\"headerlink\" title=\"2.6 Positional Encode\"></a>2.6 Positional Encode</h3><ul>\n<li>由于模型摒弃了RNN结构，所以<strong>无法获得序列的位置信息</strong>，而为了获得这种位置信息我们需要引入Positional Embedding来表示位置信息。</li>\n<li>Positional Embedding的维度同样是<script type=\"math/tex\">d_{model}</script>，并且在一开始的时候和Embedding进行相加，具体表示为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP E_{(p o s, 2 i)} &=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model } }}\\right) \\\\\nP E_{(p o s, 2 i+1)} &=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model } }}\\right)\n\\end{aligned}</script><p>其中pos代表第几个序列位置（最大值为规定的最长序列长度），i代表第几个维度（最大值为<script type=\"math/tex\">d_{mdoel} / 2</script>）</p>\n<ul>\n<li>以上公式不仅能很好的表示单词的绝对位置，还能表示出相对位置：<strong>相隔 k 个词的两个位置 pos 和 pos+k 的位置编码是由 k 的位置编码定义的一个线性变换</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nP E(p o s+k, 2 i)=P E(p o s, 2 i) P E(k, 2 i+1)+P E(p o s, 2 i+1) P E(k, 2 i) \\\\\nP E(p o s+k, 2 i+1)=P E(p o s, 2 i+1) P E(k, 2 i+1)-P E(p o s, 2 i) P E(k, 2 i)\n\\end{array}</script><ul>\n<li>采用正弦方式和学习方式position embedding结果几乎一样。但采用正弦，因为<strong>能让模型推断出比训练期间遇到的序列长度更长的序列长度</strong></li>\n</ul>\n<h1 id=\"3-模型训练\"><a href=\"#3-模型训练\" class=\"headerlink\" title=\"3 模型训练\"></a>3 模型训练</h1><h3 id=\"3-1-Optimizer-amp-amp-learning-rate\"><a href=\"#3-1-Optimizer-amp-amp-learning-rate\" class=\"headerlink\" title=\"3.1 Optimizer &amp;&amp; learning rate\"></a>3.1 Optimizer &amp;&amp; learning rate</h3><ul>\n<li><p>采用Adam优化器，参数都是模型参数：<script type=\"math/tex\">\\beta_1=0.9,\\beta_2=0.98,\\epsilon=10^{-9}</script></p>\n</li>\n<li><p>Transformer 的学习率更新公式叫作“<strong>noam</strong>”，它将 warmup 和 decay 两个部分组合在一起，总体趋势是<strong>先增加后减小</strong>，具体公式为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\text { lrate }=d_{\\text {model } }^{-0.5} \\cdot \\min \\left(\\text { step }_{-} \\text {num }^{-0.5}, \\text { step_ }_{-} \\text {num } \\cdot \\text { warmup_steps }^{-1.5}\\right)</script><ul>\n<li>公式实际上是一个以warmup_steps为分界点的分段函数。该点之前是warmup部分，采用线性函数的形式，且warmup_steps越大，斜率越小。该点之后是decay部分，采用负幂的衰减形式，衰减速度先快后慢：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-7c98b2c7ca4467ab770da064bb2b58ba_720w.jpg\" alt=\"v2-7c98b2c7ca4467ab770da064bb2b58ba_720w\"  /></p>\n<ul>\n<li><strong>设置warmup的原因：</strong>在CV领域中常常这样做，在《Deep Residual Learning for Image Recognition》中，作者训练110层的超深网络是就用过类似策略：</li>\n</ul>\n<blockquote>\n<p>In this case, we find that <strong>the initial learning rate of 0.1 is slightly too large to start converging</strong>. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.</p>\n</blockquote>\n<p>对于 Transformer 这样的大型网络，<strong>在训练初始阶段，模型尚不稳定，较大的学习率会增加收敛难度</strong>。因此，使用较小的学习率进行 warmup，等 loss 下降到一定程度后，再恢复回常规学习率。</p>\n<h3 id=\"3-2-Dropout\"><a href=\"#3-2-Dropout\" class=\"headerlink\" title=\"3.2 Dropout\"></a>3.2 Dropout</h3><ul>\n<li>在每个子块中，输出结果加入到残差结构和layer normalization之前，进行Dropout</li>\n<li>并且还在Encoder和Decoder最开始的两种embedding相加的时候，使用了Dropout</li>\n<li>Dropout的概率均为0.1</li>\n</ul>\n<h3 id=\"3-3-Label-Smoothing\"><a href=\"#3-3-Label-Smoothing\" class=\"headerlink\" title=\"3.3 Label Smoothing\"></a>3.3 Label Smoothing</h3><ul>\n<li>为了不要对正确类别”too confident”（防止过拟合），Transformer中还使用了Label Smoothing。这种方法<strong>会增大困惑度（perplexity），但是可以提高accuracy和BLEU</strong>。</li>\n<li>假设目标类别为y，任意类别为k，ground-truth 分布为q(k)，模型预测分布为p(k)。 显然，当k=y时，q(k)=1。当k<script type=\"math/tex\">\\neq</script>y时，q(k)=0。<strong>LSR（Label Smoothing Regularization）为了让模型的输出不要过于贴合单点分布，选择在gound-truth中加入噪声</strong>。即削弱y的概率，并整体叠加一个独立于训练样例的均匀分布u(k)：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nq^{\\prime}(k)=(1-\\epsilon) q(k)+\\epsilon u(k)=(1-\\epsilon) q(k)+\\epsilon / K</script><p>其中K为softmax的类别数，拆开来看就是：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{ll}\nq^{\\prime}(k)=1-\\epsilon+\\epsilon / K, & k=y \\\\\nq^{\\prime}(k)=\\epsilon / K, & k \\neq y\n\\end{array}</script><p>所有类别的概率和仍然是归一的。说白了就是把最高点砍掉一点，多出来的概率平均分给所有人。</p>\n","site":{"data":{}},"wordcount":6328,"excerpt":"","more":"<ul>\n<li>Transformer摒弃了传统的CNN/RNN模型，而是用纯注意力机制， <strong>相对于RNN，实现了并行化，并且消除了memory对于距离的依赖性（无法捕捉长距离依赖）。</strong></li>\n</ul>\n<h1 id=\"1-注意力机制\"><a href=\"#1-注意力机制\" class=\"headerlink\" title=\"1 注意力机制\"></a>1 注意力机制</h1><ul>\n<li><strong>注意力机制中分别有key、query、value（一般key=value），通过key、query之间的相似度，计算得到每个value对应的权值，再对所有value加权求和，得到一整个序列的表征。其中对于自己本身的注意力机制称为self-attention（自注意力机制），即key=value=query</strong></li>\n</ul>\n<h3 id=\"1-1-Scaled-Dot-Product-Attention-点积\"><a href=\"#1-1-Scaled-Dot-Product-Attention-点积\" class=\"headerlink\" title=\"1.1 Scaled Dot-Product Attention(点积)\"></a>1.1 Scaled Dot-Product Attention(点积)</h3><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112222285.png\" alt=\"image-20220512112222285\" style=\"zoom: 37%;\" /></p>\n<ul>\n<li>在计算时，我们是将query、key、value（分别为<script type=\"math/tex\">d_k, d_k, d_v</script>维）打包成Q，K<script type=\"math/tex\">\\in R^{N \\times d_k}</script>，V<script type=\"math/tex\">\\in R^{N \\times d_v}</script>，具体做法是：</li>\n</ul>\n<blockquote>\n<p>将送进来的输入input<script type=\"math/tex\">\\in R^{N \\times d_{model} }</script>（其中<script type=\"math/tex\">d_{model}</script>为embedding的维度，且q、k、v三者的input可能各自不同），input分别乘<script type=\"math/tex\">W^Q、W^K \\in R^{d_{model} \\times d_k}</script>，<script type=\"math/tex\">W^V \\in R^{ {d_{model} \\times d_v} }</script>即可得到Q、K、V</p>\n</blockquote>\n<ul>\n<li>在计算权值时，将Q、K相乘，再除以<script type=\"math/tex\">\\sqrt{d_k}</script>，再softmax得到权值。<strong>除以<script type=\"math/tex\">\\sqrt{d_k}</script>的原因</strong>：</li>\n</ul>\n<blockquote>\n<p><strong>维度过大会使Q、K相乘的结果过大，容易把softmax的区域推向梯度极小的区域。并且实验证明在<script type=\"math/tex\">d_k</script>较小时，其实除不除效果差不多</strong></p>\n</blockquote>\n<ul>\n<li>得到权重后再和V相乘，总过程为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T} }{\\sqrt{d_{k} }}\\right) V</script><ul>\n<li>还有一种较常用的注意力机制叫Additive attention， 是使用一个单隐藏层的全连接网络计算权重，两者效果差不多，<strong>但是dot-product会快得多</strong></li>\n</ul>\n<h3 id=\"1-2-Multi-Head-Attention-多头注意力机制\"><a href=\"#1-2-Multi-Head-Attention-多头注意力机制\" class=\"headerlink\" title=\"1.2 Multi-Head Attention(多头注意力机制)\"></a>1.2 Multi-Head Attention(多头注意力机制)</h3><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512112235945.png\" alt=\"image-20220512112235945\" style=\"zoom:35%;\" /></p>\n<ul>\n<li>多头注意力机制本质上就是<strong>做多次Scaled Dot-Product Attention</strong></li>\n<li>具体做法是：</li>\n</ul>\n<blockquote>\n<p><strong>重复做h次Scaled Dot-Product Attention（每次的W权重矩阵分别独立），将每次得到的结果<script type=\"math/tex\">Z \\in R^{N \\times d_v}</script>在第二维连结，形状变为<script type=\"math/tex\">R^{N \\times hd_v}</script>，再乘一个<script type=\"math/tex\">W^O \\in R^{hd_v \\times d_{model} }</script>，即可得到形状为<script type=\"math/tex\">R^{N \\times d_{model} }</script>的最终结果</strong></p>\n</blockquote>\n<ul>\n<li>总过程为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{MultiHead}(Q, K, V) &=\\operatorname{Concat}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{\\mathrm{h} }\\right) W^{O} \\\\\n\\text { where head } &=\\operatorname{Attention}\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)\n\\end{aligned}</script><ul>\n<li><p>在base模型中取的<script type=\"math/tex\">h = 8</script>，且<script type=\"math/tex\">d_k = d_v = d_{model}/h = 64</script></p>\n</li>\n<li><p><strong>多头注意力的好处：类似于CNN中的通道，能提取到不同子空间下的特征。多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。</strong>如果单纯使用单注意力头+平均化，会抑制这一点</p>\n</li>\n</ul>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  </p>\n</blockquote>\n<h3 id=\"1-3-自注意力机制的好处\"><a href=\"#1-3-自注意力机制的好处\" class=\"headerlink\" title=\"1.3 自注意力机制的好处\"></a>1.3 自注意力机制的好处</h3><ul>\n<li><p>自注意力机制最大的好处肯定是实现了<strong>并行化，加快了训练速度</strong>。并且得到的结果相比于其他方法（如全局平均池化），<strong>更具有解释性</strong>，self-attention是可以退化成平均的，所以结果肯定好于平均。</p>\n</li>\n<li><p>论文从每层的总计算复杂度、可并行化的计算数量（用顺序操作的最小量来衡量）、长距离依赖的距离三个方面进行了对比：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512114401561.png\" alt=\"image-20220512114401561\"></p>\n<ul>\n<li><strong>并且单个注意力头不仅清楚地学习执行不同的任务，而且许多似乎表现出与句子的句法和语义结构相关的行为</strong></li>\n</ul>\n<blockquote>\n<p>Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences  </p>\n</blockquote>\n<h1 id=\"2-模型结构\"><a href=\"#2-模型结构\" class=\"headerlink\" title=\"2 模型结构\"></a>2 模型结构</h1><p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220512121448150.png\" alt=\"image-20220512121448150\" style=\"zoom:28%;\" /></p>\n<ul>\n<li>图中多头注意力层的输入从左到右依次为V、K、Q</li>\n</ul>\n<h3 id=\"2-1-Encoder和Decoder\"><a href=\"#2-1-Encoder和Decoder\" class=\"headerlink\" title=\"2.1 Encoder和Decoder\"></a>2.1 Encoder和Decoder</h3><ul>\n<li>Encoder由N=6个相同的块组成，每个块有两个子层，一个多头注意力层，一个全连接层，子层输入输出都是都是<script type=\"math/tex\">R^{N \\times d_{model} }</script>，其中N为时间步个数，也就是词数。<strong>并且在每个子层都有一个残差结构+LayerNorm</strong>，先残差后LayerNorm：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nLayerNorm(x + Sublayer(x))</script><ul>\n<li>Decoder同样是由N=6个相同的块组成，每个块有3个子层，有两个和Encoder中一模一样。增加了一个带Mask的多头注意力层。<strong>Decoder最开始的输入在训练时和预测时不一样</strong>，在<strong>训练时是把所有的翻译结果一次性输入</strong>，并行化提高速度。而<strong>预测时是类似于RNN一样的串行方式</strong>，第一次给Decoder输入句子的开始符号，然后得到第一个翻译结果，再将第一个翻译结果当作输入送入Decoder。总结来说就是：<strong>每次Decoder的输入为之前所有时间步的结果</strong>。而在训练时，是一次导入所有结果，所以需要<strong>Mask掉未来时间步的翻译结果</strong>。</li>\n</ul>\n<h3 id=\"2-2-多头注意力层\"><a href=\"#2-2-多头注意力层\" class=\"headerlink\" title=\"2.2 多头注意力层\"></a>2.2 多头注意力层</h3><ul>\n<li><p>进行的操作其实就是上文提到的多头注意力机制：<strong>将输入分别乘一个矩阵W，转换成Q、K、V，再计算权重并加权平均，得到Z。将上述过程进行h次，每次使用的是相互独立的W，再将Z连结，再乘一个权重矩阵，得到最终结果。</strong></p>\n</li>\n<li><p>需要注意的是Decoder中的Masked Multi-Head Attention。我们在<strong>预测时，肯定是无法知道未来的信息的（也就是之后时间步的输出），但是在训练时我们是将翻译结果一次性使用一个矩阵导入的</strong>。所以为了保持一致性，我们需要在<strong>训练时屏蔽掉未来的信息，即当前时间步t的输出只取决于t-1及其之前的时间步。</strong></p>\n</li>\n<li>下方为一个Attention Map，每个单元代表该行对四个列对应的权值。如第一行代表”I”分别对”I”、”have”、”a”、”dream”的权值。</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/64.jpg\" alt=\"64\" style=\"zoom:50%;\" /></p>\n<p>显然在通过”I”预测”have”时，是不知道后面的”have”、”a”、”dream”的，所以需要通过Mask屏蔽掉未来的信息，其他时间步的时候类似：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/56.jpg\" alt=\"56\" style=\"zoom:50%;\" /></p>\n<p>上图就是经过Mask后的Attention Map，将每个时间步未来的信息进行了屏蔽，具体的做法是：<strong>在计算V的权重时，softmax之前将对应的值设为<script type=\"math/tex\">-\\infty</script></strong></p>\n<h3 id=\"2-3-全连接层\"><a href=\"#2-3-全连接层\" class=\"headerlink\" title=\"2.3 全连接层\"></a>2.3 全连接层</h3><ul>\n<li>每个全连接子层有两个层，进行的运算为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}</script><p>第二层是没有激活函数的</p>\n<ul>\n<li>层的输入为<script type=\"math/tex\">d_{model} = 512</script>，经过第一层变为<script type=\"math/tex\">d_{ff} = 2048</script>，经过第二层又变为512维。</li>\n</ul>\n<h3 id=\"2-4-Layer-Normalization\"><a href=\"#2-4-Layer-Normalization\" class=\"headerlink\" title=\"2.4 Layer Normalization\"></a>2.4 Layer Normalization</h3><ul>\n<li><p>BN是对一个batch-size样本内的每个特征做归一化，LN是对每个样本的每个时间步中所有特征做归一化</p>\n</li>\n<li><p><strong>使用LN，假设输入X，X.shape = (batch_size,time_step,embedding_dim) 那么mean.shape = std.shape = (batch_size,time_step,1)。即对embedding维做归一化，另外LN中同样是有放缩的参数的</strong></p>\n</li>\n<li><p>而使用BN，那么mean.shape = std.shape = (1,time_step,embedding_dim)，即对batch维做归一化</p>\n</li>\n<li><p><strong>选用LN而弃用BN的原因：</strong>BN需要较大的batch_size来保证对于期望、方差的统计的可靠性，对于CNN、Dense层来说则好统计。但是在天然变长的NLP任务中，如果选用BN需要<strong>对每个时间步的状态进行统计</strong>，这会导致在偏长的序列中，<strong>靠后的时间步的统计量不足</strong>。相比之下使用LN则不会有这种限制</p>\n</li>\n<li><p><strong>而对embedding层进行归一化也更具有解释性，因为embedding层的每个值都是直接相关的</strong></p>\n</li>\n</ul>\n<h3 id=\"2-5-词嵌入\"><a href=\"#2-5-词嵌入\" class=\"headerlink\" title=\"2.5 词嵌入\"></a>2.5 词嵌入</h3><ul>\n<li>Transformer中的embedding是训练出来的，所以总的结构类似于跳字模型或者连续词袋模型，具体可看<a href=\"https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#1-2-%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B%EF%BC%88skip-gram%EF%BC%89\">跳字模型</a>中的具体实现，简单来说就是：一个单隐藏层的全连接网络，输入one-hot向量，乘一个V矩阵，得到隐藏层值，再乘一个U矩阵，得到输入层值，再softmax计算概率最后梯度下降。<strong>而Decoder的前后就是分别为乘V和乘U两个操作，分别称为embedding转换和pre-softmax linear transformation</strong></li>\n<li>在一般的词嵌入模型当中，U、V矩阵一般是两个不同的矩阵，而Transformer中使用了<strong>Weight Tying</strong>，即U、V使用同一矩阵<strong>（注意只是共用权重矩阵，偏差还是相互独立的）</strong></li>\n<li>one-hot向量和对U的操作是“指定抽取”，即取出某个单词的向量行；pre-softmax对V的操作是“逐个点积”，对隐层的输出，依次计算其和每个单词向量行的变换结果。虽然具体的操作不同，<strong>但在本质上，U和V都是对任一的单词进行向量化表示，然后按词表序stack起来。因此，两个权重矩阵在语义上是相通的</strong>。</li>\n<li>也是由于上面两种操作方式的不同，且V的更新在靠近输出层，所以<strong>U在反向传播中不如V训练得充分</strong>，将两者绑定在一起缓和了这一问题，可以训练得到质量更高的新矩阵。并且<strong>Weight Tying 可以显著减小模型的参数量</strong>。</li>\n<li>在embdding层中，<strong>为了让embedding层的权重值不至于过小，乘以<script type=\"math/tex\">\\sqrt{d_{model} }</script>后与位置编码的值差不多，可以保护原有向量空间不被破坏</strong>。</li>\n</ul>\n<h3 id=\"2-6-Positional-Encode\"><a href=\"#2-6-Positional-Encode\" class=\"headerlink\" title=\"2.6 Positional Encode\"></a>2.6 Positional Encode</h3><ul>\n<li>由于模型摒弃了RNN结构，所以<strong>无法获得序列的位置信息</strong>，而为了获得这种位置信息我们需要引入Positional Embedding来表示位置信息。</li>\n<li>Positional Embedding的维度同样是<script type=\"math/tex\">d_{model}</script>，并且在一开始的时候和Embedding进行相加，具体表示为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP E_{(p o s, 2 i)} &=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model } }}\\right) \\\\\nP E_{(p o s, 2 i+1)} &=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model } }}\\right)\n\\end{aligned}</script><p>其中pos代表第几个序列位置（最大值为规定的最长序列长度），i代表第几个维度（最大值为<script type=\"math/tex\">d_{mdoel} / 2</script>）</p>\n<ul>\n<li>以上公式不仅能很好的表示单词的绝对位置，还能表示出相对位置：<strong>相隔 k 个词的两个位置 pos 和 pos+k 的位置编码是由 k 的位置编码定义的一个线性变换</strong>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nP E(p o s+k, 2 i)=P E(p o s, 2 i) P E(k, 2 i+1)+P E(p o s, 2 i+1) P E(k, 2 i) \\\\\nP E(p o s+k, 2 i+1)=P E(p o s, 2 i+1) P E(k, 2 i+1)-P E(p o s, 2 i) P E(k, 2 i)\n\\end{array}</script><ul>\n<li>采用正弦方式和学习方式position embedding结果几乎一样。但采用正弦，因为<strong>能让模型推断出比训练期间遇到的序列长度更长的序列长度</strong></li>\n</ul>\n<h1 id=\"3-模型训练\"><a href=\"#3-模型训练\" class=\"headerlink\" title=\"3 模型训练\"></a>3 模型训练</h1><h3 id=\"3-1-Optimizer-amp-amp-learning-rate\"><a href=\"#3-1-Optimizer-amp-amp-learning-rate\" class=\"headerlink\" title=\"3.1 Optimizer &amp;&amp; learning rate\"></a>3.1 Optimizer &amp;&amp; learning rate</h3><ul>\n<li><p>采用Adam优化器，参数都是模型参数：<script type=\"math/tex\">\\beta_1=0.9,\\beta_2=0.98,\\epsilon=10^{-9}</script></p>\n</li>\n<li><p>Transformer 的学习率更新公式叫作“<strong>noam</strong>”，它将 warmup 和 decay 两个部分组合在一起，总体趋势是<strong>先增加后减小</strong>，具体公式为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\text { lrate }=d_{\\text {model } }^{-0.5} \\cdot \\min \\left(\\text { step }_{-} \\text {num }^{-0.5}, \\text { step_ }_{-} \\text {num } \\cdot \\text { warmup_steps }^{-1.5}\\right)</script><ul>\n<li>公式实际上是一个以warmup_steps为分界点的分段函数。该点之前是warmup部分，采用线性函数的形式，且warmup_steps越大，斜率越小。该点之后是decay部分，采用负幂的衰减形式，衰减速度先快后慢：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-7c98b2c7ca4467ab770da064bb2b58ba_720w.jpg\" alt=\"v2-7c98b2c7ca4467ab770da064bb2b58ba_720w\"  /></p>\n<ul>\n<li><strong>设置warmup的原因：</strong>在CV领域中常常这样做，在《Deep Residual Learning for Image Recognition》中，作者训练110层的超深网络是就用过类似策略：</li>\n</ul>\n<blockquote>\n<p>In this case, we find that <strong>the initial learning rate of 0.1 is slightly too large to start converging</strong>. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.</p>\n</blockquote>\n<p>对于 Transformer 这样的大型网络，<strong>在训练初始阶段，模型尚不稳定，较大的学习率会增加收敛难度</strong>。因此，使用较小的学习率进行 warmup，等 loss 下降到一定程度后，再恢复回常规学习率。</p>\n<h3 id=\"3-2-Dropout\"><a href=\"#3-2-Dropout\" class=\"headerlink\" title=\"3.2 Dropout\"></a>3.2 Dropout</h3><ul>\n<li>在每个子块中，输出结果加入到残差结构和layer normalization之前，进行Dropout</li>\n<li>并且还在Encoder和Decoder最开始的两种embedding相加的时候，使用了Dropout</li>\n<li>Dropout的概率均为0.1</li>\n</ul>\n<h3 id=\"3-3-Label-Smoothing\"><a href=\"#3-3-Label-Smoothing\" class=\"headerlink\" title=\"3.3 Label Smoothing\"></a>3.3 Label Smoothing</h3><ul>\n<li>为了不要对正确类别”too confident”（防止过拟合），Transformer中还使用了Label Smoothing。这种方法<strong>会增大困惑度（perplexity），但是可以提高accuracy和BLEU</strong>。</li>\n<li>假设目标类别为y，任意类别为k，ground-truth 分布为q(k)，模型预测分布为p(k)。 显然，当k=y时，q(k)=1。当k<script type=\"math/tex\">\\neq</script>y时，q(k)=0。<strong>LSR（Label Smoothing Regularization）为了让模型的输出不要过于贴合单点分布，选择在gound-truth中加入噪声</strong>。即削弱y的概率，并整体叠加一个独立于训练样例的均匀分布u(k)：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nq^{\\prime}(k)=(1-\\epsilon) q(k)+\\epsilon u(k)=(1-\\epsilon) q(k)+\\epsilon / K</script><p>其中K为softmax的类别数，拆开来看就是：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{ll}\nq^{\\prime}(k)=1-\\epsilon+\\epsilon / K, & k=y \\\\\nq^{\\prime}(k)=\\epsilon / K, & k \\neq y\n\\end{array}</script><p>所有类别的概率和仍然是归一的。说白了就是把最高点砍掉一点，多出来的概率平均分给所有人。</p>\n"},{"title":"代码总结（持续更新）","math":true,"date":"2022-07-16T16:00:00.000Z","_content":"\n# NLTK用法\n\n- nltk用于**英文**分词分句等应用\n\n### 基本的预处理\n\n- **分句：nltk.sent_tokenize(text, language=\"english\"):**\n\n> 输入：一个str段落\n>\n> 输出：一个list，每个元素是一个str句子\n\n- **分词：nltk.word_tokenize(text, language=\"english\", preserve_line=False):**\n\n> 输入：一个str句子\n>\n> 输出：一个list，每个元素是一个str词\n\n- **词性标注（POS_tag）：nltk.postag(tokens)**\n\n> 输入：一个list，每个元素是一个str，一个句子分好词的结果\n>\n> 输入：一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[('football', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('family', 'NN')]\n>\n> 词性所对应的意义，大致来说**N开头对应名词NOUN，V开头对应动词VERB，J开头对应形容词ADJ，R开头对应副词ADV**\n>\n> 词性具体所对应意义如下：\n>\n> | 标记 | 含义 | 示例                            |\n> | ---- | ---- | ------------------------------- |\n> | CC   | 连词 | and, or,but, if, while,although |\n> | CD   | 数词 | twenty-four, fourth, 1991,14:24 |\n> |DT | 限定词|  the, a, some, most,every, no |\n> |EX| 存在量词| there, there’s|\n> |FW |外来词 |dolce, ersatz, esprit, quo,maitre|\n> |IN |介词连词| on, of,at, with,by,into, under|\n> |JJ |形容词 |new,good, high, special, big, local|\n> |JJR| 比较级词语| bleaker braver breezier briefer brighter brisker|\n> |JJS| 最高级词语| calmest cheapest choicest classiest cleanest clearest|\n> |LS| 标记| A A. B B. C C. D E F First G H I J K|\n> |MD| 情态动词| can cannot could couldn’t|\n> |NN| 名词| year,home, costs, time, education|\n> |NNS| 名词复数| undergraduates scotches|\n> |NNP| 专有名词| Alison,Africa,April,Washington|\n> |NNPS| 专有名词复数| Americans Americas Amharas Amityvilles|\n> |PDT| 前限定词| all both half many|\n> |POS| 所有格标记 ’| 's|\n> |PRP| 人称代词| hers herself him himself hisself|\n> |PRP$| 所有格| her his mine my our ours|\n> |RB| 副词| occasionally unabatingly maddeningly|\n> |RBR |副词比较级| further gloomier grander|\n> |RBS| 副词最高级 |best biggest bluntest earliest|\n> |RP |虚词| aboard about across along apart|\n> |SYM |符号 |% & ’ ‘’ ‘’. ) )|\n> |TO |词to| to|\n> |UH| 感叹词| Goodbye Goody Gosh Wow|\n> |VB |动词| ask assemble assess|\n> |VBD| 动词过去式 |dipped pleaded swiped|\n> |VBG| 动词现在分词| telegraphing stirring focusing|\n> |VBN| 动词过去分词| multihulled dilapidated aerosolized|\n> |VBP| 动词现在式非第三人称时态| predominate wrap resort sue|\n> |VBZ| 动词现在式第三人称时态| bases reconstructs marks|\n> |WDT| Wh限定词| who,which,when,what,where,how|\n> |WP| WH代词| that what whatever|\n> |WP$| WH代词所有格 |whose|\n> |WRB| WH副词||\n> \n\n- **词形还原Lemmatization：nltk.stem.WordNetLemmatizer().lemmatize(word, pos=\"n\")：**\n\n> 这是一个类的内置方法，首先需要创建WordNetLemmatizer对象（假设为wnl），则通过`wnl.lemmatize()`调用\n>\n> 输入：word是单个词，pos是其对应的单词词性（n对应noun，v对应verb，a对应adj，r对应adv，s对应satellite adjectives(不咋用)）\n>\n> 输出：word经过还原后的词，如cars还原为car\n\n\n\n### NER\n\n- 先介绍一下命名实体识别（Named Entity Recognition，NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。\n- nltk中对NER类别的分类如下：\n\n![img](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20191219223810610.png)\n\n其中LOCATION和GPE有重合，GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。\n\n- **进行NER：ne_chunk(tagged_tokens, binary=False)**\n\n> 输入：tagged_tokens为`pos_tag()`函数的输出，一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[('football', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('family', 'NN')]\n>\n> 输出：同样的是一个list，每个元素变为了一个封装对象（Tree类），和输入一一对应\n>\n> - **对于该封装对象：**\n>\n> 输出的list中，有些元素是没有NER的结果的，其对应的封装对象没有label属性，**可使用`hasattr(ne_word, 'label')`函数判断是否有NER结果**\n>\n> 假设一个该对象命名为ne_word，调用`ne_word.leaves()`可返回一个list，每个元素为一个tuple，tuple[0]为token，tuple[1]为该token的pos_tag。对于NER，list中只有一个tuple，返回结果示例：[('FIFA', 'NNP')]\n>\n> 调用`ne_word.label()`函数可返回该token对应的NER结果，一定要先使用`hasattr()`函数才能使用`label()`函数\n\n\n\n### 计算BLEU\n\n- [BLEU的定义](https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#5-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91)\n\n- **nltk.translate.bleu_score.sentence_bleu(references, hypothesis, ..., smoothing_function=None, ...)**\n\n> references：参照序列，label。**type=list(list(str))，其中的每个str为词或字**\n>\n> hypothesis：候选序列，也就是预测出的序列。**type=list(str)，其中的每个str为词或字**\n>\n> smoothing_function，是论文中使用到的平滑技巧，一般输入`nltk.translate.bleu_score.SmoothingFunction().methodi()`，最后的i为0~7\n\n- 另外还可以使用`corpus_bleu()`计算多个句子的BLEU；用`modified_precision()`计算修正的n-gram精确度\n\n\n\n# Pyltp用法\n\n- Pyltp主要用于中文的预处理等，包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注\n- 使用Pyltp时注意其编码使用的都是utf-8，而Windows终端使用GBK编码，如果出现乱码可以将输出重定向到文件，然后用utf-8编码查看\n- **分句：pyltp.SentenceSplitter()**\n\n> 使用时先创建实例：`sp = SentenceSplitter()`，再进行分句：`sents = sp.split(doc)`\n\n- **分词：pyltp.Segmentor()**\n\n> Segmentor加载模型可以用`load()`也可以用`load_with_lexicon()`，后者还要加一个用户词典的参数\n\n- **词性标注：pyltp.Postagger()**\n\n> 直接举个栗子吧：\n>\n> ```python\n> sent = \"据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。\"\n> # 加载模型\n> segmentor = Segmentor()\n> segmentor.load_with_lexicon(cws_model_path, lexicon_path)\n> postagger = Postagger()\n> postagger.load(pos_model_path)\n> # 分词和词性标注\n> words = segmentor.segment(sent)\n> postags = postagger.postag(words)\n> print(list(postags))\n> # 释放模型\n> segmentor.release()\n> postagger.release()\n> ```\n>\n> 注意：进行这些处理后返回的都是VectorOfString类，是一个可迭代类，也可用list()转换为列表\n>\n> 词性标注的词性如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724143414199.png\" alt=\"image-20220724143414199\" style=\"zoom: 50%;\" />\n>\n> 词性标注也可以添加用户词典\n\n- **NER识别：pyltp.NamedEntityRecognizer()**\n\n> ```python\n> from pyltp import NamedEntityRecognizer\n> recognizer = NamedEntityRecognizer()\n> recognizer.load(ner_model_path)\n> ner_results = recognizer.recognize(words, postags)\n> ```\n>\n> LTP 采用 BIESO 标注体系。**B 表示实体开始词，I表示实体中间词，E表示实体结束词，S表示单独成实体，O表示不构成命名实体。**\n>\n> LTP 提供的命名实体类型为：**人名（Nh）、地名（Ns）、机构名（Ni）**。\n>\n> B、I、E、S位置标签和实体类型标签之间用一个横线 `-` 相连；O标签后没有类型标签。\n\n- **依存句法分析：pyltp.Parser()**\n\n> 依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系\n>\n> 首先也是创建实例和加载模型，然后`parse_results = parser.parse(words, postags)`，得到的结果是一个可迭代对象，其中的每一个元素也是个自定义类（假设parse_results[i]为result），调用`result.head`和`result.relation`可分别获得words[i]对应的关系词和关系，其中head是返回关系词所对应的索引+1，如果为0则为\"Root\"，表示无对应关系词\n>\n> 举个栗子：\n>\n> ```python\n> relations = [result.relation for result in parse_results]\n> heads = [words[result.head - 1] if result.head else \"Root\" for result in parse_results]\n> for i in range(len(words)):\n>  print(f\"{relations[i]} : ({words[i]}, {heads[i]})\")\n> ```\n>\n> 输出结果：\n> <div align=center><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211345194.png\" alt=\"image-20220724211345194\" style=\"zoom: 80%;\" />\n>\n>\n> 依存句法关系如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724153110907.png\" alt=\"image-20220724153110907\" style=\"zoom:50%;\" />\n\n- **语义角色标注：pyltp.SementicRoleLabeller()**\n\n> 语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等\n> 进行语义角色标注时首先同样是创建实例，再加载模型，然后调用方法：`roles = labeller.label(words, postags, parse_results)`。结果得到一个可迭代对象，**其中的每一个元素也是一个自定义类（假设每个元素为role），则`role.index`为谓语对应的索引，`role.arguments`又是一个可迭代对象，每个元素对应一个和该谓语对应的语义角色，也是一个自定义类（假设为argument），**那么可以通过`argument.name`获取角色和谓语的关系，`argument.range.start`和`argument.range.end`对应该角色的开始和结束索引（**end要算上的**）\n>\n> 给个示例：\n>\n> ```python\n> roles = labeller.label(words, postags, parse_results)\n> for role in roles:\n>  arguments = role.arguments\n>  index = role.index\n>  print(f\"谓语: {words[index]} (索引: {index})\")\n>  for argument in arguments:\n>      start, end = argument.range.start, argument.range.end\n>      obj = \"\"\n>      for word in words[start : end+1]:\n>          obj += word\n>      print(f\"{argument.name}: {obj} (索引: {start}:{end})\")\n>  print(\"\\n\")\n> ```\n>\n> 输出结果：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211302209.png\" alt=\"image-20220724211302209\" style=\"zoom:67%;\" />\n>\n> 语义角色如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211544134.png\" alt=\"image-20220724211544134\" style=\"zoom:60%;\" />\n\n\n\n\n\n# rouge用法\n\n- rouge和bleu一样也是一种对序列质量的评估标准，但是相比于bleu更关注召回率而非精准率（通过计算F1_score时改变$$\\beta$$参数），rouge-n计算公式如下：\n\n$$\nRough-N=\\frac{\\sum_{S \\in\\{\\text { ReferemceSummaries }\\}} \\sum_{\\text {gram }_{n} \\in S} \\text { Count }_{\\text {match }}\\left(\\text { gram }_{n}\\right)}{\\sum_{S \\in\\{\\text { ReferenceSummaries }\\}} \\text { gram }_{n} \\in S}\n$$\n\n- rouge-1和rouge-2都可通过上述公式计算出来，和bleu中的P1和P2计算公式很像。可以在下面看到，每个rough-N都会输出p和r，其实就是精准率和召回率，两者也就是分母不一样（一个分母是refs的n-gram个数，一个分母是hyps的n-gram个数）\n- 而rouge-l是rouge-N的改进：\n\n$$\n\\begin{array}{l}\nR_{l c s}=\\frac{L C S(X, Y)}{m} \\\\\nP_{l c s}=\\frac{L C S(X, Y)}{n} \\\\\nF_{l c s}=\\frac{\\left(1+\\beta^{2}\\right) R_{l c s} P_{l c s}}{R_{l c s}+\\beta^{2} P_{l c s}}\n\\end{array}\n$$\n\n其中X、Y为hyps和refs，m、n为他们的长度，LCS(X, Y)为X、Y的最长共同序列\n\n- **rouge.Rouge.get_scores(hyps, refs, avg=False, ignore_empty=False)**\n\n> **注意：输入的hyps和refs两个字符串，不管是中文英文，都需要每个字或词使用空格间隔**\n>\n> 输出是list(dict(dict))\n\n- 直接给栗子吧：\n\n```python\nfrom rouge import Rouge \n\nhypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n\nreference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n\nrouge = Rouge()\nscores = rouge.get_scores(hypothesis, reference)\n```\n\n**output：**\n\n```\n[\n  {\n    \"rouge-1\": {\n      \"f\": 0.4786324739396596,\n      \"p\": 0.6363636363636364,\n      \"r\": 0.3835616438356164\n    },\n    \"rouge-2\": {\n      \"f\": 0.2608695605353498,\n      \"p\": 0.3488372093023256,\n      \"r\": 0.20833333333333334\n    },\n    \"rouge-l\": {\n      \"f\": 0.44705881864636676,\n      \"p\": 0.5277777777777778,\n      \"r\": 0.3877551020408163\n    }\n  }\n]\n```\n\n其中p、r、f分别为精准率、召回率、F1_score\n\n\n\n\n\n# zhconv用法\n\n- 该库用于中文简繁体转换，但是也可以用**OpenCC库，精准度更高、覆盖率更高、速度更快**\n\n- **逐字转换：zhconv.convert(s, locale, update=None)**\n\n- **基于MediaWiki的转换：zhconv.ocnvert_for_mw(s, locale, update=None)**\n\n> 两个函数用法相同\n>\n> 示例：\n>\n> ```python\n> sent2 = \"計算機軟體\"\n> print(convert(sent2, \"zh-hans\"))\n> ```\n>\n> locale可为以下值：\n>\n> `zh-cn` 大陆简体、`zh-tw` 台灣正體、`zh-hk` 香港繁體、` zh-sg` 马新简体、`zh-hans` 简体、`zh-hant` 繁體\n\n\n\n\n\n# gensim用法\n\n- gensim主要用于创建语料库，和计算各种特征（如相似度、tf-idf）等任务\n\n### 创建语料库和计算相似度\n\n- **创建语料库类：gensim.corpora.Dictionary(texts)**\n\n> 输入：整个语料库，分好句和分好词的结果，一个list，每个元素又是一个list，里面的每个子元素是一个str词\n>\n> 输出：一个Dictionary对象\n\n- **将句子转化为词袋形式：Dictionary.doc2bow(text)**\n\n> 输入：分好词的一个句子，一个list，每个元素是一个str词\n>\n> 输入：一个list，每个元素是一个二元tuple，tuple[0]是句子中存在的词的索引，tuple[1]是其在句子中出现的次数（出现0次的不计入）\n>\n> 该类的用法和python自带的字典对象基本相同，values、key、item之类的\n\n- **获取索引字典：Dictionary.token2id**\n\n> 返回一个{token : id}的字典\n\n- **创建相似度类：gensim.similarities.Similarity(output_prefix, corpus, num_features)**\n\n> out_prefix是存储这个对象文件的名称\n>\n> corpus是整个语料库，但是必须先转化为词袋模型\n>\n> num_fuatures是整个词典的数量，一般使用len(dictionary)表示\n>\n> 对于得到的对象（取名为similarity），具体的用法为：similarity[test]，其中test为要比较的对象，可以是单个句子，也可以是整个语料库\n\n**举个栗子：**\n\n```python\nfrom nltk import word_tokenize\nfrom gensim.corpora import Dictionary\nfrom gensim.similarities import Similarity\n\nsent1 = \"I love sky, I love sea.\"\nsent2 = \"I like running, I love reading.\"\nsents = [sent1, sent2]\n# 分词\ntexts = [word_tokenize(sent) for sent in sents]\n\n# 创建字典对象\ndictionary = Dictionary(texts)\n# 获得词袋模型表示的词料库\ncorpus = [dictionary.doc2bow(text) for text in texts]\nprint(f\"The corpus is : {corpus}\")\n# 创建相似度对象\nsimilarity = Similarity(\"Similarity-excise1\", corpus, num_features=len(dictionary))\nprint(f\"Created class : {similarity}\")\n# 计算余弦相似度\ntest_corpus = dictionary.doc2bow(word_tokenize(sent1))\nprint(similarity[test_corpus])\n```\n\n输出：\n\n![image-20220706160148320](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706160148320.png)\n\n\n\n### 计算TF-IDF\n\n- **首先介绍一下tf-idf：**\n\n词频（term frequency，tf）指的是某一个给定的词语在该文件中对应的频率\n\n逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量，可由文件总数除以包含该词的文件的总数求得\n\n具体的计算公式如下：\n$$\n\\mathrm{tf}_{\\mathrm{i}, \\mathrm{j}}=\\frac{n_{i, j}}{\\sum_{k} n_{k, j}} \\\\\n\\operatorname{idf}_{\\mathrm{i}}=\\lg \\frac{|D|}{\\left|\\left\\{j: t_{i} \\in d_{j}\\right\\}\\right|}\\\\\n\\operatorname{tfidf}_{i, j}=\\mathrm{tf}_{\\mathrm{i}, \\mathrm{j}} \\times \\mathrm{idf}_{\\mathrm{i}}\n$$\n其中$$tf_{i, j}$$为$$word_i$$在$$doc_j$$中出现的词频，$$n_{i, j}$$为$$word_i$$在$$doc_j$$中出现的个数\n\n$$idf_i$$为$$word_i$$的逆向文件频率，$$|D|$$为文档总数，$$\\left|\\left\\{j: t_{i} \\in d_{j}\\right\\}\\right|$$为包含$$word_i$$的文档个数\n\n对于idf，底数可用2、e、10，并且分母可以+1，避免除以0\n\n- **创建tfidf模型类：gensim.models.TfidfModel(corpus)**\n\n> 输入为整个语料库，必须先转化为词袋模型\n>\n> 在具体使用时，和Similarity类似，tfidf_model[test]，其中test可以是单个句子也可以是整个语料库，返回一个列表，列表中每个元素为一个tuple，tuple[0]为词索引，tuple[1]为对应的tfidf值\n>\n> **gensim算出来的tf-idf是经过了规范化的，每个句子的tfidf值，是一个单位向量**\n\n**举个栗子：**\n\n```python\n# texts的构建是每个段落分词，有三个段落，则len(texts)=3\ntexts = [text1, text2, text3]\ntexts = [get_tokens(text) for text in texts]\ndictionary = Dictionary(texts)\nid2token_dict = {v : k for k, v in dictionary.token2id.items()}\ncorpus = [dictionary.doc2bow(text) for text in texts]\ntfidf_model = TfidfModel(corpus)\nresult = tfidf_model[corpus]\nfor i in result:\n    print(i)\n```\n\n输出：\n\n![image-20220706210417422](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706210417422.png)\n\n\n\n\n\n# Pandas用法\n\n- **创建DataFrame对象：pd.DataFrame(data, columns, index)**\n\n> 输入：data是一个list, 每个对象是一个tuple或list，tuple的长度和columns的个数对应。data也可以是numpy形式\n>\n> index是一个list，是行索引值，可以用于自行设置行索引**（索引没有规定一定是int）**\n>\n> 示例：pd.DataFrame([(1, 2), (3, 4), (5, 6)], columns=[\"name\", \"NER result\"])，输出为：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220711120934316.png\" alt=\"image-20220711120934316\" style=\"zoom: 80%;\" />\n\n- **获取DataFrame特征：DataFrame.describe()**\n- **获取DataFrame的某列：DataFrame[\"column_name\"]**\n\n>  这样可获取到column列，结果是一个Series类（**只有一个列则是Series，多个列则是DataFrame**）\n>\n>  如果不想出现重复结果，可调用`unique()`函数，示例：data[\"ner_result\"].unique()，返回的结果是一个ndarray，也可使用`list()`将其转换为列表\n>\n>  如果想把这个Series类转换为ndarray，则可使用Series.values，这是Series类其中的一个属性，不是方法，类型是一个ndarrary，也可使用`list()`转换为列表\n\n- **获取DataFrame列名：DataFrame.columns**\n\n- **DataFrame根据某列，进行分组：DataFrame.groupby(\"column_name\")**\n\n> 将DataFrame通过column_name列进行分组\n>\n> 结果是一个可迭代的DataFrameGroupBy类，该类的用法和DataFrame几乎一样，比如可使用`DataFrameGroupBy[\"column_name\"]`将其转换为SeriesGroupby类\n>\n> 也可转化为list，转换后，其中每一个元素为一个tuple，tuple[0]为column_name列对应的值，tuple[1]为原DataFrame的一部分，根据column_name的值进行截断。如果是将SeriesGroupby类转换为list，则tuple[1]变为一个Series类\n>\n> 举个例子：\n> input_data为一个DataFrame，值如下：\n>\n> <div align=center><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123006189.png\" alt=\"image-20220716123006189\" style=\"zoom:67%;\" />\n>\n> 现在将input_data根据sent_order列进行分组：input_data.groupby(\"sent_order\")，结果如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123836380.png\" alt=\"image-20220716123836380\" style=\"zoom: 67%;\" />\n>\n> 只展示了前三个元组，每个元组，tuple[0]为sent_order的值，tuple[1]为对应的DataFrame\n\n- **应用自定义方法：DaraFrame.apply(func, axis=0, ...)**\n\n> Series、DataFrame和GroupbyDataFrame等均可使用\n>\n> func是一个自定义函数\n>\n> axis决定func的输入是什么，axis=0则输入为行，axis=1则输入为列\n>\n> Series无需指定axis，DataFrame可调节axis更换操作的维度，**DaraFrameGroupby也无需指定，func的输入为分组后的每个DataFrame，最后再将每个分组返回的结果总和起来**\n\n- **DataFrame的count()方法：**\n\n> `count()`有很多种用法，结果是可以对Series、DataFrame和GroupbyDataFrame等使用，结果是返回一个Series或DataFrame\n>\n> 下面举几个示例，其中使用的df如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725142611139.png\" alt=\"image-20220725142611139\" style=\"zoom: 60%;\" />\n>\n> ```python\n> df.count()    # DataFrame执行count()，对每一列分别执行count，结果返回一个Series\n> df.data[\"length\"].count()    # Series执行count()，直接返回length列的长度，类型为int\n> df.groupy(\"length\").count()   # 对DataFrameGroupby类执行，返回一个DataFrame，index为length的各指，colums为出length的其他列执行count的结果\n> df.groupy(\"length\")[\"evaluation\"].count()  # 对SeriesGroupby类执行，则只返回evaluation列的count结果，类型为Series\n> ```\n>\n> 上述四句语句的执行结果：\n>\n> - 第一句：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143928089.png\" alt=\"image-20220725143928089\" style=\"zoom:67%;\" />\n>\n> - 第二句：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143939723.png\" alt=\"image-20220725143939723\" style=\"zoom:67%;\" />\n>\n> - 第三句：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143953768.png\" alt=\"image-20220725143953768\" style=\"zoom:50%;\" />\n>\n> - 第四句：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725144005108.png\" alt=\"image-20220725144005108\" style=\"zoom:50%;\" />\n\n- **读取/保存csv文件：**\n\n```python\ndf.to_csv(file_name, index=False)    # 要加index=False\npd.read_csv(file_name)\n```\n\n\n\n### 处理缺失值\n\n- **判断空值：DataFrame.isnull()或DataFrame.notnull()**\n\n> DataFrame和Series都可用，结果也是返回一个DataFrame或Series，和输入一一对应，其中每个值为布尔值\n\n- **删除缺失值：DataFrame.dropna(axis=0, inplace=False, how=None)**\n\n> axis：删除行还是列，{0 or ‘index’, 1 or ‘columns’}\n>\n> how : 如果等于any则任何值为空都删除，如果等于all则所有值都为空才删除\n\n- **填充缺失值：DataFrame.fillna(value, method, axis, ...)**\n\n> value：用于填充的值，可以是单个值，或者字典（用于不同列填充不同值的情况，key是列名，value是值）\n> method : 等于\"ffill\"使用前一个；不为空的值填充forword fill；等于\"bfill\"使用后一个不为空的值填充backword fill\n> axis : 按行还是列填充，{0 or ‘index’, 1 or ‘columns’}\n\n\n\n### Padas中DataFrame的增删查改\n\n- **增加列：**\n\n```python\ndf = DataFrame(...)    # 有三行数据\ncities = [\"成都\", \"上海\", \"北京\"]\n# 三种插入方式\ndf.insert(0, \"city\", citys)    # 参数分别为：插入列的位置、列名、插入内容\ndf[\"city\"] = cities\ndf.loc[:, \"city\"] = cities\n```\n\n- **增加行：**\n\n```python\ndf = DataFrame(...)  # 有两列 \ndf.loc[3] = [\"1\", \"2\"]       # 如果已经存在index=3的行，则修改值；反之直接添加该行\ndf = df.append(df_insert)    # 合成两个DataFrame，列要相同才行\n```\n\n- **loc[]和iloc[]的使用：**\n\n> 两者功能是相同的，区别在于loc中对于列，只能使用列名（str类型）进行搜索，而iloc对于列只能使用整数索引进行搜索，DataFrame的查和改基本都是基于两者的\n\n```python\ndf[\"column_name\"]或df.loc[:, \"column_name\"]   # 查找column_name列\ndf.loc[\"index_name\"]    # 查找index_name行\ndf.loc[\"index_name\", \"column_name\"]     # 查找(index_name, column_name)处的值\n# 还可以使用list或者切片来代替，一次操作多行或多列\ndf.loc[[\"index_name1\", \"index_name2\"], [\"column_name1\", \"column_name2\"]]  # 同时操作index_name1&2行和column_name1&2列\ndf.loc[0:3, \"column_name\"]    # 进行切片，对0、1、2、3行操作\n```\n\n> `iloc[]`是使用整数列索引，比如：`df.iloc[:3, 2:6]`\n>\n> 注意`loc[]`中的切片，是包含了end所指的索引（和list的索引稍有不同），`iloc[]`是不包含end的索引的\n\n- **删除：**\n\n```python\ndf.drop(0, axis=0)   # 删除第0行\n# 删除列的三种方法\ndf.drop(\"column_name\", axis=1)\ndel df[\"column_name\"]\nndf = df.pop(\"column_name\")\n# 使用drop的时候，第一个参数也可以是list，一次操作多行或多列\ndf.drop([0, 1, 2], axis=0)\ndf.drop([\"column_name1\", \"column_name2\"], axis=1)\n```\n\n\n\n\n\n# Pickle用法\n\n- **储存pk文件：pickle.dump(obj, file)**\n\n- **加载pk文件：pickle.load(file)**\n\n> 可以存储和加载对象，其中`dump()`中的obj是要存的对象，file是`open()`函数返回的文件描述符\n>\n> `load()`返回值为所存储的对象\n>\n> 储存的文件后缀为.pk\n>\n> 举个栗子：\n>\n> ```python\n> a = [1, 2]\n> file_name = \"test.pk\"\n> # 存\n> with open(file_name, \"wb\") as f:\n>     pickle.dump(a, f)\n> # 取\n> with open(file_name, \"rb\") as f:\n>     b = pickle.load(f)\n>     \n> # a和b是一样的\n> ```\n\n\n\n\n\n# Json用法\n\n- **json可进行Python对象和json格式之间的互换**\n\n- **Python转json：json.dumps(obj, ensure_ascii=True, ..., indent=None, ...)**\n- **json转Python：json.loads(s, ...)**\n\n> obj为Python对象，s为json对象\n>\n> ensure_ascii：如果为False，则返回值可以包含非 ASCII\n>\n> indent：为json中每个元素的间隔数，如果为0或None，则每个元素之间紧凑排版\n>\n> **注意：**\n>\n> **1. 以上两个函数只是进行对象的转换，而不会进行保存，所以是一般配合`open()`、`f.write()`、`f.read()`使用，给两个示例：**\n>\n> ```python\n> # 存储json对象\n> with open(file_name, \"w\", encoding=\"utf-8\") as f:\n>  f.write(json.dumps(data, ensure_ascii=False, indent=2))\n> # 读取json对象\n> with open(file_name, 'r', encoding='utf-8') as f:\n>  data = json.loads(f.read())\n> ```\n>\n\n\n\n\n\n# Yaml\n\n- **读取yaml文件：**`config = yaml.safe_load(open(\"config.yaml\", \"r\", encoding=\"utf-8\").read())`\n- 在.yaml文件中，可用缩进表示层级关系，并且写浮点数不能用python的科学技术法的写法：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_25.png\" alt=\"IMG_25\" style=\"zoom:50%;\" />\n\n\n\n\n\n# Argparse\n\n- **基本流程：**\n\n```python\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--batch_size\", type=int, default=1) \nargs = parser.parse_args()\n```\n\n- 其中`--`表示可选参数，在运行命令指定`--batch_size=xxx`即可\n\n\n\n\n\n# Matplotlib用法\n\n- **折线图：plt.plot(x, y)**\n- **直方图：plt.bar(x, y)**\n- **散点图：plt.scatter(x, y)**\n\n> 上述函数还可以指定颜色、大小等参数：\n>\n> - color：颜色\n>\n> > 可为：\"b\"：蓝色、\"g\"：绿色、\"r\"：红色、\"c\"：青色、\"m\"：品红、\"y\"：黄色、\"k\"：黑色、\"w\"：白色\n> >\n> > 也只直接用全名，如\"blue\"\n>\n> - s：点的大小\n> - linewidth：线宽\n> - linestyle：线样式，如设为\"dashed\"将线设为虚线\n>\n> - label：名字， 若要打印label，则使用`plt.legend()`\n\n- **设置标题：plt.title()**\n\n> 同样可以使用fontsize指定字体大小\n\n- **设置x、y轴标题：plt.xlabel()和plt.ylabel()**\n- **保存图片：plt.savefig()**\n- **画完图之后记得关闭：plt.close()**\n- **画一条水平的线：plt.hlines(y, xmin, xmax)**\n- **画一条垂直的线：plt.vlines(x, ymin, ymax)**\n- **在指定座标处加注释：plt.text(x, y, s)**\n\n\n\n\n\n# Numpy用法\n\n- **获取最大值的位置（索引）np.argmax(a, axis=None, ...)：**\n\n> axis：如果为None, 则代表把整个数组当作一维的，然后返回最大的索引值。如果指定axis，则只在该维度搜索最大值，示例：\n>\n> ```python\n> array([[10, 11, 12],\n>        [13, 14, 15]])\n> >>> np.argmax(a)\n> 5\n> >>> np.argmax(a, axis=0)\n> array([1, 1, 1])\n> >>> np.argmax(a, axis=1)\n> array([2, 2])\n> ```\n>\n> axis指定为哪个维度，则stack哪个维度\n\n- **减少维度：np.squeeze(a, axis=None)**\n\n> axis如果为None，则丢弃所有大小为1的维度\n\n\n\n# Tensorflow用法\n\n### 禁用GPU\n\n```python\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n```\n\n\n\n### 数据预处理\n\n- **填充序列：tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)**\n\n> 输入：\n> sequences是要填充的序列\n>\n> maxlen是要填充的最大长度，如果为None，则设为所有序列中最长的那个的长度\n>\n> padding和truncating可以设为\"pre\"或者\"post\"，选择是在序列前面还是后面填充/截断\n>\n> value为填充的值\n>\n> 返回值是一个ndarray\n\n- **划分训练、测试集：sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)**\n\n> arrays是要划分的数据，可以使用list、numpy array、matric、dataframe**（不能用Tensor）**，示例：\n>\n> `x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)`\n>\n> train_size和test_size指定一个就行\n>\n> random_state为随机种子\n>\n> shuffle决定是否打乱数据\n>\n> stratify决定是否进行数据分层，stratify不是bool，是直接指定分层的依据变量，即通过什么来分层，一般就是label或者Series（在输入数据是DataFrame时，stratify指定为Series，即根据那一列来分层）\n>\n> **输入的数据是什么类型，输出的数据就是什么类型，返回值依次为：x_train, x_test, y_train, y_test**\n\n\n\n### 各种层\n\n- **tf.keras.layers.Dense(units, activation=None, use_bias=True, ...)**\n- **tf.keras.layers.Embedding(input_dim, output_dim,...., mask_zero=False, input_length=None)**\n\n> input_dim：词汇表大小\n>\n> output_dim：词嵌入向量的维度\n>\n> mask_zero：决定输入值0是否是应被屏蔽的mask“填充”值。如果为True，则使用mask，相应的input_dim也应该为词汇表大小+1（因为增加了一个mask token，索引为0）\n>\n> intput_length：输入序列的长度，如果为None，则自动为最长序列的长度\n\n- **tf.keras.layers.LSTM(units, activation=\"tanh\", recurrent_activation=\"sigmoid\", ..., return_sequences=False)**\n\n> 其中return_sequences决定是否在最后返回所有时间步的输出，若为False只会返回最后一个时间步的输出，如果下一层需要使用到所有时间步的输出则需要设为True\n\n- **tf.keras.layers.Bidirectional(layer, merge_mode=\"concat\", ...)**\n\n> 是RNN的双向包装器，可将单向RNN变为双向RNN\n>\n> layer是要包装的层，merge_mode决定前后信息汇总的 方式，可为：\"sum\"、\"mul\"、\"concat\"、\"ave\"、None，如果为None，则不会合并，而是将他们作为列表输出\n\n- **tf.keras.layers.Softmax(axis=-1)**\n\n> axis：决定对那个维度执行softmax，默认为最后一个维度\n\n\n\n### 模型的常用方法\n\n- **序列容器：tf.keras.models.Sequential()**\n\n> 可以和`add()`配合使用，也可以里面直接加个列表Sequential([...])\n\n- **配置模型：model.compile(optimizer='rmsprop', loss=None, metrics=None,...)**\n\n> optimizer：字符串或者Opitimizer实例\n>\n> loss：字符串或Loss示例或一个函数\n>\n> metrics：字符串或Metric示例或一个函数\n\n- **训练模型：model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0., validation_data=None, ...)**\n\n> x/y：可以是numpy数组或者Tensor或list或迭代器等，这样x/y应一一对应。x也可为DataSet、Generator、Sequence，但是这种情况下就不应该指定y，因为y已经在x中了\n>\n> batch_size如果未指定则默认32\n>\n> callbacks：一个列表，每个元素是一个callbacks中的实例，如EarlyStopping类\n>\n> validation_split和validaton_data只需要指定一个，前者是验证集所占比例，后者是直接指定验证集**（如果指定了该参数，则x/y只能Tensor或ndarray）**\n\n- **画模型：tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=False, show_dtype=False, show_layer_names=True, ...)**\n\n> model是要打印的模型，一个keras model实例\n\n- **测试模型：model.evaluate(x=None, y=None, batch_size=None, verbose=1, ...)**\n\n> 用法和fit基本一样\n\n- **采用另一种方法训练模型：**\n\n> ```python\n> with tf.GradientTape() as tape:\n>     predictions = model(inputs, training = True)\n>     loss = loss_func(labels, predictions)\n> gradients = tape.gradient(loss, model.trainable_variables)\n> optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n> ```\n\n\n\n### 保存和加载模型\n\n- 既有keras保存方式，也有tf原生方式保存方式，**但是前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署**。既可以直接保存整个模型，也可以分别保存模型的权重和结构\n\n> ```python\n> # 直接保存\n> model.save(\"model.h5\", save_format = \"h5\")\n> model = tf.keras.models.load_model(\"model.h5\")\n> \n> # 分别保存模型的权重和结构\n> json_str = model.to_json()\n> model.save_weights(\"model_weights.h5\", save_format = \"h5\")\n> model = tf.keras.models.model_from_json(json_str)\n> model.compile(...)    # 这种方法重新加载后需要重新配置优化器\n> model.load_weights（\"model_weights.h5\"）\n> \n> # 上面演示的是keras保存方式\n> # 要使用tf原生方式，需要在model.save()和model.save_weights()时添加参数save_format=\"tf\"，并且更改文件后缀\n> # 演示一下：\n> model.save_weights('tf_model_weights.ckpt',save_format=\"tf\")\n> model.save('tf_model_savedmodel', save_format=\"tf\")\n> ```\n\n\n\n### 模型的反馈\n\n- **设置Early Stop：tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, ..., restore_best_weights=False)**\n\n> monitor：要监视的量，如：\"val_loss\"、\"val_accuracy\"、\"train_loss\"等\n>\n> min_delta：若小于min_delta的绝对变化，将被视为没有改进\n>\n> patience：容忍可以没有提升的epochs数\n>\n> restore_best_weights：是否保存整个训练过程中最好的一个epoch\n\n- **设置学习率衰减：tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=10, verbose=0, ..., min_delta=0.0001, ..., min_lr=0)**\n\n> 当经过patience个epochs仍没有改进，则降低学习率，学习率*factor来降低\n>\n> factor：每次降低的倍数\n>\n> min_lr：学习率可降低到的最小值\n\n\n\n### Dataset用法\n\n- 在`model.fit`中需要可以选择分别用list等类型指定x和y，也可以直接使用Dataset等类指定x而忽略y。对于前者，需要将所有数据一次性读入内存，内存负担较大。而后者可以使用TFRecordDataset类进行外存到内存的流式读取，只需要牺牲小部分IO性能，可大大减轻内存的负担\n- 简单来说，一个.tfrecords文件包含多个样本**（官方建议每个文件大小100-200M）**，每个样本都对应着一个`tf.train.Example`类，每个样本可能有多个特征，每个特征用`tf.train.Feature()`封装\n- tf.train.Featrue可接收以下三种类型，大多数其他通用类型也可以强制转换成下面的其中一种：\n\n> 1. **tf.train.BytesList：**string、byte**（非标量数据也使用这个，但是需要先将Tensor序列化）**\n> 2. **tf.train.FloatList：**float、double\n> 3. **tf.train.Int64List：**bool、enum、int32、unit32、int64、uint64g\n\n- **首先进行.tfrecords文件的存入：**\n\n```python\ndef save_tfrecords(data, label, desfile):\n    with tf.io.TFRecordWriter(desfile) as writer:\n        for i in range(len(data)):\n            features = tf.train.Features(\n                feature = {\n                    \"data\":tf.train.Feature(bytes_list = tf.train.BytesList(value =[tf.io.serialize_tensor(data[i]).numpy()])),\n                    \"label\":tf.train.Feature(float_list = tf.train.FloatList(value = label[i])),\n                }\n            )\n            example = tf.train.Example(features = features)\n            serialized = example.SerializeToString()\n            writer.write(serialized)\n   \n\nsave_tfrecords(x_in_sample1, y_in_sample1, \"path1.tfrecords\")\nsave_tfrecords(x_in_sample2, y_in_sample2, \"path2.tfrecords\")\n```\n\n> - 首先要用`tf.io.TFRecordWriter()`打开写入器，data和label是两个列表，每个元素对应一个Example的特征，desfile是文件名\n> - 对于每个Example的特征，使用一个字典表示，字典还需要被`tf.train.Features()`封装起来，字典的每个值可以是上述三种类型之一，需要使用`tf.train.Feature()`封装，其中value的值必须为一个list**（注意前者是Features，后者是Feature）**。并且**由于data是非标量的高维数据，需要先使用`tf.io.serialize_tensor()`进行序列化，然后再使用其numpy值**\n> - 最后通过`tf.train.Example`封装feature，然后调用`SerializeToString()`函数，将编码后的字符串写入\n\n- **接下来进行数据读取和使用：**\n\n```python\n# TFR数据反编译\ndef map_func(example):\n    feature_description = {\n        'data': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.float32),\n    }\n    parsed_example = tf.io.parse_single_example(example, features=feature_description)\n    \n    x_sample = tf.io.parse_tensor(parsed_example['data'], tf.float32)\n    y_sample = parsed_example['label']\n    \n    return x_sample, y_sample\n\n# 加载数据集\ndef load_dataset(filepaths):\n    shuffle_buffer_size = 3000\n    batch_size = 256\n\n    dataset = tf.data.TFRecordDataset(filepaths)\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(map_func=map_func, num_parallel_calls=tf.data.AOTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(64)\n    \n    return dataset\n\n\ntrain_set = load_dataset([\"path1.tfrecords\",\"path2.tfrecords\"])\nvalid_set = load_dataset([\"path3.tfrecords\",\"path4.tfrecords\"])\nmodel.fit(train_set,epochs=model_epochs, validation_data=valid_set, callbacks=[early_stopping])\n```\n\n> - 首先要生成`tf.data.TFRecordDataset`类，传入参数是一个list，其中元素是.tfrecords文件名\n> - 每次读取后要先打乱\n> - 之后调用`map()`，其中指定的函数是**对每个样本操作的**，由于储存时序列化为了二进制字符串，所以数据是什么类型是的信息是丢失了的，解码时需要自己制定数据类型，所以在`map()`中首先需要指定feature_description，说明每个数据的类型\n> - 然后使用`tf.io.parse_single_example()`解码单个样本，结果返回一个字典，字典的每个元素对应一个特征，其中**由于data是string类型，由于是编码后的非标量数据，所以还需要使用`tf.io.parse_tensor`解码一次（非标量数据的shape的编码解码的过程中是会丢失的，所以需要把shape一起储存）**\n> - 如果储存的是列表，需要在`tf.io.FixedLenFeature()`指定shape，也就是(len(list), )\n\n\n\n\n\n# keras_bert用法\n\n- **加载预训练模型：keras_bert.load_trained_model_from_checkpoint(config_file, checkpoint_file, training=False, trainable=None, output_layer_num=1, seq_len=int(1e9), kwargs)**\n\n> config_file和checkpoint_file分别为bert_config.json文件和bert_model.ckpt文件\n>\n> training：如果为True，则将返回带有 MLM 和 NSP输出的模型；否则，将返回输入层和最后一个特征提取层。\n>\n> trainable：模型是否可训练（也可通过for循环分别制定每层是否可训练）\n>\n> seq_len为最大序列长度\n\n- **编码输入：keras_bert.Tokenizer(token_dict, ...)**\n\n> bert模型的输入有两个，一个是语义token，一个是序列token，需要创建Tokenizer实例，再对句子进行编码，产生两个token\n>\n> token_dict：是输入的映射字典，在vocab.txt中可以获取所有token，但是需要自行建立token -> id的字典\n>\n> 创建Tokenizer实例后，调用`Tokenizer.encode(first, second=None, max_len=None)`进行编码，结果返回一个list，元素分别为两个list，分别对应两个token\n\n- **将数据输入模型：**\n\n> 在`model.fit()`中，由于是多输入，采用`x=[train_token_embeddings, train_seq_embeddings]`和`validation_data=([test_token_embeddings, test_seq_embeddings], test_labels)`这种形式进行输入\n\n- **加载模型时所需要注意的：**\n\n> 加载保存好的模型时同样是使用`tf.keras.models.load_model()`，但是会出现如下报错：\n>\n> **ValueError: Unknown layer: TokenEmbedding**\n>\n> 这种情况需要：先`from keras_bert import get_custom_objects`，再在`load_model()`时添加参数`custom_objects=get_custom_objects()`\n\n\n\n\n\n# bert4keras用法\n\n- **由于keras和tf.keras的兼容问题，所以最开始需要添加环境变量TF_KERAS=1：**\n\n```python\nimport os\nos.environ[\"TF_KERAS\"] = \"1\"\n```\n\n- 直接看[民间的API文档](https://github.com/Sniper970119/bert4keras_document)，写的还可以\n\n\n\n\n\n# Pytorch\n\n### 基本操作\n\n- **读取数据**\n\n> - 自建一个类，继承`torch.utils.data.Dataset`，还要实现`__init__()`、`__getitem__(index)`、`__len__()`三个函数\n> - 实例化Dataset，并用`torch.utils.data.DataLoader`封装：`DataLoader(data, batch_size=1)`\n> - 直接for循环取每个batch：`for input_ids, attention_mask, labels in data: `\n\n- **搭建模型：**\n\n> 1. 继承`torch.nn.Module `，并实现`__init__()`、`forward()`\n\n- **损失函数：**\n\n> - 以交叉熵举例\n> - 创建：`loss = torch.nn.CrossEntropyLoss(reduction=\"none\").cuda() `\n> - 其中`reduction=Union[“mean”, “sum”, “none”]`，表示对每个sample的loss怎么处理\n> - **踩个坑：如果loss的输入（模型的输出置信度）超过两维，如token级别的分类，`shape=[batch_size, max_seq_len, num_class]`，需要把num_class移到第二维，即执行`output.transpose(1, 2) `，现在`shape=[batch_size, num_class, max_seq_len]`**\n>\n> ![IMG_27](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_27.png)\n\n- **优化器：**`torch.optim.Adam(model.parameters(), lr=0.01`\n- **反向传播：**\n\n> ```python\n> model.zero_grad()\n> loss.backward()\n> optimizer.step()\n> ```\n\n- **模型保存和加载：**\n\n```python\n# 可以选择保存整个模型，也可以选择只保存权重\n# 保存整个模型：\ntorch.save(model, file_path)   # file_path以.pt .pth结尾\n# 加载整个模型\nmodel = torch.load(file_path)\n\n# 保存权重\ntorch.save(model.state_dict(), file_path)\n# 加载权重\nmodel = MyModel()\nmodel.load_state_dict(torch.load(file_path))\n```\n\n- **模型训练、测试模式转换：**`model.train()`、`model.eval()`\n\n\n\n### 多卡并行\n\n- **运行命令：**`python3 -m torch.distributed.launch --nproc_per_node=2 {file_name}`\n\n> 其中nproc_per_node代表GPU数量\n\n- **命令行参数**\n\n> 1. 进程编号：`parser.add_argument(\"--local_rank\", type=int, default=-1) `，local_rank为0代表主进程\n> 2. GPU数量：`gpu_num = int(os.environ['WORLD_SIZE']) `\n> 3. 需要手动设置环境变量：`os.environ[\"CUDA_VISIBLE_DEVICES\"] = “0, ..., gpu_num-1”`\n\n- **初始化：**\n\n> ```python\n> torch.cuda.set_device(local_rank)\n> if gpu_num > 1 :\n>  \ttorch.distributed.init_process_group(backend=\"nccl”)\n> ```\n\n- **读取数据：**\n\n> ```python \n> data = Dataset()\n> data_sampler = torch.utils.data.distributed.DistributedSampler(data)\n> data = DataLoader(data, batch_size=1, sampler=data_sampler)   # 需要在DataLoader封装时指定sampler\n> ```\n\n- **模型：**\n\n> - 模型需要用`torch.nn.parallel.DistributedDataParallel `封装：\n> ```python\n> model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=False) `\n> ```\n> - 其中find_unused_parameters默认为False，为False效率更高，如果报错则改为True\n\n- **打乱数据：**\n\n> - 在每个epoch开始的时候需要`data.sampler.set_epoch(epoch) `\n> - 是为了让sample打乱数据，使每个epoch的训练数据顺序不同\n\n- **同步每个进程的数据：**\n\n> - 每个进程中的数据是不同步的，如train_loss、val_loss等\n> - 通过`torch.distributed.all_reduce(value)`来对value进行同步\n> - 同步之后是所有进程该值的和，所以一般需要除以gpu_num\n> - value必须是Tensor，不能是标量\n\n- **模型保存和加载：**\n\n> - 如果model用DDP封装，保存时保存`model.module`或者`model.module.state_dict()`\n> - **一般只在主进程进行模型保存、打印、tensorboard记录**\n\n\n\n### Huggingface Transformers\n\n- **Tokenizer加载：**`AutoTokenizer.from_pretrained(checkpoint)`\n- **Tokenizer编码：**\n\n> - `tokenizer.encode(text, return_tensors=\"pt\") `：只返回词典映射后的编码结果，`text=str`\n> - `tokenizer(texts, padding=\"max_length\", truncation=True,max_length=150, return_tensors=\"pt”)`：返回一个字典，包括模型所需要的所有输入，一般为input_ids和attention_mask。`texts=Union[str, list(str)]`，`padding=Union[“max_length”, True, False]`\n> - **tokenizer()的结果如果需要组成batch，字典中的每个Tensor需要将第一维去掉：**\n>\n> ```python\n> for key, val in inputs.items():\n>     inputs[key] = val.squeeze(0)\n> ```\n\n- **Tokenizer解码：**`tokenizer.decode(ids, skip_special_tokens=True) `\n\n> 其中ids为input_ids\n\n- **指定Tokenizer填充和截断方向：**\n> - `tokenizer.padding_side=Union[“left”, “right”]`\n> -  `tokenizer.truncation_side=Union[“left”, “right”] `\n- **模型：**\n\n> - 加载：`AutoModel.from_pretrained(checkpoint) `\n> - 前向计算：\n>\n> ```python\n> inputs = tokenizer().    # inputs是一个dict\n> output = model(**inputs)。   # 输出的output是一个dict\n> ```\n\n- **生成式模型进行生成预测：**`model.generate(ids, max_length=None, num_beams=1,do_sample=False, top_k=1, early_stopping=False, num_return_sequences=1)`\n\n> - 一些生成模型如BloomForCausalModel自带生成函数\n> - 其中ids为encode之后的结果，每次generate的ids只对应一个句子，无法并行\n> - 需要do_sanple=True，才能启用top_k、top_p\n\n\n\n### Tensorboard\n\n- 直接给个🌰：\n\n> ```python\n> # 初始化\n> time = “{0:%Y-%m-%d-%H:%M:%S/}\".format(datetime.datetime.now())\n> tb_path = config[\"tb_root_path\"] + time\n> print(f\"Start Tensorboard with 'tensorboard --logdir={tb_path} --bind_all', view at http://localhost:6006/\")\n> writer = SummaryWriter(tb_path)\n> # 保存标量\n> writer.add_scalar(\"train/loss\", loss, step)\n> ```\n>\n> - 运行完之后直接运行`tensorboard --logdir={tb_path} --bind_all`，然后访问本地6006端口即可\n> - 在多卡时，一般只在主进程采用tensorboard\n\n","source":"_posts/代码总结.md","raw":"---\ntitle: 代码总结（持续更新）\nmath: true\ndate: 2022-7-17\n---\n\n# NLTK用法\n\n- nltk用于**英文**分词分句等应用\n\n### 基本的预处理\n\n- **分句：nltk.sent_tokenize(text, language=\"english\"):**\n\n> 输入：一个str段落\n>\n> 输出：一个list，每个元素是一个str句子\n\n- **分词：nltk.word_tokenize(text, language=\"english\", preserve_line=False):**\n\n> 输入：一个str句子\n>\n> 输出：一个list，每个元素是一个str词\n\n- **词性标注（POS_tag）：nltk.postag(tokens)**\n\n> 输入：一个list，每个元素是一个str，一个句子分好词的结果\n>\n> 输入：一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[('football', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('family', 'NN')]\n>\n> 词性所对应的意义，大致来说**N开头对应名词NOUN，V开头对应动词VERB，J开头对应形容词ADJ，R开头对应副词ADV**\n>\n> 词性具体所对应意义如下：\n>\n> | 标记 | 含义 | 示例                            |\n> | ---- | ---- | ------------------------------- |\n> | CC   | 连词 | and, or,but, if, while,although |\n> | CD   | 数词 | twenty-four, fourth, 1991,14:24 |\n> |DT | 限定词|  the, a, some, most,every, no |\n> |EX| 存在量词| there, there’s|\n> |FW |外来词 |dolce, ersatz, esprit, quo,maitre|\n> |IN |介词连词| on, of,at, with,by,into, under|\n> |JJ |形容词 |new,good, high, special, big, local|\n> |JJR| 比较级词语| bleaker braver breezier briefer brighter brisker|\n> |JJS| 最高级词语| calmest cheapest choicest classiest cleanest clearest|\n> |LS| 标记| A A. B B. C C. D E F First G H I J K|\n> |MD| 情态动词| can cannot could couldn’t|\n> |NN| 名词| year,home, costs, time, education|\n> |NNS| 名词复数| undergraduates scotches|\n> |NNP| 专有名词| Alison,Africa,April,Washington|\n> |NNPS| 专有名词复数| Americans Americas Amharas Amityvilles|\n> |PDT| 前限定词| all both half many|\n> |POS| 所有格标记 ’| 's|\n> |PRP| 人称代词| hers herself him himself hisself|\n> |PRP$| 所有格| her his mine my our ours|\n> |RB| 副词| occasionally unabatingly maddeningly|\n> |RBR |副词比较级| further gloomier grander|\n> |RBS| 副词最高级 |best biggest bluntest earliest|\n> |RP |虚词| aboard about across along apart|\n> |SYM |符号 |% & ’ ‘’ ‘’. ) )|\n> |TO |词to| to|\n> |UH| 感叹词| Goodbye Goody Gosh Wow|\n> |VB |动词| ask assemble assess|\n> |VBD| 动词过去式 |dipped pleaded swiped|\n> |VBG| 动词现在分词| telegraphing stirring focusing|\n> |VBN| 动词过去分词| multihulled dilapidated aerosolized|\n> |VBP| 动词现在式非第三人称时态| predominate wrap resort sue|\n> |VBZ| 动词现在式第三人称时态| bases reconstructs marks|\n> |WDT| Wh限定词| who,which,when,what,where,how|\n> |WP| WH代词| that what whatever|\n> |WP$| WH代词所有格 |whose|\n> |WRB| WH副词||\n> \n\n- **词形还原Lemmatization：nltk.stem.WordNetLemmatizer().lemmatize(word, pos=\"n\")：**\n\n> 这是一个类的内置方法，首先需要创建WordNetLemmatizer对象（假设为wnl），则通过`wnl.lemmatize()`调用\n>\n> 输入：word是单个词，pos是其对应的单词词性（n对应noun，v对应verb，a对应adj，r对应adv，s对应satellite adjectives(不咋用)）\n>\n> 输出：word经过还原后的词，如cars还原为car\n\n\n\n### NER\n\n- 先介绍一下命名实体识别（Named Entity Recognition，NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。\n- nltk中对NER类别的分类如下：\n\n![img](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20191219223810610.png)\n\n其中LOCATION和GPE有重合，GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。\n\n- **进行NER：ne_chunk(tagged_tokens, binary=False)**\n\n> 输入：tagged_tokens为`pos_tag()`函数的输出，一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[('football', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('family', 'NN')]\n>\n> 输出：同样的是一个list，每个元素变为了一个封装对象（Tree类），和输入一一对应\n>\n> - **对于该封装对象：**\n>\n> 输出的list中，有些元素是没有NER的结果的，其对应的封装对象没有label属性，**可使用`hasattr(ne_word, 'label')`函数判断是否有NER结果**\n>\n> 假设一个该对象命名为ne_word，调用`ne_word.leaves()`可返回一个list，每个元素为一个tuple，tuple[0]为token，tuple[1]为该token的pos_tag。对于NER，list中只有一个tuple，返回结果示例：[('FIFA', 'NNP')]\n>\n> 调用`ne_word.label()`函数可返回该token对应的NER结果，一定要先使用`hasattr()`函数才能使用`label()`函数\n\n\n\n### 计算BLEU\n\n- [BLEU的定义](https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#5-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91)\n\n- **nltk.translate.bleu_score.sentence_bleu(references, hypothesis, ..., smoothing_function=None, ...)**\n\n> references：参照序列，label。**type=list(list(str))，其中的每个str为词或字**\n>\n> hypothesis：候选序列，也就是预测出的序列。**type=list(str)，其中的每个str为词或字**\n>\n> smoothing_function，是论文中使用到的平滑技巧，一般输入`nltk.translate.bleu_score.SmoothingFunction().methodi()`，最后的i为0~7\n\n- 另外还可以使用`corpus_bleu()`计算多个句子的BLEU；用`modified_precision()`计算修正的n-gram精确度\n\n\n\n# Pyltp用法\n\n- Pyltp主要用于中文的预处理等，包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注\n- 使用Pyltp时注意其编码使用的都是utf-8，而Windows终端使用GBK编码，如果出现乱码可以将输出重定向到文件，然后用utf-8编码查看\n- **分句：pyltp.SentenceSplitter()**\n\n> 使用时先创建实例：`sp = SentenceSplitter()`，再进行分句：`sents = sp.split(doc)`\n\n- **分词：pyltp.Segmentor()**\n\n> Segmentor加载模型可以用`load()`也可以用`load_with_lexicon()`，后者还要加一个用户词典的参数\n\n- **词性标注：pyltp.Postagger()**\n\n> 直接举个栗子吧：\n>\n> ```python\n> sent = \"据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。\"\n> # 加载模型\n> segmentor = Segmentor()\n> segmentor.load_with_lexicon(cws_model_path, lexicon_path)\n> postagger = Postagger()\n> postagger.load(pos_model_path)\n> # 分词和词性标注\n> words = segmentor.segment(sent)\n> postags = postagger.postag(words)\n> print(list(postags))\n> # 释放模型\n> segmentor.release()\n> postagger.release()\n> ```\n>\n> 注意：进行这些处理后返回的都是VectorOfString类，是一个可迭代类，也可用list()转换为列表\n>\n> 词性标注的词性如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724143414199.png\" alt=\"image-20220724143414199\" style=\"zoom: 50%;\" />\n>\n> 词性标注也可以添加用户词典\n\n- **NER识别：pyltp.NamedEntityRecognizer()**\n\n> ```python\n> from pyltp import NamedEntityRecognizer\n> recognizer = NamedEntityRecognizer()\n> recognizer.load(ner_model_path)\n> ner_results = recognizer.recognize(words, postags)\n> ```\n>\n> LTP 采用 BIESO 标注体系。**B 表示实体开始词，I表示实体中间词，E表示实体结束词，S表示单独成实体，O表示不构成命名实体。**\n>\n> LTP 提供的命名实体类型为：**人名（Nh）、地名（Ns）、机构名（Ni）**。\n>\n> B、I、E、S位置标签和实体类型标签之间用一个横线 `-` 相连；O标签后没有类型标签。\n\n- **依存句法分析：pyltp.Parser()**\n\n> 依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系\n>\n> 首先也是创建实例和加载模型，然后`parse_results = parser.parse(words, postags)`，得到的结果是一个可迭代对象，其中的每一个元素也是个自定义类（假设parse_results[i]为result），调用`result.head`和`result.relation`可分别获得words[i]对应的关系词和关系，其中head是返回关系词所对应的索引+1，如果为0则为\"Root\"，表示无对应关系词\n>\n> 举个栗子：\n>\n> ```python\n> relations = [result.relation for result in parse_results]\n> heads = [words[result.head - 1] if result.head else \"Root\" for result in parse_results]\n> for i in range(len(words)):\n>  print(f\"{relations[i]} : ({words[i]}, {heads[i]})\")\n> ```\n>\n> 输出结果：\n> <div align=center><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211345194.png\" alt=\"image-20220724211345194\" style=\"zoom: 80%;\" />\n>\n>\n> 依存句法关系如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724153110907.png\" alt=\"image-20220724153110907\" style=\"zoom:50%;\" />\n\n- **语义角色标注：pyltp.SementicRoleLabeller()**\n\n> 语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等\n> 进行语义角色标注时首先同样是创建实例，再加载模型，然后调用方法：`roles = labeller.label(words, postags, parse_results)`。结果得到一个可迭代对象，**其中的每一个元素也是一个自定义类（假设每个元素为role），则`role.index`为谓语对应的索引，`role.arguments`又是一个可迭代对象，每个元素对应一个和该谓语对应的语义角色，也是一个自定义类（假设为argument），**那么可以通过`argument.name`获取角色和谓语的关系，`argument.range.start`和`argument.range.end`对应该角色的开始和结束索引（**end要算上的**）\n>\n> 给个示例：\n>\n> ```python\n> roles = labeller.label(words, postags, parse_results)\n> for role in roles:\n>  arguments = role.arguments\n>  index = role.index\n>  print(f\"谓语: {words[index]} (索引: {index})\")\n>  for argument in arguments:\n>      start, end = argument.range.start, argument.range.end\n>      obj = \"\"\n>      for word in words[start : end+1]:\n>          obj += word\n>      print(f\"{argument.name}: {obj} (索引: {start}:{end})\")\n>  print(\"\\n\")\n> ```\n>\n> 输出结果：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211302209.png\" alt=\"image-20220724211302209\" style=\"zoom:67%;\" />\n>\n> 语义角色如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211544134.png\" alt=\"image-20220724211544134\" style=\"zoom:60%;\" />\n\n\n\n\n\n# rouge用法\n\n- rouge和bleu一样也是一种对序列质量的评估标准，但是相比于bleu更关注召回率而非精准率（通过计算F1_score时改变$$\\beta$$参数），rouge-n计算公式如下：\n\n$$\nRough-N=\\frac{\\sum_{S \\in\\{\\text { ReferemceSummaries }\\}} \\sum_{\\text {gram }_{n} \\in S} \\text { Count }_{\\text {match }}\\left(\\text { gram }_{n}\\right)}{\\sum_{S \\in\\{\\text { ReferenceSummaries }\\}} \\text { gram }_{n} \\in S}\n$$\n\n- rouge-1和rouge-2都可通过上述公式计算出来，和bleu中的P1和P2计算公式很像。可以在下面看到，每个rough-N都会输出p和r，其实就是精准率和召回率，两者也就是分母不一样（一个分母是refs的n-gram个数，一个分母是hyps的n-gram个数）\n- 而rouge-l是rouge-N的改进：\n\n$$\n\\begin{array}{l}\nR_{l c s}=\\frac{L C S(X, Y)}{m} \\\\\nP_{l c s}=\\frac{L C S(X, Y)}{n} \\\\\nF_{l c s}=\\frac{\\left(1+\\beta^{2}\\right) R_{l c s} P_{l c s}}{R_{l c s}+\\beta^{2} P_{l c s}}\n\\end{array}\n$$\n\n其中X、Y为hyps和refs，m、n为他们的长度，LCS(X, Y)为X、Y的最长共同序列\n\n- **rouge.Rouge.get_scores(hyps, refs, avg=False, ignore_empty=False)**\n\n> **注意：输入的hyps和refs两个字符串，不管是中文英文，都需要每个字或词使用空格间隔**\n>\n> 输出是list(dict(dict))\n\n- 直接给栗子吧：\n\n```python\nfrom rouge import Rouge \n\nhypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n\nreference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n\nrouge = Rouge()\nscores = rouge.get_scores(hypothesis, reference)\n```\n\n**output：**\n\n```\n[\n  {\n    \"rouge-1\": {\n      \"f\": 0.4786324739396596,\n      \"p\": 0.6363636363636364,\n      \"r\": 0.3835616438356164\n    },\n    \"rouge-2\": {\n      \"f\": 0.2608695605353498,\n      \"p\": 0.3488372093023256,\n      \"r\": 0.20833333333333334\n    },\n    \"rouge-l\": {\n      \"f\": 0.44705881864636676,\n      \"p\": 0.5277777777777778,\n      \"r\": 0.3877551020408163\n    }\n  }\n]\n```\n\n其中p、r、f分别为精准率、召回率、F1_score\n\n\n\n\n\n# zhconv用法\n\n- 该库用于中文简繁体转换，但是也可以用**OpenCC库，精准度更高、覆盖率更高、速度更快**\n\n- **逐字转换：zhconv.convert(s, locale, update=None)**\n\n- **基于MediaWiki的转换：zhconv.ocnvert_for_mw(s, locale, update=None)**\n\n> 两个函数用法相同\n>\n> 示例：\n>\n> ```python\n> sent2 = \"計算機軟體\"\n> print(convert(sent2, \"zh-hans\"))\n> ```\n>\n> locale可为以下值：\n>\n> `zh-cn` 大陆简体、`zh-tw` 台灣正體、`zh-hk` 香港繁體、` zh-sg` 马新简体、`zh-hans` 简体、`zh-hant` 繁體\n\n\n\n\n\n# gensim用法\n\n- gensim主要用于创建语料库，和计算各种特征（如相似度、tf-idf）等任务\n\n### 创建语料库和计算相似度\n\n- **创建语料库类：gensim.corpora.Dictionary(texts)**\n\n> 输入：整个语料库，分好句和分好词的结果，一个list，每个元素又是一个list，里面的每个子元素是一个str词\n>\n> 输出：一个Dictionary对象\n\n- **将句子转化为词袋形式：Dictionary.doc2bow(text)**\n\n> 输入：分好词的一个句子，一个list，每个元素是一个str词\n>\n> 输入：一个list，每个元素是一个二元tuple，tuple[0]是句子中存在的词的索引，tuple[1]是其在句子中出现的次数（出现0次的不计入）\n>\n> 该类的用法和python自带的字典对象基本相同，values、key、item之类的\n\n- **获取索引字典：Dictionary.token2id**\n\n> 返回一个{token : id}的字典\n\n- **创建相似度类：gensim.similarities.Similarity(output_prefix, corpus, num_features)**\n\n> out_prefix是存储这个对象文件的名称\n>\n> corpus是整个语料库，但是必须先转化为词袋模型\n>\n> num_fuatures是整个词典的数量，一般使用len(dictionary)表示\n>\n> 对于得到的对象（取名为similarity），具体的用法为：similarity[test]，其中test为要比较的对象，可以是单个句子，也可以是整个语料库\n\n**举个栗子：**\n\n```python\nfrom nltk import word_tokenize\nfrom gensim.corpora import Dictionary\nfrom gensim.similarities import Similarity\n\nsent1 = \"I love sky, I love sea.\"\nsent2 = \"I like running, I love reading.\"\nsents = [sent1, sent2]\n# 分词\ntexts = [word_tokenize(sent) for sent in sents]\n\n# 创建字典对象\ndictionary = Dictionary(texts)\n# 获得词袋模型表示的词料库\ncorpus = [dictionary.doc2bow(text) for text in texts]\nprint(f\"The corpus is : {corpus}\")\n# 创建相似度对象\nsimilarity = Similarity(\"Similarity-excise1\", corpus, num_features=len(dictionary))\nprint(f\"Created class : {similarity}\")\n# 计算余弦相似度\ntest_corpus = dictionary.doc2bow(word_tokenize(sent1))\nprint(similarity[test_corpus])\n```\n\n输出：\n\n![image-20220706160148320](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706160148320.png)\n\n\n\n### 计算TF-IDF\n\n- **首先介绍一下tf-idf：**\n\n词频（term frequency，tf）指的是某一个给定的词语在该文件中对应的频率\n\n逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量，可由文件总数除以包含该词的文件的总数求得\n\n具体的计算公式如下：\n$$\n\\mathrm{tf}_{\\mathrm{i}, \\mathrm{j}}=\\frac{n_{i, j}}{\\sum_{k} n_{k, j}} \\\\\n\\operatorname{idf}_{\\mathrm{i}}=\\lg \\frac{|D|}{\\left|\\left\\{j: t_{i} \\in d_{j}\\right\\}\\right|}\\\\\n\\operatorname{tfidf}_{i, j}=\\mathrm{tf}_{\\mathrm{i}, \\mathrm{j}} \\times \\mathrm{idf}_{\\mathrm{i}}\n$$\n其中$$tf_{i, j}$$为$$word_i$$在$$doc_j$$中出现的词频，$$n_{i, j}$$为$$word_i$$在$$doc_j$$中出现的个数\n\n$$idf_i$$为$$word_i$$的逆向文件频率，$$|D|$$为文档总数，$$\\left|\\left\\{j: t_{i} \\in d_{j}\\right\\}\\right|$$为包含$$word_i$$的文档个数\n\n对于idf，底数可用2、e、10，并且分母可以+1，避免除以0\n\n- **创建tfidf模型类：gensim.models.TfidfModel(corpus)**\n\n> 输入为整个语料库，必须先转化为词袋模型\n>\n> 在具体使用时，和Similarity类似，tfidf_model[test]，其中test可以是单个句子也可以是整个语料库，返回一个列表，列表中每个元素为一个tuple，tuple[0]为词索引，tuple[1]为对应的tfidf值\n>\n> **gensim算出来的tf-idf是经过了规范化的，每个句子的tfidf值，是一个单位向量**\n\n**举个栗子：**\n\n```python\n# texts的构建是每个段落分词，有三个段落，则len(texts)=3\ntexts = [text1, text2, text3]\ntexts = [get_tokens(text) for text in texts]\ndictionary = Dictionary(texts)\nid2token_dict = {v : k for k, v in dictionary.token2id.items()}\ncorpus = [dictionary.doc2bow(text) for text in texts]\ntfidf_model = TfidfModel(corpus)\nresult = tfidf_model[corpus]\nfor i in result:\n    print(i)\n```\n\n输出：\n\n![image-20220706210417422](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706210417422.png)\n\n\n\n\n\n# Pandas用法\n\n- **创建DataFrame对象：pd.DataFrame(data, columns, index)**\n\n> 输入：data是一个list, 每个对象是一个tuple或list，tuple的长度和columns的个数对应。data也可以是numpy形式\n>\n> index是一个list，是行索引值，可以用于自行设置行索引**（索引没有规定一定是int）**\n>\n> 示例：pd.DataFrame([(1, 2), (3, 4), (5, 6)], columns=[\"name\", \"NER result\"])，输出为：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220711120934316.png\" alt=\"image-20220711120934316\" style=\"zoom: 80%;\" />\n\n- **获取DataFrame特征：DataFrame.describe()**\n- **获取DataFrame的某列：DataFrame[\"column_name\"]**\n\n>  这样可获取到column列，结果是一个Series类（**只有一个列则是Series，多个列则是DataFrame**）\n>\n>  如果不想出现重复结果，可调用`unique()`函数，示例：data[\"ner_result\"].unique()，返回的结果是一个ndarray，也可使用`list()`将其转换为列表\n>\n>  如果想把这个Series类转换为ndarray，则可使用Series.values，这是Series类其中的一个属性，不是方法，类型是一个ndarrary，也可使用`list()`转换为列表\n\n- **获取DataFrame列名：DataFrame.columns**\n\n- **DataFrame根据某列，进行分组：DataFrame.groupby(\"column_name\")**\n\n> 将DataFrame通过column_name列进行分组\n>\n> 结果是一个可迭代的DataFrameGroupBy类，该类的用法和DataFrame几乎一样，比如可使用`DataFrameGroupBy[\"column_name\"]`将其转换为SeriesGroupby类\n>\n> 也可转化为list，转换后，其中每一个元素为一个tuple，tuple[0]为column_name列对应的值，tuple[1]为原DataFrame的一部分，根据column_name的值进行截断。如果是将SeriesGroupby类转换为list，则tuple[1]变为一个Series类\n>\n> 举个例子：\n> input_data为一个DataFrame，值如下：\n>\n> <div align=center><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123006189.png\" alt=\"image-20220716123006189\" style=\"zoom:67%;\" />\n>\n> 现在将input_data根据sent_order列进行分组：input_data.groupby(\"sent_order\")，结果如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123836380.png\" alt=\"image-20220716123836380\" style=\"zoom: 67%;\" />\n>\n> 只展示了前三个元组，每个元组，tuple[0]为sent_order的值，tuple[1]为对应的DataFrame\n\n- **应用自定义方法：DaraFrame.apply(func, axis=0, ...)**\n\n> Series、DataFrame和GroupbyDataFrame等均可使用\n>\n> func是一个自定义函数\n>\n> axis决定func的输入是什么，axis=0则输入为行，axis=1则输入为列\n>\n> Series无需指定axis，DataFrame可调节axis更换操作的维度，**DaraFrameGroupby也无需指定，func的输入为分组后的每个DataFrame，最后再将每个分组返回的结果总和起来**\n\n- **DataFrame的count()方法：**\n\n> `count()`有很多种用法，结果是可以对Series、DataFrame和GroupbyDataFrame等使用，结果是返回一个Series或DataFrame\n>\n> 下面举几个示例，其中使用的df如下：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725142611139.png\" alt=\"image-20220725142611139\" style=\"zoom: 60%;\" />\n>\n> ```python\n> df.count()    # DataFrame执行count()，对每一列分别执行count，结果返回一个Series\n> df.data[\"length\"].count()    # Series执行count()，直接返回length列的长度，类型为int\n> df.groupy(\"length\").count()   # 对DataFrameGroupby类执行，返回一个DataFrame，index为length的各指，colums为出length的其他列执行count的结果\n> df.groupy(\"length\")[\"evaluation\"].count()  # 对SeriesGroupby类执行，则只返回evaluation列的count结果，类型为Series\n> ```\n>\n> 上述四句语句的执行结果：\n>\n> - 第一句：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143928089.png\" alt=\"image-20220725143928089\" style=\"zoom:67%;\" />\n>\n> - 第二句：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143939723.png\" alt=\"image-20220725143939723\" style=\"zoom:67%;\" />\n>\n> - 第三句：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143953768.png\" alt=\"image-20220725143953768\" style=\"zoom:50%;\" />\n>\n> - 第四句：\n>\n> <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725144005108.png\" alt=\"image-20220725144005108\" style=\"zoom:50%;\" />\n\n- **读取/保存csv文件：**\n\n```python\ndf.to_csv(file_name, index=False)    # 要加index=False\npd.read_csv(file_name)\n```\n\n\n\n### 处理缺失值\n\n- **判断空值：DataFrame.isnull()或DataFrame.notnull()**\n\n> DataFrame和Series都可用，结果也是返回一个DataFrame或Series，和输入一一对应，其中每个值为布尔值\n\n- **删除缺失值：DataFrame.dropna(axis=0, inplace=False, how=None)**\n\n> axis：删除行还是列，{0 or ‘index’, 1 or ‘columns’}\n>\n> how : 如果等于any则任何值为空都删除，如果等于all则所有值都为空才删除\n\n- **填充缺失值：DataFrame.fillna(value, method, axis, ...)**\n\n> value：用于填充的值，可以是单个值，或者字典（用于不同列填充不同值的情况，key是列名，value是值）\n> method : 等于\"ffill\"使用前一个；不为空的值填充forword fill；等于\"bfill\"使用后一个不为空的值填充backword fill\n> axis : 按行还是列填充，{0 or ‘index’, 1 or ‘columns’}\n\n\n\n### Padas中DataFrame的增删查改\n\n- **增加列：**\n\n```python\ndf = DataFrame(...)    # 有三行数据\ncities = [\"成都\", \"上海\", \"北京\"]\n# 三种插入方式\ndf.insert(0, \"city\", citys)    # 参数分别为：插入列的位置、列名、插入内容\ndf[\"city\"] = cities\ndf.loc[:, \"city\"] = cities\n```\n\n- **增加行：**\n\n```python\ndf = DataFrame(...)  # 有两列 \ndf.loc[3] = [\"1\", \"2\"]       # 如果已经存在index=3的行，则修改值；反之直接添加该行\ndf = df.append(df_insert)    # 合成两个DataFrame，列要相同才行\n```\n\n- **loc[]和iloc[]的使用：**\n\n> 两者功能是相同的，区别在于loc中对于列，只能使用列名（str类型）进行搜索，而iloc对于列只能使用整数索引进行搜索，DataFrame的查和改基本都是基于两者的\n\n```python\ndf[\"column_name\"]或df.loc[:, \"column_name\"]   # 查找column_name列\ndf.loc[\"index_name\"]    # 查找index_name行\ndf.loc[\"index_name\", \"column_name\"]     # 查找(index_name, column_name)处的值\n# 还可以使用list或者切片来代替，一次操作多行或多列\ndf.loc[[\"index_name1\", \"index_name2\"], [\"column_name1\", \"column_name2\"]]  # 同时操作index_name1&2行和column_name1&2列\ndf.loc[0:3, \"column_name\"]    # 进行切片，对0、1、2、3行操作\n```\n\n> `iloc[]`是使用整数列索引，比如：`df.iloc[:3, 2:6]`\n>\n> 注意`loc[]`中的切片，是包含了end所指的索引（和list的索引稍有不同），`iloc[]`是不包含end的索引的\n\n- **删除：**\n\n```python\ndf.drop(0, axis=0)   # 删除第0行\n# 删除列的三种方法\ndf.drop(\"column_name\", axis=1)\ndel df[\"column_name\"]\nndf = df.pop(\"column_name\")\n# 使用drop的时候，第一个参数也可以是list，一次操作多行或多列\ndf.drop([0, 1, 2], axis=0)\ndf.drop([\"column_name1\", \"column_name2\"], axis=1)\n```\n\n\n\n\n\n# Pickle用法\n\n- **储存pk文件：pickle.dump(obj, file)**\n\n- **加载pk文件：pickle.load(file)**\n\n> 可以存储和加载对象，其中`dump()`中的obj是要存的对象，file是`open()`函数返回的文件描述符\n>\n> `load()`返回值为所存储的对象\n>\n> 储存的文件后缀为.pk\n>\n> 举个栗子：\n>\n> ```python\n> a = [1, 2]\n> file_name = \"test.pk\"\n> # 存\n> with open(file_name, \"wb\") as f:\n>     pickle.dump(a, f)\n> # 取\n> with open(file_name, \"rb\") as f:\n>     b = pickle.load(f)\n>     \n> # a和b是一样的\n> ```\n\n\n\n\n\n# Json用法\n\n- **json可进行Python对象和json格式之间的互换**\n\n- **Python转json：json.dumps(obj, ensure_ascii=True, ..., indent=None, ...)**\n- **json转Python：json.loads(s, ...)**\n\n> obj为Python对象，s为json对象\n>\n> ensure_ascii：如果为False，则返回值可以包含非 ASCII\n>\n> indent：为json中每个元素的间隔数，如果为0或None，则每个元素之间紧凑排版\n>\n> **注意：**\n>\n> **1. 以上两个函数只是进行对象的转换，而不会进行保存，所以是一般配合`open()`、`f.write()`、`f.read()`使用，给两个示例：**\n>\n> ```python\n> # 存储json对象\n> with open(file_name, \"w\", encoding=\"utf-8\") as f:\n>  f.write(json.dumps(data, ensure_ascii=False, indent=2))\n> # 读取json对象\n> with open(file_name, 'r', encoding='utf-8') as f:\n>  data = json.loads(f.read())\n> ```\n>\n\n\n\n\n\n# Yaml\n\n- **读取yaml文件：**`config = yaml.safe_load(open(\"config.yaml\", \"r\", encoding=\"utf-8\").read())`\n- 在.yaml文件中，可用缩进表示层级关系，并且写浮点数不能用python的科学技术法的写法：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_25.png\" alt=\"IMG_25\" style=\"zoom:50%;\" />\n\n\n\n\n\n# Argparse\n\n- **基本流程：**\n\n```python\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--batch_size\", type=int, default=1) \nargs = parser.parse_args()\n```\n\n- 其中`--`表示可选参数，在运行命令指定`--batch_size=xxx`即可\n\n\n\n\n\n# Matplotlib用法\n\n- **折线图：plt.plot(x, y)**\n- **直方图：plt.bar(x, y)**\n- **散点图：plt.scatter(x, y)**\n\n> 上述函数还可以指定颜色、大小等参数：\n>\n> - color：颜色\n>\n> > 可为：\"b\"：蓝色、\"g\"：绿色、\"r\"：红色、\"c\"：青色、\"m\"：品红、\"y\"：黄色、\"k\"：黑色、\"w\"：白色\n> >\n> > 也只直接用全名，如\"blue\"\n>\n> - s：点的大小\n> - linewidth：线宽\n> - linestyle：线样式，如设为\"dashed\"将线设为虚线\n>\n> - label：名字， 若要打印label，则使用`plt.legend()`\n\n- **设置标题：plt.title()**\n\n> 同样可以使用fontsize指定字体大小\n\n- **设置x、y轴标题：plt.xlabel()和plt.ylabel()**\n- **保存图片：plt.savefig()**\n- **画完图之后记得关闭：plt.close()**\n- **画一条水平的线：plt.hlines(y, xmin, xmax)**\n- **画一条垂直的线：plt.vlines(x, ymin, ymax)**\n- **在指定座标处加注释：plt.text(x, y, s)**\n\n\n\n\n\n# Numpy用法\n\n- **获取最大值的位置（索引）np.argmax(a, axis=None, ...)：**\n\n> axis：如果为None, 则代表把整个数组当作一维的，然后返回最大的索引值。如果指定axis，则只在该维度搜索最大值，示例：\n>\n> ```python\n> array([[10, 11, 12],\n>        [13, 14, 15]])\n> >>> np.argmax(a)\n> 5\n> >>> np.argmax(a, axis=0)\n> array([1, 1, 1])\n> >>> np.argmax(a, axis=1)\n> array([2, 2])\n> ```\n>\n> axis指定为哪个维度，则stack哪个维度\n\n- **减少维度：np.squeeze(a, axis=None)**\n\n> axis如果为None，则丢弃所有大小为1的维度\n\n\n\n# Tensorflow用法\n\n### 禁用GPU\n\n```python\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n```\n\n\n\n### 数据预处理\n\n- **填充序列：tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)**\n\n> 输入：\n> sequences是要填充的序列\n>\n> maxlen是要填充的最大长度，如果为None，则设为所有序列中最长的那个的长度\n>\n> padding和truncating可以设为\"pre\"或者\"post\"，选择是在序列前面还是后面填充/截断\n>\n> value为填充的值\n>\n> 返回值是一个ndarray\n\n- **划分训练、测试集：sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)**\n\n> arrays是要划分的数据，可以使用list、numpy array、matric、dataframe**（不能用Tensor）**，示例：\n>\n> `x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)`\n>\n> train_size和test_size指定一个就行\n>\n> random_state为随机种子\n>\n> shuffle决定是否打乱数据\n>\n> stratify决定是否进行数据分层，stratify不是bool，是直接指定分层的依据变量，即通过什么来分层，一般就是label或者Series（在输入数据是DataFrame时，stratify指定为Series，即根据那一列来分层）\n>\n> **输入的数据是什么类型，输出的数据就是什么类型，返回值依次为：x_train, x_test, y_train, y_test**\n\n\n\n### 各种层\n\n- **tf.keras.layers.Dense(units, activation=None, use_bias=True, ...)**\n- **tf.keras.layers.Embedding(input_dim, output_dim,...., mask_zero=False, input_length=None)**\n\n> input_dim：词汇表大小\n>\n> output_dim：词嵌入向量的维度\n>\n> mask_zero：决定输入值0是否是应被屏蔽的mask“填充”值。如果为True，则使用mask，相应的input_dim也应该为词汇表大小+1（因为增加了一个mask token，索引为0）\n>\n> intput_length：输入序列的长度，如果为None，则自动为最长序列的长度\n\n- **tf.keras.layers.LSTM(units, activation=\"tanh\", recurrent_activation=\"sigmoid\", ..., return_sequences=False)**\n\n> 其中return_sequences决定是否在最后返回所有时间步的输出，若为False只会返回最后一个时间步的输出，如果下一层需要使用到所有时间步的输出则需要设为True\n\n- **tf.keras.layers.Bidirectional(layer, merge_mode=\"concat\", ...)**\n\n> 是RNN的双向包装器，可将单向RNN变为双向RNN\n>\n> layer是要包装的层，merge_mode决定前后信息汇总的 方式，可为：\"sum\"、\"mul\"、\"concat\"、\"ave\"、None，如果为None，则不会合并，而是将他们作为列表输出\n\n- **tf.keras.layers.Softmax(axis=-1)**\n\n> axis：决定对那个维度执行softmax，默认为最后一个维度\n\n\n\n### 模型的常用方法\n\n- **序列容器：tf.keras.models.Sequential()**\n\n> 可以和`add()`配合使用，也可以里面直接加个列表Sequential([...])\n\n- **配置模型：model.compile(optimizer='rmsprop', loss=None, metrics=None,...)**\n\n> optimizer：字符串或者Opitimizer实例\n>\n> loss：字符串或Loss示例或一个函数\n>\n> metrics：字符串或Metric示例或一个函数\n\n- **训练模型：model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0., validation_data=None, ...)**\n\n> x/y：可以是numpy数组或者Tensor或list或迭代器等，这样x/y应一一对应。x也可为DataSet、Generator、Sequence，但是这种情况下就不应该指定y，因为y已经在x中了\n>\n> batch_size如果未指定则默认32\n>\n> callbacks：一个列表，每个元素是一个callbacks中的实例，如EarlyStopping类\n>\n> validation_split和validaton_data只需要指定一个，前者是验证集所占比例，后者是直接指定验证集**（如果指定了该参数，则x/y只能Tensor或ndarray）**\n\n- **画模型：tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=False, show_dtype=False, show_layer_names=True, ...)**\n\n> model是要打印的模型，一个keras model实例\n\n- **测试模型：model.evaluate(x=None, y=None, batch_size=None, verbose=1, ...)**\n\n> 用法和fit基本一样\n\n- **采用另一种方法训练模型：**\n\n> ```python\n> with tf.GradientTape() as tape:\n>     predictions = model(inputs, training = True)\n>     loss = loss_func(labels, predictions)\n> gradients = tape.gradient(loss, model.trainable_variables)\n> optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n> ```\n\n\n\n### 保存和加载模型\n\n- 既有keras保存方式，也有tf原生方式保存方式，**但是前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署**。既可以直接保存整个模型，也可以分别保存模型的权重和结构\n\n> ```python\n> # 直接保存\n> model.save(\"model.h5\", save_format = \"h5\")\n> model = tf.keras.models.load_model(\"model.h5\")\n> \n> # 分别保存模型的权重和结构\n> json_str = model.to_json()\n> model.save_weights(\"model_weights.h5\", save_format = \"h5\")\n> model = tf.keras.models.model_from_json(json_str)\n> model.compile(...)    # 这种方法重新加载后需要重新配置优化器\n> model.load_weights（\"model_weights.h5\"）\n> \n> # 上面演示的是keras保存方式\n> # 要使用tf原生方式，需要在model.save()和model.save_weights()时添加参数save_format=\"tf\"，并且更改文件后缀\n> # 演示一下：\n> model.save_weights('tf_model_weights.ckpt',save_format=\"tf\")\n> model.save('tf_model_savedmodel', save_format=\"tf\")\n> ```\n\n\n\n### 模型的反馈\n\n- **设置Early Stop：tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, ..., restore_best_weights=False)**\n\n> monitor：要监视的量，如：\"val_loss\"、\"val_accuracy\"、\"train_loss\"等\n>\n> min_delta：若小于min_delta的绝对变化，将被视为没有改进\n>\n> patience：容忍可以没有提升的epochs数\n>\n> restore_best_weights：是否保存整个训练过程中最好的一个epoch\n\n- **设置学习率衰减：tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=10, verbose=0, ..., min_delta=0.0001, ..., min_lr=0)**\n\n> 当经过patience个epochs仍没有改进，则降低学习率，学习率*factor来降低\n>\n> factor：每次降低的倍数\n>\n> min_lr：学习率可降低到的最小值\n\n\n\n### Dataset用法\n\n- 在`model.fit`中需要可以选择分别用list等类型指定x和y，也可以直接使用Dataset等类指定x而忽略y。对于前者，需要将所有数据一次性读入内存，内存负担较大。而后者可以使用TFRecordDataset类进行外存到内存的流式读取，只需要牺牲小部分IO性能，可大大减轻内存的负担\n- 简单来说，一个.tfrecords文件包含多个样本**（官方建议每个文件大小100-200M）**，每个样本都对应着一个`tf.train.Example`类，每个样本可能有多个特征，每个特征用`tf.train.Feature()`封装\n- tf.train.Featrue可接收以下三种类型，大多数其他通用类型也可以强制转换成下面的其中一种：\n\n> 1. **tf.train.BytesList：**string、byte**（非标量数据也使用这个，但是需要先将Tensor序列化）**\n> 2. **tf.train.FloatList：**float、double\n> 3. **tf.train.Int64List：**bool、enum、int32、unit32、int64、uint64g\n\n- **首先进行.tfrecords文件的存入：**\n\n```python\ndef save_tfrecords(data, label, desfile):\n    with tf.io.TFRecordWriter(desfile) as writer:\n        for i in range(len(data)):\n            features = tf.train.Features(\n                feature = {\n                    \"data\":tf.train.Feature(bytes_list = tf.train.BytesList(value =[tf.io.serialize_tensor(data[i]).numpy()])),\n                    \"label\":tf.train.Feature(float_list = tf.train.FloatList(value = label[i])),\n                }\n            )\n            example = tf.train.Example(features = features)\n            serialized = example.SerializeToString()\n            writer.write(serialized)\n   \n\nsave_tfrecords(x_in_sample1, y_in_sample1, \"path1.tfrecords\")\nsave_tfrecords(x_in_sample2, y_in_sample2, \"path2.tfrecords\")\n```\n\n> - 首先要用`tf.io.TFRecordWriter()`打开写入器，data和label是两个列表，每个元素对应一个Example的特征，desfile是文件名\n> - 对于每个Example的特征，使用一个字典表示，字典还需要被`tf.train.Features()`封装起来，字典的每个值可以是上述三种类型之一，需要使用`tf.train.Feature()`封装，其中value的值必须为一个list**（注意前者是Features，后者是Feature）**。并且**由于data是非标量的高维数据，需要先使用`tf.io.serialize_tensor()`进行序列化，然后再使用其numpy值**\n> - 最后通过`tf.train.Example`封装feature，然后调用`SerializeToString()`函数，将编码后的字符串写入\n\n- **接下来进行数据读取和使用：**\n\n```python\n# TFR数据反编译\ndef map_func(example):\n    feature_description = {\n        'data': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.float32),\n    }\n    parsed_example = tf.io.parse_single_example(example, features=feature_description)\n    \n    x_sample = tf.io.parse_tensor(parsed_example['data'], tf.float32)\n    y_sample = parsed_example['label']\n    \n    return x_sample, y_sample\n\n# 加载数据集\ndef load_dataset(filepaths):\n    shuffle_buffer_size = 3000\n    batch_size = 256\n\n    dataset = tf.data.TFRecordDataset(filepaths)\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(map_func=map_func, num_parallel_calls=tf.data.AOTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(64)\n    \n    return dataset\n\n\ntrain_set = load_dataset([\"path1.tfrecords\",\"path2.tfrecords\"])\nvalid_set = load_dataset([\"path3.tfrecords\",\"path4.tfrecords\"])\nmodel.fit(train_set,epochs=model_epochs, validation_data=valid_set, callbacks=[early_stopping])\n```\n\n> - 首先要生成`tf.data.TFRecordDataset`类，传入参数是一个list，其中元素是.tfrecords文件名\n> - 每次读取后要先打乱\n> - 之后调用`map()`，其中指定的函数是**对每个样本操作的**，由于储存时序列化为了二进制字符串，所以数据是什么类型是的信息是丢失了的，解码时需要自己制定数据类型，所以在`map()`中首先需要指定feature_description，说明每个数据的类型\n> - 然后使用`tf.io.parse_single_example()`解码单个样本，结果返回一个字典，字典的每个元素对应一个特征，其中**由于data是string类型，由于是编码后的非标量数据，所以还需要使用`tf.io.parse_tensor`解码一次（非标量数据的shape的编码解码的过程中是会丢失的，所以需要把shape一起储存）**\n> - 如果储存的是列表，需要在`tf.io.FixedLenFeature()`指定shape，也就是(len(list), )\n\n\n\n\n\n# keras_bert用法\n\n- **加载预训练模型：keras_bert.load_trained_model_from_checkpoint(config_file, checkpoint_file, training=False, trainable=None, output_layer_num=1, seq_len=int(1e9), kwargs)**\n\n> config_file和checkpoint_file分别为bert_config.json文件和bert_model.ckpt文件\n>\n> training：如果为True，则将返回带有 MLM 和 NSP输出的模型；否则，将返回输入层和最后一个特征提取层。\n>\n> trainable：模型是否可训练（也可通过for循环分别制定每层是否可训练）\n>\n> seq_len为最大序列长度\n\n- **编码输入：keras_bert.Tokenizer(token_dict, ...)**\n\n> bert模型的输入有两个，一个是语义token，一个是序列token，需要创建Tokenizer实例，再对句子进行编码，产生两个token\n>\n> token_dict：是输入的映射字典，在vocab.txt中可以获取所有token，但是需要自行建立token -> id的字典\n>\n> 创建Tokenizer实例后，调用`Tokenizer.encode(first, second=None, max_len=None)`进行编码，结果返回一个list，元素分别为两个list，分别对应两个token\n\n- **将数据输入模型：**\n\n> 在`model.fit()`中，由于是多输入，采用`x=[train_token_embeddings, train_seq_embeddings]`和`validation_data=([test_token_embeddings, test_seq_embeddings], test_labels)`这种形式进行输入\n\n- **加载模型时所需要注意的：**\n\n> 加载保存好的模型时同样是使用`tf.keras.models.load_model()`，但是会出现如下报错：\n>\n> **ValueError: Unknown layer: TokenEmbedding**\n>\n> 这种情况需要：先`from keras_bert import get_custom_objects`，再在`load_model()`时添加参数`custom_objects=get_custom_objects()`\n\n\n\n\n\n# bert4keras用法\n\n- **由于keras和tf.keras的兼容问题，所以最开始需要添加环境变量TF_KERAS=1：**\n\n```python\nimport os\nos.environ[\"TF_KERAS\"] = \"1\"\n```\n\n- 直接看[民间的API文档](https://github.com/Sniper970119/bert4keras_document)，写的还可以\n\n\n\n\n\n# Pytorch\n\n### 基本操作\n\n- **读取数据**\n\n> - 自建一个类，继承`torch.utils.data.Dataset`，还要实现`__init__()`、`__getitem__(index)`、`__len__()`三个函数\n> - 实例化Dataset，并用`torch.utils.data.DataLoader`封装：`DataLoader(data, batch_size=1)`\n> - 直接for循环取每个batch：`for input_ids, attention_mask, labels in data: `\n\n- **搭建模型：**\n\n> 1. 继承`torch.nn.Module `，并实现`__init__()`、`forward()`\n\n- **损失函数：**\n\n> - 以交叉熵举例\n> - 创建：`loss = torch.nn.CrossEntropyLoss(reduction=\"none\").cuda() `\n> - 其中`reduction=Union[“mean”, “sum”, “none”]`，表示对每个sample的loss怎么处理\n> - **踩个坑：如果loss的输入（模型的输出置信度）超过两维，如token级别的分类，`shape=[batch_size, max_seq_len, num_class]`，需要把num_class移到第二维，即执行`output.transpose(1, 2) `，现在`shape=[batch_size, num_class, max_seq_len]`**\n>\n> ![IMG_27](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_27.png)\n\n- **优化器：**`torch.optim.Adam(model.parameters(), lr=0.01`\n- **反向传播：**\n\n> ```python\n> model.zero_grad()\n> loss.backward()\n> optimizer.step()\n> ```\n\n- **模型保存和加载：**\n\n```python\n# 可以选择保存整个模型，也可以选择只保存权重\n# 保存整个模型：\ntorch.save(model, file_path)   # file_path以.pt .pth结尾\n# 加载整个模型\nmodel = torch.load(file_path)\n\n# 保存权重\ntorch.save(model.state_dict(), file_path)\n# 加载权重\nmodel = MyModel()\nmodel.load_state_dict(torch.load(file_path))\n```\n\n- **模型训练、测试模式转换：**`model.train()`、`model.eval()`\n\n\n\n### 多卡并行\n\n- **运行命令：**`python3 -m torch.distributed.launch --nproc_per_node=2 {file_name}`\n\n> 其中nproc_per_node代表GPU数量\n\n- **命令行参数**\n\n> 1. 进程编号：`parser.add_argument(\"--local_rank\", type=int, default=-1) `，local_rank为0代表主进程\n> 2. GPU数量：`gpu_num = int(os.environ['WORLD_SIZE']) `\n> 3. 需要手动设置环境变量：`os.environ[\"CUDA_VISIBLE_DEVICES\"] = “0, ..., gpu_num-1”`\n\n- **初始化：**\n\n> ```python\n> torch.cuda.set_device(local_rank)\n> if gpu_num > 1 :\n>  \ttorch.distributed.init_process_group(backend=\"nccl”)\n> ```\n\n- **读取数据：**\n\n> ```python \n> data = Dataset()\n> data_sampler = torch.utils.data.distributed.DistributedSampler(data)\n> data = DataLoader(data, batch_size=1, sampler=data_sampler)   # 需要在DataLoader封装时指定sampler\n> ```\n\n- **模型：**\n\n> - 模型需要用`torch.nn.parallel.DistributedDataParallel `封装：\n> ```python\n> model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=False) `\n> ```\n> - 其中find_unused_parameters默认为False，为False效率更高，如果报错则改为True\n\n- **打乱数据：**\n\n> - 在每个epoch开始的时候需要`data.sampler.set_epoch(epoch) `\n> - 是为了让sample打乱数据，使每个epoch的训练数据顺序不同\n\n- **同步每个进程的数据：**\n\n> - 每个进程中的数据是不同步的，如train_loss、val_loss等\n> - 通过`torch.distributed.all_reduce(value)`来对value进行同步\n> - 同步之后是所有进程该值的和，所以一般需要除以gpu_num\n> - value必须是Tensor，不能是标量\n\n- **模型保存和加载：**\n\n> - 如果model用DDP封装，保存时保存`model.module`或者`model.module.state_dict()`\n> - **一般只在主进程进行模型保存、打印、tensorboard记录**\n\n\n\n### Huggingface Transformers\n\n- **Tokenizer加载：**`AutoTokenizer.from_pretrained(checkpoint)`\n- **Tokenizer编码：**\n\n> - `tokenizer.encode(text, return_tensors=\"pt\") `：只返回词典映射后的编码结果，`text=str`\n> - `tokenizer(texts, padding=\"max_length\", truncation=True,max_length=150, return_tensors=\"pt”)`：返回一个字典，包括模型所需要的所有输入，一般为input_ids和attention_mask。`texts=Union[str, list(str)]`，`padding=Union[“max_length”, True, False]`\n> - **tokenizer()的结果如果需要组成batch，字典中的每个Tensor需要将第一维去掉：**\n>\n> ```python\n> for key, val in inputs.items():\n>     inputs[key] = val.squeeze(0)\n> ```\n\n- **Tokenizer解码：**`tokenizer.decode(ids, skip_special_tokens=True) `\n\n> 其中ids为input_ids\n\n- **指定Tokenizer填充和截断方向：**\n> - `tokenizer.padding_side=Union[“left”, “right”]`\n> -  `tokenizer.truncation_side=Union[“left”, “right”] `\n- **模型：**\n\n> - 加载：`AutoModel.from_pretrained(checkpoint) `\n> - 前向计算：\n>\n> ```python\n> inputs = tokenizer().    # inputs是一个dict\n> output = model(**inputs)。   # 输出的output是一个dict\n> ```\n\n- **生成式模型进行生成预测：**`model.generate(ids, max_length=None, num_beams=1,do_sample=False, top_k=1, early_stopping=False, num_return_sequences=1)`\n\n> - 一些生成模型如BloomForCausalModel自带生成函数\n> - 其中ids为encode之后的结果，每次generate的ids只对应一个句子，无法并行\n> - 需要do_sanple=True，才能启用top_k、top_p\n\n\n\n### Tensorboard\n\n- 直接给个🌰：\n\n> ```python\n> # 初始化\n> time = “{0:%Y-%m-%d-%H:%M:%S/}\".format(datetime.datetime.now())\n> tb_path = config[\"tb_root_path\"] + time\n> print(f\"Start Tensorboard with 'tensorboard --logdir={tb_path} --bind_all', view at http://localhost:6006/\")\n> writer = SummaryWriter(tb_path)\n> # 保存标量\n> writer.add_scalar(\"train/loss\", loss, step)\n> ```\n>\n> - 运行完之后直接运行`tensorboard --logdir={tb_path} --bind_all`，然后访问本地6006端口即可\n> - 在多卡时，一般只在主进程采用tensorboard\n\n","slug":"代码总结","published":1,"updated":"2023-03-29T16:17:03.144Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1j000g7csz1mspbsh0","content":"<h1 id=\"NLTK用法\"><a href=\"#NLTK用法\" class=\"headerlink\" title=\"NLTK用法\"></a>NLTK用法</h1><ul>\n<li>nltk用于<strong>英文</strong>分词分句等应用</li>\n</ul>\n<h3 id=\"基本的预处理\"><a href=\"#基本的预处理\" class=\"headerlink\" title=\"基本的预处理\"></a>基本的预处理</h3><ul>\n<li><strong>分句：nltk.sent_tokenize(text, language=”english”):</strong></li>\n</ul>\n<blockquote>\n<p>输入：一个str段落</p>\n<p>输出：一个list，每个元素是一个str句子</p>\n</blockquote>\n<ul>\n<li><strong>分词：nltk.word_tokenize(text, language=”english”, preserve_line=False):</strong></li>\n</ul>\n<blockquote>\n<p>输入：一个str句子</p>\n<p>输出：一个list，每个元素是一个str词</p>\n</blockquote>\n<ul>\n<li><strong>词性标注（POS_tag）：nltk.postag(tokens)</strong></li>\n</ul>\n<blockquote>\n<p>输入：一个list，每个元素是一个str，一个句子分好词的结果</p>\n<p>输入：一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[(‘football’, ‘NN’), (‘is’, ‘VBZ’), (‘a’, ‘DT’), (‘family’, ‘NN’)]</p>\n<p>词性所对应的意义，大致来说<strong>N开头对应名词NOUN，V开头对应动词VERB，J开头对应形容词ADJ，R开头对应副词ADV</strong></p>\n<p>词性具体所对应意义如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>标记</th>\n<th>含义</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CC</td>\n<td>连词</td>\n<td>and, or,but, if, while,although</td>\n</tr>\n<tr>\n<td>CD</td>\n<td>数词</td>\n<td>twenty-four, fourth, 1991,14:24</td>\n</tr>\n<tr>\n<td>DT</td>\n<td>限定词</td>\n<td>the, a, some, most,every, no</td>\n</tr>\n<tr>\n<td>EX</td>\n<td>存在量词</td>\n<td>there, there’s</td>\n</tr>\n<tr>\n<td>FW</td>\n<td>外来词</td>\n<td>dolce, ersatz, esprit, quo,maitre</td>\n</tr>\n<tr>\n<td>IN</td>\n<td>介词连词</td>\n<td>on, of,at, with,by,into, under</td>\n</tr>\n<tr>\n<td>JJ</td>\n<td>形容词</td>\n<td>new,good, high, special, big, local</td>\n</tr>\n<tr>\n<td>JJR</td>\n<td>比较级词语</td>\n<td>bleaker braver breezier briefer brighter brisker</td>\n</tr>\n<tr>\n<td>JJS</td>\n<td>最高级词语</td>\n<td>calmest cheapest choicest classiest cleanest clearest</td>\n</tr>\n<tr>\n<td>LS</td>\n<td>标记</td>\n<td>A A. B B. C C. D E F First G H I J K</td>\n</tr>\n<tr>\n<td>MD</td>\n<td>情态动词</td>\n<td>can cannot could couldn’t</td>\n</tr>\n<tr>\n<td>NN</td>\n<td>名词</td>\n<td>year,home, costs, time, education</td>\n</tr>\n<tr>\n<td>NNS</td>\n<td>名词复数</td>\n<td>undergraduates scotches</td>\n</tr>\n<tr>\n<td>NNP</td>\n<td>专有名词</td>\n<td>Alison,Africa,April,Washington</td>\n</tr>\n<tr>\n<td>NNPS</td>\n<td>专有名词复数</td>\n<td>Americans Americas Amharas Amityvilles</td>\n</tr>\n<tr>\n<td>PDT</td>\n<td>前限定词</td>\n<td>all both half many</td>\n</tr>\n<tr>\n<td>POS</td>\n<td>所有格标记 ’</td>\n<td>‘s</td>\n</tr>\n<tr>\n<td>PRP</td>\n<td>人称代词</td>\n<td>hers herself him himself hisself</td>\n</tr>\n<tr>\n<td>PRP$</td>\n<td>所有格</td>\n<td>her his mine my our ours</td>\n</tr>\n<tr>\n<td>RB</td>\n<td>副词</td>\n<td>occasionally unabatingly maddeningly</td>\n</tr>\n<tr>\n<td>RBR</td>\n<td>副词比较级</td>\n<td>further gloomier grander</td>\n</tr>\n<tr>\n<td>RBS</td>\n<td>副词最高级</td>\n<td>best biggest bluntest earliest</td>\n</tr>\n<tr>\n<td>RP</td>\n<td>虚词</td>\n<td>aboard about across along apart</td>\n</tr>\n<tr>\n<td>SYM</td>\n<td>符号</td>\n<td>% &amp; ’ ‘’ ‘’. ) )</td>\n</tr>\n<tr>\n<td>TO</td>\n<td>词to</td>\n<td>to</td>\n</tr>\n<tr>\n<td>UH</td>\n<td>感叹词</td>\n<td>Goodbye Goody Gosh Wow</td>\n</tr>\n<tr>\n<td>VB</td>\n<td>动词</td>\n<td>ask assemble assess</td>\n</tr>\n<tr>\n<td>VBD</td>\n<td>动词过去式</td>\n<td>dipped pleaded swiped</td>\n</tr>\n<tr>\n<td>VBG</td>\n<td>动词现在分词</td>\n<td>telegraphing stirring focusing</td>\n</tr>\n<tr>\n<td>VBN</td>\n<td>动词过去分词</td>\n<td>multihulled dilapidated aerosolized</td>\n</tr>\n<tr>\n<td>VBP</td>\n<td>动词现在式非第三人称时态</td>\n<td>predominate wrap resort sue</td>\n</tr>\n<tr>\n<td>VBZ</td>\n<td>动词现在式第三人称时态</td>\n<td>bases reconstructs marks</td>\n</tr>\n<tr>\n<td>WDT</td>\n<td>Wh限定词</td>\n<td>who,which,when,what,where,how</td>\n</tr>\n<tr>\n<td>WP</td>\n<td>WH代词</td>\n<td>that what whatever</td>\n</tr>\n<tr>\n<td>WP$</td>\n<td>WH代词所有格</td>\n<td>whose</td>\n</tr>\n<tr>\n<td>WRB</td>\n<td>WH副词</td>\n</tr>\n</tbody>\n</table>\n</div>\n</blockquote>\n<ul>\n<li><strong>词形还原Lemmatization：nltk.stem.WordNetLemmatizer().lemmatize(word, pos=”n”)：</strong></li>\n</ul>\n<blockquote>\n<p>这是一个类的内置方法，首先需要创建WordNetLemmatizer对象（假设为wnl），则通过<code>wnl.lemmatize()</code>调用</p>\n<p>输入：word是单个词，pos是其对应的单词词性（n对应noun，v对应verb，a对应adj，r对应adv，s对应satellite adjectives(不咋用)）</p>\n<p>输出：word经过还原后的词，如cars还原为car</p>\n</blockquote>\n<h3 id=\"NER\"><a href=\"#NER\" class=\"headerlink\" title=\"NER\"></a>NER</h3><ul>\n<li>先介绍一下命名实体识别（Named Entity Recognition，NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。</li>\n<li>nltk中对NER类别的分类如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20191219223810610.png\" alt=\"img\"></p>\n<p>其中LOCATION和GPE有重合，GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。</p>\n<ul>\n<li><strong>进行NER：ne_chunk(tagged_tokens, binary=False)</strong></li>\n</ul>\n<blockquote>\n<p>输入：tagged_tokens为<code>pos_tag()</code>函数的输出，一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[(‘football’, ‘NN’), (‘is’, ‘VBZ’), (‘a’, ‘DT’), (‘family’, ‘NN’)]</p>\n<p>输出：同样的是一个list，每个元素变为了一个封装对象（Tree类），和输入一一对应</p>\n<ul>\n<li><strong>对于该封装对象：</strong></li>\n</ul>\n<p>输出的list中，有些元素是没有NER的结果的，其对应的封装对象没有label属性，<strong>可使用<code>hasattr(ne_word, &#39;label&#39;)</code>函数判断是否有NER结果</strong></p>\n<p>假设一个该对象命名为ne_word，调用<code>ne_word.leaves()</code>可返回一个list，每个元素为一个tuple，tuple[0]为token，tuple[1]为该token的pos_tag。对于NER，list中只有一个tuple，返回结果示例：[(‘FIFA’, ‘NNP’)]</p>\n<p>调用<code>ne_word.label()</code>函数可返回该token对应的NER结果，一定要先使用<code>hasattr()</code>函数才能使用<code>label()</code>函数</p>\n</blockquote>\n<h3 id=\"计算BLEU\"><a href=\"#计算BLEU\" class=\"headerlink\" title=\"计算BLEU\"></a>计算BLEU</h3><ul>\n<li><p><a href=\"https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#5-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91\">BLEU的定义</a></p>\n</li>\n<li><p><strong>nltk.translate.bleu_score.sentence_bleu(references, hypothesis, …, smoothing_function=None, …)</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>references：参照序列，label。<strong>type=list(list(str))，其中的每个str为词或字</strong></p>\n<p>hypothesis：候选序列，也就是预测出的序列。<strong>type=list(str)，其中的每个str为词或字</strong></p>\n<p>smoothing_function，是论文中使用到的平滑技巧，一般输入<code>nltk.translate.bleu_score.SmoothingFunction().methodi()</code>，最后的i为0~7</p>\n</blockquote>\n<ul>\n<li>另外还可以使用<code>corpus_bleu()</code>计算多个句子的BLEU；用<code>modified_precision()</code>计算修正的n-gram精确度</li>\n</ul>\n<h1 id=\"Pyltp用法\"><a href=\"#Pyltp用法\" class=\"headerlink\" title=\"Pyltp用法\"></a>Pyltp用法</h1><ul>\n<li>Pyltp主要用于中文的预处理等，包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注</li>\n<li>使用Pyltp时注意其编码使用的都是utf-8，而Windows终端使用GBK编码，如果出现乱码可以将输出重定向到文件，然后用utf-8编码查看</li>\n<li><strong>分句：pyltp.SentenceSplitter()</strong></li>\n</ul>\n<blockquote>\n<p>使用时先创建实例：<code>sp = SentenceSplitter()</code>，再进行分句：<code>sents = sp.split(doc)</code></p>\n</blockquote>\n<ul>\n<li><strong>分词：pyltp.Segmentor()</strong></li>\n</ul>\n<blockquote>\n<p>Segmentor加载模型可以用<code>load()</code>也可以用<code>load_with_lexicon()</code>，后者还要加一个用户词典的参数</p>\n</blockquote>\n<ul>\n<li><strong>词性标注：pyltp.Postagger()</strong></li>\n</ul>\n<blockquote>\n<p>直接举个栗子吧：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">sent = <span class=\"hljs-string\">&quot;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&quot;</span><br><span class=\"hljs-comment\"># 加载模型</span><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br>postagger = Postagger()<br>postagger.load(pos_model_path)<br><span class=\"hljs-comment\"># 分词和词性标注</span><br>words = segmentor.segment(sent)<br>postags = postagger.postag(words)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">list</span>(postags))<br><span class=\"hljs-comment\"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br></code></pre></td></tr></table></figure>\n<p>注意：进行这些处理后返回的都是VectorOfString类，是一个可迭代类，也可用list()转换为列表</p>\n<p>词性标注的词性如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724143414199.png\" alt=\"image-20220724143414199\" style=\"zoom: 50%;\" /></p>\n<p>词性标注也可以添加用户词典</p>\n</blockquote>\n<ul>\n<li><strong>NER识别：pyltp.NamedEntityRecognizer()</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> pyltp <span class=\"hljs-keyword\">import</span> NamedEntityRecognizer<br>recognizer = NamedEntityRecognizer()<br>recognizer.load(ner_model_path)<br>ner_results = recognizer.recognize(words, postags)<br></code></pre></td></tr></table></figure>\n<p>LTP 采用 BIESO 标注体系。<strong>B 表示实体开始词，I表示实体中间词，E表示实体结束词，S表示单独成实体，O表示不构成命名实体。</strong></p>\n<p>LTP 提供的命名实体类型为：<strong>人名（Nh）、地名（Ns）、机构名（Ni）</strong>。</p>\n<p>B、I、E、S位置标签和实体类型标签之间用一个横线 <code>-</code> 相连；O标签后没有类型标签。</p>\n</blockquote>\n<ul>\n<li><strong>依存句法分析：pyltp.Parser()</strong></li>\n</ul>\n<blockquote>\n<p>依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系</p>\n<p>首先也是创建实例和加载模型，然后<code>parse_results = parser.parse(words, postags)</code>，得到的结果是一个可迭代对象，其中的每一个元素也是个自定义类（假设parse_results[i]为result），调用<code>result.head</code>和<code>result.relation</code>可分别获得words[i]对应的关系词和关系，其中head是返回关系词所对应的索引+1，如果为0则为”Root”，表示无对应关系词</p>\n<p>举个栗子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">relations = [result.relation <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> parse_results]<br>heads = [words[result.head - <span class=\"hljs-number\">1</span>] <span class=\"hljs-keyword\">if</span> result.head <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">&quot;Root&quot;</span> <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> parse_results]<br><span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(words)):<br> <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;<span class=\"hljs-subst\">&#123;relations[i]&#125;</span> : (<span class=\"hljs-subst\">&#123;words[i]&#125;</span>, <span class=\"hljs-subst\">&#123;heads[i]&#125;</span>)&quot;</span>)<br></code></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<p><div align=center><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211345194.png\" alt=\"image-20220724211345194\" style=\"zoom: 80%;\" /></p>\n<p>依存句法关系如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724153110907.png\" alt=\"image-20220724153110907\" style=\"zoom:50%;\" /></p>\n</blockquote>\n<ul>\n<li><strong>语义角色标注：pyltp.SementicRoleLabeller()</strong></li>\n</ul>\n<blockquote>\n<p>语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等<br>进行语义角色标注时首先同样是创建实例，再加载模型，然后调用方法：<code>roles = labeller.label(words, postags, parse_results)</code>。结果得到一个可迭代对象，<strong>其中的每一个元素也是一个自定义类（假设每个元素为role），则<code>role.index</code>为谓语对应的索引，<code>role.arguments</code>又是一个可迭代对象，每个元素对应一个和该谓语对应的语义角色，也是一个自定义类（假设为argument），</strong>那么可以通过<code>argument.name</code>获取角色和谓语的关系，<code>argument.range.start</code>和<code>argument.range.end</code>对应该角色的开始和结束索引（<strong>end要算上的</strong>）</p>\n<p>给个示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">roles = labeller.label(words, postags, parse_results)<br><span class=\"hljs-keyword\">for</span> role <span class=\"hljs-keyword\">in</span> roles:<br> arguments = role.arguments<br> index = role.index<br> <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;谓语: <span class=\"hljs-subst\">&#123;words[index]&#125;</span> (索引: <span class=\"hljs-subst\">&#123;index&#125;</span>)&quot;</span>)<br> <span class=\"hljs-keyword\">for</span> argument <span class=\"hljs-keyword\">in</span> arguments:<br>     start, end = argument.<span class=\"hljs-built_in\">range</span>.start, argument.<span class=\"hljs-built_in\">range</span>.end<br>     obj = <span class=\"hljs-string\">&quot;&quot;</span><br>     <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words[start : end+<span class=\"hljs-number\">1</span>]:<br>         obj += word<br>     <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;<span class=\"hljs-subst\">&#123;argument.name&#125;</span>: <span class=\"hljs-subst\">&#123;obj&#125;</span> (索引: <span class=\"hljs-subst\">&#123;start&#125;</span>:<span class=\"hljs-subst\">&#123;end&#125;</span>)&quot;</span>)<br> <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;\\n&quot;</span>)<br></code></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211302209.png\" alt=\"image-20220724211302209\" style=\"zoom:67%;\" /></p>\n<p>语义角色如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211544134.png\" alt=\"image-20220724211544134\" style=\"zoom:60%;\" /></p>\n</blockquote>\n<h1 id=\"rouge用法\"><a href=\"#rouge用法\" class=\"headerlink\" title=\"rouge用法\"></a>rouge用法</h1><ul>\n<li>rouge和bleu一样也是一种对序列质量的评估标准，但是相比于bleu更关注召回率而非精准率（通过计算F1_score时改变<script type=\"math/tex\">\\beta</script>参数），rouge-n计算公式如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nRough-N=\\frac{\\sum_{S \\in\\{\\text { ReferemceSummaries }\\}} \\sum_{\\text {gram }_{n} \\in S} \\text { Count }_{\\text {match }}\\left(\\text { gram }_{n}\\right)}{\\sum_{S \\in\\{\\text { ReferenceSummaries }\\}} \\text { gram }_{n} \\in S}</script><ul>\n<li>rouge-1和rouge-2都可通过上述公式计算出来，和bleu中的P1和P2计算公式很像。可以在下面看到，每个rough-N都会输出p和r，其实就是精准率和召回率，两者也就是分母不一样（一个分母是refs的n-gram个数，一个分母是hyps的n-gram个数）</li>\n<li>而rouge-l是rouge-N的改进：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nR_{l c s}=\\frac{L C S(X, Y)}{m} \\\\\nP_{l c s}=\\frac{L C S(X, Y)}{n} \\\\\nF_{l c s}=\\frac{\\left(1+\\beta^{2}\\right) R_{l c s} P_{l c s}}{R_{l c s}+\\beta^{2} P_{l c s}}\n\\end{array}</script><p>其中X、Y为hyps和refs，m、n为他们的长度，LCS(X, Y)为X、Y的最长共同序列</p>\n<ul>\n<li><strong>rouge.Rouge.get_scores(hyps, refs, avg=False, ignore_empty=False)</strong></li>\n</ul>\n<blockquote>\n<p><strong>注意：输入的hyps和refs两个字符串，不管是中文英文，都需要每个字或词使用空格间隔</strong></p>\n<p>输出是list(dict(dict))</p>\n</blockquote>\n<ul>\n<li>直接给栗子吧：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> rouge <span class=\"hljs-keyword\">import</span> Rouge <br><br>hypothesis = <span class=\"hljs-string\">&quot;the #### transcript is a written version of each day &#x27;s cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news&quot;</span><br><br>reference = <span class=\"hljs-string\">&quot;this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students &#x27; knowledge of even ts in the news&quot;</span><br><br>rouge = Rouge()<br>scores = rouge.get_scores(hypothesis, reference)<br></code></pre></td></tr></table></figure>\n<p><strong>output：</strong></p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs json\">[<br>  &#123;<br>    <span class=\"hljs-attr\">&quot;rouge-1&quot;</span>: &#123;<br>      <span class=\"hljs-attr\">&quot;f&quot;</span>: <span class=\"hljs-number\">0.4786324739396596</span>,<br>      <span class=\"hljs-attr\">&quot;p&quot;</span>: <span class=\"hljs-number\">0.6363636363636364</span>,<br>      <span class=\"hljs-attr\">&quot;r&quot;</span>: <span class=\"hljs-number\">0.3835616438356164</span><br>    &#125;,<br>    <span class=\"hljs-attr\">&quot;rouge-2&quot;</span>: &#123;<br>      <span class=\"hljs-attr\">&quot;f&quot;</span>: <span class=\"hljs-number\">0.2608695605353498</span>,<br>      <span class=\"hljs-attr\">&quot;p&quot;</span>: <span class=\"hljs-number\">0.3488372093023256</span>,<br>      <span class=\"hljs-attr\">&quot;r&quot;</span>: <span class=\"hljs-number\">0.20833333333333334</span><br>    &#125;,<br>    <span class=\"hljs-attr\">&quot;rouge-l&quot;</span>: &#123;<br>      <span class=\"hljs-attr\">&quot;f&quot;</span>: <span class=\"hljs-number\">0.44705881864636676</span>,<br>      <span class=\"hljs-attr\">&quot;p&quot;</span>: <span class=\"hljs-number\">0.5277777777777778</span>,<br>      <span class=\"hljs-attr\">&quot;r&quot;</span>: <span class=\"hljs-number\">0.3877551020408163</span><br>    &#125;<br>  &#125;<br>]<br></code></pre></td></tr></table></figure>\n<p>其中p、r、f分别为精准率、召回率、F1_score</p>\n<h1 id=\"zhconv用法\"><a href=\"#zhconv用法\" class=\"headerlink\" title=\"zhconv用法\"></a>zhconv用法</h1><ul>\n<li><p>该库用于中文简繁体转换，但是也可以用<strong>OpenCC库，精准度更高、覆盖率更高、速度更快</strong></p>\n</li>\n<li><p><strong>逐字转换：zhconv.convert(s, locale, update=None)</strong></p>\n</li>\n<li><p><strong>基于MediaWiki的转换：zhconv.ocnvert_for_mw(s, locale, update=None)</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>两个函数用法相同</p>\n<p>示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">sent2 = <span class=\"hljs-string\">&quot;計算機軟體&quot;</span><br><span class=\"hljs-built_in\">print</span>(convert(sent2, <span class=\"hljs-string\">&quot;zh-hans&quot;</span>))<br></code></pre></td></tr></table></figure>\n<p>locale可为以下值：</p>\n<p><code>zh-cn</code> 大陆简体、<code>zh-tw</code> 台灣正體、<code>zh-hk</code> 香港繁體、<code>zh-sg</code> 马新简体、<code>zh-hans</code> 简体、<code>zh-hant</code> 繁體</p>\n</blockquote>\n<h1 id=\"gensim用法\"><a href=\"#gensim用法\" class=\"headerlink\" title=\"gensim用法\"></a>gensim用法</h1><ul>\n<li>gensim主要用于创建语料库，和计算各种特征（如相似度、tf-idf）等任务</li>\n</ul>\n<h3 id=\"创建语料库和计算相似度\"><a href=\"#创建语料库和计算相似度\" class=\"headerlink\" title=\"创建语料库和计算相似度\"></a>创建语料库和计算相似度</h3><ul>\n<li><strong>创建语料库类：gensim.corpora.Dictionary(texts)</strong></li>\n</ul>\n<blockquote>\n<p>输入：整个语料库，分好句和分好词的结果，一个list，每个元素又是一个list，里面的每个子元素是一个str词</p>\n<p>输出：一个Dictionary对象</p>\n</blockquote>\n<ul>\n<li><strong>将句子转化为词袋形式：Dictionary.doc2bow(text)</strong></li>\n</ul>\n<blockquote>\n<p>输入：分好词的一个句子，一个list，每个元素是一个str词</p>\n<p>输入：一个list，每个元素是一个二元tuple，tuple[0]是句子中存在的词的索引，tuple[1]是其在句子中出现的次数（出现0次的不计入）</p>\n<p>该类的用法和python自带的字典对象基本相同，values、key、item之类的</p>\n</blockquote>\n<ul>\n<li><strong>获取索引字典：Dictionary.token2id</strong></li>\n</ul>\n<blockquote>\n<p>返回一个{token : id}的字典</p>\n</blockquote>\n<ul>\n<li><strong>创建相似度类：gensim.similarities.Similarity(output_prefix, corpus, num_features)</strong></li>\n</ul>\n<blockquote>\n<p>out_prefix是存储这个对象文件的名称</p>\n<p>corpus是整个语料库，但是必须先转化为词袋模型</p>\n<p>num_fuatures是整个词典的数量，一般使用len(dictionary)表示</p>\n<p>对于得到的对象（取名为similarity），具体的用法为：similarity[test]，其中test为要比较的对象，可以是单个句子，也可以是整个语料库</p>\n</blockquote>\n<p><strong>举个栗子：</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> nltk <span class=\"hljs-keyword\">import</span> word_tokenize<br><span class=\"hljs-keyword\">from</span> gensim.corpora <span class=\"hljs-keyword\">import</span> Dictionary<br><span class=\"hljs-keyword\">from</span> gensim.similarities <span class=\"hljs-keyword\">import</span> Similarity<br><br>sent1 = <span class=\"hljs-string\">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class=\"hljs-string\">&quot;I like running, I love reading.&quot;</span><br>sents = [sent1, sent2]<br><span class=\"hljs-comment\"># 分词</span><br>texts = [word_tokenize(sent) <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> sents]<br><br><span class=\"hljs-comment\"># 创建字典对象</span><br>dictionary = Dictionary(texts)<br><span class=\"hljs-comment\"># 获得词袋模型表示的词料库</span><br>corpus = [dictionary.doc2bow(text) <span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> texts]<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;The corpus is : <span class=\"hljs-subst\">&#123;corpus&#125;</span>&quot;</span>)<br><span class=\"hljs-comment\"># 创建相似度对象</span><br>similarity = Similarity(<span class=\"hljs-string\">&quot;Similarity-excise1&quot;</span>, corpus, num_features=<span class=\"hljs-built_in\">len</span>(dictionary))<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;Created class : <span class=\"hljs-subst\">&#123;similarity&#125;</span>&quot;</span>)<br><span class=\"hljs-comment\"># 计算余弦相似度</span><br>test_corpus = dictionary.doc2bow(word_tokenize(sent1))<br><span class=\"hljs-built_in\">print</span>(similarity[test_corpus])<br></code></pre></td></tr></table></figure>\n<p>输出：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706160148320.png\" alt=\"image-20220706160148320\"></p>\n<h3 id=\"计算TF-IDF\"><a href=\"#计算TF-IDF\" class=\"headerlink\" title=\"计算TF-IDF\"></a>计算TF-IDF</h3><ul>\n<li><strong>首先介绍一下tf-idf：</strong></li>\n</ul>\n<p>词频（term frequency，tf）指的是某一个给定的词语在该文件中对应的频率</p>\n<p>逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量，可由文件总数除以包含该词的文件的总数求得</p>\n<p>具体的计算公式如下：</p>\n<script type=\"math/tex; mode=display\">\n\\mathrm{tf}_{\\mathrm{i}, \\mathrm{j}}=\\frac{n_{i, j}}{\\sum_{k} n_{k, j}} \\\\\n\\operatorname{idf}_{\\mathrm{i}}=\\lg \\frac{|D|}{\\left|\\left\\{j: t_{i} \\in d_{j}\\right\\}\\right|}\\\\\n\\operatorname{tfidf}_{i, j}=\\mathrm{tf}_{\\mathrm{i}, \\mathrm{j}} \\times \\mathrm{idf}_{\\mathrm{i}}</script><p>其中<script type=\"math/tex\">tf_{i, j}</script>为<script type=\"math/tex\">word_i</script>在<script type=\"math/tex\">doc_j</script>中出现的词频，<script type=\"math/tex\">n_{i, j}</script>为<script type=\"math/tex\">word_i</script>在<script type=\"math/tex\">doc_j</script>中出现的个数</p>\n<p><script type=\"math/tex\">idf_i</script>为<script type=\"math/tex\">word_i</script>的逆向文件频率，<script type=\"math/tex\">|D|</script>为文档总数，<script type=\"math/tex\">\\left|\\left\\{j: t_{i} \\in d_{j}\\right\\}\\right|</script>为包含<script type=\"math/tex\">word_i</script>的文档个数</p>\n<p>对于idf，底数可用2、e、10，并且分母可以+1，避免除以0</p>\n<ul>\n<li><strong>创建tfidf模型类：gensim.models.TfidfModel(corpus)</strong></li>\n</ul>\n<blockquote>\n<p>输入为整个语料库，必须先转化为词袋模型</p>\n<p>在具体使用时，和Similarity类似，tfidf_model[test]，其中test可以是单个句子也可以是整个语料库，返回一个列表，列表中每个元素为一个tuple，tuple[0]为词索引，tuple[1]为对应的tfidf值</p>\n<p><strong>gensim算出来的tf-idf是经过了规范化的，每个句子的tfidf值，是一个单位向量</strong></p>\n</blockquote>\n<p><strong>举个栗子：</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># texts的构建是每个段落分词，有三个段落，则len(texts)=3</span><br>texts = [text1, text2, text3]<br>texts = [get_tokens(text) <span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> texts]<br>dictionary = Dictionary(texts)<br>id2token_dict = &#123;v : k <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> dictionary.token2id.items()&#125;<br>corpus = [dictionary.doc2bow(text) <span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> texts]<br>tfidf_model = TfidfModel(corpus)<br>result = tfidf_model[corpus]<br><span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> result:<br>    <span class=\"hljs-built_in\">print</span>(i)<br></code></pre></td></tr></table></figure>\n<p>输出：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706210417422.png\" alt=\"image-20220706210417422\"></p>\n<h1 id=\"Pandas用法\"><a href=\"#Pandas用法\" class=\"headerlink\" title=\"Pandas用法\"></a>Pandas用法</h1><ul>\n<li><strong>创建DataFrame对象：pd.DataFrame(data, columns, index)</strong></li>\n</ul>\n<blockquote>\n<p>输入：data是一个list, 每个对象是一个tuple或list，tuple的长度和columns的个数对应。data也可以是numpy形式</p>\n<p>index是一个list，是行索引值，可以用于自行设置行索引<strong>（索引没有规定一定是int）</strong></p>\n<p>示例：pd.DataFrame([(1, 2), (3, 4), (5, 6)], columns=[“name”, “NER result”])，输出为：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220711120934316.png\" alt=\"image-20220711120934316\" style=\"zoom: 80%;\" /></p>\n</blockquote>\n<ul>\n<li><strong>获取DataFrame特征：DataFrame.describe()</strong></li>\n<li><strong>获取DataFrame的某列：DataFrame[“column_name”]</strong></li>\n</ul>\n<blockquote>\n<p> 这样可获取到column列，结果是一个Series类（<strong>只有一个列则是Series，多个列则是DataFrame</strong>）</p>\n<p> 如果不想出现重复结果，可调用<code>unique()</code>函数，示例：data[“ner_result”].unique()，返回的结果是一个ndarray，也可使用<code>list()</code>将其转换为列表</p>\n<p> 如果想把这个Series类转换为ndarray，则可使用Series.values，这是Series类其中的一个属性，不是方法，类型是一个ndarrary，也可使用<code>list()</code>转换为列表</p>\n</blockquote>\n<ul>\n<li><p><strong>获取DataFrame列名：DataFrame.columns</strong></p>\n</li>\n<li><p><strong>DataFrame根据某列，进行分组：DataFrame.groupby(“column_name”)</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>将DataFrame通过column_name列进行分组</p>\n<p>结果是一个可迭代的DataFrameGroupBy类，该类的用法和DataFrame几乎一样，比如可使用<code>DataFrameGroupBy[&quot;column_name&quot;]</code>将其转换为SeriesGroupby类</p>\n<p>也可转化为list，转换后，其中每一个元素为一个tuple，tuple[0]为column_name列对应的值，tuple[1]为原DataFrame的一部分，根据column_name的值进行截断。如果是将SeriesGroupby类转换为list，则tuple[1]变为一个Series类</p>\n<p>举个例子：<br>input_data为一个DataFrame，值如下：</p>\n<p><div align=center><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123006189.png\" alt=\"image-20220716123006189\" style=\"zoom:67%;\" /></p>\n<p>现在将input_data根据sent_order列进行分组：input_data.groupby(“sent_order”)，结果如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123836380.png\" alt=\"image-20220716123836380\" style=\"zoom: 67%;\" /></p>\n<p>只展示了前三个元组，每个元组，tuple[0]为sent_order的值，tuple[1]为对应的DataFrame</p>\n</blockquote>\n<ul>\n<li><strong>应用自定义方法：DaraFrame.apply(func, axis=0, …)</strong></li>\n</ul>\n<blockquote>\n<p>Series、DataFrame和GroupbyDataFrame等均可使用</p>\n<p>func是一个自定义函数</p>\n<p>axis决定func的输入是什么，axis=0则输入为行，axis=1则输入为列</p>\n<p>Series无需指定axis，DataFrame可调节axis更换操作的维度，<strong>DaraFrameGroupby也无需指定，func的输入为分组后的每个DataFrame，最后再将每个分组返回的结果总和起来</strong></p>\n</blockquote>\n<ul>\n<li><strong>DataFrame的count()方法：</strong></li>\n</ul>\n<blockquote>\n<p><code>count()</code>有很多种用法，结果是可以对Series、DataFrame和GroupbyDataFrame等使用，结果是返回一个Series或DataFrame</p>\n<p>下面举几个示例，其中使用的df如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725142611139.png\" alt=\"image-20220725142611139\" style=\"zoom: 60%;\" /></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df.count()    <span class=\"hljs-comment\"># DataFrame执行count()，对每一列分别执行count，结果返回一个Series</span><br>df.data[<span class=\"hljs-string\">&quot;length&quot;</span>].count()    <span class=\"hljs-comment\"># Series执行count()，直接返回length列的长度，类型为int</span><br>df.groupy(<span class=\"hljs-string\">&quot;length&quot;</span>).count()   <span class=\"hljs-comment\"># 对DataFrameGroupby类执行，返回一个DataFrame，index为length的各指，colums为出length的其他列执行count的结果</span><br>df.groupy(<span class=\"hljs-string\">&quot;length&quot;</span>)[<span class=\"hljs-string\">&quot;evaluation&quot;</span>].count()  <span class=\"hljs-comment\"># 对SeriesGroupby类执行，则只返回evaluation列的count结果，类型为Series</span><br></code></pre></td></tr></table></figure>\n<p>上述四句语句的执行结果：</p>\n<ul>\n<li>第一句：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143928089.png\" alt=\"image-20220725143928089\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>第二句：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143939723.png\" alt=\"image-20220725143939723\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>第三句：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143953768.png\" alt=\"image-20220725143953768\" style=\"zoom:50%;\" /></p>\n<ul>\n<li>第四句：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725144005108.png\" alt=\"image-20220725144005108\" style=\"zoom:50%;\" /></p>\n</blockquote>\n<ul>\n<li><strong>读取/保存csv文件：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df.to_csv(file_name, index=<span class=\"hljs-literal\">False</span>)    <span class=\"hljs-comment\"># 要加index=False</span><br>pd.read_csv(file_name)<br></code></pre></td></tr></table></figure>\n<h3 id=\"处理缺失值\"><a href=\"#处理缺失值\" class=\"headerlink\" title=\"处理缺失值\"></a>处理缺失值</h3><ul>\n<li><strong>判断空值：DataFrame.isnull()或DataFrame.notnull()</strong></li>\n</ul>\n<blockquote>\n<p>DataFrame和Series都可用，结果也是返回一个DataFrame或Series，和输入一一对应，其中每个值为布尔值</p>\n</blockquote>\n<ul>\n<li><strong>删除缺失值：DataFrame.dropna(axis=0, inplace=False, how=None)</strong></li>\n</ul>\n<blockquote>\n<p>axis：删除行还是列，{0 or ‘index’, 1 or ‘columns’}</p>\n<p>how : 如果等于any则任何值为空都删除，如果等于all则所有值都为空才删除</p>\n</blockquote>\n<ul>\n<li><strong>填充缺失值：DataFrame.fillna(value, method, axis, …)</strong></li>\n</ul>\n<blockquote>\n<p>value：用于填充的值，可以是单个值，或者字典（用于不同列填充不同值的情况，key是列名，value是值）<br>method : 等于”ffill”使用前一个；不为空的值填充forword fill；等于”bfill”使用后一个不为空的值填充backword fill<br>axis : 按行还是列填充，{0 or ‘index’, 1 or ‘columns’}</p>\n</blockquote>\n<h3 id=\"Padas中DataFrame的增删查改\"><a href=\"#Padas中DataFrame的增删查改\" class=\"headerlink\" title=\"Padas中DataFrame的增删查改\"></a>Padas中DataFrame的增删查改</h3><ul>\n<li><strong>增加列：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df = DataFrame(...)    <span class=\"hljs-comment\"># 有三行数据</span><br>cities = [<span class=\"hljs-string\">&quot;成都&quot;</span>, <span class=\"hljs-string\">&quot;上海&quot;</span>, <span class=\"hljs-string\">&quot;北京&quot;</span>]<br><span class=\"hljs-comment\"># 三种插入方式</span><br>df.insert(<span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">&quot;city&quot;</span>, citys)    <span class=\"hljs-comment\"># 参数分别为：插入列的位置、列名、插入内容</span><br>df[<span class=\"hljs-string\">&quot;city&quot;</span>] = cities<br>df.loc[:, <span class=\"hljs-string\">&quot;city&quot;</span>] = cities<br></code></pre></td></tr></table></figure>\n<ul>\n<li><strong>增加行：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df = DataFrame(...)  <span class=\"hljs-comment\"># 有两列 </span><br>df.loc[<span class=\"hljs-number\">3</span>] = [<span class=\"hljs-string\">&quot;1&quot;</span>, <span class=\"hljs-string\">&quot;2&quot;</span>]       <span class=\"hljs-comment\"># 如果已经存在index=3的行，则修改值；反之直接添加该行</span><br>df = df.append(df_insert)    <span class=\"hljs-comment\"># 合成两个DataFrame，列要相同才行</span><br></code></pre></td></tr></table></figure>\n<ul>\n<li><strong>loc[]和iloc[]的使用：</strong></li>\n</ul>\n<blockquote>\n<p>两者功能是相同的，区别在于loc中对于列，只能使用列名（str类型）进行搜索，而iloc对于列只能使用整数索引进行搜索，DataFrame的查和改基本都是基于两者的</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df[<span class=\"hljs-string\">&quot;column_name&quot;</span>]或df.loc[:, <span class=\"hljs-string\">&quot;column_name&quot;</span>]   <span class=\"hljs-comment\"># 查找column_name列</span><br>df.loc[<span class=\"hljs-string\">&quot;index_name&quot;</span>]    <span class=\"hljs-comment\"># 查找index_name行</span><br>df.loc[<span class=\"hljs-string\">&quot;index_name&quot;</span>, <span class=\"hljs-string\">&quot;column_name&quot;</span>]     <span class=\"hljs-comment\"># 查找(index_name, column_name)处的值</span><br><span class=\"hljs-comment\"># 还可以使用list或者切片来代替，一次操作多行或多列</span><br>df.loc[[<span class=\"hljs-string\">&quot;index_name1&quot;</span>, <span class=\"hljs-string\">&quot;index_name2&quot;</span>], [<span class=\"hljs-string\">&quot;column_name1&quot;</span>, <span class=\"hljs-string\">&quot;column_name2&quot;</span>]]  <span class=\"hljs-comment\"># 同时操作index_name1&amp;2行和column_name1&amp;2列</span><br>df.loc[<span class=\"hljs-number\">0</span>:<span class=\"hljs-number\">3</span>, <span class=\"hljs-string\">&quot;column_name&quot;</span>]    <span class=\"hljs-comment\"># 进行切片，对0、1、2、3行操作</span><br></code></pre></td></tr></table></figure>\n<blockquote>\n<p><code>iloc[]</code>是使用整数列索引，比如：<code>df.iloc[:3, 2:6]</code></p>\n<p>注意<code>loc[]</code>中的切片，是包含了end所指的索引（和list的索引稍有不同），<code>iloc[]</code>是不包含end的索引的</p>\n</blockquote>\n<ul>\n<li><strong>删除：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df.drop(<span class=\"hljs-number\">0</span>, axis=<span class=\"hljs-number\">0</span>)   <span class=\"hljs-comment\"># 删除第0行</span><br><span class=\"hljs-comment\"># 删除列的三种方法</span><br>df.drop(<span class=\"hljs-string\">&quot;column_name&quot;</span>, axis=<span class=\"hljs-number\">1</span>)<br><span class=\"hljs-keyword\">del</span> df[<span class=\"hljs-string\">&quot;column_name&quot;</span>]<br>ndf = df.pop(<span class=\"hljs-string\">&quot;column_name&quot;</span>)<br><span class=\"hljs-comment\"># 使用drop的时候，第一个参数也可以是list，一次操作多行或多列</span><br>df.drop([<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>], axis=<span class=\"hljs-number\">0</span>)<br>df.drop([<span class=\"hljs-string\">&quot;column_name1&quot;</span>, <span class=\"hljs-string\">&quot;column_name2&quot;</span>], axis=<span class=\"hljs-number\">1</span>)<br></code></pre></td></tr></table></figure>\n<h1 id=\"Pickle用法\"><a href=\"#Pickle用法\" class=\"headerlink\" title=\"Pickle用法\"></a>Pickle用法</h1><ul>\n<li><p><strong>储存pk文件：pickle.dump(obj, file)</strong></p>\n</li>\n<li><p><strong>加载pk文件：pickle.load(file)</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>可以存储和加载对象，其中<code>dump()</code>中的obj是要存的对象，file是<code>open()</code>函数返回的文件描述符</p>\n<p><code>load()</code>返回值为所存储的对象</p>\n<p>储存的文件后缀为.pk</p>\n<p>举个栗子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">a = [<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>]<br>file_name = <span class=\"hljs-string\">&quot;test.pk&quot;</span><br><span class=\"hljs-comment\"># 存</span><br><span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">&quot;wb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>    pickle.dump(a, f)<br><span class=\"hljs-comment\"># 取</span><br><span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">&quot;rb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>    b = pickle.load(f)<br>    <br><span class=\"hljs-comment\"># a和b是一样的</span><br></code></pre></td></tr></table></figure>\n</blockquote>\n<h1 id=\"Json用法\"><a href=\"#Json用法\" class=\"headerlink\" title=\"Json用法\"></a>Json用法</h1><ul>\n<li><p><strong>json可进行Python对象和json格式之间的互换</strong></p>\n</li>\n<li><p><strong>Python转json：json.dumps(obj, ensure_ascii=True, …, indent=None, …)</strong></p>\n</li>\n<li><strong>json转Python：json.loads(s, …)</strong></li>\n</ul>\n<blockquote>\n<p>obj为Python对象，s为json对象</p>\n<p>ensure_ascii：如果为False，则返回值可以包含非 ASCII</p>\n<p>indent：为json中每个元素的间隔数，如果为0或None，则每个元素之间紧凑排版</p>\n<p><strong>注意：</strong></p>\n<p><strong>1. 以上两个函数只是进行对象的转换，而不会进行保存，所以是一般配合<code>open()</code>、<code>f.write()</code>、<code>f.read()</code>使用，给两个示例：</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 存储json对象</span><br><span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">&quot;w&quot;</span>, encoding=<span class=\"hljs-string\">&quot;utf-8&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br> f.write(json.dumps(data, ensure_ascii=<span class=\"hljs-literal\">False</span>, indent=<span class=\"hljs-number\">2</span>))<br><span class=\"hljs-comment\"># 读取json对象</span><br><span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">&#x27;r&#x27;</span>, encoding=<span class=\"hljs-string\">&#x27;utf-8&#x27;</span>) <span class=\"hljs-keyword\">as</span> f:<br> data = json.loads(f.read())<br></code></pre></td></tr></table></figure>\n</blockquote>\n<h1 id=\"Yaml\"><a href=\"#Yaml\" class=\"headerlink\" title=\"Yaml\"></a>Yaml</h1><ul>\n<li><strong>读取yaml文件：</strong><code>config = yaml.safe_load(open(&quot;config.yaml&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;).read())</code></li>\n<li>在.yaml文件中，可用缩进表示层级关系，并且写浮点数不能用python的科学技术法的写法：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_25.png\" alt=\"IMG_25\" style=\"zoom:50%;\" /></p>\n<h1 id=\"Argparse\"><a href=\"#Argparse\" class=\"headerlink\" title=\"Argparse\"></a>Argparse</h1><ul>\n<li><strong>基本流程：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">parser = argparse.ArgumentParser()<br>parser.add_argument(<span class=\"hljs-string\">&quot;--batch_size&quot;</span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>, default=<span class=\"hljs-number\">1</span>) <br>args = parser.parse_args()<br></code></pre></td></tr></table></figure>\n<ul>\n<li>其中<code>--</code>表示可选参数，在运行命令指定<code>--batch_size=xxx</code>即可</li>\n</ul>\n<h1 id=\"Matplotlib用法\"><a href=\"#Matplotlib用法\" class=\"headerlink\" title=\"Matplotlib用法\"></a>Matplotlib用法</h1><ul>\n<li><strong>折线图：plt.plot(x, y)</strong></li>\n<li><strong>直方图：plt.bar(x, y)</strong></li>\n<li><strong>散点图：plt.scatter(x, y)</strong></li>\n</ul>\n<blockquote>\n<p>上述函数还可以指定颜色、大小等参数：</p>\n<ul>\n<li>color：颜色</li>\n</ul>\n<blockquote>\n<p>可为：”b”：蓝色、”g”：绿色、”r”：红色、”c”：青色、”m”：品红、”y”：黄色、”k”：黑色、”w”：白色</p>\n<p>也只直接用全名，如”blue”</p>\n</blockquote>\n<ul>\n<li>s：点的大小</li>\n<li>linewidth：线宽</li>\n<li><p>linestyle：线样式，如设为”dashed”将线设为虚线</p>\n</li>\n<li><p>label：名字， 若要打印label，则使用<code>plt.legend()</code></p>\n</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>设置标题：plt.title()</strong></li>\n</ul>\n<blockquote>\n<p>同样可以使用fontsize指定字体大小</p>\n</blockquote>\n<ul>\n<li><strong>设置x、y轴标题：plt.xlabel()和plt.ylabel()</strong></li>\n<li><strong>保存图片：plt.savefig()</strong></li>\n<li><strong>画完图之后记得关闭：plt.close()</strong></li>\n<li><strong>画一条水平的线：plt.hlines(y, xmin, xmax)</strong></li>\n<li><strong>画一条垂直的线：plt.vlines(x, ymin, ymax)</strong></li>\n<li><strong>在指定座标处加注释：plt.text(x, y, s)</strong></li>\n</ul>\n<h1 id=\"Numpy用法\"><a href=\"#Numpy用法\" class=\"headerlink\" title=\"Numpy用法\"></a>Numpy用法</h1><ul>\n<li><strong>获取最大值的位置（索引）np.argmax(a, axis=None, …)：</strong></li>\n</ul>\n<blockquote>\n<p>axis：如果为None, 则代表把整个数组当作一维的，然后返回最大的索引值。如果指定axis，则只在该维度搜索最大值，示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">array([[<span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-number\">12</span>],<br>       [<span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>]])<br><span class=\"hljs-meta\">&gt;&gt;&gt; </span>np.argmax(a)<br><span class=\"hljs-number\">5</span><br><span class=\"hljs-meta\">&gt;&gt;&gt; </span>np.argmax(a, axis=<span class=\"hljs-number\">0</span>)<br>array([<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>])<br><span class=\"hljs-meta\">&gt;&gt;&gt; </span>np.argmax(a, axis=<span class=\"hljs-number\">1</span>)<br>array([<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>])<br></code></pre></td></tr></table></figure>\n<p>axis指定为哪个维度，则stack哪个维度</p>\n</blockquote>\n<ul>\n<li><strong>减少维度：np.squeeze(a, axis=None)</strong></li>\n</ul>\n<blockquote>\n<p>axis如果为None，则丢弃所有大小为1的维度</p>\n</blockquote>\n<h1 id=\"Tensorflow用法\"><a href=\"#Tensorflow用法\" class=\"headerlink\" title=\"Tensorflow用法\"></a>Tensorflow用法</h1><h3 id=\"禁用GPU\"><a href=\"#禁用GPU\" class=\"headerlink\" title=\"禁用GPU\"></a>禁用GPU</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> os<br>os.environ[<span class=\"hljs-string\">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class=\"hljs-string\">&quot;-1&quot;</span><br></code></pre></td></tr></table></figure>\n<h3 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h3><ul>\n<li><strong>填充序列：tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.)</strong></li>\n</ul>\n<blockquote>\n<p>输入：<br>sequences是要填充的序列</p>\n<p>maxlen是要填充的最大长度，如果为None，则设为所有序列中最长的那个的长度</p>\n<p>padding和truncating可以设为”pre”或者”post”，选择是在序列前面还是后面填充/截断</p>\n<p>value为填充的值</p>\n<p>返回值是一个ndarray</p>\n</blockquote>\n<ul>\n<li><strong>划分训练、测试集：sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</strong></li>\n</ul>\n<blockquote>\n<p>arrays是要划分的数据，可以使用list、numpy array、matric、dataframe<strong>（不能用Tensor）</strong>，示例：</p>\n<p><code>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)</code></p>\n<p>train_size和test_size指定一个就行</p>\n<p>random_state为随机种子</p>\n<p>shuffle决定是否打乱数据</p>\n<p>stratify决定是否进行数据分层，stratify不是bool，是直接指定分层的依据变量，即通过什么来分层，一般就是label或者Series（在输入数据是DataFrame时，stratify指定为Series，即根据那一列来分层）</p>\n<p><strong>输入的数据是什么类型，输出的数据就是什么类型，返回值依次为：x_train, x_test, y_train, y_test</strong></p>\n</blockquote>\n<h3 id=\"各种层\"><a href=\"#各种层\" class=\"headerlink\" title=\"各种层\"></a>各种层</h3><ul>\n<li><strong>tf.keras.layers.Dense(units, activation=None, use_bias=True, …)</strong></li>\n<li><strong>tf.keras.layers.Embedding(input_dim, output_dim,…., mask_zero=False, input_length=None)</strong></li>\n</ul>\n<blockquote>\n<p>input_dim：词汇表大小</p>\n<p>output_dim：词嵌入向量的维度</p>\n<p>mask_zero：决定输入值0是否是应被屏蔽的mask“填充”值。如果为True，则使用mask，相应的input_dim也应该为词汇表大小+1（因为增加了一个mask token，索引为0）</p>\n<p>intput_length：输入序列的长度，如果为None，则自动为最长序列的长度</p>\n</blockquote>\n<ul>\n<li><strong>tf.keras.layers.LSTM(units, activation=”tanh”, recurrent_activation=”sigmoid”, …, return_sequences=False)</strong></li>\n</ul>\n<blockquote>\n<p>其中return_sequences决定是否在最后返回所有时间步的输出，若为False只会返回最后一个时间步的输出，如果下一层需要使用到所有时间步的输出则需要设为True</p>\n</blockquote>\n<ul>\n<li><strong>tf.keras.layers.Bidirectional(layer, merge_mode=”concat”, …)</strong></li>\n</ul>\n<blockquote>\n<p>是RNN的双向包装器，可将单向RNN变为双向RNN</p>\n<p>layer是要包装的层，merge_mode决定前后信息汇总的 方式，可为：”sum”、”mul”、”concat”、”ave”、None，如果为None，则不会合并，而是将他们作为列表输出</p>\n</blockquote>\n<ul>\n<li><strong>tf.keras.layers.Softmax(axis=-1)</strong></li>\n</ul>\n<blockquote>\n<p>axis：决定对那个维度执行softmax，默认为最后一个维度</p>\n</blockquote>\n<h3 id=\"模型的常用方法\"><a href=\"#模型的常用方法\" class=\"headerlink\" title=\"模型的常用方法\"></a>模型的常用方法</h3><ul>\n<li><strong>序列容器：tf.keras.models.Sequential()</strong></li>\n</ul>\n<blockquote>\n<p>可以和<code>add()</code>配合使用，也可以里面直接加个列表Sequential([…])</p>\n</blockquote>\n<ul>\n<li><strong>配置模型：model.compile(optimizer=’rmsprop’, loss=None, metrics=None,…)</strong></li>\n</ul>\n<blockquote>\n<p>optimizer：字符串或者Opitimizer实例</p>\n<p>loss：字符串或Loss示例或一个函数</p>\n<p>metrics：字符串或Metric示例或一个函数</p>\n</blockquote>\n<ul>\n<li><strong>训练模型：model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0., validation_data=None, …)</strong></li>\n</ul>\n<blockquote>\n<p>x/y：可以是numpy数组或者Tensor或list或迭代器等，这样x/y应一一对应。x也可为DataSet、Generator、Sequence，但是这种情况下就不应该指定y，因为y已经在x中了</p>\n<p>batch_size如果未指定则默认32</p>\n<p>callbacks：一个列表，每个元素是一个callbacks中的实例，如EarlyStopping类</p>\n<p>validation_split和validaton_data只需要指定一个，前者是验证集所占比例，后者是直接指定验证集<strong>（如果指定了该参数，则x/y只能Tensor或ndarray）</strong></p>\n</blockquote>\n<ul>\n<li><strong>画模型：tf.keras.utils.plot_model(model, to_file=”model.png”, show_shapes=False, show_dtype=False, show_layer_names=True, …)</strong></li>\n</ul>\n<blockquote>\n<p>model是要打印的模型，一个keras model实例</p>\n</blockquote>\n<ul>\n<li><strong>测试模型：model.evaluate(x=None, y=None, batch_size=None, verbose=1, …)</strong></li>\n</ul>\n<blockquote>\n<p>用法和fit基本一样</p>\n</blockquote>\n<ul>\n<li><strong>采用另一种方法训练模型：</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">with</span> tf.GradientTape() <span class=\"hljs-keyword\">as</span> tape:<br>    predictions = model(inputs, training = <span class=\"hljs-literal\">True</span>)<br>    loss = loss_func(labels, predictions)<br>gradients = tape.gradient(loss, model.trainable_variables)<br>optimizer.apply_gradients(<span class=\"hljs-built_in\">zip</span>(gradients, model.trainable_variables))<br></code></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"保存和加载模型\"><a href=\"#保存和加载模型\" class=\"headerlink\" title=\"保存和加载模型\"></a>保存和加载模型</h3><ul>\n<li>既有keras保存方式，也有tf原生方式保存方式，<strong>但是前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署</strong>。既可以直接保存整个模型，也可以分别保存模型的权重和结构</li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 直接保存</span><br>model.save(<span class=\"hljs-string\">&quot;model.h5&quot;</span>, save_format = <span class=\"hljs-string\">&quot;h5&quot;</span>)<br>model = tf.keras.models.load_model(<span class=\"hljs-string\">&quot;model.h5&quot;</span>)<br><br><span class=\"hljs-comment\"># 分别保存模型的权重和结构</span><br>json_str = model.to_json()<br>model.save_weights(<span class=\"hljs-string\">&quot;model_weights.h5&quot;</span>, save_format = <span class=\"hljs-string\">&quot;h5&quot;</span>)<br>model = tf.keras.models.model_from_json(json_str)<br>model.<span class=\"hljs-built_in\">compile</span>(...)    <span class=\"hljs-comment\"># 这种方法重新加载后需要重新配置优化器</span><br>model.load_weights（<span class=\"hljs-string\">&quot;model_weights.h5&quot;</span>）<br><br><span class=\"hljs-comment\"># 上面演示的是keras保存方式</span><br><span class=\"hljs-comment\"># 要使用tf原生方式，需要在model.save()和model.save_weights()时添加参数save_format=&quot;tf&quot;，并且更改文件后缀</span><br><span class=\"hljs-comment\"># 演示一下：</span><br>model.save_weights(<span class=\"hljs-string\">&#x27;tf_model_weights.ckpt&#x27;</span>,save_format=<span class=\"hljs-string\">&quot;tf&quot;</span>)<br>model.save(<span class=\"hljs-string\">&#x27;tf_model_savedmodel&#x27;</span>, save_format=<span class=\"hljs-string\">&quot;tf&quot;</span>)<br></code></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"模型的反馈\"><a href=\"#模型的反馈\" class=\"headerlink\" title=\"模型的反馈\"></a>模型的反馈</h3><ul>\n<li><strong>设置Early Stop：tf.keras.callbacks.EarlyStopping(monitor=’val_loss’, min_delta=0, patience=0, …, restore_best_weights=False)</strong></li>\n</ul>\n<blockquote>\n<p>monitor：要监视的量，如：”val_loss”、”val_accuracy”、”train_loss”等</p>\n<p>min_delta：若小于min_delta的绝对变化，将被视为没有改进</p>\n<p>patience：容忍可以没有提升的epochs数</p>\n<p>restore_best_weights：是否保存整个训练过程中最好的一个epoch</p>\n</blockquote>\n<ul>\n<li><strong>设置学习率衰减：tf.keras.callbacks.ReduceLROnPlateau(monitor=”val_loss”, factor=0.1, patience=10, verbose=0, …, min_delta=0.0001, …, min_lr=0)</strong></li>\n</ul>\n<blockquote>\n<p>当经过patience个epochs仍没有改进，则降低学习率，学习率*factor来降低</p>\n<p>factor：每次降低的倍数</p>\n<p>min_lr：学习率可降低到的最小值</p>\n</blockquote>\n<h3 id=\"Dataset用法\"><a href=\"#Dataset用法\" class=\"headerlink\" title=\"Dataset用法\"></a>Dataset用法</h3><ul>\n<li>在<code>model.fit</code>中需要可以选择分别用list等类型指定x和y，也可以直接使用Dataset等类指定x而忽略y。对于前者，需要将所有数据一次性读入内存，内存负担较大。而后者可以使用TFRecordDataset类进行外存到内存的流式读取，只需要牺牲小部分IO性能，可大大减轻内存的负担</li>\n<li>简单来说，一个.tfrecords文件包含多个样本<strong>（官方建议每个文件大小100-200M）</strong>，每个样本都对应着一个<code>tf.train.Example</code>类，每个样本可能有多个特征，每个特征用<code>tf.train.Feature()</code>封装</li>\n<li>tf.train.Featrue可接收以下三种类型，大多数其他通用类型也可以强制转换成下面的其中一种：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>tf.train.BytesList：</strong>string、byte<strong>（非标量数据也使用这个，但是需要先将Tensor序列化）</strong></li>\n<li><strong>tf.train.FloatList：</strong>float、double</li>\n<li><strong>tf.train.Int64List：</strong>bool、enum、int32、unit32、int64、uint64g</li>\n</ol>\n</blockquote>\n<ul>\n<li><strong>首先进行.tfrecords文件的存入：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">save_tfrecords</span>(<span class=\"hljs-params\">data, label, desfile</span>):</span><br>    <span class=\"hljs-keyword\">with</span> tf.io.TFRecordWriter(desfile) <span class=\"hljs-keyword\">as</span> writer:<br>        <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(data)):<br>            features = tf.train.Features(<br>                feature = &#123;<br>                    <span class=\"hljs-string\">&quot;data&quot;</span>:tf.train.Feature(bytes_list = tf.train.BytesList(value =[tf.io.serialize_tensor(data[i]).numpy()])),<br>                    <span class=\"hljs-string\">&quot;label&quot;</span>:tf.train.Feature(float_list = tf.train.FloatList(value = label[i])),<br>                &#125;<br>            )<br>            example = tf.train.Example(features = features)<br>            serialized = example.SerializeToString()<br>            writer.write(serialized)<br>   <br><br>save_tfrecords(x_in_sample1, y_in_sample1, <span class=\"hljs-string\">&quot;path1.tfrecords&quot;</span>)<br>save_tfrecords(x_in_sample2, y_in_sample2, <span class=\"hljs-string\">&quot;path2.tfrecords&quot;</span>)<br></code></pre></td></tr></table></figure>\n<blockquote>\n<ul>\n<li>首先要用<code>tf.io.TFRecordWriter()</code>打开写入器，data和label是两个列表，每个元素对应一个Example的特征，desfile是文件名</li>\n<li>对于每个Example的特征，使用一个字典表示，字典还需要被<code>tf.train.Features()</code>封装起来，字典的每个值可以是上述三种类型之一，需要使用<code>tf.train.Feature()</code>封装，其中value的值必须为一个list<strong>（注意前者是Features，后者是Feature）</strong>。并且<strong>由于data是非标量的高维数据，需要先使用<code>tf.io.serialize_tensor()</code>进行序列化，然后再使用其numpy值</strong></li>\n<li>最后通过<code>tf.train.Example</code>封装feature，然后调用<code>SerializeToString()</code>函数，将编码后的字符串写入</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>接下来进行数据读取和使用：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># TFR数据反编译</span><br><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">map_func</span>(<span class=\"hljs-params\">example</span>):</span><br>    feature_description = &#123;<br>        <span class=\"hljs-string\">&#x27;data&#x27;</span>: tf.io.FixedLenFeature([], tf.string),<br>        <span class=\"hljs-string\">&#x27;label&#x27;</span>: tf.io.FixedLenFeature([], tf.float32),<br>    &#125;<br>    parsed_example = tf.io.parse_single_example(example, features=feature_description)<br>    <br>    x_sample = tf.io.parse_tensor(parsed_example[<span class=\"hljs-string\">&#x27;data&#x27;</span>], tf.float32)<br>    y_sample = parsed_example[<span class=\"hljs-string\">&#x27;label&#x27;</span>]<br>    <br>    <span class=\"hljs-keyword\">return</span> x_sample, y_sample<br><br><span class=\"hljs-comment\"># 加载数据集</span><br><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">load_dataset</span>(<span class=\"hljs-params\">filepaths</span>):</span><br>    shuffle_buffer_size = <span class=\"hljs-number\">3000</span><br>    batch_size = <span class=\"hljs-number\">256</span><br><br>    dataset = tf.data.TFRecordDataset(filepaths)<br>    dataset = dataset.shuffle(shuffle_buffer_size)<br>    dataset = dataset.<span class=\"hljs-built_in\">map</span>(map_func=map_func, num_parallel_calls=tf.data.AOTOTUNE)<br>    dataset = dataset.batch(batch_size).prefetch(<span class=\"hljs-number\">64</span>)<br>    <br>    <span class=\"hljs-keyword\">return</span> dataset<br><br><br>train_set = load_dataset([<span class=\"hljs-string\">&quot;path1.tfrecords&quot;</span>,<span class=\"hljs-string\">&quot;path2.tfrecords&quot;</span>])<br>valid_set = load_dataset([<span class=\"hljs-string\">&quot;path3.tfrecords&quot;</span>,<span class=\"hljs-string\">&quot;path4.tfrecords&quot;</span>])<br>model.fit(train_set,epochs=model_epochs, validation_data=valid_set, callbacks=[early_stopping])<br></code></pre></td></tr></table></figure>\n<blockquote>\n<ul>\n<li>首先要生成<code>tf.data.TFRecordDataset</code>类，传入参数是一个list，其中元素是.tfrecords文件名</li>\n<li>每次读取后要先打乱</li>\n<li>之后调用<code>map()</code>，其中指定的函数是<strong>对每个样本操作的</strong>，由于储存时序列化为了二进制字符串，所以数据是什么类型是的信息是丢失了的，解码时需要自己制定数据类型，所以在<code>map()</code>中首先需要指定feature_description，说明每个数据的类型</li>\n<li>然后使用<code>tf.io.parse_single_example()</code>解码单个样本，结果返回一个字典，字典的每个元素对应一个特征，其中<strong>由于data是string类型，由于是编码后的非标量数据，所以还需要使用<code>tf.io.parse_tensor</code>解码一次（非标量数据的shape的编码解码的过程中是会丢失的，所以需要把shape一起储存）</strong></li>\n<li>如果储存的是列表，需要在<code>tf.io.FixedLenFeature()</code>指定shape，也就是(len(list), )</li>\n</ul>\n</blockquote>\n<h1 id=\"keras-bert用法\"><a href=\"#keras-bert用法\" class=\"headerlink\" title=\"keras_bert用法\"></a>keras_bert用法</h1><ul>\n<li><strong>加载预训练模型：keras_bert.load_trained_model_from_checkpoint(config_file, checkpoint_file, training=False, trainable=None, output_layer_num=1, seq_len=int(1e9), kwargs)</strong></li>\n</ul>\n<blockquote>\n<p>config_file和checkpoint_file分别为bert_config.json文件和bert_model.ckpt文件</p>\n<p>training：如果为True，则将返回带有 MLM 和 NSP输出的模型；否则，将返回输入层和最后一个特征提取层。</p>\n<p>trainable：模型是否可训练（也可通过for循环分别制定每层是否可训练）</p>\n<p>seq_len为最大序列长度</p>\n</blockquote>\n<ul>\n<li><strong>编码输入：keras_bert.Tokenizer(token_dict, …)</strong></li>\n</ul>\n<blockquote>\n<p>bert模型的输入有两个，一个是语义token，一个是序列token，需要创建Tokenizer实例，再对句子进行编码，产生两个token</p>\n<p>token_dict：是输入的映射字典，在vocab.txt中可以获取所有token，但是需要自行建立token -&gt; id的字典</p>\n<p>创建Tokenizer实例后，调用<code>Tokenizer.encode(first, second=None, max_len=None)</code>进行编码，结果返回一个list，元素分别为两个list，分别对应两个token</p>\n</blockquote>\n<ul>\n<li><strong>将数据输入模型：</strong></li>\n</ul>\n<blockquote>\n<p>在<code>model.fit()</code>中，由于是多输入，采用<code>x=[train_token_embeddings, train_seq_embeddings]</code>和<code>validation_data=([test_token_embeddings, test_seq_embeddings], test_labels)</code>这种形式进行输入</p>\n</blockquote>\n<ul>\n<li><strong>加载模型时所需要注意的：</strong></li>\n</ul>\n<blockquote>\n<p>加载保存好的模型时同样是使用<code>tf.keras.models.load_model()</code>，但是会出现如下报错：</p>\n<p><strong>ValueError: Unknown layer: TokenEmbedding</strong></p>\n<p>这种情况需要：先<code>from keras_bert import get_custom_objects</code>，再在<code>load_model()</code>时添加参数<code>custom_objects=get_custom_objects()</code></p>\n</blockquote>\n<h1 id=\"bert4keras用法\"><a href=\"#bert4keras用法\" class=\"headerlink\" title=\"bert4keras用法\"></a>bert4keras用法</h1><ul>\n<li><strong>由于keras和tf.keras的兼容问题，所以最开始需要添加环境变量TF_KERAS=1：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> os<br>os.environ[<span class=\"hljs-string\">&quot;TF_KERAS&quot;</span>] = <span class=\"hljs-string\">&quot;1&quot;</span><br></code></pre></td></tr></table></figure>\n<ul>\n<li>直接看<a href=\"https://github.com/Sniper970119/bert4keras_document\">民间的API文档</a>，写的还可以</li>\n</ul>\n<h1 id=\"Pytorch\"><a href=\"#Pytorch\" class=\"headerlink\" title=\"Pytorch\"></a>Pytorch</h1><h3 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h3><ul>\n<li><strong>读取数据</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>自建一个类，继承<code>torch.utils.data.Dataset</code>，还要实现<code>__init__()</code>、<code>__getitem__(index)</code>、<code>__len__()</code>三个函数</li>\n<li>实例化Dataset，并用<code>torch.utils.data.DataLoader</code>封装：<code>DataLoader(data, batch_size=1)</code></li>\n<li>直接for循环取每个batch：<code>for input_ids, attention_mask, labels in data:</code></li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>搭建模型：</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li>继承<code>torch.nn.Module</code>，并实现<code>__init__()</code>、<code>forward()</code></li>\n</ol>\n</blockquote>\n<ul>\n<li><strong>损失函数：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>以交叉熵举例</li>\n<li>创建：<code>loss = torch.nn.CrossEntropyLoss(reduction=&quot;none&quot;).cuda()</code></li>\n<li>其中<code>reduction=Union[“mean”, “sum”, “none”]</code>，表示对每个sample的loss怎么处理</li>\n<li><strong>踩个坑：如果loss的输入（模型的输出置信度）超过两维，如token级别的分类，<code>shape=[batch_size, max_seq_len, num_class]</code>，需要把num_class移到第二维，即执行<code>output.transpose(1, 2)</code>，现在<code>shape=[batch_size, num_class, max_seq_len]</code></strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_27.png\" alt=\"IMG_27\"></p>\n</blockquote>\n<ul>\n<li><strong>优化器：</strong><code>torch.optim.Adam(model.parameters(), lr=0.01</code></li>\n<li><strong>反向传播：</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">model.zero_grad()<br>loss.backward()<br>optimizer.step()<br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>模型保存和加载：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 可以选择保存整个模型，也可以选择只保存权重</span><br><span class=\"hljs-comment\"># 保存整个模型：</span><br>torch.save(model, file_path)   <span class=\"hljs-comment\"># file_path以.pt .pth结尾</span><br><span class=\"hljs-comment\"># 加载整个模型</span><br>model = torch.load(file_path)<br><br><span class=\"hljs-comment\"># 保存权重</span><br>torch.save(model.state_dict(), file_path)<br><span class=\"hljs-comment\"># 加载权重</span><br>model = MyModel()<br>model.load_state_dict(torch.load(file_path))<br></code></pre></td></tr></table></figure>\n<ul>\n<li><strong>模型训练、测试模式转换：</strong><code>model.train()</code>、<code>model.eval()</code></li>\n</ul>\n<h3 id=\"多卡并行\"><a href=\"#多卡并行\" class=\"headerlink\" title=\"多卡并行\"></a>多卡并行</h3><ul>\n<li><strong>运行命令：</strong><code>python3 -m torch.distributed.launch --nproc_per_node=2 &#123;file_name&#125;</code></li>\n</ul>\n<blockquote>\n<p>其中nproc_per_node代表GPU数量</p>\n</blockquote>\n<ul>\n<li><strong>命令行参数</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li>进程编号：<code>parser.add_argument(&quot;--local_rank&quot;, type=int, default=-1)</code>，local_rank为0代表主进程</li>\n<li>GPU数量：<code>gpu_num = int(os.environ[&#39;WORLD_SIZE&#39;])</code></li>\n<li>需要手动设置环境变量：<code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = “0, ..., gpu_num-1”</code></li>\n</ol>\n</blockquote>\n<ul>\n<li><strong>初始化：</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">torch.cuda.set_device(local_rank)<br><span class=\"hljs-keyword\">if</span> gpu_num &gt; <span class=\"hljs-number\">1</span> :<br> \ttorch.distributed.init_process_group(backend=<span class=\"hljs-string\">&quot;nccl”)</span><br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>读取数据：</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">data = Dataset()<br>data_sampler = torch.utils.data.distributed.DistributedSampler(data)<br>data = DataLoader(data, batch_size=<span class=\"hljs-number\">1</span>, sampler=data_sampler)   <span class=\"hljs-comment\"># 需要在DataLoader封装时指定sampler</span><br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>模型：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>模型需要用<code>torch.nn.parallel.DistributedDataParallel</code>封装：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=<span class=\"hljs-literal\">False</span>) `<br></code></pre></td></tr></table></figure></li>\n<li>其中find_unused_parameters默认为False，为False效率更高，如果报错则改为True</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>打乱数据：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>在每个epoch开始的时候需要<code>data.sampler.set_epoch(epoch)</code></li>\n<li>是为了让sample打乱数据，使每个epoch的训练数据顺序不同</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>同步每个进程的数据：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>每个进程中的数据是不同步的，如train_loss、val_loss等</li>\n<li>通过<code>torch.distributed.all_reduce(value)</code>来对value进行同步</li>\n<li>同步之后是所有进程该值的和，所以一般需要除以gpu_num</li>\n<li>value必须是Tensor，不能是标量</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>模型保存和加载：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>如果model用DDP封装，保存时保存<code>model.module</code>或者<code>model.module.state_dict()</code></li>\n<li><strong>一般只在主进程进行模型保存、打印、tensorboard记录</strong></li>\n</ul>\n</blockquote>\n<h3 id=\"Huggingface-Transformers\"><a href=\"#Huggingface-Transformers\" class=\"headerlink\" title=\"Huggingface Transformers\"></a>Huggingface Transformers</h3><ul>\n<li><strong>Tokenizer加载：</strong><code>AutoTokenizer.from_pretrained(checkpoint)</code></li>\n<li><strong>Tokenizer编码：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li><code>tokenizer.encode(text, return_tensors=&quot;pt&quot;)</code>：只返回词典映射后的编码结果，<code>text=str</code></li>\n<li><code>tokenizer(texts, padding=&quot;max_length&quot;, truncation=True,max_length=150, return_tensors=&quot;pt”)</code>：返回一个字典，包括模型所需要的所有输入，一般为input_ids和attention_mask。<code>texts=Union[str, list(str)]</code>，<code>padding=Union[“max_length”, True, False]</code></li>\n<li><strong>tokenizer()的结果如果需要组成batch，字典中的每个Tensor需要将第一维去掉：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">for</span> key, val <span class=\"hljs-keyword\">in</span> inputs.items():<br>    inputs[key] = val.squeeze(<span class=\"hljs-number\">0</span>)<br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>Tokenizer解码：</strong><code>tokenizer.decode(ids, skip_special_tokens=True)</code></li>\n</ul>\n<blockquote>\n<p>其中ids为input_ids</p>\n</blockquote>\n<ul>\n<li><strong>指定Tokenizer填充和截断方向：</strong><blockquote>\n<ul>\n<li><code>tokenizer.padding_side=Union[“left”, “right”]</code></li>\n<li><code>tokenizer.truncation_side=Union[“left”, “right”]</code></li>\n</ul>\n</blockquote>\n</li>\n<li><strong>模型：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>加载：<code>AutoModel.from_pretrained(checkpoint)</code></li>\n<li>前向计算：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">inputs = tokenizer().    <span class=\"hljs-comment\"># inputs是一个dict</span><br>output = model(**inputs)。   <span class=\"hljs-comment\"># 输出的output是一个dict</span><br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>生成式模型进行生成预测：</strong><code>model.generate(ids, max_length=None, num_beams=1,do_sample=False, top_k=1, early_stopping=False, num_return_sequences=1)</code></li>\n</ul>\n<blockquote>\n<ul>\n<li>一些生成模型如BloomForCausalModel自带生成函数</li>\n<li>其中ids为encode之后的结果，每次generate的ids只对应一个句子，无法并行</li>\n<li>需要do_sanple=True，才能启用top_k、top_p</li>\n</ul>\n</blockquote>\n<h3 id=\"Tensorboard\"><a href=\"#Tensorboard\" class=\"headerlink\" title=\"Tensorboard\"></a>Tensorboard</h3><ul>\n<li>直接给个🌰：</li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 初始化</span><br>time = “&#123;<span class=\"hljs-number\">0</span>:%Y-%m-%d-%H:%M:%S/&#125;<span class=\"hljs-string\">&quot;.format(datetime.datetime.now())</span><br><span class=\"hljs-string\">tb_path = config[&quot;</span>tb_root_path<span class=\"hljs-string\">&quot;] + time</span><br><span class=\"hljs-string\">print(f&quot;</span>Start Tensorboard <span class=\"hljs-keyword\">with</span> <span class=\"hljs-string\">&#x27;tensorboard --logdir=&#123;tb_path&#125; --bind_all&#x27;</span>, view at http://localhost:<span class=\"hljs-number\">6006</span>/<span class=\"hljs-string\">&quot;)</span><br><span class=\"hljs-string\">writer = SummaryWriter(tb_path)</span><br><span class=\"hljs-string\"># 保存标量</span><br><span class=\"hljs-string\">writer.add_scalar(&quot;</span>train/loss<span class=\"hljs-string\">&quot;, loss, step)</span><br></code></pre></td></tr></table></figure>\n<ul>\n<li>运行完之后直接运行<code>tensorboard --logdir=&#123;tb_path&#125; --bind_all</code>，然后访问本地6006端口即可</li>\n<li>在多卡时，一般只在主进程采用tensorboard</li>\n</ul>\n</blockquote>\n","site":{"data":{}},"wordcount":27261,"excerpt":"","more":"<h1 id=\"NLTK用法\"><a href=\"#NLTK用法\" class=\"headerlink\" title=\"NLTK用法\"></a>NLTK用法</h1><ul>\n<li>nltk用于<strong>英文</strong>分词分句等应用</li>\n</ul>\n<h3 id=\"基本的预处理\"><a href=\"#基本的预处理\" class=\"headerlink\" title=\"基本的预处理\"></a>基本的预处理</h3><ul>\n<li><strong>分句：nltk.sent_tokenize(text, language=”english”):</strong></li>\n</ul>\n<blockquote>\n<p>输入：一个str段落</p>\n<p>输出：一个list，每个元素是一个str句子</p>\n</blockquote>\n<ul>\n<li><strong>分词：nltk.word_tokenize(text, language=”english”, preserve_line=False):</strong></li>\n</ul>\n<blockquote>\n<p>输入：一个str句子</p>\n<p>输出：一个list，每个元素是一个str词</p>\n</blockquote>\n<ul>\n<li><strong>词性标注（POS_tag）：nltk.postag(tokens)</strong></li>\n</ul>\n<blockquote>\n<p>输入：一个list，每个元素是一个str，一个句子分好词的结果</p>\n<p>输入：一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[(‘football’, ‘NN’), (‘is’, ‘VBZ’), (‘a’, ‘DT’), (‘family’, ‘NN’)]</p>\n<p>词性所对应的意义，大致来说<strong>N开头对应名词NOUN，V开头对应动词VERB，J开头对应形容词ADJ，R开头对应副词ADV</strong></p>\n<p>词性具体所对应意义如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>标记</th>\n<th>含义</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CC</td>\n<td>连词</td>\n<td>and, or,but, if, while,although</td>\n</tr>\n<tr>\n<td>CD</td>\n<td>数词</td>\n<td>twenty-four, fourth, 1991,14:24</td>\n</tr>\n<tr>\n<td>DT</td>\n<td>限定词</td>\n<td>the, a, some, most,every, no</td>\n</tr>\n<tr>\n<td>EX</td>\n<td>存在量词</td>\n<td>there, there’s</td>\n</tr>\n<tr>\n<td>FW</td>\n<td>外来词</td>\n<td>dolce, ersatz, esprit, quo,maitre</td>\n</tr>\n<tr>\n<td>IN</td>\n<td>介词连词</td>\n<td>on, of,at, with,by,into, under</td>\n</tr>\n<tr>\n<td>JJ</td>\n<td>形容词</td>\n<td>new,good, high, special, big, local</td>\n</tr>\n<tr>\n<td>JJR</td>\n<td>比较级词语</td>\n<td>bleaker braver breezier briefer brighter brisker</td>\n</tr>\n<tr>\n<td>JJS</td>\n<td>最高级词语</td>\n<td>calmest cheapest choicest classiest cleanest clearest</td>\n</tr>\n<tr>\n<td>LS</td>\n<td>标记</td>\n<td>A A. B B. C C. D E F First G H I J K</td>\n</tr>\n<tr>\n<td>MD</td>\n<td>情态动词</td>\n<td>can cannot could couldn’t</td>\n</tr>\n<tr>\n<td>NN</td>\n<td>名词</td>\n<td>year,home, costs, time, education</td>\n</tr>\n<tr>\n<td>NNS</td>\n<td>名词复数</td>\n<td>undergraduates scotches</td>\n</tr>\n<tr>\n<td>NNP</td>\n<td>专有名词</td>\n<td>Alison,Africa,April,Washington</td>\n</tr>\n<tr>\n<td>NNPS</td>\n<td>专有名词复数</td>\n<td>Americans Americas Amharas Amityvilles</td>\n</tr>\n<tr>\n<td>PDT</td>\n<td>前限定词</td>\n<td>all both half many</td>\n</tr>\n<tr>\n<td>POS</td>\n<td>所有格标记 ’</td>\n<td>‘s</td>\n</tr>\n<tr>\n<td>PRP</td>\n<td>人称代词</td>\n<td>hers herself him himself hisself</td>\n</tr>\n<tr>\n<td>PRP$</td>\n<td>所有格</td>\n<td>her his mine my our ours</td>\n</tr>\n<tr>\n<td>RB</td>\n<td>副词</td>\n<td>occasionally unabatingly maddeningly</td>\n</tr>\n<tr>\n<td>RBR</td>\n<td>副词比较级</td>\n<td>further gloomier grander</td>\n</tr>\n<tr>\n<td>RBS</td>\n<td>副词最高级</td>\n<td>best biggest bluntest earliest</td>\n</tr>\n<tr>\n<td>RP</td>\n<td>虚词</td>\n<td>aboard about across along apart</td>\n</tr>\n<tr>\n<td>SYM</td>\n<td>符号</td>\n<td>% &amp; ’ ‘’ ‘’. ) )</td>\n</tr>\n<tr>\n<td>TO</td>\n<td>词to</td>\n<td>to</td>\n</tr>\n<tr>\n<td>UH</td>\n<td>感叹词</td>\n<td>Goodbye Goody Gosh Wow</td>\n</tr>\n<tr>\n<td>VB</td>\n<td>动词</td>\n<td>ask assemble assess</td>\n</tr>\n<tr>\n<td>VBD</td>\n<td>动词过去式</td>\n<td>dipped pleaded swiped</td>\n</tr>\n<tr>\n<td>VBG</td>\n<td>动词现在分词</td>\n<td>telegraphing stirring focusing</td>\n</tr>\n<tr>\n<td>VBN</td>\n<td>动词过去分词</td>\n<td>multihulled dilapidated aerosolized</td>\n</tr>\n<tr>\n<td>VBP</td>\n<td>动词现在式非第三人称时态</td>\n<td>predominate wrap resort sue</td>\n</tr>\n<tr>\n<td>VBZ</td>\n<td>动词现在式第三人称时态</td>\n<td>bases reconstructs marks</td>\n</tr>\n<tr>\n<td>WDT</td>\n<td>Wh限定词</td>\n<td>who,which,when,what,where,how</td>\n</tr>\n<tr>\n<td>WP</td>\n<td>WH代词</td>\n<td>that what whatever</td>\n</tr>\n<tr>\n<td>WP$</td>\n<td>WH代词所有格</td>\n<td>whose</td>\n</tr>\n<tr>\n<td>WRB</td>\n<td>WH副词</td>\n</tr>\n</tbody>\n</table>\n</div>\n</blockquote>\n<ul>\n<li><strong>词形还原Lemmatization：nltk.stem.WordNetLemmatizer().lemmatize(word, pos=”n”)：</strong></li>\n</ul>\n<blockquote>\n<p>这是一个类的内置方法，首先需要创建WordNetLemmatizer对象（假设为wnl），则通过<code>wnl.lemmatize()</code>调用</p>\n<p>输入：word是单个词，pos是其对应的单词词性（n对应noun，v对应verb，a对应adj，r对应adv，s对应satellite adjectives(不咋用)）</p>\n<p>输出：word经过还原后的词，如cars还原为car</p>\n</blockquote>\n<h3 id=\"NER\"><a href=\"#NER\" class=\"headerlink\" title=\"NER\"></a>NER</h3><ul>\n<li>先介绍一下命名实体识别（Named Entity Recognition，NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。</li>\n<li>nltk中对NER类别的分类如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/20191219223810610.png\" alt=\"img\"></p>\n<p>其中LOCATION和GPE有重合，GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。</p>\n<ul>\n<li><strong>进行NER：ne_chunk(tagged_tokens, binary=False)</strong></li>\n</ul>\n<blockquote>\n<p>输入：tagged_tokens为<code>pos_tag()</code>函数的输出，一个list，每个元素是一个tuple，tuple[0]是对应的token，tuple[1]是对应的词性，示例：[(‘football’, ‘NN’), (‘is’, ‘VBZ’), (‘a’, ‘DT’), (‘family’, ‘NN’)]</p>\n<p>输出：同样的是一个list，每个元素变为了一个封装对象（Tree类），和输入一一对应</p>\n<ul>\n<li><strong>对于该封装对象：</strong></li>\n</ul>\n<p>输出的list中，有些元素是没有NER的结果的，其对应的封装对象没有label属性，<strong>可使用<code>hasattr(ne_word, &#39;label&#39;)</code>函数判断是否有NER结果</strong></p>\n<p>假设一个该对象命名为ne_word，调用<code>ne_word.leaves()</code>可返回一个list，每个元素为一个tuple，tuple[0]为token，tuple[1]为该token的pos_tag。对于NER，list中只有一个tuple，返回结果示例：[(‘FIFA’, ‘NNP’)]</p>\n<p>调用<code>ne_word.label()</code>函数可返回该token对应的NER结果，一定要先使用<code>hasattr()</code>函数才能使用<code>label()</code>函数</p>\n</blockquote>\n<h3 id=\"计算BLEU\"><a href=\"#计算BLEU\" class=\"headerlink\" title=\"计算BLEU\"></a>计算BLEU</h3><ul>\n<li><p><a href=\"https://zlkqz.top/2022/02/20/NLP%E5%9F%BA%E7%A1%80/#5-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91\">BLEU的定义</a></p>\n</li>\n<li><p><strong>nltk.translate.bleu_score.sentence_bleu(references, hypothesis, …, smoothing_function=None, …)</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>references：参照序列，label。<strong>type=list(list(str))，其中的每个str为词或字</strong></p>\n<p>hypothesis：候选序列，也就是预测出的序列。<strong>type=list(str)，其中的每个str为词或字</strong></p>\n<p>smoothing_function，是论文中使用到的平滑技巧，一般输入<code>nltk.translate.bleu_score.SmoothingFunction().methodi()</code>，最后的i为0~7</p>\n</blockquote>\n<ul>\n<li>另外还可以使用<code>corpus_bleu()</code>计算多个句子的BLEU；用<code>modified_precision()</code>计算修正的n-gram精确度</li>\n</ul>\n<h1 id=\"Pyltp用法\"><a href=\"#Pyltp用法\" class=\"headerlink\" title=\"Pyltp用法\"></a>Pyltp用法</h1><ul>\n<li>Pyltp主要用于中文的预处理等，包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注</li>\n<li>使用Pyltp时注意其编码使用的都是utf-8，而Windows终端使用GBK编码，如果出现乱码可以将输出重定向到文件，然后用utf-8编码查看</li>\n<li><strong>分句：pyltp.SentenceSplitter()</strong></li>\n</ul>\n<blockquote>\n<p>使用时先创建实例：<code>sp = SentenceSplitter()</code>，再进行分句：<code>sents = sp.split(doc)</code></p>\n</blockquote>\n<ul>\n<li><strong>分词：pyltp.Segmentor()</strong></li>\n</ul>\n<blockquote>\n<p>Segmentor加载模型可以用<code>load()</code>也可以用<code>load_with_lexicon()</code>，后者还要加一个用户词典的参数</p>\n</blockquote>\n<ul>\n<li><strong>词性标注：pyltp.Postagger()</strong></li>\n</ul>\n<blockquote>\n<p>直接举个栗子吧：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">sent = <span class=\"hljs-string\">&quot;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&quot;</span><br><span class=\"hljs-comment\"># 加载模型</span><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br>postagger = Postagger()<br>postagger.load(pos_model_path)<br><span class=\"hljs-comment\"># 分词和词性标注</span><br>words = segmentor.segment(sent)<br>postags = postagger.postag(words)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">list</span>(postags))<br><span class=\"hljs-comment\"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br></code></pre></td></tr></table></figure>\n<p>注意：进行这些处理后返回的都是VectorOfString类，是一个可迭代类，也可用list()转换为列表</p>\n<p>词性标注的词性如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724143414199.png\" alt=\"image-20220724143414199\" style=\"zoom: 50%;\" /></p>\n<p>词性标注也可以添加用户词典</p>\n</blockquote>\n<ul>\n<li><strong>NER识别：pyltp.NamedEntityRecognizer()</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> pyltp <span class=\"hljs-keyword\">import</span> NamedEntityRecognizer<br>recognizer = NamedEntityRecognizer()<br>recognizer.load(ner_model_path)<br>ner_results = recognizer.recognize(words, postags)<br></code></pre></td></tr></table></figure>\n<p>LTP 采用 BIESO 标注体系。<strong>B 表示实体开始词，I表示实体中间词，E表示实体结束词，S表示单独成实体，O表示不构成命名实体。</strong></p>\n<p>LTP 提供的命名实体类型为：<strong>人名（Nh）、地名（Ns）、机构名（Ni）</strong>。</p>\n<p>B、I、E、S位置标签和实体类型标签之间用一个横线 <code>-</code> 相连；O标签后没有类型标签。</p>\n</blockquote>\n<ul>\n<li><strong>依存句法分析：pyltp.Parser()</strong></li>\n</ul>\n<blockquote>\n<p>依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系</p>\n<p>首先也是创建实例和加载模型，然后<code>parse_results = parser.parse(words, postags)</code>，得到的结果是一个可迭代对象，其中的每一个元素也是个自定义类（假设parse_results[i]为result），调用<code>result.head</code>和<code>result.relation</code>可分别获得words[i]对应的关系词和关系，其中head是返回关系词所对应的索引+1，如果为0则为”Root”，表示无对应关系词</p>\n<p>举个栗子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">relations = [result.relation <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> parse_results]<br>heads = [words[result.head - <span class=\"hljs-number\">1</span>] <span class=\"hljs-keyword\">if</span> result.head <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">&quot;Root&quot;</span> <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> parse_results]<br><span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(words)):<br> <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;<span class=\"hljs-subst\">&#123;relations[i]&#125;</span> : (<span class=\"hljs-subst\">&#123;words[i]&#125;</span>, <span class=\"hljs-subst\">&#123;heads[i]&#125;</span>)&quot;</span>)<br></code></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<p><div align=center><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211345194.png\" alt=\"image-20220724211345194\" style=\"zoom: 80%;\" /></p>\n<p>依存句法关系如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724153110907.png\" alt=\"image-20220724153110907\" style=\"zoom:50%;\" /></p>\n</blockquote>\n<ul>\n<li><strong>语义角色标注：pyltp.SementicRoleLabeller()</strong></li>\n</ul>\n<blockquote>\n<p>语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等<br>进行语义角色标注时首先同样是创建实例，再加载模型，然后调用方法：<code>roles = labeller.label(words, postags, parse_results)</code>。结果得到一个可迭代对象，<strong>其中的每一个元素也是一个自定义类（假设每个元素为role），则<code>role.index</code>为谓语对应的索引，<code>role.arguments</code>又是一个可迭代对象，每个元素对应一个和该谓语对应的语义角色，也是一个自定义类（假设为argument），</strong>那么可以通过<code>argument.name</code>获取角色和谓语的关系，<code>argument.range.start</code>和<code>argument.range.end</code>对应该角色的开始和结束索引（<strong>end要算上的</strong>）</p>\n<p>给个示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">roles = labeller.label(words, postags, parse_results)<br><span class=\"hljs-keyword\">for</span> role <span class=\"hljs-keyword\">in</span> roles:<br> arguments = role.arguments<br> index = role.index<br> <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;谓语: <span class=\"hljs-subst\">&#123;words[index]&#125;</span> (索引: <span class=\"hljs-subst\">&#123;index&#125;</span>)&quot;</span>)<br> <span class=\"hljs-keyword\">for</span> argument <span class=\"hljs-keyword\">in</span> arguments:<br>     start, end = argument.<span class=\"hljs-built_in\">range</span>.start, argument.<span class=\"hljs-built_in\">range</span>.end<br>     obj = <span class=\"hljs-string\">&quot;&quot;</span><br>     <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words[start : end+<span class=\"hljs-number\">1</span>]:<br>         obj += word<br>     <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;<span class=\"hljs-subst\">&#123;argument.name&#125;</span>: <span class=\"hljs-subst\">&#123;obj&#125;</span> (索引: <span class=\"hljs-subst\">&#123;start&#125;</span>:<span class=\"hljs-subst\">&#123;end&#125;</span>)&quot;</span>)<br> <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;\\n&quot;</span>)<br></code></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211302209.png\" alt=\"image-20220724211302209\" style=\"zoom:67%;\" /></p>\n<p>语义角色如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220724211544134.png\" alt=\"image-20220724211544134\" style=\"zoom:60%;\" /></p>\n</blockquote>\n<h1 id=\"rouge用法\"><a href=\"#rouge用法\" class=\"headerlink\" title=\"rouge用法\"></a>rouge用法</h1><ul>\n<li>rouge和bleu一样也是一种对序列质量的评估标准，但是相比于bleu更关注召回率而非精准率（通过计算F1_score时改变<script type=\"math/tex\">\\beta</script>参数），rouge-n计算公式如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nRough-N=\\frac{\\sum_{S \\in\\{\\text { ReferemceSummaries }\\}} \\sum_{\\text {gram }_{n} \\in S} \\text { Count }_{\\text {match }}\\left(\\text { gram }_{n}\\right)}{\\sum_{S \\in\\{\\text { ReferenceSummaries }\\}} \\text { gram }_{n} \\in S}</script><ul>\n<li>rouge-1和rouge-2都可通过上述公式计算出来，和bleu中的P1和P2计算公式很像。可以在下面看到，每个rough-N都会输出p和r，其实就是精准率和召回率，两者也就是分母不一样（一个分母是refs的n-gram个数，一个分母是hyps的n-gram个数）</li>\n<li>而rouge-l是rouge-N的改进：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nR_{l c s}=\\frac{L C S(X, Y)}{m} \\\\\nP_{l c s}=\\frac{L C S(X, Y)}{n} \\\\\nF_{l c s}=\\frac{\\left(1+\\beta^{2}\\right) R_{l c s} P_{l c s}}{R_{l c s}+\\beta^{2} P_{l c s}}\n\\end{array}</script><p>其中X、Y为hyps和refs，m、n为他们的长度，LCS(X, Y)为X、Y的最长共同序列</p>\n<ul>\n<li><strong>rouge.Rouge.get_scores(hyps, refs, avg=False, ignore_empty=False)</strong></li>\n</ul>\n<blockquote>\n<p><strong>注意：输入的hyps和refs两个字符串，不管是中文英文，都需要每个字或词使用空格间隔</strong></p>\n<p>输出是list(dict(dict))</p>\n</blockquote>\n<ul>\n<li>直接给栗子吧：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> rouge <span class=\"hljs-keyword\">import</span> Rouge <br><br>hypothesis = <span class=\"hljs-string\">&quot;the #### transcript is a written version of each day &#x27;s cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news&quot;</span><br><br>reference = <span class=\"hljs-string\">&quot;this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students &#x27; knowledge of even ts in the news&quot;</span><br><br>rouge = Rouge()<br>scores = rouge.get_scores(hypothesis, reference)<br></code></pre></td></tr></table></figure>\n<p><strong>output：</strong></p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs json\">[<br>  &#123;<br>    <span class=\"hljs-attr\">&quot;rouge-1&quot;</span>: &#123;<br>      <span class=\"hljs-attr\">&quot;f&quot;</span>: <span class=\"hljs-number\">0.4786324739396596</span>,<br>      <span class=\"hljs-attr\">&quot;p&quot;</span>: <span class=\"hljs-number\">0.6363636363636364</span>,<br>      <span class=\"hljs-attr\">&quot;r&quot;</span>: <span class=\"hljs-number\">0.3835616438356164</span><br>    &#125;,<br>    <span class=\"hljs-attr\">&quot;rouge-2&quot;</span>: &#123;<br>      <span class=\"hljs-attr\">&quot;f&quot;</span>: <span class=\"hljs-number\">0.2608695605353498</span>,<br>      <span class=\"hljs-attr\">&quot;p&quot;</span>: <span class=\"hljs-number\">0.3488372093023256</span>,<br>      <span class=\"hljs-attr\">&quot;r&quot;</span>: <span class=\"hljs-number\">0.20833333333333334</span><br>    &#125;,<br>    <span class=\"hljs-attr\">&quot;rouge-l&quot;</span>: &#123;<br>      <span class=\"hljs-attr\">&quot;f&quot;</span>: <span class=\"hljs-number\">0.44705881864636676</span>,<br>      <span class=\"hljs-attr\">&quot;p&quot;</span>: <span class=\"hljs-number\">0.5277777777777778</span>,<br>      <span class=\"hljs-attr\">&quot;r&quot;</span>: <span class=\"hljs-number\">0.3877551020408163</span><br>    &#125;<br>  &#125;<br>]<br></code></pre></td></tr></table></figure>\n<p>其中p、r、f分别为精准率、召回率、F1_score</p>\n<h1 id=\"zhconv用法\"><a href=\"#zhconv用法\" class=\"headerlink\" title=\"zhconv用法\"></a>zhconv用法</h1><ul>\n<li><p>该库用于中文简繁体转换，但是也可以用<strong>OpenCC库，精准度更高、覆盖率更高、速度更快</strong></p>\n</li>\n<li><p><strong>逐字转换：zhconv.convert(s, locale, update=None)</strong></p>\n</li>\n<li><p><strong>基于MediaWiki的转换：zhconv.ocnvert_for_mw(s, locale, update=None)</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>两个函数用法相同</p>\n<p>示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">sent2 = <span class=\"hljs-string\">&quot;計算機軟體&quot;</span><br><span class=\"hljs-built_in\">print</span>(convert(sent2, <span class=\"hljs-string\">&quot;zh-hans&quot;</span>))<br></code></pre></td></tr></table></figure>\n<p>locale可为以下值：</p>\n<p><code>zh-cn</code> 大陆简体、<code>zh-tw</code> 台灣正體、<code>zh-hk</code> 香港繁體、<code>zh-sg</code> 马新简体、<code>zh-hans</code> 简体、<code>zh-hant</code> 繁體</p>\n</blockquote>\n<h1 id=\"gensim用法\"><a href=\"#gensim用法\" class=\"headerlink\" title=\"gensim用法\"></a>gensim用法</h1><ul>\n<li>gensim主要用于创建语料库，和计算各种特征（如相似度、tf-idf）等任务</li>\n</ul>\n<h3 id=\"创建语料库和计算相似度\"><a href=\"#创建语料库和计算相似度\" class=\"headerlink\" title=\"创建语料库和计算相似度\"></a>创建语料库和计算相似度</h3><ul>\n<li><strong>创建语料库类：gensim.corpora.Dictionary(texts)</strong></li>\n</ul>\n<blockquote>\n<p>输入：整个语料库，分好句和分好词的结果，一个list，每个元素又是一个list，里面的每个子元素是一个str词</p>\n<p>输出：一个Dictionary对象</p>\n</blockquote>\n<ul>\n<li><strong>将句子转化为词袋形式：Dictionary.doc2bow(text)</strong></li>\n</ul>\n<blockquote>\n<p>输入：分好词的一个句子，一个list，每个元素是一个str词</p>\n<p>输入：一个list，每个元素是一个二元tuple，tuple[0]是句子中存在的词的索引，tuple[1]是其在句子中出现的次数（出现0次的不计入）</p>\n<p>该类的用法和python自带的字典对象基本相同，values、key、item之类的</p>\n</blockquote>\n<ul>\n<li><strong>获取索引字典：Dictionary.token2id</strong></li>\n</ul>\n<blockquote>\n<p>返回一个{token : id}的字典</p>\n</blockquote>\n<ul>\n<li><strong>创建相似度类：gensim.similarities.Similarity(output_prefix, corpus, num_features)</strong></li>\n</ul>\n<blockquote>\n<p>out_prefix是存储这个对象文件的名称</p>\n<p>corpus是整个语料库，但是必须先转化为词袋模型</p>\n<p>num_fuatures是整个词典的数量，一般使用len(dictionary)表示</p>\n<p>对于得到的对象（取名为similarity），具体的用法为：similarity[test]，其中test为要比较的对象，可以是单个句子，也可以是整个语料库</p>\n</blockquote>\n<p><strong>举个栗子：</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> nltk <span class=\"hljs-keyword\">import</span> word_tokenize<br><span class=\"hljs-keyword\">from</span> gensim.corpora <span class=\"hljs-keyword\">import</span> Dictionary<br><span class=\"hljs-keyword\">from</span> gensim.similarities <span class=\"hljs-keyword\">import</span> Similarity<br><br>sent1 = <span class=\"hljs-string\">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class=\"hljs-string\">&quot;I like running, I love reading.&quot;</span><br>sents = [sent1, sent2]<br><span class=\"hljs-comment\"># 分词</span><br>texts = [word_tokenize(sent) <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> sents]<br><br><span class=\"hljs-comment\"># 创建字典对象</span><br>dictionary = Dictionary(texts)<br><span class=\"hljs-comment\"># 获得词袋模型表示的词料库</span><br>corpus = [dictionary.doc2bow(text) <span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> texts]<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;The corpus is : <span class=\"hljs-subst\">&#123;corpus&#125;</span>&quot;</span>)<br><span class=\"hljs-comment\"># 创建相似度对象</span><br>similarity = Similarity(<span class=\"hljs-string\">&quot;Similarity-excise1&quot;</span>, corpus, num_features=<span class=\"hljs-built_in\">len</span>(dictionary))<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;Created class : <span class=\"hljs-subst\">&#123;similarity&#125;</span>&quot;</span>)<br><span class=\"hljs-comment\"># 计算余弦相似度</span><br>test_corpus = dictionary.doc2bow(word_tokenize(sent1))<br><span class=\"hljs-built_in\">print</span>(similarity[test_corpus])<br></code></pre></td></tr></table></figure>\n<p>输出：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706160148320.png\" alt=\"image-20220706160148320\"></p>\n<h3 id=\"计算TF-IDF\"><a href=\"#计算TF-IDF\" class=\"headerlink\" title=\"计算TF-IDF\"></a>计算TF-IDF</h3><ul>\n<li><strong>首先介绍一下tf-idf：</strong></li>\n</ul>\n<p>词频（term frequency，tf）指的是某一个给定的词语在该文件中对应的频率</p>\n<p>逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量，可由文件总数除以包含该词的文件的总数求得</p>\n<p>具体的计算公式如下：</p>\n<script type=\"math/tex; mode=display\">\n\\mathrm{tf}_{\\mathrm{i}, \\mathrm{j}}=\\frac{n_{i, j}}{\\sum_{k} n_{k, j}} \\\\\n\\operatorname{idf}_{\\mathrm{i}}=\\lg \\frac{|D|}{\\left|\\left\\{j: t_{i} \\in d_{j}\\right\\}\\right|}\\\\\n\\operatorname{tfidf}_{i, j}=\\mathrm{tf}_{\\mathrm{i}, \\mathrm{j}} \\times \\mathrm{idf}_{\\mathrm{i}}</script><p>其中<script type=\"math/tex\">tf_{i, j}</script>为<script type=\"math/tex\">word_i</script>在<script type=\"math/tex\">doc_j</script>中出现的词频，<script type=\"math/tex\">n_{i, j}</script>为<script type=\"math/tex\">word_i</script>在<script type=\"math/tex\">doc_j</script>中出现的个数</p>\n<p><script type=\"math/tex\">idf_i</script>为<script type=\"math/tex\">word_i</script>的逆向文件频率，<script type=\"math/tex\">|D|</script>为文档总数，<script type=\"math/tex\">\\left|\\left\\{j: t_{i} \\in d_{j}\\right\\}\\right|</script>为包含<script type=\"math/tex\">word_i</script>的文档个数</p>\n<p>对于idf，底数可用2、e、10，并且分母可以+1，避免除以0</p>\n<ul>\n<li><strong>创建tfidf模型类：gensim.models.TfidfModel(corpus)</strong></li>\n</ul>\n<blockquote>\n<p>输入为整个语料库，必须先转化为词袋模型</p>\n<p>在具体使用时，和Similarity类似，tfidf_model[test]，其中test可以是单个句子也可以是整个语料库，返回一个列表，列表中每个元素为一个tuple，tuple[0]为词索引，tuple[1]为对应的tfidf值</p>\n<p><strong>gensim算出来的tf-idf是经过了规范化的，每个句子的tfidf值，是一个单位向量</strong></p>\n</blockquote>\n<p><strong>举个栗子：</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># texts的构建是每个段落分词，有三个段落，则len(texts)=3</span><br>texts = [text1, text2, text3]<br>texts = [get_tokens(text) <span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> texts]<br>dictionary = Dictionary(texts)<br>id2token_dict = &#123;v : k <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> dictionary.token2id.items()&#125;<br>corpus = [dictionary.doc2bow(text) <span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> texts]<br>tfidf_model = TfidfModel(corpus)<br>result = tfidf_model[corpus]<br><span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> result:<br>    <span class=\"hljs-built_in\">print</span>(i)<br></code></pre></td></tr></table></figure>\n<p>输出：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220706210417422.png\" alt=\"image-20220706210417422\"></p>\n<h1 id=\"Pandas用法\"><a href=\"#Pandas用法\" class=\"headerlink\" title=\"Pandas用法\"></a>Pandas用法</h1><ul>\n<li><strong>创建DataFrame对象：pd.DataFrame(data, columns, index)</strong></li>\n</ul>\n<blockquote>\n<p>输入：data是一个list, 每个对象是一个tuple或list，tuple的长度和columns的个数对应。data也可以是numpy形式</p>\n<p>index是一个list，是行索引值，可以用于自行设置行索引<strong>（索引没有规定一定是int）</strong></p>\n<p>示例：pd.DataFrame([(1, 2), (3, 4), (5, 6)], columns=[“name”, “NER result”])，输出为：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220711120934316.png\" alt=\"image-20220711120934316\" style=\"zoom: 80%;\" /></p>\n</blockquote>\n<ul>\n<li><strong>获取DataFrame特征：DataFrame.describe()</strong></li>\n<li><strong>获取DataFrame的某列：DataFrame[“column_name”]</strong></li>\n</ul>\n<blockquote>\n<p> 这样可获取到column列，结果是一个Series类（<strong>只有一个列则是Series，多个列则是DataFrame</strong>）</p>\n<p> 如果不想出现重复结果，可调用<code>unique()</code>函数，示例：data[“ner_result”].unique()，返回的结果是一个ndarray，也可使用<code>list()</code>将其转换为列表</p>\n<p> 如果想把这个Series类转换为ndarray，则可使用Series.values，这是Series类其中的一个属性，不是方法，类型是一个ndarrary，也可使用<code>list()</code>转换为列表</p>\n</blockquote>\n<ul>\n<li><p><strong>获取DataFrame列名：DataFrame.columns</strong></p>\n</li>\n<li><p><strong>DataFrame根据某列，进行分组：DataFrame.groupby(“column_name”)</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>将DataFrame通过column_name列进行分组</p>\n<p>结果是一个可迭代的DataFrameGroupBy类，该类的用法和DataFrame几乎一样，比如可使用<code>DataFrameGroupBy[&quot;column_name&quot;]</code>将其转换为SeriesGroupby类</p>\n<p>也可转化为list，转换后，其中每一个元素为一个tuple，tuple[0]为column_name列对应的值，tuple[1]为原DataFrame的一部分，根据column_name的值进行截断。如果是将SeriesGroupby类转换为list，则tuple[1]变为一个Series类</p>\n<p>举个例子：<br>input_data为一个DataFrame，值如下：</p>\n<p><div align=center><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123006189.png\" alt=\"image-20220716123006189\" style=\"zoom:67%;\" /></p>\n<p>现在将input_data根据sent_order列进行分组：input_data.groupby(“sent_order”)，结果如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220716123836380.png\" alt=\"image-20220716123836380\" style=\"zoom: 67%;\" /></p>\n<p>只展示了前三个元组，每个元组，tuple[0]为sent_order的值，tuple[1]为对应的DataFrame</p>\n</blockquote>\n<ul>\n<li><strong>应用自定义方法：DaraFrame.apply(func, axis=0, …)</strong></li>\n</ul>\n<blockquote>\n<p>Series、DataFrame和GroupbyDataFrame等均可使用</p>\n<p>func是一个自定义函数</p>\n<p>axis决定func的输入是什么，axis=0则输入为行，axis=1则输入为列</p>\n<p>Series无需指定axis，DataFrame可调节axis更换操作的维度，<strong>DaraFrameGroupby也无需指定，func的输入为分组后的每个DataFrame，最后再将每个分组返回的结果总和起来</strong></p>\n</blockquote>\n<ul>\n<li><strong>DataFrame的count()方法：</strong></li>\n</ul>\n<blockquote>\n<p><code>count()</code>有很多种用法，结果是可以对Series、DataFrame和GroupbyDataFrame等使用，结果是返回一个Series或DataFrame</p>\n<p>下面举几个示例，其中使用的df如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725142611139.png\" alt=\"image-20220725142611139\" style=\"zoom: 60%;\" /></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df.count()    <span class=\"hljs-comment\"># DataFrame执行count()，对每一列分别执行count，结果返回一个Series</span><br>df.data[<span class=\"hljs-string\">&quot;length&quot;</span>].count()    <span class=\"hljs-comment\"># Series执行count()，直接返回length列的长度，类型为int</span><br>df.groupy(<span class=\"hljs-string\">&quot;length&quot;</span>).count()   <span class=\"hljs-comment\"># 对DataFrameGroupby类执行，返回一个DataFrame，index为length的各指，colums为出length的其他列执行count的结果</span><br>df.groupy(<span class=\"hljs-string\">&quot;length&quot;</span>)[<span class=\"hljs-string\">&quot;evaluation&quot;</span>].count()  <span class=\"hljs-comment\"># 对SeriesGroupby类执行，则只返回evaluation列的count结果，类型为Series</span><br></code></pre></td></tr></table></figure>\n<p>上述四句语句的执行结果：</p>\n<ul>\n<li>第一句：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143928089.png\" alt=\"image-20220725143928089\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>第二句：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143939723.png\" alt=\"image-20220725143939723\" style=\"zoom:67%;\" /></p>\n<ul>\n<li>第三句：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725143953768.png\" alt=\"image-20220725143953768\" style=\"zoom:50%;\" /></p>\n<ul>\n<li>第四句：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220725144005108.png\" alt=\"image-20220725144005108\" style=\"zoom:50%;\" /></p>\n</blockquote>\n<ul>\n<li><strong>读取/保存csv文件：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df.to_csv(file_name, index=<span class=\"hljs-literal\">False</span>)    <span class=\"hljs-comment\"># 要加index=False</span><br>pd.read_csv(file_name)<br></code></pre></td></tr></table></figure>\n<h3 id=\"处理缺失值\"><a href=\"#处理缺失值\" class=\"headerlink\" title=\"处理缺失值\"></a>处理缺失值</h3><ul>\n<li><strong>判断空值：DataFrame.isnull()或DataFrame.notnull()</strong></li>\n</ul>\n<blockquote>\n<p>DataFrame和Series都可用，结果也是返回一个DataFrame或Series，和输入一一对应，其中每个值为布尔值</p>\n</blockquote>\n<ul>\n<li><strong>删除缺失值：DataFrame.dropna(axis=0, inplace=False, how=None)</strong></li>\n</ul>\n<blockquote>\n<p>axis：删除行还是列，{0 or ‘index’, 1 or ‘columns’}</p>\n<p>how : 如果等于any则任何值为空都删除，如果等于all则所有值都为空才删除</p>\n</blockquote>\n<ul>\n<li><strong>填充缺失值：DataFrame.fillna(value, method, axis, …)</strong></li>\n</ul>\n<blockquote>\n<p>value：用于填充的值，可以是单个值，或者字典（用于不同列填充不同值的情况，key是列名，value是值）<br>method : 等于”ffill”使用前一个；不为空的值填充forword fill；等于”bfill”使用后一个不为空的值填充backword fill<br>axis : 按行还是列填充，{0 or ‘index’, 1 or ‘columns’}</p>\n</blockquote>\n<h3 id=\"Padas中DataFrame的增删查改\"><a href=\"#Padas中DataFrame的增删查改\" class=\"headerlink\" title=\"Padas中DataFrame的增删查改\"></a>Padas中DataFrame的增删查改</h3><ul>\n<li><strong>增加列：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df = DataFrame(...)    <span class=\"hljs-comment\"># 有三行数据</span><br>cities = [<span class=\"hljs-string\">&quot;成都&quot;</span>, <span class=\"hljs-string\">&quot;上海&quot;</span>, <span class=\"hljs-string\">&quot;北京&quot;</span>]<br><span class=\"hljs-comment\"># 三种插入方式</span><br>df.insert(<span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">&quot;city&quot;</span>, citys)    <span class=\"hljs-comment\"># 参数分别为：插入列的位置、列名、插入内容</span><br>df[<span class=\"hljs-string\">&quot;city&quot;</span>] = cities<br>df.loc[:, <span class=\"hljs-string\">&quot;city&quot;</span>] = cities<br></code></pre></td></tr></table></figure>\n<ul>\n<li><strong>增加行：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df = DataFrame(...)  <span class=\"hljs-comment\"># 有两列 </span><br>df.loc[<span class=\"hljs-number\">3</span>] = [<span class=\"hljs-string\">&quot;1&quot;</span>, <span class=\"hljs-string\">&quot;2&quot;</span>]       <span class=\"hljs-comment\"># 如果已经存在index=3的行，则修改值；反之直接添加该行</span><br>df = df.append(df_insert)    <span class=\"hljs-comment\"># 合成两个DataFrame，列要相同才行</span><br></code></pre></td></tr></table></figure>\n<ul>\n<li><strong>loc[]和iloc[]的使用：</strong></li>\n</ul>\n<blockquote>\n<p>两者功能是相同的，区别在于loc中对于列，只能使用列名（str类型）进行搜索，而iloc对于列只能使用整数索引进行搜索，DataFrame的查和改基本都是基于两者的</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df[<span class=\"hljs-string\">&quot;column_name&quot;</span>]或df.loc[:, <span class=\"hljs-string\">&quot;column_name&quot;</span>]   <span class=\"hljs-comment\"># 查找column_name列</span><br>df.loc[<span class=\"hljs-string\">&quot;index_name&quot;</span>]    <span class=\"hljs-comment\"># 查找index_name行</span><br>df.loc[<span class=\"hljs-string\">&quot;index_name&quot;</span>, <span class=\"hljs-string\">&quot;column_name&quot;</span>]     <span class=\"hljs-comment\"># 查找(index_name, column_name)处的值</span><br><span class=\"hljs-comment\"># 还可以使用list或者切片来代替，一次操作多行或多列</span><br>df.loc[[<span class=\"hljs-string\">&quot;index_name1&quot;</span>, <span class=\"hljs-string\">&quot;index_name2&quot;</span>], [<span class=\"hljs-string\">&quot;column_name1&quot;</span>, <span class=\"hljs-string\">&quot;column_name2&quot;</span>]]  <span class=\"hljs-comment\"># 同时操作index_name1&amp;2行和column_name1&amp;2列</span><br>df.loc[<span class=\"hljs-number\">0</span>:<span class=\"hljs-number\">3</span>, <span class=\"hljs-string\">&quot;column_name&quot;</span>]    <span class=\"hljs-comment\"># 进行切片，对0、1、2、3行操作</span><br></code></pre></td></tr></table></figure>\n<blockquote>\n<p><code>iloc[]</code>是使用整数列索引，比如：<code>df.iloc[:3, 2:6]</code></p>\n<p>注意<code>loc[]</code>中的切片，是包含了end所指的索引（和list的索引稍有不同），<code>iloc[]</code>是不包含end的索引的</p>\n</blockquote>\n<ul>\n<li><strong>删除：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">df.drop(<span class=\"hljs-number\">0</span>, axis=<span class=\"hljs-number\">0</span>)   <span class=\"hljs-comment\"># 删除第0行</span><br><span class=\"hljs-comment\"># 删除列的三种方法</span><br>df.drop(<span class=\"hljs-string\">&quot;column_name&quot;</span>, axis=<span class=\"hljs-number\">1</span>)<br><span class=\"hljs-keyword\">del</span> df[<span class=\"hljs-string\">&quot;column_name&quot;</span>]<br>ndf = df.pop(<span class=\"hljs-string\">&quot;column_name&quot;</span>)<br><span class=\"hljs-comment\"># 使用drop的时候，第一个参数也可以是list，一次操作多行或多列</span><br>df.drop([<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>], axis=<span class=\"hljs-number\">0</span>)<br>df.drop([<span class=\"hljs-string\">&quot;column_name1&quot;</span>, <span class=\"hljs-string\">&quot;column_name2&quot;</span>], axis=<span class=\"hljs-number\">1</span>)<br></code></pre></td></tr></table></figure>\n<h1 id=\"Pickle用法\"><a href=\"#Pickle用法\" class=\"headerlink\" title=\"Pickle用法\"></a>Pickle用法</h1><ul>\n<li><p><strong>储存pk文件：pickle.dump(obj, file)</strong></p>\n</li>\n<li><p><strong>加载pk文件：pickle.load(file)</strong></p>\n</li>\n</ul>\n<blockquote>\n<p>可以存储和加载对象，其中<code>dump()</code>中的obj是要存的对象，file是<code>open()</code>函数返回的文件描述符</p>\n<p><code>load()</code>返回值为所存储的对象</p>\n<p>储存的文件后缀为.pk</p>\n<p>举个栗子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">a = [<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>]<br>file_name = <span class=\"hljs-string\">&quot;test.pk&quot;</span><br><span class=\"hljs-comment\"># 存</span><br><span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">&quot;wb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>    pickle.dump(a, f)<br><span class=\"hljs-comment\"># 取</span><br><span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">&quot;rb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>    b = pickle.load(f)<br>    <br><span class=\"hljs-comment\"># a和b是一样的</span><br></code></pre></td></tr></table></figure>\n</blockquote>\n<h1 id=\"Json用法\"><a href=\"#Json用法\" class=\"headerlink\" title=\"Json用法\"></a>Json用法</h1><ul>\n<li><p><strong>json可进行Python对象和json格式之间的互换</strong></p>\n</li>\n<li><p><strong>Python转json：json.dumps(obj, ensure_ascii=True, …, indent=None, …)</strong></p>\n</li>\n<li><strong>json转Python：json.loads(s, …)</strong></li>\n</ul>\n<blockquote>\n<p>obj为Python对象，s为json对象</p>\n<p>ensure_ascii：如果为False，则返回值可以包含非 ASCII</p>\n<p>indent：为json中每个元素的间隔数，如果为0或None，则每个元素之间紧凑排版</p>\n<p><strong>注意：</strong></p>\n<p><strong>1. 以上两个函数只是进行对象的转换，而不会进行保存，所以是一般配合<code>open()</code>、<code>f.write()</code>、<code>f.read()</code>使用，给两个示例：</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 存储json对象</span><br><span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">&quot;w&quot;</span>, encoding=<span class=\"hljs-string\">&quot;utf-8&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br> f.write(json.dumps(data, ensure_ascii=<span class=\"hljs-literal\">False</span>, indent=<span class=\"hljs-number\">2</span>))<br><span class=\"hljs-comment\"># 读取json对象</span><br><span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_name, <span class=\"hljs-string\">&#x27;r&#x27;</span>, encoding=<span class=\"hljs-string\">&#x27;utf-8&#x27;</span>) <span class=\"hljs-keyword\">as</span> f:<br> data = json.loads(f.read())<br></code></pre></td></tr></table></figure>\n</blockquote>\n<h1 id=\"Yaml\"><a href=\"#Yaml\" class=\"headerlink\" title=\"Yaml\"></a>Yaml</h1><ul>\n<li><strong>读取yaml文件：</strong><code>config = yaml.safe_load(open(&quot;config.yaml&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;).read())</code></li>\n<li>在.yaml文件中，可用缩进表示层级关系，并且写浮点数不能用python的科学技术法的写法：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_25.png\" alt=\"IMG_25\" style=\"zoom:50%;\" /></p>\n<h1 id=\"Argparse\"><a href=\"#Argparse\" class=\"headerlink\" title=\"Argparse\"></a>Argparse</h1><ul>\n<li><strong>基本流程：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">parser = argparse.ArgumentParser()<br>parser.add_argument(<span class=\"hljs-string\">&quot;--batch_size&quot;</span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>, default=<span class=\"hljs-number\">1</span>) <br>args = parser.parse_args()<br></code></pre></td></tr></table></figure>\n<ul>\n<li>其中<code>--</code>表示可选参数，在运行命令指定<code>--batch_size=xxx</code>即可</li>\n</ul>\n<h1 id=\"Matplotlib用法\"><a href=\"#Matplotlib用法\" class=\"headerlink\" title=\"Matplotlib用法\"></a>Matplotlib用法</h1><ul>\n<li><strong>折线图：plt.plot(x, y)</strong></li>\n<li><strong>直方图：plt.bar(x, y)</strong></li>\n<li><strong>散点图：plt.scatter(x, y)</strong></li>\n</ul>\n<blockquote>\n<p>上述函数还可以指定颜色、大小等参数：</p>\n<ul>\n<li>color：颜色</li>\n</ul>\n<blockquote>\n<p>可为：”b”：蓝色、”g”：绿色、”r”：红色、”c”：青色、”m”：品红、”y”：黄色、”k”：黑色、”w”：白色</p>\n<p>也只直接用全名，如”blue”</p>\n</blockquote>\n<ul>\n<li>s：点的大小</li>\n<li>linewidth：线宽</li>\n<li><p>linestyle：线样式，如设为”dashed”将线设为虚线</p>\n</li>\n<li><p>label：名字， 若要打印label，则使用<code>plt.legend()</code></p>\n</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>设置标题：plt.title()</strong></li>\n</ul>\n<blockquote>\n<p>同样可以使用fontsize指定字体大小</p>\n</blockquote>\n<ul>\n<li><strong>设置x、y轴标题：plt.xlabel()和plt.ylabel()</strong></li>\n<li><strong>保存图片：plt.savefig()</strong></li>\n<li><strong>画完图之后记得关闭：plt.close()</strong></li>\n<li><strong>画一条水平的线：plt.hlines(y, xmin, xmax)</strong></li>\n<li><strong>画一条垂直的线：plt.vlines(x, ymin, ymax)</strong></li>\n<li><strong>在指定座标处加注释：plt.text(x, y, s)</strong></li>\n</ul>\n<h1 id=\"Numpy用法\"><a href=\"#Numpy用法\" class=\"headerlink\" title=\"Numpy用法\"></a>Numpy用法</h1><ul>\n<li><strong>获取最大值的位置（索引）np.argmax(a, axis=None, …)：</strong></li>\n</ul>\n<blockquote>\n<p>axis：如果为None, 则代表把整个数组当作一维的，然后返回最大的索引值。如果指定axis，则只在该维度搜索最大值，示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">array([[<span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-number\">12</span>],<br>       [<span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>]])<br><span class=\"hljs-meta\">&gt;&gt;&gt; </span>np.argmax(a)<br><span class=\"hljs-number\">5</span><br><span class=\"hljs-meta\">&gt;&gt;&gt; </span>np.argmax(a, axis=<span class=\"hljs-number\">0</span>)<br>array([<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>])<br><span class=\"hljs-meta\">&gt;&gt;&gt; </span>np.argmax(a, axis=<span class=\"hljs-number\">1</span>)<br>array([<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>])<br></code></pre></td></tr></table></figure>\n<p>axis指定为哪个维度，则stack哪个维度</p>\n</blockquote>\n<ul>\n<li><strong>减少维度：np.squeeze(a, axis=None)</strong></li>\n</ul>\n<blockquote>\n<p>axis如果为None，则丢弃所有大小为1的维度</p>\n</blockquote>\n<h1 id=\"Tensorflow用法\"><a href=\"#Tensorflow用法\" class=\"headerlink\" title=\"Tensorflow用法\"></a>Tensorflow用法</h1><h3 id=\"禁用GPU\"><a href=\"#禁用GPU\" class=\"headerlink\" title=\"禁用GPU\"></a>禁用GPU</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> os<br>os.environ[<span class=\"hljs-string\">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class=\"hljs-string\">&quot;-1&quot;</span><br></code></pre></td></tr></table></figure>\n<h3 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h3><ul>\n<li><strong>填充序列：tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.)</strong></li>\n</ul>\n<blockquote>\n<p>输入：<br>sequences是要填充的序列</p>\n<p>maxlen是要填充的最大长度，如果为None，则设为所有序列中最长的那个的长度</p>\n<p>padding和truncating可以设为”pre”或者”post”，选择是在序列前面还是后面填充/截断</p>\n<p>value为填充的值</p>\n<p>返回值是一个ndarray</p>\n</blockquote>\n<ul>\n<li><strong>划分训练、测试集：sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</strong></li>\n</ul>\n<blockquote>\n<p>arrays是要划分的数据，可以使用list、numpy array、matric、dataframe<strong>（不能用Tensor）</strong>，示例：</p>\n<p><code>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)</code></p>\n<p>train_size和test_size指定一个就行</p>\n<p>random_state为随机种子</p>\n<p>shuffle决定是否打乱数据</p>\n<p>stratify决定是否进行数据分层，stratify不是bool，是直接指定分层的依据变量，即通过什么来分层，一般就是label或者Series（在输入数据是DataFrame时，stratify指定为Series，即根据那一列来分层）</p>\n<p><strong>输入的数据是什么类型，输出的数据就是什么类型，返回值依次为：x_train, x_test, y_train, y_test</strong></p>\n</blockquote>\n<h3 id=\"各种层\"><a href=\"#各种层\" class=\"headerlink\" title=\"各种层\"></a>各种层</h3><ul>\n<li><strong>tf.keras.layers.Dense(units, activation=None, use_bias=True, …)</strong></li>\n<li><strong>tf.keras.layers.Embedding(input_dim, output_dim,…., mask_zero=False, input_length=None)</strong></li>\n</ul>\n<blockquote>\n<p>input_dim：词汇表大小</p>\n<p>output_dim：词嵌入向量的维度</p>\n<p>mask_zero：决定输入值0是否是应被屏蔽的mask“填充”值。如果为True，则使用mask，相应的input_dim也应该为词汇表大小+1（因为增加了一个mask token，索引为0）</p>\n<p>intput_length：输入序列的长度，如果为None，则自动为最长序列的长度</p>\n</blockquote>\n<ul>\n<li><strong>tf.keras.layers.LSTM(units, activation=”tanh”, recurrent_activation=”sigmoid”, …, return_sequences=False)</strong></li>\n</ul>\n<blockquote>\n<p>其中return_sequences决定是否在最后返回所有时间步的输出，若为False只会返回最后一个时间步的输出，如果下一层需要使用到所有时间步的输出则需要设为True</p>\n</blockquote>\n<ul>\n<li><strong>tf.keras.layers.Bidirectional(layer, merge_mode=”concat”, …)</strong></li>\n</ul>\n<blockquote>\n<p>是RNN的双向包装器，可将单向RNN变为双向RNN</p>\n<p>layer是要包装的层，merge_mode决定前后信息汇总的 方式，可为：”sum”、”mul”、”concat”、”ave”、None，如果为None，则不会合并，而是将他们作为列表输出</p>\n</blockquote>\n<ul>\n<li><strong>tf.keras.layers.Softmax(axis=-1)</strong></li>\n</ul>\n<blockquote>\n<p>axis：决定对那个维度执行softmax，默认为最后一个维度</p>\n</blockquote>\n<h3 id=\"模型的常用方法\"><a href=\"#模型的常用方法\" class=\"headerlink\" title=\"模型的常用方法\"></a>模型的常用方法</h3><ul>\n<li><strong>序列容器：tf.keras.models.Sequential()</strong></li>\n</ul>\n<blockquote>\n<p>可以和<code>add()</code>配合使用，也可以里面直接加个列表Sequential([…])</p>\n</blockquote>\n<ul>\n<li><strong>配置模型：model.compile(optimizer=’rmsprop’, loss=None, metrics=None,…)</strong></li>\n</ul>\n<blockquote>\n<p>optimizer：字符串或者Opitimizer实例</p>\n<p>loss：字符串或Loss示例或一个函数</p>\n<p>metrics：字符串或Metric示例或一个函数</p>\n</blockquote>\n<ul>\n<li><strong>训练模型：model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0., validation_data=None, …)</strong></li>\n</ul>\n<blockquote>\n<p>x/y：可以是numpy数组或者Tensor或list或迭代器等，这样x/y应一一对应。x也可为DataSet、Generator、Sequence，但是这种情况下就不应该指定y，因为y已经在x中了</p>\n<p>batch_size如果未指定则默认32</p>\n<p>callbacks：一个列表，每个元素是一个callbacks中的实例，如EarlyStopping类</p>\n<p>validation_split和validaton_data只需要指定一个，前者是验证集所占比例，后者是直接指定验证集<strong>（如果指定了该参数，则x/y只能Tensor或ndarray）</strong></p>\n</blockquote>\n<ul>\n<li><strong>画模型：tf.keras.utils.plot_model(model, to_file=”model.png”, show_shapes=False, show_dtype=False, show_layer_names=True, …)</strong></li>\n</ul>\n<blockquote>\n<p>model是要打印的模型，一个keras model实例</p>\n</blockquote>\n<ul>\n<li><strong>测试模型：model.evaluate(x=None, y=None, batch_size=None, verbose=1, …)</strong></li>\n</ul>\n<blockquote>\n<p>用法和fit基本一样</p>\n</blockquote>\n<ul>\n<li><strong>采用另一种方法训练模型：</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">with</span> tf.GradientTape() <span class=\"hljs-keyword\">as</span> tape:<br>    predictions = model(inputs, training = <span class=\"hljs-literal\">True</span>)<br>    loss = loss_func(labels, predictions)<br>gradients = tape.gradient(loss, model.trainable_variables)<br>optimizer.apply_gradients(<span class=\"hljs-built_in\">zip</span>(gradients, model.trainable_variables))<br></code></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"保存和加载模型\"><a href=\"#保存和加载模型\" class=\"headerlink\" title=\"保存和加载模型\"></a>保存和加载模型</h3><ul>\n<li>既有keras保存方式，也有tf原生方式保存方式，<strong>但是前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署</strong>。既可以直接保存整个模型，也可以分别保存模型的权重和结构</li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 直接保存</span><br>model.save(<span class=\"hljs-string\">&quot;model.h5&quot;</span>, save_format = <span class=\"hljs-string\">&quot;h5&quot;</span>)<br>model = tf.keras.models.load_model(<span class=\"hljs-string\">&quot;model.h5&quot;</span>)<br><br><span class=\"hljs-comment\"># 分别保存模型的权重和结构</span><br>json_str = model.to_json()<br>model.save_weights(<span class=\"hljs-string\">&quot;model_weights.h5&quot;</span>, save_format = <span class=\"hljs-string\">&quot;h5&quot;</span>)<br>model = tf.keras.models.model_from_json(json_str)<br>model.<span class=\"hljs-built_in\">compile</span>(...)    <span class=\"hljs-comment\"># 这种方法重新加载后需要重新配置优化器</span><br>model.load_weights（<span class=\"hljs-string\">&quot;model_weights.h5&quot;</span>）<br><br><span class=\"hljs-comment\"># 上面演示的是keras保存方式</span><br><span class=\"hljs-comment\"># 要使用tf原生方式，需要在model.save()和model.save_weights()时添加参数save_format=&quot;tf&quot;，并且更改文件后缀</span><br><span class=\"hljs-comment\"># 演示一下：</span><br>model.save_weights(<span class=\"hljs-string\">&#x27;tf_model_weights.ckpt&#x27;</span>,save_format=<span class=\"hljs-string\">&quot;tf&quot;</span>)<br>model.save(<span class=\"hljs-string\">&#x27;tf_model_savedmodel&#x27;</span>, save_format=<span class=\"hljs-string\">&quot;tf&quot;</span>)<br></code></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"模型的反馈\"><a href=\"#模型的反馈\" class=\"headerlink\" title=\"模型的反馈\"></a>模型的反馈</h3><ul>\n<li><strong>设置Early Stop：tf.keras.callbacks.EarlyStopping(monitor=’val_loss’, min_delta=0, patience=0, …, restore_best_weights=False)</strong></li>\n</ul>\n<blockquote>\n<p>monitor：要监视的量，如：”val_loss”、”val_accuracy”、”train_loss”等</p>\n<p>min_delta：若小于min_delta的绝对变化，将被视为没有改进</p>\n<p>patience：容忍可以没有提升的epochs数</p>\n<p>restore_best_weights：是否保存整个训练过程中最好的一个epoch</p>\n</blockquote>\n<ul>\n<li><strong>设置学习率衰减：tf.keras.callbacks.ReduceLROnPlateau(monitor=”val_loss”, factor=0.1, patience=10, verbose=0, …, min_delta=0.0001, …, min_lr=0)</strong></li>\n</ul>\n<blockquote>\n<p>当经过patience个epochs仍没有改进，则降低学习率，学习率*factor来降低</p>\n<p>factor：每次降低的倍数</p>\n<p>min_lr：学习率可降低到的最小值</p>\n</blockquote>\n<h3 id=\"Dataset用法\"><a href=\"#Dataset用法\" class=\"headerlink\" title=\"Dataset用法\"></a>Dataset用法</h3><ul>\n<li>在<code>model.fit</code>中需要可以选择分别用list等类型指定x和y，也可以直接使用Dataset等类指定x而忽略y。对于前者，需要将所有数据一次性读入内存，内存负担较大。而后者可以使用TFRecordDataset类进行外存到内存的流式读取，只需要牺牲小部分IO性能，可大大减轻内存的负担</li>\n<li>简单来说，一个.tfrecords文件包含多个样本<strong>（官方建议每个文件大小100-200M）</strong>，每个样本都对应着一个<code>tf.train.Example</code>类，每个样本可能有多个特征，每个特征用<code>tf.train.Feature()</code>封装</li>\n<li>tf.train.Featrue可接收以下三种类型，大多数其他通用类型也可以强制转换成下面的其中一种：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>tf.train.BytesList：</strong>string、byte<strong>（非标量数据也使用这个，但是需要先将Tensor序列化）</strong></li>\n<li><strong>tf.train.FloatList：</strong>float、double</li>\n<li><strong>tf.train.Int64List：</strong>bool、enum、int32、unit32、int64、uint64g</li>\n</ol>\n</blockquote>\n<ul>\n<li><strong>首先进行.tfrecords文件的存入：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">save_tfrecords</span>(<span class=\"hljs-params\">data, label, desfile</span>):</span><br>    <span class=\"hljs-keyword\">with</span> tf.io.TFRecordWriter(desfile) <span class=\"hljs-keyword\">as</span> writer:<br>        <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(data)):<br>            features = tf.train.Features(<br>                feature = &#123;<br>                    <span class=\"hljs-string\">&quot;data&quot;</span>:tf.train.Feature(bytes_list = tf.train.BytesList(value =[tf.io.serialize_tensor(data[i]).numpy()])),<br>                    <span class=\"hljs-string\">&quot;label&quot;</span>:tf.train.Feature(float_list = tf.train.FloatList(value = label[i])),<br>                &#125;<br>            )<br>            example = tf.train.Example(features = features)<br>            serialized = example.SerializeToString()<br>            writer.write(serialized)<br>   <br><br>save_tfrecords(x_in_sample1, y_in_sample1, <span class=\"hljs-string\">&quot;path1.tfrecords&quot;</span>)<br>save_tfrecords(x_in_sample2, y_in_sample2, <span class=\"hljs-string\">&quot;path2.tfrecords&quot;</span>)<br></code></pre></td></tr></table></figure>\n<blockquote>\n<ul>\n<li>首先要用<code>tf.io.TFRecordWriter()</code>打开写入器，data和label是两个列表，每个元素对应一个Example的特征，desfile是文件名</li>\n<li>对于每个Example的特征，使用一个字典表示，字典还需要被<code>tf.train.Features()</code>封装起来，字典的每个值可以是上述三种类型之一，需要使用<code>tf.train.Feature()</code>封装，其中value的值必须为一个list<strong>（注意前者是Features，后者是Feature）</strong>。并且<strong>由于data是非标量的高维数据，需要先使用<code>tf.io.serialize_tensor()</code>进行序列化，然后再使用其numpy值</strong></li>\n<li>最后通过<code>tf.train.Example</code>封装feature，然后调用<code>SerializeToString()</code>函数，将编码后的字符串写入</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>接下来进行数据读取和使用：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># TFR数据反编译</span><br><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">map_func</span>(<span class=\"hljs-params\">example</span>):</span><br>    feature_description = &#123;<br>        <span class=\"hljs-string\">&#x27;data&#x27;</span>: tf.io.FixedLenFeature([], tf.string),<br>        <span class=\"hljs-string\">&#x27;label&#x27;</span>: tf.io.FixedLenFeature([], tf.float32),<br>    &#125;<br>    parsed_example = tf.io.parse_single_example(example, features=feature_description)<br>    <br>    x_sample = tf.io.parse_tensor(parsed_example[<span class=\"hljs-string\">&#x27;data&#x27;</span>], tf.float32)<br>    y_sample = parsed_example[<span class=\"hljs-string\">&#x27;label&#x27;</span>]<br>    <br>    <span class=\"hljs-keyword\">return</span> x_sample, y_sample<br><br><span class=\"hljs-comment\"># 加载数据集</span><br><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">load_dataset</span>(<span class=\"hljs-params\">filepaths</span>):</span><br>    shuffle_buffer_size = <span class=\"hljs-number\">3000</span><br>    batch_size = <span class=\"hljs-number\">256</span><br><br>    dataset = tf.data.TFRecordDataset(filepaths)<br>    dataset = dataset.shuffle(shuffle_buffer_size)<br>    dataset = dataset.<span class=\"hljs-built_in\">map</span>(map_func=map_func, num_parallel_calls=tf.data.AOTOTUNE)<br>    dataset = dataset.batch(batch_size).prefetch(<span class=\"hljs-number\">64</span>)<br>    <br>    <span class=\"hljs-keyword\">return</span> dataset<br><br><br>train_set = load_dataset([<span class=\"hljs-string\">&quot;path1.tfrecords&quot;</span>,<span class=\"hljs-string\">&quot;path2.tfrecords&quot;</span>])<br>valid_set = load_dataset([<span class=\"hljs-string\">&quot;path3.tfrecords&quot;</span>,<span class=\"hljs-string\">&quot;path4.tfrecords&quot;</span>])<br>model.fit(train_set,epochs=model_epochs, validation_data=valid_set, callbacks=[early_stopping])<br></code></pre></td></tr></table></figure>\n<blockquote>\n<ul>\n<li>首先要生成<code>tf.data.TFRecordDataset</code>类，传入参数是一个list，其中元素是.tfrecords文件名</li>\n<li>每次读取后要先打乱</li>\n<li>之后调用<code>map()</code>，其中指定的函数是<strong>对每个样本操作的</strong>，由于储存时序列化为了二进制字符串，所以数据是什么类型是的信息是丢失了的，解码时需要自己制定数据类型，所以在<code>map()</code>中首先需要指定feature_description，说明每个数据的类型</li>\n<li>然后使用<code>tf.io.parse_single_example()</code>解码单个样本，结果返回一个字典，字典的每个元素对应一个特征，其中<strong>由于data是string类型，由于是编码后的非标量数据，所以还需要使用<code>tf.io.parse_tensor</code>解码一次（非标量数据的shape的编码解码的过程中是会丢失的，所以需要把shape一起储存）</strong></li>\n<li>如果储存的是列表，需要在<code>tf.io.FixedLenFeature()</code>指定shape，也就是(len(list), )</li>\n</ul>\n</blockquote>\n<h1 id=\"keras-bert用法\"><a href=\"#keras-bert用法\" class=\"headerlink\" title=\"keras_bert用法\"></a>keras_bert用法</h1><ul>\n<li><strong>加载预训练模型：keras_bert.load_trained_model_from_checkpoint(config_file, checkpoint_file, training=False, trainable=None, output_layer_num=1, seq_len=int(1e9), kwargs)</strong></li>\n</ul>\n<blockquote>\n<p>config_file和checkpoint_file分别为bert_config.json文件和bert_model.ckpt文件</p>\n<p>training：如果为True，则将返回带有 MLM 和 NSP输出的模型；否则，将返回输入层和最后一个特征提取层。</p>\n<p>trainable：模型是否可训练（也可通过for循环分别制定每层是否可训练）</p>\n<p>seq_len为最大序列长度</p>\n</blockquote>\n<ul>\n<li><strong>编码输入：keras_bert.Tokenizer(token_dict, …)</strong></li>\n</ul>\n<blockquote>\n<p>bert模型的输入有两个，一个是语义token，一个是序列token，需要创建Tokenizer实例，再对句子进行编码，产生两个token</p>\n<p>token_dict：是输入的映射字典，在vocab.txt中可以获取所有token，但是需要自行建立token -&gt; id的字典</p>\n<p>创建Tokenizer实例后，调用<code>Tokenizer.encode(first, second=None, max_len=None)</code>进行编码，结果返回一个list，元素分别为两个list，分别对应两个token</p>\n</blockquote>\n<ul>\n<li><strong>将数据输入模型：</strong></li>\n</ul>\n<blockquote>\n<p>在<code>model.fit()</code>中，由于是多输入，采用<code>x=[train_token_embeddings, train_seq_embeddings]</code>和<code>validation_data=([test_token_embeddings, test_seq_embeddings], test_labels)</code>这种形式进行输入</p>\n</blockquote>\n<ul>\n<li><strong>加载模型时所需要注意的：</strong></li>\n</ul>\n<blockquote>\n<p>加载保存好的模型时同样是使用<code>tf.keras.models.load_model()</code>，但是会出现如下报错：</p>\n<p><strong>ValueError: Unknown layer: TokenEmbedding</strong></p>\n<p>这种情况需要：先<code>from keras_bert import get_custom_objects</code>，再在<code>load_model()</code>时添加参数<code>custom_objects=get_custom_objects()</code></p>\n</blockquote>\n<h1 id=\"bert4keras用法\"><a href=\"#bert4keras用法\" class=\"headerlink\" title=\"bert4keras用法\"></a>bert4keras用法</h1><ul>\n<li><strong>由于keras和tf.keras的兼容问题，所以最开始需要添加环境变量TF_KERAS=1：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> os<br>os.environ[<span class=\"hljs-string\">&quot;TF_KERAS&quot;</span>] = <span class=\"hljs-string\">&quot;1&quot;</span><br></code></pre></td></tr></table></figure>\n<ul>\n<li>直接看<a href=\"https://github.com/Sniper970119/bert4keras_document\">民间的API文档</a>，写的还可以</li>\n</ul>\n<h1 id=\"Pytorch\"><a href=\"#Pytorch\" class=\"headerlink\" title=\"Pytorch\"></a>Pytorch</h1><h3 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h3><ul>\n<li><strong>读取数据</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>自建一个类，继承<code>torch.utils.data.Dataset</code>，还要实现<code>__init__()</code>、<code>__getitem__(index)</code>、<code>__len__()</code>三个函数</li>\n<li>实例化Dataset，并用<code>torch.utils.data.DataLoader</code>封装：<code>DataLoader(data, batch_size=1)</code></li>\n<li>直接for循环取每个batch：<code>for input_ids, attention_mask, labels in data:</code></li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>搭建模型：</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li>继承<code>torch.nn.Module</code>，并实现<code>__init__()</code>、<code>forward()</code></li>\n</ol>\n</blockquote>\n<ul>\n<li><strong>损失函数：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>以交叉熵举例</li>\n<li>创建：<code>loss = torch.nn.CrossEntropyLoss(reduction=&quot;none&quot;).cuda()</code></li>\n<li>其中<code>reduction=Union[“mean”, “sum”, “none”]</code>，表示对每个sample的loss怎么处理</li>\n<li><strong>踩个坑：如果loss的输入（模型的输出置信度）超过两维，如token级别的分类，<code>shape=[batch_size, max_seq_len, num_class]</code>，需要把num_class移到第二维，即执行<code>output.transpose(1, 2)</code>，现在<code>shape=[batch_size, num_class, max_seq_len]</code></strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/IMG_27.png\" alt=\"IMG_27\"></p>\n</blockquote>\n<ul>\n<li><strong>优化器：</strong><code>torch.optim.Adam(model.parameters(), lr=0.01</code></li>\n<li><strong>反向传播：</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">model.zero_grad()<br>loss.backward()<br>optimizer.step()<br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>模型保存和加载：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 可以选择保存整个模型，也可以选择只保存权重</span><br><span class=\"hljs-comment\"># 保存整个模型：</span><br>torch.save(model, file_path)   <span class=\"hljs-comment\"># file_path以.pt .pth结尾</span><br><span class=\"hljs-comment\"># 加载整个模型</span><br>model = torch.load(file_path)<br><br><span class=\"hljs-comment\"># 保存权重</span><br>torch.save(model.state_dict(), file_path)<br><span class=\"hljs-comment\"># 加载权重</span><br>model = MyModel()<br>model.load_state_dict(torch.load(file_path))<br></code></pre></td></tr></table></figure>\n<ul>\n<li><strong>模型训练、测试模式转换：</strong><code>model.train()</code>、<code>model.eval()</code></li>\n</ul>\n<h3 id=\"多卡并行\"><a href=\"#多卡并行\" class=\"headerlink\" title=\"多卡并行\"></a>多卡并行</h3><ul>\n<li><strong>运行命令：</strong><code>python3 -m torch.distributed.launch --nproc_per_node=2 &#123;file_name&#125;</code></li>\n</ul>\n<blockquote>\n<p>其中nproc_per_node代表GPU数量</p>\n</blockquote>\n<ul>\n<li><strong>命令行参数</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li>进程编号：<code>parser.add_argument(&quot;--local_rank&quot;, type=int, default=-1)</code>，local_rank为0代表主进程</li>\n<li>GPU数量：<code>gpu_num = int(os.environ[&#39;WORLD_SIZE&#39;])</code></li>\n<li>需要手动设置环境变量：<code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = “0, ..., gpu_num-1”</code></li>\n</ol>\n</blockquote>\n<ul>\n<li><strong>初始化：</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">torch.cuda.set_device(local_rank)<br><span class=\"hljs-keyword\">if</span> gpu_num &gt; <span class=\"hljs-number\">1</span> :<br> \ttorch.distributed.init_process_group(backend=<span class=\"hljs-string\">&quot;nccl”)</span><br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>读取数据：</strong></li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">data = Dataset()<br>data_sampler = torch.utils.data.distributed.DistributedSampler(data)<br>data = DataLoader(data, batch_size=<span class=\"hljs-number\">1</span>, sampler=data_sampler)   <span class=\"hljs-comment\"># 需要在DataLoader封装时指定sampler</span><br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>模型：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>模型需要用<code>torch.nn.parallel.DistributedDataParallel</code>封装：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=<span class=\"hljs-literal\">False</span>) `<br></code></pre></td></tr></table></figure></li>\n<li>其中find_unused_parameters默认为False，为False效率更高，如果报错则改为True</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>打乱数据：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>在每个epoch开始的时候需要<code>data.sampler.set_epoch(epoch)</code></li>\n<li>是为了让sample打乱数据，使每个epoch的训练数据顺序不同</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>同步每个进程的数据：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>每个进程中的数据是不同步的，如train_loss、val_loss等</li>\n<li>通过<code>torch.distributed.all_reduce(value)</code>来对value进行同步</li>\n<li>同步之后是所有进程该值的和，所以一般需要除以gpu_num</li>\n<li>value必须是Tensor，不能是标量</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>模型保存和加载：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>如果model用DDP封装，保存时保存<code>model.module</code>或者<code>model.module.state_dict()</code></li>\n<li><strong>一般只在主进程进行模型保存、打印、tensorboard记录</strong></li>\n</ul>\n</blockquote>\n<h3 id=\"Huggingface-Transformers\"><a href=\"#Huggingface-Transformers\" class=\"headerlink\" title=\"Huggingface Transformers\"></a>Huggingface Transformers</h3><ul>\n<li><strong>Tokenizer加载：</strong><code>AutoTokenizer.from_pretrained(checkpoint)</code></li>\n<li><strong>Tokenizer编码：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li><code>tokenizer.encode(text, return_tensors=&quot;pt&quot;)</code>：只返回词典映射后的编码结果，<code>text=str</code></li>\n<li><code>tokenizer(texts, padding=&quot;max_length&quot;, truncation=True,max_length=150, return_tensors=&quot;pt”)</code>：返回一个字典，包括模型所需要的所有输入，一般为input_ids和attention_mask。<code>texts=Union[str, list(str)]</code>，<code>padding=Union[“max_length”, True, False]</code></li>\n<li><strong>tokenizer()的结果如果需要组成batch，字典中的每个Tensor需要将第一维去掉：</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">for</span> key, val <span class=\"hljs-keyword\">in</span> inputs.items():<br>    inputs[key] = val.squeeze(<span class=\"hljs-number\">0</span>)<br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>Tokenizer解码：</strong><code>tokenizer.decode(ids, skip_special_tokens=True)</code></li>\n</ul>\n<blockquote>\n<p>其中ids为input_ids</p>\n</blockquote>\n<ul>\n<li><strong>指定Tokenizer填充和截断方向：</strong><blockquote>\n<ul>\n<li><code>tokenizer.padding_side=Union[“left”, “right”]</code></li>\n<li><code>tokenizer.truncation_side=Union[“left”, “right”]</code></li>\n</ul>\n</blockquote>\n</li>\n<li><strong>模型：</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li>加载：<code>AutoModel.from_pretrained(checkpoint)</code></li>\n<li>前向计算：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">inputs = tokenizer().    <span class=\"hljs-comment\"># inputs是一个dict</span><br>output = model(**inputs)。   <span class=\"hljs-comment\"># 输出的output是一个dict</span><br></code></pre></td></tr></table></figure>\n</blockquote>\n<ul>\n<li><strong>生成式模型进行生成预测：</strong><code>model.generate(ids, max_length=None, num_beams=1,do_sample=False, top_k=1, early_stopping=False, num_return_sequences=1)</code></li>\n</ul>\n<blockquote>\n<ul>\n<li>一些生成模型如BloomForCausalModel自带生成函数</li>\n<li>其中ids为encode之后的结果，每次generate的ids只对应一个句子，无法并行</li>\n<li>需要do_sanple=True，才能启用top_k、top_p</li>\n</ul>\n</blockquote>\n<h3 id=\"Tensorboard\"><a href=\"#Tensorboard\" class=\"headerlink\" title=\"Tensorboard\"></a>Tensorboard</h3><ul>\n<li>直接给个🌰：</li>\n</ul>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 初始化</span><br>time = “&#123;<span class=\"hljs-number\">0</span>:%Y-%m-%d-%H:%M:%S/&#125;<span class=\"hljs-string\">&quot;.format(datetime.datetime.now())</span><br><span class=\"hljs-string\">tb_path = config[&quot;</span>tb_root_path<span class=\"hljs-string\">&quot;] + time</span><br><span class=\"hljs-string\">print(f&quot;</span>Start Tensorboard <span class=\"hljs-keyword\">with</span> <span class=\"hljs-string\">&#x27;tensorboard --logdir=&#123;tb_path&#125; --bind_all&#x27;</span>, view at http://localhost:<span class=\"hljs-number\">6006</span>/<span class=\"hljs-string\">&quot;)</span><br><span class=\"hljs-string\">writer = SummaryWriter(tb_path)</span><br><span class=\"hljs-string\"># 保存标量</span><br><span class=\"hljs-string\">writer.add_scalar(&quot;</span>train/loss<span class=\"hljs-string\">&quot;, loss, step)</span><br></code></pre></td></tr></table></figure>\n<ul>\n<li>运行完之后直接运行<code>tensorboard --logdir=&#123;tb_path&#125; --bind_all</code>，然后访问本地6006端口即可</li>\n<li>在多卡时，一般只在主进程采用tensorboard</li>\n</ul>\n</blockquote>\n"},{"title":"决策树总结","math":true,"date":"2022-05-06T16:00:00.000Z","_content":"\n\n\n# 1 基本概念\n\n- 一颗决策树包括一个根结点、若干内部结点和若干叶结点，叶结点对应于决策结果，易知：\n\n> * 每个非叶节点表示一个特征属性测试。\n> * 每个分支代表这个特征属性在某个值域上的输出。\n> * 每个叶子节点存放一个类别。\n> * 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。\n\n- **决策树的构造：**决策树的构造是一个递归的过程，有三种情形会导致递归返回：\n  1. 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别\n  2.  当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别\n  3. 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。\n\n  算法的基本流程如下图所示：\n\n  <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc728ecc27fe.png\" alt=\"2.png\" style=\"zoom:80%;\" />\n\n\n\n\n\n# 2 划分算法\n\n- 各种算法的不同之处在于用什么指标来选择属性，来对每个结点划分\n\n### 2.1 ID3算法\n\n- **信息熵**是度量样本集合程度的最常用的一种指标，设当前样本集合$$D$$中第$$k$$类样本所占比例为$$p_k$$，则$$D$$的信息熵定义为：\n\n$$\n\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k} \\log _{2} p_{k}\n$$\n\n Ent(D)的值越小，D的纯度越高\n\n- 假设属性a有V个可能的取值$$\\{a^1, ..., a^V\\}$$，若用a进行划分，则会产生V个分支结点，其中第v个分支包含的数据集为在D中取$$a = a^v$$的样本集，记为$$D^v$$，则选择使用a进行划分的**信息增益**为： \n\n$$\n\\operatorname{Gain}(D, a)=\\operatorname{Ent}(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Ent}\\left(D^{v}\\right)\n$$\n\n其中$$|D^v|/|D|$$表示给不同的分支赋予权重，即样本数越多的分支结点影响越大。一般而言信息增益越大，代表使用a进行划分获得的“纯度提升”越大。而ID3算法就是选择$$a_{*}=\\underset{a \\in A}{\\arg \\max } \\operatorname{Gain}(D, a)$$\n\n\n\n### 2.2 C4.5算法\n\n- **信息增益准则对可取值数目较多的属性有所偏好**，为减少这种偏好可能带来的不好影响，可以改用**增益率**来进行划分。增益率定义如下：\n\n$$\n\\begin{array}{c}\n\\text { Gain_ratio }(D, a)=\\frac{\\operatorname{Gain}(D, a)}{\\operatorname{IV}(a)} \\\\\n\\operatorname{IV}(a)=-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\log _{2} \\frac{\\left|D^{v}\\right|}{|D|}\n\\end{array}\n$$\n\n其中IV(a)成为属性a的**固有值**，属性a可能取值越多，则IV(a)越大。\n\n- **但是增益率准则对可取值数目较少的属性有所偏好**。所以C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：**先从候选属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的**\n\n\n\n### 2.3 CART算法\n\n- CART算法采用**基尼指数**来选择划分属性，数据集D的纯度可用**基尼值**来度量：\n\n$$\n\\begin{aligned}\n\\operatorname{Gini}(D) &=\\sum_{k=1}^{|\\mathcal{Y}|} \\sum_{k^{\\prime} \\neq k} p_{k} p_{k^{\\prime}} \\\\\n&=1-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k}^{2}\n\\end{aligned}\n$$\n\n直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其标记不一致的概率。因此，Gini(D)越小，则数据集纯度越高\n\n- 属性a的基尼指数定义为：\n\n$$\n\\text { Gini_index }(D, a)=\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Gini}\\left(D^{v}\\right)\n$$\n\n每次选择基尼指数最小的属性$$a_{*}=\\underset{a \\in A}{\\arg \\min } \\text { Gini_index }(D, a)$$\n\n\n\n\n\n# 3 剪枝\n\n- 剪枝(pruning)是决策树学习算法对付 “过拟合” 的主要手段.。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得 “太好”了，以致于把训练集自身 的一些特点当作所有数据都具有的一般性质而导致过拟合。因此, 可通过主动去掉一些分支来降低过拟合的风险\n\n- 剪枝的基本策略有：\n> - **预剪枝：**指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点\n>\n> - **后剪枝：**先从训练集生成一棵完成的决策树，然后自底向上地对非叶结点（每个属性）进行考察，若将该结点对应的子树替换成叶结点能带来决策树泛化性能提升，则将该子树替换成叶结点\n\n- 下面将根据下图（未剪枝的决策树）来讲解预剪枝和后剪枝：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921212202836.png\" alt=\"未剪枝决策树\" style=\"zoom:67%;\" />\n\n### 3.1 预剪枝\n\n- **预剪枝步骤：**\n> 1. 在生成决策树的时候，最开始是在根节点，基于信息增益准则，选择属性脐部进行划分。\n> 2. 首先将该结点当成叶结点，然后根据其包含的数据集，决定该结点类别（选择数据集中最多的类别，好瓜or坏瓜），然后用该决策树在验证集上跑，得出准确率$$acc_i$$。\n> 3. 然后选择用脐部进行属性划分，得到三个分支，再分别将这三个分支当作叶结点，并确定每个叶结点的类别，再用该决策树跑一遍验证集，得到准确率$$acc_j$$\n> 4. 若$$acc_i < acc_j$$则划分，反之不划分（两者相等时，由于“奥卡姆剃刀准则”，是不进行划分的）\n> 5. 重复上述步骤，直到属性选完\n\n预剪枝得到的决策树如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921214432617.png\" alt=\"预剪枝得到的决策树\" style=\"zoom:73%;\" />\n\n- 预剪枝不仅降低了过拟合的风险，还减少了训练和测试时间开销。但是由于预剪枝基于“贪心“本质禁止展开这些分支，**所以可能会带来欠拟合的风险**。并且一些分支可能刚展开时泛化性能会下降，但是后续划分会导致泛化性能提升\n\n\n\n### 3.2 后剪枝\n\n- **后剪枝步骤：**\n\n> 1. 剪枝顺序是自底向上，用例子中的图，则是依次考察6、5、2、3、1结点\n> 2. 每次考察都将该节点替换为叶结点，然后通过对应数据集确定类别，再在验证集上跑，若准确率得到提升或不变，则执行剪枝\n\n后剪枝得到的决策树如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921215744390.png\" alt=\"后剪枝得到的决策树\" style=\"zoom:73%;\" />\n\n- 一般来说，后剪枝欠拟合的风险很小，泛化性能往往优于预剪枝。但是要自底向上对所有非叶结点进行考察，所以训练开销要大得多\n\n\n\n\n\n# 4 连续值处理\n\n- 在对连续值处理时，可采用**连续属性离散化技术**，最简单的策略是采用**二分法**，C4.5决策树算法正使用了这种机制\n- 给定数据集D和连续属性a，将a在D上的取值从小到大排列$${a^1, ..., a^n}$$。基于划分点t可将数据集D分为$$D^+_t$$和$$D^-_t$$，分别代表在属性a上大于t和不大于t的样本。显然对相邻的取值$$a^i$$和$$a^{i+1}$$，选择$$t \\in [a^i, a^{i+1})$$的划分结果都是相同的，所以可以考察n-1个候选划分点：\n\n$$\nT_{a}=\\left\\{\\frac{a^{i}+a^{i+1}}{2} \\mid 1 \\leqslant i \\leqslant n-1\\right\\}\n$$\n\n即把$$[a^i, a^{i+1})$$的中位点作为候选划分点，然后就可以像离散值一样处理，比如计算信息增益：\n$$\n\\begin{aligned}\n\\operatorname{Gain}(D, a) &=\\max _{t \\in T_{a}} \\operatorname{Gain}(D, a, t) \\\\\n&=\\max _{t \\in T_{a}} \\operatorname{Ent}(D)-\\sum_{\\lambda \\in\\{-,+\\}} \\frac{\\left|D_{t}^{\\lambda}\\right|}{|D|} \\operatorname{Ent}\\left(D_{t}^{\\lambda}\\right)\n\\end{aligned}\n$$\n\n- 在选择划分点的时候，可以不使用中位点，而是将中位点换成在训练集中出现过的不大于中位点的最大值，从而使得最终决策树使用的划分点在训练集中出现过\n\n\n\n\n\n# 5 缺失值处理\n\n- 当存在属性值缺失的时候，有两个问题需要解决：(1).如何在属性值缺失的情况下进行划分属性的选择；(2).给定划分属性，若样本在该属性值上缺失，则如何划分该样本\n- 给定数据集D和属性a，$$\\tilde{D}$$表示属性a的值不缺失的样本子集，$$\\tilde{D}^v$$表示$$\\tilde{D}$$的a属性取值为$$a^v$$的样本子集，$$\\tilde{D}_k$$表示$$\\tilde{D}$$的类别为k的子集。我们可以为每个样本赋予一个权重$$w_x$$**（训练开始时将根节点所有样本权重初始化为1）**，并定义：\n\n$$\n\\rho=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}}{ }^{w_{\\boldsymbol{x}}}}{\\sum_{\\boldsymbol{x} \\in D} w_{\\boldsymbol{x}}}\n$$\n\n$$\n\\tilde{p}_{k}=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}_{k}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant k \\leqslant|\\mathcal{Y}|)\n$$\n\n$$\n\\tilde{r}_{v}=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}^{v}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant v \\leqslant V)\n$$\n\n直观地看，对于属性a，$$\\rho$$表示五确实样本所占比例，$$\\tilde{p}_k$$表示无缺失样本中第k类样本所占比例，$$\\tilde{r}_v$$表示无缺失样本中在属性a上取值$$a^v$$的样本所占比例\n\n- 基于上述定义，可将信息增益的公式推广为：\n\n$$\n\\begin{aligned}\n\\operatorname{Gain}(D, a) &=\\rho \\times \\operatorname{Gain}(\\tilde{D}, a) \\\\\n&=\\rho \\times\\left(\\operatorname{Ent}(\\tilde{D})-\\sum_{v=1}^{V} \\tilde{r}_{v} \\operatorname{Ent}\\left(\\tilde{D}^{v}\\right)\\right) \\\\\n& \\operatorname{Ent}(\\tilde{D})=-\\sum_{k=1}^{|\\mathcal{Y}|} \\tilde{p}_{k} \\log _{2} \\tilde{p}_{k}\n\\end{aligned}\n$$\n\n- **若样本$$x$$在a上的取值已知，则正常划入子结点，样本权值仍然保持为$$w_x$$；若在a上的取值未知，则将该样本划入所有子结点，样本权值在每个分支上分别调整为$$\\tilde{r}_{v} \\cdot w_{\\boldsymbol{x}}$$**","source":"_posts/决策树.md","raw":"---\ntitle: 决策树总结\nmath: true\ndate: 2022-5-7\n---\n\n\n\n# 1 基本概念\n\n- 一颗决策树包括一个根结点、若干内部结点和若干叶结点，叶结点对应于决策结果，易知：\n\n> * 每个非叶节点表示一个特征属性测试。\n> * 每个分支代表这个特征属性在某个值域上的输出。\n> * 每个叶子节点存放一个类别。\n> * 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。\n\n- **决策树的构造：**决策树的构造是一个递归的过程，有三种情形会导致递归返回：\n  1. 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别\n  2.  当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别\n  3. 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。\n\n  算法的基本流程如下图所示：\n\n  <img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc728ecc27fe.png\" alt=\"2.png\" style=\"zoom:80%;\" />\n\n\n\n\n\n# 2 划分算法\n\n- 各种算法的不同之处在于用什么指标来选择属性，来对每个结点划分\n\n### 2.1 ID3算法\n\n- **信息熵**是度量样本集合程度的最常用的一种指标，设当前样本集合$$D$$中第$$k$$类样本所占比例为$$p_k$$，则$$D$$的信息熵定义为：\n\n$$\n\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k} \\log _{2} p_{k}\n$$\n\n Ent(D)的值越小，D的纯度越高\n\n- 假设属性a有V个可能的取值$$\\{a^1, ..., a^V\\}$$，若用a进行划分，则会产生V个分支结点，其中第v个分支包含的数据集为在D中取$$a = a^v$$的样本集，记为$$D^v$$，则选择使用a进行划分的**信息增益**为： \n\n$$\n\\operatorname{Gain}(D, a)=\\operatorname{Ent}(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Ent}\\left(D^{v}\\right)\n$$\n\n其中$$|D^v|/|D|$$表示给不同的分支赋予权重，即样本数越多的分支结点影响越大。一般而言信息增益越大，代表使用a进行划分获得的“纯度提升”越大。而ID3算法就是选择$$a_{*}=\\underset{a \\in A}{\\arg \\max } \\operatorname{Gain}(D, a)$$\n\n\n\n### 2.2 C4.5算法\n\n- **信息增益准则对可取值数目较多的属性有所偏好**，为减少这种偏好可能带来的不好影响，可以改用**增益率**来进行划分。增益率定义如下：\n\n$$\n\\begin{array}{c}\n\\text { Gain_ratio }(D, a)=\\frac{\\operatorname{Gain}(D, a)}{\\operatorname{IV}(a)} \\\\\n\\operatorname{IV}(a)=-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\log _{2} \\frac{\\left|D^{v}\\right|}{|D|}\n\\end{array}\n$$\n\n其中IV(a)成为属性a的**固有值**，属性a可能取值越多，则IV(a)越大。\n\n- **但是增益率准则对可取值数目较少的属性有所偏好**。所以C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：**先从候选属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的**\n\n\n\n### 2.3 CART算法\n\n- CART算法采用**基尼指数**来选择划分属性，数据集D的纯度可用**基尼值**来度量：\n\n$$\n\\begin{aligned}\n\\operatorname{Gini}(D) &=\\sum_{k=1}^{|\\mathcal{Y}|} \\sum_{k^{\\prime} \\neq k} p_{k} p_{k^{\\prime}} \\\\\n&=1-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k}^{2}\n\\end{aligned}\n$$\n\n直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其标记不一致的概率。因此，Gini(D)越小，则数据集纯度越高\n\n- 属性a的基尼指数定义为：\n\n$$\n\\text { Gini_index }(D, a)=\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Gini}\\left(D^{v}\\right)\n$$\n\n每次选择基尼指数最小的属性$$a_{*}=\\underset{a \\in A}{\\arg \\min } \\text { Gini_index }(D, a)$$\n\n\n\n\n\n# 3 剪枝\n\n- 剪枝(pruning)是决策树学习算法对付 “过拟合” 的主要手段.。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得 “太好”了，以致于把训练集自身 的一些特点当作所有数据都具有的一般性质而导致过拟合。因此, 可通过主动去掉一些分支来降低过拟合的风险\n\n- 剪枝的基本策略有：\n> - **预剪枝：**指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点\n>\n> - **后剪枝：**先从训练集生成一棵完成的决策树，然后自底向上地对非叶结点（每个属性）进行考察，若将该结点对应的子树替换成叶结点能带来决策树泛化性能提升，则将该子树替换成叶结点\n\n- 下面将根据下图（未剪枝的决策树）来讲解预剪枝和后剪枝：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921212202836.png\" alt=\"未剪枝决策树\" style=\"zoom:67%;\" />\n\n### 3.1 预剪枝\n\n- **预剪枝步骤：**\n> 1. 在生成决策树的时候，最开始是在根节点，基于信息增益准则，选择属性脐部进行划分。\n> 2. 首先将该结点当成叶结点，然后根据其包含的数据集，决定该结点类别（选择数据集中最多的类别，好瓜or坏瓜），然后用该决策树在验证集上跑，得出准确率$$acc_i$$。\n> 3. 然后选择用脐部进行属性划分，得到三个分支，再分别将这三个分支当作叶结点，并确定每个叶结点的类别，再用该决策树跑一遍验证集，得到准确率$$acc_j$$\n> 4. 若$$acc_i < acc_j$$则划分，反之不划分（两者相等时，由于“奥卡姆剃刀准则”，是不进行划分的）\n> 5. 重复上述步骤，直到属性选完\n\n预剪枝得到的决策树如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921214432617.png\" alt=\"预剪枝得到的决策树\" style=\"zoom:73%;\" />\n\n- 预剪枝不仅降低了过拟合的风险，还减少了训练和测试时间开销。但是由于预剪枝基于“贪心“本质禁止展开这些分支，**所以可能会带来欠拟合的风险**。并且一些分支可能刚展开时泛化性能会下降，但是后续划分会导致泛化性能提升\n\n\n\n### 3.2 后剪枝\n\n- **后剪枝步骤：**\n\n> 1. 剪枝顺序是自底向上，用例子中的图，则是依次考察6、5、2、3、1结点\n> 2. 每次考察都将该节点替换为叶结点，然后通过对应数据集确定类别，再在验证集上跑，若准确率得到提升或不变，则执行剪枝\n\n后剪枝得到的决策树如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921215744390.png\" alt=\"后剪枝得到的决策树\" style=\"zoom:73%;\" />\n\n- 一般来说，后剪枝欠拟合的风险很小，泛化性能往往优于预剪枝。但是要自底向上对所有非叶结点进行考察，所以训练开销要大得多\n\n\n\n\n\n# 4 连续值处理\n\n- 在对连续值处理时，可采用**连续属性离散化技术**，最简单的策略是采用**二分法**，C4.5决策树算法正使用了这种机制\n- 给定数据集D和连续属性a，将a在D上的取值从小到大排列$${a^1, ..., a^n}$$。基于划分点t可将数据集D分为$$D^+_t$$和$$D^-_t$$，分别代表在属性a上大于t和不大于t的样本。显然对相邻的取值$$a^i$$和$$a^{i+1}$$，选择$$t \\in [a^i, a^{i+1})$$的划分结果都是相同的，所以可以考察n-1个候选划分点：\n\n$$\nT_{a}=\\left\\{\\frac{a^{i}+a^{i+1}}{2} \\mid 1 \\leqslant i \\leqslant n-1\\right\\}\n$$\n\n即把$$[a^i, a^{i+1})$$的中位点作为候选划分点，然后就可以像离散值一样处理，比如计算信息增益：\n$$\n\\begin{aligned}\n\\operatorname{Gain}(D, a) &=\\max _{t \\in T_{a}} \\operatorname{Gain}(D, a, t) \\\\\n&=\\max _{t \\in T_{a}} \\operatorname{Ent}(D)-\\sum_{\\lambda \\in\\{-,+\\}} \\frac{\\left|D_{t}^{\\lambda}\\right|}{|D|} \\operatorname{Ent}\\left(D_{t}^{\\lambda}\\right)\n\\end{aligned}\n$$\n\n- 在选择划分点的时候，可以不使用中位点，而是将中位点换成在训练集中出现过的不大于中位点的最大值，从而使得最终决策树使用的划分点在训练集中出现过\n\n\n\n\n\n# 5 缺失值处理\n\n- 当存在属性值缺失的时候，有两个问题需要解决：(1).如何在属性值缺失的情况下进行划分属性的选择；(2).给定划分属性，若样本在该属性值上缺失，则如何划分该样本\n- 给定数据集D和属性a，$$\\tilde{D}$$表示属性a的值不缺失的样本子集，$$\\tilde{D}^v$$表示$$\\tilde{D}$$的a属性取值为$$a^v$$的样本子集，$$\\tilde{D}_k$$表示$$\\tilde{D}$$的类别为k的子集。我们可以为每个样本赋予一个权重$$w_x$$**（训练开始时将根节点所有样本权重初始化为1）**，并定义：\n\n$$\n\\rho=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}}{ }^{w_{\\boldsymbol{x}}}}{\\sum_{\\boldsymbol{x} \\in D} w_{\\boldsymbol{x}}}\n$$\n\n$$\n\\tilde{p}_{k}=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}_{k}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant k \\leqslant|\\mathcal{Y}|)\n$$\n\n$$\n\\tilde{r}_{v}=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}^{v}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant v \\leqslant V)\n$$\n\n直观地看，对于属性a，$$\\rho$$表示五确实样本所占比例，$$\\tilde{p}_k$$表示无缺失样本中第k类样本所占比例，$$\\tilde{r}_v$$表示无缺失样本中在属性a上取值$$a^v$$的样本所占比例\n\n- 基于上述定义，可将信息增益的公式推广为：\n\n$$\n\\begin{aligned}\n\\operatorname{Gain}(D, a) &=\\rho \\times \\operatorname{Gain}(\\tilde{D}, a) \\\\\n&=\\rho \\times\\left(\\operatorname{Ent}(\\tilde{D})-\\sum_{v=1}^{V} \\tilde{r}_{v} \\operatorname{Ent}\\left(\\tilde{D}^{v}\\right)\\right) \\\\\n& \\operatorname{Ent}(\\tilde{D})=-\\sum_{k=1}^{|\\mathcal{Y}|} \\tilde{p}_{k} \\log _{2} \\tilde{p}_{k}\n\\end{aligned}\n$$\n\n- **若样本$$x$$在a上的取值已知，则正常划入子结点，样本权值仍然保持为$$w_x$$；若在a上的取值未知，则将该样本划入所有子结点，样本权值在每个分支上分别调整为$$\\tilde{r}_{v} \\cdot w_{\\boldsymbol{x}}$$**","slug":"决策树","published":1,"updated":"2022-12-20T06:20:49.282Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1k000h7csz66ifbs9t","content":"<h1 id=\"1-基本概念\"><a href=\"#1-基本概念\" class=\"headerlink\" title=\"1 基本概念\"></a>1 基本概念</h1><ul>\n<li>一颗决策树包括一个根结点、若干内部结点和若干叶结点，叶结点对应于决策结果，易知：</li>\n</ul>\n<blockquote>\n<ul>\n<li>每个非叶节点表示一个特征属性测试。</li>\n<li>每个分支代表这个特征属性在某个值域上的输出。</li>\n<li>每个叶子节点存放一个类别。</li>\n<li>每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</li>\n</ul>\n</blockquote>\n<ul>\n<li><p><strong>决策树的构造：</strong>决策树的构造是一个递归的过程，有三种情形会导致递归返回：</p>\n<ol>\n<li>当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别</li>\n<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别</li>\n<li>当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。</li>\n</ol>\n<p>算法的基本流程如下图所示：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc728ecc27fe.png\" alt=\"2.png\" style=\"zoom:80%;\" /></p>\n</li>\n</ul>\n<h1 id=\"2-划分算法\"><a href=\"#2-划分算法\" class=\"headerlink\" title=\"2 划分算法\"></a>2 划分算法</h1><ul>\n<li>各种算法的不同之处在于用什么指标来选择属性，来对每个结点划分</li>\n</ul>\n<h3 id=\"2-1-ID3算法\"><a href=\"#2-1-ID3算法\" class=\"headerlink\" title=\"2.1 ID3算法\"></a>2.1 ID3算法</h3><ul>\n<li><strong>信息熵</strong>是度量样本集合程度的最常用的一种指标，设当前样本集合<script type=\"math/tex\">D</script>中第<script type=\"math/tex\">k</script>类样本所占比例为<script type=\"math/tex\">p_k</script>，则<script type=\"math/tex\">D</script>的信息熵定义为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k} \\log _{2} p_{k}</script><p> Ent(D)的值越小，D的纯度越高</p>\n<ul>\n<li>假设属性a有V个可能的取值<script type=\"math/tex\">\\{a^1, ..., a^V\\}</script>，若用a进行划分，则会产生V个分支结点，其中第v个分支包含的数据集为在D中取<script type=\"math/tex\">a = a^v</script>的样本集，记为<script type=\"math/tex\">D^v</script>，则选择使用a进行划分的<strong>信息增益</strong>为： </li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{Gain}(D, a)=\\operatorname{Ent}(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Ent}\\left(D^{v}\\right)</script><p>其中<script type=\"math/tex\">|D^v|/|D|</script>表示给不同的分支赋予权重，即样本数越多的分支结点影响越大。一般而言信息增益越大，代表使用a进行划分获得的“纯度提升”越大。而ID3算法就是选择<script type=\"math/tex\">a_{*}=\\underset{a \\in A}{\\arg \\max } \\operatorname{Gain}(D, a)</script></p>\n<h3 id=\"2-2-C4-5算法\"><a href=\"#2-2-C4-5算法\" class=\"headerlink\" title=\"2.2 C4.5算法\"></a>2.2 C4.5算法</h3><ul>\n<li><strong>信息增益准则对可取值数目较多的属性有所偏好</strong>，为减少这种偏好可能带来的不好影响，可以改用<strong>增益率</strong>来进行划分。增益率定义如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\text { Gain_ratio }(D, a)=\\frac{\\operatorname{Gain}(D, a)}{\\operatorname{IV}(a)} \\\\\n\\operatorname{IV}(a)=-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\log _{2} \\frac{\\left|D^{v}\\right|}{|D|}\n\\end{array}</script><p>其中IV(a)成为属性a的<strong>固有值</strong>，属性a可能取值越多，则IV(a)越大。</p>\n<ul>\n<li><strong>但是增益率准则对可取值数目较少的属性有所偏好</strong>。所以C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：<strong>先从候选属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的</strong></li>\n</ul>\n<h3 id=\"2-3-CART算法\"><a href=\"#2-3-CART算法\" class=\"headerlink\" title=\"2.3 CART算法\"></a>2.3 CART算法</h3><ul>\n<li>CART算法采用<strong>基尼指数</strong>来选择划分属性，数据集D的纯度可用<strong>基尼值</strong>来度量：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{Gini}(D) &=\\sum_{k=1}^{|\\mathcal{Y}|} \\sum_{k^{\\prime} \\neq k} p_{k} p_{k^{\\prime}} \\\\\n&=1-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k}^{2}\n\\end{aligned}</script><p>直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其标记不一致的概率。因此，Gini(D)越小，则数据集纯度越高</p>\n<ul>\n<li>属性a的基尼指数定义为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\text { Gini_index }(D, a)=\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Gini}\\left(D^{v}\\right)</script><p>每次选择基尼指数最小的属性<script type=\"math/tex\">a_{*}=\\underset{a \\in A}{\\arg \\min } \\text { Gini_index }(D, a)</script></p>\n<h1 id=\"3-剪枝\"><a href=\"#3-剪枝\" class=\"headerlink\" title=\"3 剪枝\"></a>3 剪枝</h1><ul>\n<li><p>剪枝(pruning)是决策树学习算法对付 “过拟合” 的主要手段.。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得 “太好”了，以致于把训练集自身 的一些特点当作所有数据都具有的一般性质而导致过拟合。因此, 可通过主动去掉一些分支来降低过拟合的风险</p>\n</li>\n<li><p>剪枝的基本策略有：</p>\n<blockquote>\n<ul>\n<li><p><strong>预剪枝：</strong>指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点</p>\n</li>\n<li><p><strong>后剪枝：</strong>先从训练集生成一棵完成的决策树，然后自底向上地对非叶结点（每个属性）进行考察，若将该结点对应的子树替换成叶结点能带来决策树泛化性能提升，则将该子树替换成叶结点</p>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>下面将根据下图（未剪枝的决策树）来讲解预剪枝和后剪枝：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921212202836.png\" alt=\"未剪枝决策树\" style=\"zoom:67%;\" /></p>\n<h3 id=\"3-1-预剪枝\"><a href=\"#3-1-预剪枝\" class=\"headerlink\" title=\"3.1 预剪枝\"></a>3.1 预剪枝</h3><ul>\n<li><strong>预剪枝步骤：</strong><blockquote>\n<ol>\n<li>在生成决策树的时候，最开始是在根节点，基于信息增益准则，选择属性脐部进行划分。</li>\n<li>首先将该结点当成叶结点，然后根据其包含的数据集，决定该结点类别（选择数据集中最多的类别，好瓜or坏瓜），然后用该决策树在验证集上跑，得出准确率<script type=\"math/tex\">acc_i</script>。</li>\n<li>然后选择用脐部进行属性划分，得到三个分支，再分别将这三个分支当作叶结点，并确定每个叶结点的类别，再用该决策树跑一遍验证集，得到准确率<script type=\"math/tex\">acc_j</script></li>\n<li>若<script type=\"math/tex\">acc_i < acc_j</script>则划分，反之不划分（两者相等时，由于“奥卡姆剃刀准则”，是不进行划分的）</li>\n<li>重复上述步骤，直到属性选完</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n<p>预剪枝得到的决策树如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921214432617.png\" alt=\"预剪枝得到的决策树\" style=\"zoom:73%;\" /></p>\n<ul>\n<li>预剪枝不仅降低了过拟合的风险，还减少了训练和测试时间开销。但是由于预剪枝基于“贪心“本质禁止展开这些分支，<strong>所以可能会带来欠拟合的风险</strong>。并且一些分支可能刚展开时泛化性能会下降，但是后续划分会导致泛化性能提升</li>\n</ul>\n<h3 id=\"3-2-后剪枝\"><a href=\"#3-2-后剪枝\" class=\"headerlink\" title=\"3.2 后剪枝\"></a>3.2 后剪枝</h3><ul>\n<li><strong>后剪枝步骤：</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li>剪枝顺序是自底向上，用例子中的图，则是依次考察6、5、2、3、1结点</li>\n<li>每次考察都将该节点替换为叶结点，然后通过对应数据集确定类别，再在验证集上跑，若准确率得到提升或不变，则执行剪枝</li>\n</ol>\n</blockquote>\n<p>后剪枝得到的决策树如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921215744390.png\" alt=\"后剪枝得到的决策树\" style=\"zoom:73%;\" /></p>\n<ul>\n<li>一般来说，后剪枝欠拟合的风险很小，泛化性能往往优于预剪枝。但是要自底向上对所有非叶结点进行考察，所以训练开销要大得多</li>\n</ul>\n<h1 id=\"4-连续值处理\"><a href=\"#4-连续值处理\" class=\"headerlink\" title=\"4 连续值处理\"></a>4 连续值处理</h1><ul>\n<li>在对连续值处理时，可采用<strong>连续属性离散化技术</strong>，最简单的策略是采用<strong>二分法</strong>，C4.5决策树算法正使用了这种机制</li>\n<li>给定数据集D和连续属性a，将a在D上的取值从小到大排列<script type=\"math/tex\">{a^1, ..., a^n}</script>。基于划分点t可将数据集D分为<script type=\"math/tex\">D^+_t</script>和<script type=\"math/tex\">D^-_t</script>，分别代表在属性a上大于t和不大于t的样本。显然对相邻的取值<script type=\"math/tex\">a^i</script>和<script type=\"math/tex\">a^{i+1}</script>，选择<script type=\"math/tex\">t \\in [a^i, a^{i+1})</script>的划分结果都是相同的，所以可以考察n-1个候选划分点：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nT_{a}=\\left\\{\\frac{a^{i}+a^{i+1}}{2} \\mid 1 \\leqslant i \\leqslant n-1\\right\\}</script><p>即把<script type=\"math/tex\">[a^i, a^{i+1})</script>的中位点作为候选划分点，然后就可以像离散值一样处理，比如计算信息增益：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{Gain}(D, a) &=\\max _{t \\in T_{a}} \\operatorname{Gain}(D, a, t) \\\\\n&=\\max _{t \\in T_{a}} \\operatorname{Ent}(D)-\\sum_{\\lambda \\in\\{-,+\\}} \\frac{\\left|D_{t}^{\\lambda}\\right|}{|D|} \\operatorname{Ent}\\left(D_{t}^{\\lambda}\\right)\n\\end{aligned}</script><ul>\n<li>在选择划分点的时候，可以不使用中位点，而是将中位点换成在训练集中出现过的不大于中位点的最大值，从而使得最终决策树使用的划分点在训练集中出现过</li>\n</ul>\n<h1 id=\"5-缺失值处理\"><a href=\"#5-缺失值处理\" class=\"headerlink\" title=\"5 缺失值处理\"></a>5 缺失值处理</h1><ul>\n<li>当存在属性值缺失的时候，有两个问题需要解决：(1).如何在属性值缺失的情况下进行划分属性的选择；(2).给定划分属性，若样本在该属性值上缺失，则如何划分该样本</li>\n<li>给定数据集D和属性a，<script type=\"math/tex\">\\tilde{D}</script>表示属性a的值不缺失的样本子集，<script type=\"math/tex\">\\tilde{D}^v</script>表示<script type=\"math/tex\">\\tilde{D}</script>的a属性取值为<script type=\"math/tex\">a^v</script>的样本子集，<script type=\"math/tex\">\\tilde{D}_k</script>表示<script type=\"math/tex\">\\tilde{D}</script>的类别为k的子集。我们可以为每个样本赋予一个权重<script type=\"math/tex\">w_x</script><strong>（训练开始时将根节点所有样本权重初始化为1）</strong>，并定义：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\rho=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}}{ }^{w_{\\boldsymbol{x}}}}{\\sum_{\\boldsymbol{x} \\in D} w_{\\boldsymbol{x}}}</script><script type=\"math/tex; mode=display\">\n\\tilde{p}_{k}=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}_{k}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant k \\leqslant|\\mathcal{Y}|)</script><script type=\"math/tex; mode=display\">\n\\tilde{r}_{v}=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}^{v}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant v \\leqslant V)</script><p>直观地看，对于属性a，<script type=\"math/tex\">\\rho</script>表示五确实样本所占比例，<script type=\"math/tex\">\\tilde{p}_k</script>表示无缺失样本中第k类样本所占比例，<script type=\"math/tex\">\\tilde{r}_v</script>表示无缺失样本中在属性a上取值<script type=\"math/tex\">a^v</script>的样本所占比例</p>\n<ul>\n<li>基于上述定义，可将信息增益的公式推广为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{Gain}(D, a) &=\\rho \\times \\operatorname{Gain}(\\tilde{D}, a) \\\\\n&=\\rho \\times\\left(\\operatorname{Ent}(\\tilde{D})-\\sum_{v=1}^{V} \\tilde{r}_{v} \\operatorname{Ent}\\left(\\tilde{D}^{v}\\right)\\right) \\\\\n& \\operatorname{Ent}(\\tilde{D})=-\\sum_{k=1}^{|\\mathcal{Y}|} \\tilde{p}_{k} \\log _{2} \\tilde{p}_{k}\n\\end{aligned}</script><ul>\n<li><strong>若样本<script type=\"math/tex\">x</script>在a上的取值已知，则正常划入子结点，样本权值仍然保持为<script type=\"math/tex\">w_x</script>；若在a上的取值未知，则将该样本划入所有子结点，样本权值在每个分支上分别调整为<script type=\"math/tex\">\\tilde{r}_{v} \\cdot w_{\\boldsymbol{x}}</script></strong></li>\n</ul>\n","site":{"data":{}},"wordcount":4498,"excerpt":"","more":"<h1 id=\"1-基本概念\"><a href=\"#1-基本概念\" class=\"headerlink\" title=\"1 基本概念\"></a>1 基本概念</h1><ul>\n<li>一颗决策树包括一个根结点、若干内部结点和若干叶结点，叶结点对应于决策结果，易知：</li>\n</ul>\n<blockquote>\n<ul>\n<li>每个非叶节点表示一个特征属性测试。</li>\n<li>每个分支代表这个特征属性在某个值域上的输出。</li>\n<li>每个叶子节点存放一个类别。</li>\n<li>每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</li>\n</ul>\n</blockquote>\n<ul>\n<li><p><strong>决策树的构造：</strong>决策树的构造是一个递归的过程，有三种情形会导致递归返回：</p>\n<ol>\n<li>当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别</li>\n<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别</li>\n<li>当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。</li>\n</ol>\n<p>算法的基本流程如下图所示：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/5bc728ecc27fe.png\" alt=\"2.png\" style=\"zoom:80%;\" /></p>\n</li>\n</ul>\n<h1 id=\"2-划分算法\"><a href=\"#2-划分算法\" class=\"headerlink\" title=\"2 划分算法\"></a>2 划分算法</h1><ul>\n<li>各种算法的不同之处在于用什么指标来选择属性，来对每个结点划分</li>\n</ul>\n<h3 id=\"2-1-ID3算法\"><a href=\"#2-1-ID3算法\" class=\"headerlink\" title=\"2.1 ID3算法\"></a>2.1 ID3算法</h3><ul>\n<li><strong>信息熵</strong>是度量样本集合程度的最常用的一种指标，设当前样本集合<script type=\"math/tex\">D</script>中第<script type=\"math/tex\">k</script>类样本所占比例为<script type=\"math/tex\">p_k</script>，则<script type=\"math/tex\">D</script>的信息熵定义为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k} \\log _{2} p_{k}</script><p> Ent(D)的值越小，D的纯度越高</p>\n<ul>\n<li>假设属性a有V个可能的取值<script type=\"math/tex\">\\{a^1, ..., a^V\\}</script>，若用a进行划分，则会产生V个分支结点，其中第v个分支包含的数据集为在D中取<script type=\"math/tex\">a = a^v</script>的样本集，记为<script type=\"math/tex\">D^v</script>，则选择使用a进行划分的<strong>信息增益</strong>为： </li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{Gain}(D, a)=\\operatorname{Ent}(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Ent}\\left(D^{v}\\right)</script><p>其中<script type=\"math/tex\">|D^v|/|D|</script>表示给不同的分支赋予权重，即样本数越多的分支结点影响越大。一般而言信息增益越大，代表使用a进行划分获得的“纯度提升”越大。而ID3算法就是选择<script type=\"math/tex\">a_{*}=\\underset{a \\in A}{\\arg \\max } \\operatorname{Gain}(D, a)</script></p>\n<h3 id=\"2-2-C4-5算法\"><a href=\"#2-2-C4-5算法\" class=\"headerlink\" title=\"2.2 C4.5算法\"></a>2.2 C4.5算法</h3><ul>\n<li><strong>信息增益准则对可取值数目较多的属性有所偏好</strong>，为减少这种偏好可能带来的不好影响，可以改用<strong>增益率</strong>来进行划分。增益率定义如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\text { Gain_ratio }(D, a)=\\frac{\\operatorname{Gain}(D, a)}{\\operatorname{IV}(a)} \\\\\n\\operatorname{IV}(a)=-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\log _{2} \\frac{\\left|D^{v}\\right|}{|D|}\n\\end{array}</script><p>其中IV(a)成为属性a的<strong>固有值</strong>，属性a可能取值越多，则IV(a)越大。</p>\n<ul>\n<li><strong>但是增益率准则对可取值数目较少的属性有所偏好</strong>。所以C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：<strong>先从候选属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的</strong></li>\n</ul>\n<h3 id=\"2-3-CART算法\"><a href=\"#2-3-CART算法\" class=\"headerlink\" title=\"2.3 CART算法\"></a>2.3 CART算法</h3><ul>\n<li>CART算法采用<strong>基尼指数</strong>来选择划分属性，数据集D的纯度可用<strong>基尼值</strong>来度量：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{Gini}(D) &=\\sum_{k=1}^{|\\mathcal{Y}|} \\sum_{k^{\\prime} \\neq k} p_{k} p_{k^{\\prime}} \\\\\n&=1-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k}^{2}\n\\end{aligned}</script><p>直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其标记不一致的概率。因此，Gini(D)越小，则数据集纯度越高</p>\n<ul>\n<li>属性a的基尼指数定义为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\text { Gini_index }(D, a)=\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Gini}\\left(D^{v}\\right)</script><p>每次选择基尼指数最小的属性<script type=\"math/tex\">a_{*}=\\underset{a \\in A}{\\arg \\min } \\text { Gini_index }(D, a)</script></p>\n<h1 id=\"3-剪枝\"><a href=\"#3-剪枝\" class=\"headerlink\" title=\"3 剪枝\"></a>3 剪枝</h1><ul>\n<li><p>剪枝(pruning)是决策树学习算法对付 “过拟合” 的主要手段.。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得 “太好”了，以致于把训练集自身 的一些特点当作所有数据都具有的一般性质而导致过拟合。因此, 可通过主动去掉一些分支来降低过拟合的风险</p>\n</li>\n<li><p>剪枝的基本策略有：</p>\n<blockquote>\n<ul>\n<li><p><strong>预剪枝：</strong>指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点</p>\n</li>\n<li><p><strong>后剪枝：</strong>先从训练集生成一棵完成的决策树，然后自底向上地对非叶结点（每个属性）进行考察，若将该结点对应的子树替换成叶结点能带来决策树泛化性能提升，则将该子树替换成叶结点</p>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>下面将根据下图（未剪枝的决策树）来讲解预剪枝和后剪枝：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921212202836.png\" alt=\"未剪枝决策树\" style=\"zoom:67%;\" /></p>\n<h3 id=\"3-1-预剪枝\"><a href=\"#3-1-预剪枝\" class=\"headerlink\" title=\"3.1 预剪枝\"></a>3.1 预剪枝</h3><ul>\n<li><strong>预剪枝步骤：</strong><blockquote>\n<ol>\n<li>在生成决策树的时候，最开始是在根节点，基于信息增益准则，选择属性脐部进行划分。</li>\n<li>首先将该结点当成叶结点，然后根据其包含的数据集，决定该结点类别（选择数据集中最多的类别，好瓜or坏瓜），然后用该决策树在验证集上跑，得出准确率<script type=\"math/tex\">acc_i</script>。</li>\n<li>然后选择用脐部进行属性划分，得到三个分支，再分别将这三个分支当作叶结点，并确定每个叶结点的类别，再用该决策树跑一遍验证集，得到准确率<script type=\"math/tex\">acc_j</script></li>\n<li>若<script type=\"math/tex\">acc_i < acc_j</script>则划分，反之不划分（两者相等时，由于“奥卡姆剃刀准则”，是不进行划分的）</li>\n<li>重复上述步骤，直到属性选完</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n<p>预剪枝得到的决策树如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921214432617.png\" alt=\"预剪枝得到的决策树\" style=\"zoom:73%;\" /></p>\n<ul>\n<li>预剪枝不仅降低了过拟合的风险，还减少了训练和测试时间开销。但是由于预剪枝基于“贪心“本质禁止展开这些分支，<strong>所以可能会带来欠拟合的风险</strong>。并且一些分支可能刚展开时泛化性能会下降，但是后续划分会导致泛化性能提升</li>\n</ul>\n<h3 id=\"3-2-后剪枝\"><a href=\"#3-2-后剪枝\" class=\"headerlink\" title=\"3.2 后剪枝\"></a>3.2 后剪枝</h3><ul>\n<li><strong>后剪枝步骤：</strong></li>\n</ul>\n<blockquote>\n<ol>\n<li>剪枝顺序是自底向上，用例子中的图，则是依次考察6、5、2、3、1结点</li>\n<li>每次考察都将该节点替换为叶结点，然后通过对应数据集确定类别，再在验证集上跑，若准确率得到提升或不变，则执行剪枝</li>\n</ol>\n</blockquote>\n<p>后剪枝得到的决策树如下：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220921215744390.png\" alt=\"后剪枝得到的决策树\" style=\"zoom:73%;\" /></p>\n<ul>\n<li>一般来说，后剪枝欠拟合的风险很小，泛化性能往往优于预剪枝。但是要自底向上对所有非叶结点进行考察，所以训练开销要大得多</li>\n</ul>\n<h1 id=\"4-连续值处理\"><a href=\"#4-连续值处理\" class=\"headerlink\" title=\"4 连续值处理\"></a>4 连续值处理</h1><ul>\n<li>在对连续值处理时，可采用<strong>连续属性离散化技术</strong>，最简单的策略是采用<strong>二分法</strong>，C4.5决策树算法正使用了这种机制</li>\n<li>给定数据集D和连续属性a，将a在D上的取值从小到大排列<script type=\"math/tex\">{a^1, ..., a^n}</script>。基于划分点t可将数据集D分为<script type=\"math/tex\">D^+_t</script>和<script type=\"math/tex\">D^-_t</script>，分别代表在属性a上大于t和不大于t的样本。显然对相邻的取值<script type=\"math/tex\">a^i</script>和<script type=\"math/tex\">a^{i+1}</script>，选择<script type=\"math/tex\">t \\in [a^i, a^{i+1})</script>的划分结果都是相同的，所以可以考察n-1个候选划分点：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nT_{a}=\\left\\{\\frac{a^{i}+a^{i+1}}{2} \\mid 1 \\leqslant i \\leqslant n-1\\right\\}</script><p>即把<script type=\"math/tex\">[a^i, a^{i+1})</script>的中位点作为候选划分点，然后就可以像离散值一样处理，比如计算信息增益：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{Gain}(D, a) &=\\max _{t \\in T_{a}} \\operatorname{Gain}(D, a, t) \\\\\n&=\\max _{t \\in T_{a}} \\operatorname{Ent}(D)-\\sum_{\\lambda \\in\\{-,+\\}} \\frac{\\left|D_{t}^{\\lambda}\\right|}{|D|} \\operatorname{Ent}\\left(D_{t}^{\\lambda}\\right)\n\\end{aligned}</script><ul>\n<li>在选择划分点的时候，可以不使用中位点，而是将中位点换成在训练集中出现过的不大于中位点的最大值，从而使得最终决策树使用的划分点在训练集中出现过</li>\n</ul>\n<h1 id=\"5-缺失值处理\"><a href=\"#5-缺失值处理\" class=\"headerlink\" title=\"5 缺失值处理\"></a>5 缺失值处理</h1><ul>\n<li>当存在属性值缺失的时候，有两个问题需要解决：(1).如何在属性值缺失的情况下进行划分属性的选择；(2).给定划分属性，若样本在该属性值上缺失，则如何划分该样本</li>\n<li>给定数据集D和属性a，<script type=\"math/tex\">\\tilde{D}</script>表示属性a的值不缺失的样本子集，<script type=\"math/tex\">\\tilde{D}^v</script>表示<script type=\"math/tex\">\\tilde{D}</script>的a属性取值为<script type=\"math/tex\">a^v</script>的样本子集，<script type=\"math/tex\">\\tilde{D}_k</script>表示<script type=\"math/tex\">\\tilde{D}</script>的类别为k的子集。我们可以为每个样本赋予一个权重<script type=\"math/tex\">w_x</script><strong>（训练开始时将根节点所有样本权重初始化为1）</strong>，并定义：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\rho=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}}{ }^{w_{\\boldsymbol{x}}}}{\\sum_{\\boldsymbol{x} \\in D} w_{\\boldsymbol{x}}}</script><script type=\"math/tex; mode=display\">\n\\tilde{p}_{k}=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}_{k}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant k \\leqslant|\\mathcal{Y}|)</script><script type=\"math/tex; mode=display\">\n\\tilde{r}_{v}=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}^{v}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant v \\leqslant V)</script><p>直观地看，对于属性a，<script type=\"math/tex\">\\rho</script>表示五确实样本所占比例，<script type=\"math/tex\">\\tilde{p}_k</script>表示无缺失样本中第k类样本所占比例，<script type=\"math/tex\">\\tilde{r}_v</script>表示无缺失样本中在属性a上取值<script type=\"math/tex\">a^v</script>的样本所占比例</p>\n<ul>\n<li>基于上述定义，可将信息增益的公式推广为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\operatorname{Gain}(D, a) &=\\rho \\times \\operatorname{Gain}(\\tilde{D}, a) \\\\\n&=\\rho \\times\\left(\\operatorname{Ent}(\\tilde{D})-\\sum_{v=1}^{V} \\tilde{r}_{v} \\operatorname{Ent}\\left(\\tilde{D}^{v}\\right)\\right) \\\\\n& \\operatorname{Ent}(\\tilde{D})=-\\sum_{k=1}^{|\\mathcal{Y}|} \\tilde{p}_{k} \\log _{2} \\tilde{p}_{k}\n\\end{aligned}</script><ul>\n<li><strong>若样本<script type=\"math/tex\">x</script>在a上的取值已知，则正常划入子结点，样本权值仍然保持为<script type=\"math/tex\">w_x</script>；若在a上的取值未知，则将该样本划入所有子结点，样本权值在每个分支上分别调整为<script type=\"math/tex\">\\tilde{r}_{v} \\cdot w_{\\boldsymbol{x}}</script></strong></li>\n</ul>\n"},{"title":"激活函数的作用和比较","math":true,"date":"2021-10-19T16:00:00.000Z","_content":"\n\n\n# 1 线性结构\n\n如果满足：\n$$\ny = wx + b\n$$\n则可称y、x间具有线性关系\n\n而对于神经网络，相邻两层之间的输出之间满足：\n$$\nX^{[l]} = w^{[l]}X^{[l - 1]} + b^{[l]}\n$$\n则可称其满足线性结构\n\n\n\n\n\n# 2 激活函数的作用\n\n- 我们先假设不用激活函数，假设神经网络有3层，每层的权重为$w^{[l]}$，偏差都为$b^{[l]}$，输入为$X$，输出为$Y$，则从头到尾的计算为：\n\n$$\nY = w^{[3]}(w^{[2]}(w^{[1]}X + b^{[1]}) + b^{[2]}) + b^{[3]}\n$$\n\n就相当于做一次线性运算：\n$$\nY = w'X + b'\n$$\n则从头到尾都是线性结构，这在某些情况是适用的，但是大多数时候输入和输出的关系是非线性的，**这时候我们就需要引入非线性因素，即激活函数，来使得神经网络有足够的capacity来抓取复杂的pattern**\n\n所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。\n\n\n\n- **能用线性拟合的情况：**\n\n<img src=\"https://i.loli.net/2021/11/11/NdbYFTOkwyBj7xh.png\" alt=\"image-20211111115632781\" style=\"zoom:50%;\" />\n\n- **无法用线性拟合的情况：**\n\n<img src=\"https://i.loli.net/2021/11/11/Zvo5O17LesqBbVF.png\" alt=\"image-20211111115745462\" style=\"zoom:50%;\" />\n\n\n\n\n\n# 3 常用激活函数的优缺点\n\n### 3.1 Sigmoid函数\n\n1. Sigmoid具有一个很好的特性就是能将输入值映射到$(0, 1)$，便于我们将值转化为概率，并且sigmoid平滑，便于求导\n\n2. 但sigmoid还有三大缺点：\n\n- **Gradient Vanishing：**\n\n![image-20211111155149051](https://i.loli.net/2021/11/11/TWxNvl6CrD4kuAU.png)\n\n**由图可以看出，在输入值较大或者较小时，其导数是趋于0，而在梯度下降时，其值就基本不会被更新**\n\n\n\n- **函数输出并不是zero-centered**\n\n我们以一个二维的情况举例：\n$$\nf(\\vec{x} ; \\vec{w}, b)=f\\left(w_{0} x_{0}+w_{1} x_{1}+b\\right)\n$$\n现在假设，参数 $w_0, w_1$ 的最优解 $w_0^∗, w_1^∗$ 满足条件:\n$$\n\\left\\{\\begin{array}{l}\nw_{0}<w_{0}^{*} \\\\\nw_{1} \\geqslant w_{1}^{*}\n\\end{array}\\right.\n$$\n所以我们现在就是要让$w_0$变大，$w_1$变小，而：\n$$\n\\frac{\\partial L}{\\partial w_0} = x_0 \\frac{\\partial L}{\\partial f}, \\\\\n\\frac{\\partial L}{\\partial w_1} = x_1 \\frac{\\partial L}{\\partial f}\n$$\n由于上一层sigmoid输出的$x_0, x_1$为正值，所以$w_0, w_1$的梯度一定同正或同负，所以我们要逼近最小值点，只能走下图红色线路：\n\n<img src=\"https://i.loli.net/2021/11/11/IXqb8Brk9KOH7WG.png\" alt=\"image-20211111160220315\" style=\"zoom:50%;\" />\n\n而显然绿色线路才是最快的，但是由于绿色路线的$$\\frac{\\partial L}{\\partial w_0},\\frac{\\partial L}{\\partial w_1}$$符号相反，所以不能走，所以这会**影响梯度下降的速度**\n\n\n\n- **幂运算相对来说比较耗时**\n\n\n\n### 3.2 Tanh函数\n\n- tanh一般优于sigmoid，就是因为tanh是zero-centered，但是tanh仍然没有解决sigmoid的其他两个问题\n\n\n\n### 3.3 ReLu函数\n\n1. 优点：\n\n- 解决了gradient vanishing问题 (在正区间)\n- 计算速度非常快，只需要判断输入是否大于0\n- 收敛速度快\n\n2. 缺点：\n\n- ReLU的输出不是zero-centered\n- **Dead ReLU Problem：**\n\n指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。\n\n在x<0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应\n\n**产生原因：**\n\n1. 参数初始化问题（比较少见）\n\n2. learning rate太高导致在训练过程中参数更新太大\n\n\n\n而为了防止这种情况，我们可以使用改进的ReLu函数，如Leaky ReLu：\n$$\nf(x) = max(0.01x, x)\n$$\n\n但是大部分时候我们还是使用ReLu，很多时候LReLu的表现和ReLu其实相差不多，只有神经元大量坏死，ReLu表现不好的时候，我们才会考虑使用LReLu，甚至更复杂的PReLu和ELU\n","source":"_posts/激活函数的作用和比较.md","raw":"---\ntitle: 激活函数的作用和比较\nmath: true\ndate: 2021-10-20\n---\n\n\n\n# 1 线性结构\n\n如果满足：\n$$\ny = wx + b\n$$\n则可称y、x间具有线性关系\n\n而对于神经网络，相邻两层之间的输出之间满足：\n$$\nX^{[l]} = w^{[l]}X^{[l - 1]} + b^{[l]}\n$$\n则可称其满足线性结构\n\n\n\n\n\n# 2 激活函数的作用\n\n- 我们先假设不用激活函数，假设神经网络有3层，每层的权重为$w^{[l]}$，偏差都为$b^{[l]}$，输入为$X$，输出为$Y$，则从头到尾的计算为：\n\n$$\nY = w^{[3]}(w^{[2]}(w^{[1]}X + b^{[1]}) + b^{[2]}) + b^{[3]}\n$$\n\n就相当于做一次线性运算：\n$$\nY = w'X + b'\n$$\n则从头到尾都是线性结构，这在某些情况是适用的，但是大多数时候输入和输出的关系是非线性的，**这时候我们就需要引入非线性因素，即激活函数，来使得神经网络有足够的capacity来抓取复杂的pattern**\n\n所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。\n\n\n\n- **能用线性拟合的情况：**\n\n<img src=\"https://i.loli.net/2021/11/11/NdbYFTOkwyBj7xh.png\" alt=\"image-20211111115632781\" style=\"zoom:50%;\" />\n\n- **无法用线性拟合的情况：**\n\n<img src=\"https://i.loli.net/2021/11/11/Zvo5O17LesqBbVF.png\" alt=\"image-20211111115745462\" style=\"zoom:50%;\" />\n\n\n\n\n\n# 3 常用激活函数的优缺点\n\n### 3.1 Sigmoid函数\n\n1. Sigmoid具有一个很好的特性就是能将输入值映射到$(0, 1)$，便于我们将值转化为概率，并且sigmoid平滑，便于求导\n\n2. 但sigmoid还有三大缺点：\n\n- **Gradient Vanishing：**\n\n![image-20211111155149051](https://i.loli.net/2021/11/11/TWxNvl6CrD4kuAU.png)\n\n**由图可以看出，在输入值较大或者较小时，其导数是趋于0，而在梯度下降时，其值就基本不会被更新**\n\n\n\n- **函数输出并不是zero-centered**\n\n我们以一个二维的情况举例：\n$$\nf(\\vec{x} ; \\vec{w}, b)=f\\left(w_{0} x_{0}+w_{1} x_{1}+b\\right)\n$$\n现在假设，参数 $w_0, w_1$ 的最优解 $w_0^∗, w_1^∗$ 满足条件:\n$$\n\\left\\{\\begin{array}{l}\nw_{0}<w_{0}^{*} \\\\\nw_{1} \\geqslant w_{1}^{*}\n\\end{array}\\right.\n$$\n所以我们现在就是要让$w_0$变大，$w_1$变小，而：\n$$\n\\frac{\\partial L}{\\partial w_0} = x_0 \\frac{\\partial L}{\\partial f}, \\\\\n\\frac{\\partial L}{\\partial w_1} = x_1 \\frac{\\partial L}{\\partial f}\n$$\n由于上一层sigmoid输出的$x_0, x_1$为正值，所以$w_0, w_1$的梯度一定同正或同负，所以我们要逼近最小值点，只能走下图红色线路：\n\n<img src=\"https://i.loli.net/2021/11/11/IXqb8Brk9KOH7WG.png\" alt=\"image-20211111160220315\" style=\"zoom:50%;\" />\n\n而显然绿色线路才是最快的，但是由于绿色路线的$$\\frac{\\partial L}{\\partial w_0},\\frac{\\partial L}{\\partial w_1}$$符号相反，所以不能走，所以这会**影响梯度下降的速度**\n\n\n\n- **幂运算相对来说比较耗时**\n\n\n\n### 3.2 Tanh函数\n\n- tanh一般优于sigmoid，就是因为tanh是zero-centered，但是tanh仍然没有解决sigmoid的其他两个问题\n\n\n\n### 3.3 ReLu函数\n\n1. 优点：\n\n- 解决了gradient vanishing问题 (在正区间)\n- 计算速度非常快，只需要判断输入是否大于0\n- 收敛速度快\n\n2. 缺点：\n\n- ReLU的输出不是zero-centered\n- **Dead ReLU Problem：**\n\n指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。\n\n在x<0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应\n\n**产生原因：**\n\n1. 参数初始化问题（比较少见）\n\n2. learning rate太高导致在训练过程中参数更新太大\n\n\n\n而为了防止这种情况，我们可以使用改进的ReLu函数，如Leaky ReLu：\n$$\nf(x) = max(0.01x, x)\n$$\n\n但是大部分时候我们还是使用ReLu，很多时候LReLu的表现和ReLu其实相差不多，只有神经元大量坏死，ReLu表现不好的时候，我们才会考虑使用LReLu，甚至更复杂的PReLu和ELU\n","slug":"激活函数的作用和比较","published":1,"updated":"2022-12-20T06:15:18.573Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1k000i7csz5nt62uvg","content":"<h1 id=\"1-线性结构\"><a href=\"#1-线性结构\" class=\"headerlink\" title=\"1 线性结构\"></a>1 线性结构</h1><p>如果满足：</p>\n<script type=\"math/tex; mode=display\">\ny = wx + b</script><p>则可称y、x间具有线性关系</p>\n<p>而对于神经网络，相邻两层之间的输出之间满足：</p>\n<script type=\"math/tex; mode=display\">\nX^{[l]} = w^{[l]}X^{[l - 1]} + b^{[l]}</script><p>则可称其满足线性结构</p>\n<h1 id=\"2-激活函数的作用\"><a href=\"#2-激活函数的作用\" class=\"headerlink\" title=\"2 激活函数的作用\"></a>2 激活函数的作用</h1><ul>\n<li>我们先假设不用激活函数，假设神经网络有3层，每层的权重为$w^{[l]}$，偏差都为$b^{[l]}$，输入为$X$，输出为$Y$，则从头到尾的计算为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nY = w^{[3]}(w^{[2]}(w^{[1]}X + b^{[1]}) + b^{[2]}) + b^{[3]}</script><p>就相当于做一次线性运算：</p>\n<script type=\"math/tex; mode=display\">\nY = w'X + b'</script><p>则从头到尾都是线性结构，这在某些情况是适用的，但是大多数时候输入和输出的关系是非线性的，<strong>这时候我们就需要引入非线性因素，即激活函数，来使得神经网络有足够的capacity来抓取复杂的pattern</strong></p>\n<p>所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。</p>\n<ul>\n<li><strong>能用线性拟合的情况：</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/11/NdbYFTOkwyBj7xh.png\" alt=\"image-20211111115632781\" style=\"zoom:50%;\" /></p>\n<ul>\n<li><strong>无法用线性拟合的情况：</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/11/Zvo5O17LesqBbVF.png\" alt=\"image-20211111115745462\" style=\"zoom:50%;\" /></p>\n<h1 id=\"3-常用激活函数的优缺点\"><a href=\"#3-常用激活函数的优缺点\" class=\"headerlink\" title=\"3 常用激活函数的优缺点\"></a>3 常用激活函数的优缺点</h1><h3 id=\"3-1-Sigmoid函数\"><a href=\"#3-1-Sigmoid函数\" class=\"headerlink\" title=\"3.1 Sigmoid函数\"></a>3.1 Sigmoid函数</h3><ol>\n<li><p>Sigmoid具有一个很好的特性就是能将输入值映射到$(0, 1)$，便于我们将值转化为概率，并且sigmoid平滑，便于求导</p>\n</li>\n<li><p>但sigmoid还有三大缺点：</p>\n</li>\n</ol>\n<ul>\n<li><strong>Gradient Vanishing：</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/11/TWxNvl6CrD4kuAU.png\" alt=\"image-20211111155149051\"></p>\n<p><strong>由图可以看出，在输入值较大或者较小时，其导数是趋于0，而在梯度下降时，其值就基本不会被更新</strong></p>\n<ul>\n<li><strong>函数输出并不是zero-centered</strong></li>\n</ul>\n<p>我们以一个二维的情况举例：</p>\n<script type=\"math/tex; mode=display\">\nf(\\vec{x} ; \\vec{w}, b)=f\\left(w_{0} x_{0}+w_{1} x_{1}+b\\right)</script><p>现在假设，参数 $w_0, w_1$ 的最优解 $w_0^∗, w_1^∗$ 满足条件:</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{array}{l}\nw_{0}<w_{0}^{*} \\\\\nw_{1} \\geqslant w_{1}^{*}\n\\end{array}\\right.</script><p>所以我们现在就是要让$w_0$变大，$w_1$变小，而：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial w_0} = x_0 \\frac{\\partial L}{\\partial f}, \\\\\n\\frac{\\partial L}{\\partial w_1} = x_1 \\frac{\\partial L}{\\partial f}</script><p>由于上一层sigmoid输出的$x_0, x_1$为正值，所以$w_0, w_1$的梯度一定同正或同负，所以我们要逼近最小值点，只能走下图红色线路：</p>\n<p><img src=\"https://i.loli.net/2021/11/11/IXqb8Brk9KOH7WG.png\" alt=\"image-20211111160220315\" style=\"zoom:50%;\" /></p>\n<p>而显然绿色线路才是最快的，但是由于绿色路线的<script type=\"math/tex\">\\frac{\\partial L}{\\partial w_0},\\frac{\\partial L}{\\partial w_1}</script>符号相反，所以不能走，所以这会<strong>影响梯度下降的速度</strong></p>\n<ul>\n<li><strong>幂运算相对来说比较耗时</strong></li>\n</ul>\n<h3 id=\"3-2-Tanh函数\"><a href=\"#3-2-Tanh函数\" class=\"headerlink\" title=\"3.2 Tanh函数\"></a>3.2 Tanh函数</h3><ul>\n<li>tanh一般优于sigmoid，就是因为tanh是zero-centered，但是tanh仍然没有解决sigmoid的其他两个问题</li>\n</ul>\n<h3 id=\"3-3-ReLu函数\"><a href=\"#3-3-ReLu函数\" class=\"headerlink\" title=\"3.3 ReLu函数\"></a>3.3 ReLu函数</h3><ol>\n<li>优点：</li>\n</ol>\n<ul>\n<li>解决了gradient vanishing问题 (在正区间)</li>\n<li>计算速度非常快，只需要判断输入是否大于0</li>\n<li>收敛速度快</li>\n</ul>\n<ol>\n<li>缺点：</li>\n</ol>\n<ul>\n<li>ReLU的输出不是zero-centered</li>\n<li><strong>Dead ReLU Problem：</strong></li>\n</ul>\n<p>指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。</p>\n<p>在x&lt;0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应</p>\n<p><strong>产生原因：</strong></p>\n<ol>\n<li><p>参数初始化问题（比较少见）</p>\n</li>\n<li><p>learning rate太高导致在训练过程中参数更新太大</p>\n</li>\n</ol>\n<p>而为了防止这种情况，我们可以使用改进的ReLu函数，如Leaky ReLu：</p>\n<script type=\"math/tex; mode=display\">\nf(x) = max(0.01x, x)</script><p>但是大部分时候我们还是使用ReLu，很多时候LReLu的表现和ReLu其实相差不多，只有神经元大量坏死，ReLu表现不好的时候，我们才会考虑使用LReLu，甚至更复杂的PReLu和ELU</p>\n","site":{"data":{}},"wordcount":751,"excerpt":"","more":"<h1 id=\"1-线性结构\"><a href=\"#1-线性结构\" class=\"headerlink\" title=\"1 线性结构\"></a>1 线性结构</h1><p>如果满足：</p>\n<script type=\"math/tex; mode=display\">\ny = wx + b</script><p>则可称y、x间具有线性关系</p>\n<p>而对于神经网络，相邻两层之间的输出之间满足：</p>\n<script type=\"math/tex; mode=display\">\nX^{[l]} = w^{[l]}X^{[l - 1]} + b^{[l]}</script><p>则可称其满足线性结构</p>\n<h1 id=\"2-激活函数的作用\"><a href=\"#2-激活函数的作用\" class=\"headerlink\" title=\"2 激活函数的作用\"></a>2 激活函数的作用</h1><ul>\n<li>我们先假设不用激活函数，假设神经网络有3层，每层的权重为$w^{[l]}$，偏差都为$b^{[l]}$，输入为$X$，输出为$Y$，则从头到尾的计算为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nY = w^{[3]}(w^{[2]}(w^{[1]}X + b^{[1]}) + b^{[2]}) + b^{[3]}</script><p>就相当于做一次线性运算：</p>\n<script type=\"math/tex; mode=display\">\nY = w'X + b'</script><p>则从头到尾都是线性结构，这在某些情况是适用的，但是大多数时候输入和输出的关系是非线性的，<strong>这时候我们就需要引入非线性因素，即激活函数，来使得神经网络有足够的capacity来抓取复杂的pattern</strong></p>\n<p>所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。</p>\n<ul>\n<li><strong>能用线性拟合的情况：</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/11/NdbYFTOkwyBj7xh.png\" alt=\"image-20211111115632781\" style=\"zoom:50%;\" /></p>\n<ul>\n<li><strong>无法用线性拟合的情况：</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/11/Zvo5O17LesqBbVF.png\" alt=\"image-20211111115745462\" style=\"zoom:50%;\" /></p>\n<h1 id=\"3-常用激活函数的优缺点\"><a href=\"#3-常用激活函数的优缺点\" class=\"headerlink\" title=\"3 常用激活函数的优缺点\"></a>3 常用激活函数的优缺点</h1><h3 id=\"3-1-Sigmoid函数\"><a href=\"#3-1-Sigmoid函数\" class=\"headerlink\" title=\"3.1 Sigmoid函数\"></a>3.1 Sigmoid函数</h3><ol>\n<li><p>Sigmoid具有一个很好的特性就是能将输入值映射到$(0, 1)$，便于我们将值转化为概率，并且sigmoid平滑，便于求导</p>\n</li>\n<li><p>但sigmoid还有三大缺点：</p>\n</li>\n</ol>\n<ul>\n<li><strong>Gradient Vanishing：</strong></li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/11/TWxNvl6CrD4kuAU.png\" alt=\"image-20211111155149051\"></p>\n<p><strong>由图可以看出，在输入值较大或者较小时，其导数是趋于0，而在梯度下降时，其值就基本不会被更新</strong></p>\n<ul>\n<li><strong>函数输出并不是zero-centered</strong></li>\n</ul>\n<p>我们以一个二维的情况举例：</p>\n<script type=\"math/tex; mode=display\">\nf(\\vec{x} ; \\vec{w}, b)=f\\left(w_{0} x_{0}+w_{1} x_{1}+b\\right)</script><p>现在假设，参数 $w_0, w_1$ 的最优解 $w_0^∗, w_1^∗$ 满足条件:</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{array}{l}\nw_{0}<w_{0}^{*} \\\\\nw_{1} \\geqslant w_{1}^{*}\n\\end{array}\\right.</script><p>所以我们现在就是要让$w_0$变大，$w_1$变小，而：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial w_0} = x_0 \\frac{\\partial L}{\\partial f}, \\\\\n\\frac{\\partial L}{\\partial w_1} = x_1 \\frac{\\partial L}{\\partial f}</script><p>由于上一层sigmoid输出的$x_0, x_1$为正值，所以$w_0, w_1$的梯度一定同正或同负，所以我们要逼近最小值点，只能走下图红色线路：</p>\n<p><img src=\"https://i.loli.net/2021/11/11/IXqb8Brk9KOH7WG.png\" alt=\"image-20211111160220315\" style=\"zoom:50%;\" /></p>\n<p>而显然绿色线路才是最快的，但是由于绿色路线的<script type=\"math/tex\">\\frac{\\partial L}{\\partial w_0},\\frac{\\partial L}{\\partial w_1}</script>符号相反，所以不能走，所以这会<strong>影响梯度下降的速度</strong></p>\n<ul>\n<li><strong>幂运算相对来说比较耗时</strong></li>\n</ul>\n<h3 id=\"3-2-Tanh函数\"><a href=\"#3-2-Tanh函数\" class=\"headerlink\" title=\"3.2 Tanh函数\"></a>3.2 Tanh函数</h3><ul>\n<li>tanh一般优于sigmoid，就是因为tanh是zero-centered，但是tanh仍然没有解决sigmoid的其他两个问题</li>\n</ul>\n<h3 id=\"3-3-ReLu函数\"><a href=\"#3-3-ReLu函数\" class=\"headerlink\" title=\"3.3 ReLu函数\"></a>3.3 ReLu函数</h3><ol>\n<li>优点：</li>\n</ol>\n<ul>\n<li>解决了gradient vanishing问题 (在正区间)</li>\n<li>计算速度非常快，只需要判断输入是否大于0</li>\n<li>收敛速度快</li>\n</ul>\n<ol>\n<li>缺点：</li>\n</ol>\n<ul>\n<li>ReLU的输出不是zero-centered</li>\n<li><strong>Dead ReLU Problem：</strong></li>\n</ul>\n<p>指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。</p>\n<p>在x&lt;0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应</p>\n<p><strong>产生原因：</strong></p>\n<ol>\n<li><p>参数初始化问题（比较少见）</p>\n</li>\n<li><p>learning rate太高导致在训练过程中参数更新太大</p>\n</li>\n</ol>\n<p>而为了防止这种情况，我们可以使用改进的ReLu函数，如Leaky ReLu：</p>\n<script type=\"math/tex; mode=display\">\nf(x) = max(0.01x, x)</script><p>但是大部分时候我们还是使用ReLu，很多时候LReLu的表现和ReLu其实相差不多，只有神经元大量坏死，ReLu表现不好的时候，我们才会考虑使用LReLu，甚至更复杂的PReLu和ELU</p>\n"},{"title":"优化算法","math":true,"date":"2021-12-06T16:00:00.000Z","_content":"\n\n\n# 1 沿梯度方向函数值一定下降\n\n- 证明上述的这个结论（这里只证明了一维输入的时候），首先通过泰勒公式：\n\n$$\nf(x)=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n !}(x-a)^{n}\n$$\n\n- 将上式的x和a替换为$$x + \\epsilon$$和x，得到：\n\n$$\nf(x+\\epsilon) \\approx f(x)+f^{\\prime}(x) \\epsilon+\\mathcal{O}\\left(\\epsilon^{2}\\right)\n$$\n\n- 当$$\\epsilon$$足够小时，$$\\mathcal{O}(\\epsilon^2)$$可以忽略不计：\n\n$$\nf(x+\\epsilon) \\approx f(x)+f^{\\prime}(x) \\epsilon\n$$\n\n- 如果存在$$\\eta > 0$$，使得$$|\\eta f'(x)|$$足够小，那么：\n\n$$\nf\\left(x-\\eta f^{\\prime}(x)\\right) \\approx f(x)-\\eta f^{\\prime}(x)^{2} \\lesssim f(x)\n$$\n\n- 对于多维输入时的证明，可以看[牛顿法的原理](https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#4-1-%E7%89%9B%E9%A1%BF%E6%B3%95)\n\n\n\n\n\n# 2 梯度下降法\n\n- 梯度下降法即使用所有样本或部分样本关于loss的梯度和，作为整体的梯度：\n\n$$\n\\begin{array}{c}\nf(\\boldsymbol{x})=\\frac{1}{n} \\sum_{i=1}^{n} f_{i}(\\boldsymbol{x}) . \\\\\n\\nabla f(\\boldsymbol{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\boldsymbol{x}) .\n\\end{array}\n$$\n\n- 不同batch-size的梯度下降有一定的区别，具体可看[不同batch size梯度下降的影响](https://zlkqz.site/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/)\n\n\n\n\n\n# 3 动量法（Momentum）\n\n### 3.1 梯度下降法的问题\n\n- 举个栗子，考虑函数$$f(x) = 0.1x_1^2 + 2x_2^2$$，**输入中不同维度变量的梯度相差较大**，假设使用较小的学习率，其迭代路线如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221104163804950.png\" alt=\"image-20221104163804950\" style=\"zoom:80%;\" />\n\n- **可以看到，由于使用了较小的学习率，所以梯度较小的维度收敛得很慢，但是学习率又不能的过大，因为有较大的梯度的维度会无法收敛，**\n\n\n\n### 3.2 指数加权移动平均\n\n- 定义如下：\n\n$$\ny_{t}=\\gamma y_{t-1}+(1-\\gamma) x_{t} \\\\\ny_0 = 0\n$$\n\n- 是可以对$$y_t$$进行展开的：\n\n$$\n\\begin{aligned}\ny_{t} &=(1-\\gamma) x_{t}+\\gamma y_{t-1} \\\\\n&=(1-\\gamma) x_{t}+(1-\\gamma) \\cdot \\gamma x_{t-1}+\\gamma^{2} y_{t-2} \\\\\n&=(1-\\gamma) x_{t}+(1-\\gamma) \\cdot \\gamma x_{t-1}+(1-\\gamma) \\cdot \\gamma^{2} x_{t-2}+\\gamma^{3} y_{t-3} \\\\\n&= \\quad ......\n\\end{aligned}\n$$\n\n- 从上面可以看出，**可以将$$y_t$$当作前面时间步的$$x_i$$的加权平均**。但是我们需要一些近似，由于越往前的$$x_i$$涉及到的连乘越多，越接近0，**所以对于权值为$$\\mathcal{O}(\\gamma^{1 / (1-\\gamma)})$$的项进行忽略，即把$$y_t$$看作是对最近$$1/(1 − \\gamma)$$个时间步的$$x_i$$值的加权平均，且越接近当前时间步，权重越大**\n\n\n\n### 3.3 算法流程\n\n- 算法流程如下：\n\n$$\n\\begin{array}{l}\n\\boldsymbol{v}_{t} \\leftarrow \\gamma \\boldsymbol{v}_{t-1}+\\eta \\boldsymbol{g}_{t} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{v}_{t}\n\\end{array}\n$$\n\n其中$$g_t$$为现在step的小批量梯度，最开始$$v_0 = 0$$，动量超参数$$\\gamma$$满⾜$$0 \\le \\gamma < 1$$。当$$\\gamma = 0$$时，动量法等价于小批量随机梯度下降\n\n\n\n### 3.4 算法原理\n\n- 动量法其实就是对指数加权移动平均中$$x_t$$做了一个变形：\n\n$$\n\\boldsymbol{v}_{t} \\leftarrow \\gamma \\boldsymbol{v}_{t-1}+(1-\\gamma)\\left(\\frac{\\eta}{1-\\gamma} g_{t}\\right)\n$$\n\n- 由上式可知，动量法其实就是对最近$$1/(1 − \\gamma)$$个时间步的梯度（只是梯度进行了一个放缩）进行加权平均\n\n- **所以，在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决于当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致**。如果梯度不一致，则代表梯度过大，那么动量法就会在加权平均时减去相反的梯度，以达到减少梯度的目的\n\n\n\n\n\n# 4 AdaGrad算法\n\n- 在前面梯度下降的问题中，造成这种现象的原因是**不同维度使用了相同的学习率**。而AdaGrad则自变量在每个维度的梯度值的大小来**调整各个维度上的学习率**，从而避免统⼀的学习率难以适应所有维度的问题\n\n\n\n### 4.1 算法流程\n\n- 算法流程如下：\n\n$$\n\\boldsymbol{s}_{t} \\leftarrow \\boldsymbol{s}_{t-1}+g_{t} \\odot g_{t} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\frac{\\eta}{\\sqrt{\\boldsymbol{s}_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t},\n$$\n\n其中$$\\epsilon$$是为了防止除以0，一般为$$10^{-6}$$，$$\\odot$$为按元素相乘，在最开始时$$s_0 = 0$$\n\n\n\n### 4.2 算法原理\n\n- 小批量随机梯度按元素平方的累加变量$$s_t$$出现在学习率的分母项中。**因此，如果 目标函数有关目变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；反之， 如果目标函数有关目变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢**\n\n- 但是AdaGrad算法也有缺点，由于$$s_t$$⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的**学习率在迭代过程中⼀直在降低**。所以，**当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有用的解**\n\n\n\n\n\n# 5 RMSProp算法\n\n- 针对上述AdaGrad的问题，学习率会一直下降，而RMSProp对AdaGrad做了一点微小的改动，**使用了指数加权移动平均：**\n\n$$\n\\begin{array}{c}\n\\boldsymbol{s}_{t} \\leftarrow \\gamma \\boldsymbol{s}_{t-1}+(1-\\gamma) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t} . \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\frac{\\eta}{\\sqrt{s_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t}\n\\end{array}\n$$\n\n- 可以看到，**AdaGrad中的$$s_t$$是所有steps中梯度的平方和，而RMSProp中的$$s_t$$是最近$$1 / (1-\\gamma)$$个steps的加权平均**，这样学习率就不会一直降低\n\n\n\n\n\n# 6 AdaDelta算法\n\n- AdaDelta同样是针对AdaGrad的问题进行了改进，值得注意的是**AdaDelta没有学习率这一超参**\n\n\n\n### 6.1 算法流程\n\n- AdaDelta和RMSProp同样使用了指数加权移动平均，以减少梯度惩罚致使学习率过小的影响，给定超参数$$\\rho$$（即前面中的$$\\gamma$$），首先和RMSProp一样，计算：\n\n$$\n\\boldsymbol{s}_{t} \\leftarrow \\rho \\boldsymbol{s}_{t-1}+(1-\\rho) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t}\n$$\n\n- 另外，他还维护另一个变量$$\\Delta x_t$$：\n\n$$\n\\boldsymbol{g}_{t}^{\\prime} \\leftarrow \\sqrt{\\frac{\\Delta \\boldsymbol{x}_{t-1}+\\epsilon}{\\boldsymbol{s}_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t} \\\\\n\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{g}_{t}^{\\prime}\n$$\n\n- $$\\Delta x_t$$同样是使用指数加权移动平均：\n\n$$\n\\Delta \\boldsymbol{x}_{t} \\leftarrow \\rho \\Delta \\boldsymbol{x}_{t-1}+(1-\\rho) \\boldsymbol{g}_{t}^{\\prime} \\odot \\boldsymbol{g}_{t}^{\\prime}\n$$\n\n\n\n### 6.2 算法原理\n\n- 首先是牛顿法中的更新公式：\n\n$$\nx_t = x_{t-1} - H^{-1}g\n$$\n\n其中$$H^{-1}$$为黑塞矩阵的逆\n\n- 以及梯度下降中的更新公式：\n\n$$\nx_t = x_{t-1} - \\eta g\n$$\n\n其中$$\\eta$$为学习率\n\n- 从上面的两个式子可以得出：**我们并需要显式的设置学习率，而可以直接用$$H^{-1}$$来代替学习率**\n\n- 而由于黑塞矩阵的计算复杂度过高，所以我们需要进行一些迭代逼近，由于：\n\n$$\nx_t - x_{t-1} = \\Delta x = -H^{-1}g\n$$\n\n所以：\n$$\nH^{-1} = -\\frac{\\Delta x}{g}\n$$\n我们就使用这个式子来进行迭代逼近，其中$$\\Delta x$$和$$g$$使用前面时间步的指数加权平均来取近似\n\n- 所以就得到了：\n\n$$\n\\Delta x_{t}=-\\frac{\\operatorname{RMS}[\\Delta x]_{t-1}}{\\operatorname{RMS}[g]_{t}} g_{t}\n$$\n\n其中$$\\operatorname{RMS}[g]_{t}=\\sqrt{E\\left[g^{2}\\right]_{t}+\\epsilon}$$，$$E[g^2]_t$$为最近t个时间步的$$g^2$$的指数加权移动平均，$$\\operatorname{RMS}[\\Delta x]_{t-1}$$同理\n\n\n\n### 6.3 二阶导和一阶导\n\n- AdaDelta中使用了黑塞矩阵，涉及到二阶导，**而作者认为使用二阶导是优于一阶导的**\n\n- 一阶导可以得到一组近似：（RMSProp举例）\n\n$$\n\\Delta x \\propto g \\propto \\frac{\\partial f}{\\partial x} \\propto \\frac{1}{x}\n$$\n\n- 而二阶导又可以得到另一组近似：（牛顿法举例）\n\n$$\n\\Delta x \\propto H^{-1}g \\propto \\frac{\\frac{\\partial f}{\\partial x}}{\\frac{\\partial^2 f}{\\partial x^2}} \\propto x\n$$\n\n- 由上面两个式子可以得出：**一阶方法最终正比于1/x，即与参数逆相关：参数逐渐变大的时候，更新值反而变小；而二阶方法最终正比于x，即与参数正相关：参数逐渐变大的时候，更新值也会变大。因此，作者称Hessian方法得到了Correct Units(正确的更新单元)。**\n\n\n\n\n\n# 7 Adam算法\n\n- Adam其实就是**动量法和RMSProp算法的结合**\n- 其实就是在RMSProp有惩罚项的基础上，对梯度也做了指数加权平均：\n\n$$\n\\boldsymbol{v}_{t} \\leftarrow \\beta_{1} \\boldsymbol{v}_{t-1}+\\left(1-\\beta_{1}\\right) \\boldsymbol{g}_{t} \\\\\n\\boldsymbol{s}_{t} \\leftarrow \\beta_{2} \\boldsymbol{s}_{t-1}+\\left(1-\\beta_{2}\\right) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t}\n$$\n\n其中超参建议值：$$\\beta_1 = 0.9, \\beta_2 = 0.999$$\n\n- 将$$v_t$$展开：\n\n$$\n\\boldsymbol{v}_{t}=\\left(1-\\beta_{1}\\right) \\sum_{i=1}^{t} \\beta_{1}^{t-i} \\boldsymbol{g}_{i \\circ}\n$$\n\n将所有权值相加得到和为$$1 - \\beta_1^t$$，所以在t较小时，其权值和是不等于1的，所以进行了一个**偏差修正**：\n$$\n\\begin{array}{l}\n\\hat{\\boldsymbol{v}}_{t} \\leftarrow \\frac{\\boldsymbol{v}_{t}}{1-\\beta_{1}^{t}} \\\\\n\\hat{\\boldsymbol{s}}_{t} \\leftarrow \\frac{\\boldsymbol{s}_{t}}{1-\\beta_{2}^{t}}\n\\end{array}\n$$\n这样权重值相加就等于1了\n\n- 然后使用偏差修正后的变量进行更新：\n\n$$\n\\begin{array}{l}\n\\boldsymbol{g}_{t}^{\\prime} \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{v}}_{t}}{\\sqrt{\\hat{\\boldsymbol{s}}_{t}}+\\epsilon} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{g}_{t}^{\\prime}\n\\end{array}\n$$\n\n","source":"_posts/优化算法.md","raw":"---\ntitle: 优化算法\nmath: true\ndate: 2021-12-7\n---\n\n\n\n# 1 沿梯度方向函数值一定下降\n\n- 证明上述的这个结论（这里只证明了一维输入的时候），首先通过泰勒公式：\n\n$$\nf(x)=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n !}(x-a)^{n}\n$$\n\n- 将上式的x和a替换为$$x + \\epsilon$$和x，得到：\n\n$$\nf(x+\\epsilon) \\approx f(x)+f^{\\prime}(x) \\epsilon+\\mathcal{O}\\left(\\epsilon^{2}\\right)\n$$\n\n- 当$$\\epsilon$$足够小时，$$\\mathcal{O}(\\epsilon^2)$$可以忽略不计：\n\n$$\nf(x+\\epsilon) \\approx f(x)+f^{\\prime}(x) \\epsilon\n$$\n\n- 如果存在$$\\eta > 0$$，使得$$|\\eta f'(x)|$$足够小，那么：\n\n$$\nf\\left(x-\\eta f^{\\prime}(x)\\right) \\approx f(x)-\\eta f^{\\prime}(x)^{2} \\lesssim f(x)\n$$\n\n- 对于多维输入时的证明，可以看[牛顿法的原理](https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#4-1-%E7%89%9B%E9%A1%BF%E6%B3%95)\n\n\n\n\n\n# 2 梯度下降法\n\n- 梯度下降法即使用所有样本或部分样本关于loss的梯度和，作为整体的梯度：\n\n$$\n\\begin{array}{c}\nf(\\boldsymbol{x})=\\frac{1}{n} \\sum_{i=1}^{n} f_{i}(\\boldsymbol{x}) . \\\\\n\\nabla f(\\boldsymbol{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\boldsymbol{x}) .\n\\end{array}\n$$\n\n- 不同batch-size的梯度下降有一定的区别，具体可看[不同batch size梯度下降的影响](https://zlkqz.site/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/)\n\n\n\n\n\n# 3 动量法（Momentum）\n\n### 3.1 梯度下降法的问题\n\n- 举个栗子，考虑函数$$f(x) = 0.1x_1^2 + 2x_2^2$$，**输入中不同维度变量的梯度相差较大**，假设使用较小的学习率，其迭代路线如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221104163804950.png\" alt=\"image-20221104163804950\" style=\"zoom:80%;\" />\n\n- **可以看到，由于使用了较小的学习率，所以梯度较小的维度收敛得很慢，但是学习率又不能的过大，因为有较大的梯度的维度会无法收敛，**\n\n\n\n### 3.2 指数加权移动平均\n\n- 定义如下：\n\n$$\ny_{t}=\\gamma y_{t-1}+(1-\\gamma) x_{t} \\\\\ny_0 = 0\n$$\n\n- 是可以对$$y_t$$进行展开的：\n\n$$\n\\begin{aligned}\ny_{t} &=(1-\\gamma) x_{t}+\\gamma y_{t-1} \\\\\n&=(1-\\gamma) x_{t}+(1-\\gamma) \\cdot \\gamma x_{t-1}+\\gamma^{2} y_{t-2} \\\\\n&=(1-\\gamma) x_{t}+(1-\\gamma) \\cdot \\gamma x_{t-1}+(1-\\gamma) \\cdot \\gamma^{2} x_{t-2}+\\gamma^{3} y_{t-3} \\\\\n&= \\quad ......\n\\end{aligned}\n$$\n\n- 从上面可以看出，**可以将$$y_t$$当作前面时间步的$$x_i$$的加权平均**。但是我们需要一些近似，由于越往前的$$x_i$$涉及到的连乘越多，越接近0，**所以对于权值为$$\\mathcal{O}(\\gamma^{1 / (1-\\gamma)})$$的项进行忽略，即把$$y_t$$看作是对最近$$1/(1 − \\gamma)$$个时间步的$$x_i$$值的加权平均，且越接近当前时间步，权重越大**\n\n\n\n### 3.3 算法流程\n\n- 算法流程如下：\n\n$$\n\\begin{array}{l}\n\\boldsymbol{v}_{t} \\leftarrow \\gamma \\boldsymbol{v}_{t-1}+\\eta \\boldsymbol{g}_{t} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{v}_{t}\n\\end{array}\n$$\n\n其中$$g_t$$为现在step的小批量梯度，最开始$$v_0 = 0$$，动量超参数$$\\gamma$$满⾜$$0 \\le \\gamma < 1$$。当$$\\gamma = 0$$时，动量法等价于小批量随机梯度下降\n\n\n\n### 3.4 算法原理\n\n- 动量法其实就是对指数加权移动平均中$$x_t$$做了一个变形：\n\n$$\n\\boldsymbol{v}_{t} \\leftarrow \\gamma \\boldsymbol{v}_{t-1}+(1-\\gamma)\\left(\\frac{\\eta}{1-\\gamma} g_{t}\\right)\n$$\n\n- 由上式可知，动量法其实就是对最近$$1/(1 − \\gamma)$$个时间步的梯度（只是梯度进行了一个放缩）进行加权平均\n\n- **所以，在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决于当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致**。如果梯度不一致，则代表梯度过大，那么动量法就会在加权平均时减去相反的梯度，以达到减少梯度的目的\n\n\n\n\n\n# 4 AdaGrad算法\n\n- 在前面梯度下降的问题中，造成这种现象的原因是**不同维度使用了相同的学习率**。而AdaGrad则自变量在每个维度的梯度值的大小来**调整各个维度上的学习率**，从而避免统⼀的学习率难以适应所有维度的问题\n\n\n\n### 4.1 算法流程\n\n- 算法流程如下：\n\n$$\n\\boldsymbol{s}_{t} \\leftarrow \\boldsymbol{s}_{t-1}+g_{t} \\odot g_{t} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\frac{\\eta}{\\sqrt{\\boldsymbol{s}_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t},\n$$\n\n其中$$\\epsilon$$是为了防止除以0，一般为$$10^{-6}$$，$$\\odot$$为按元素相乘，在最开始时$$s_0 = 0$$\n\n\n\n### 4.2 算法原理\n\n- 小批量随机梯度按元素平方的累加变量$$s_t$$出现在学习率的分母项中。**因此，如果 目标函数有关目变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；反之， 如果目标函数有关目变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢**\n\n- 但是AdaGrad算法也有缺点，由于$$s_t$$⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的**学习率在迭代过程中⼀直在降低**。所以，**当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有用的解**\n\n\n\n\n\n# 5 RMSProp算法\n\n- 针对上述AdaGrad的问题，学习率会一直下降，而RMSProp对AdaGrad做了一点微小的改动，**使用了指数加权移动平均：**\n\n$$\n\\begin{array}{c}\n\\boldsymbol{s}_{t} \\leftarrow \\gamma \\boldsymbol{s}_{t-1}+(1-\\gamma) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t} . \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\frac{\\eta}{\\sqrt{s_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t}\n\\end{array}\n$$\n\n- 可以看到，**AdaGrad中的$$s_t$$是所有steps中梯度的平方和，而RMSProp中的$$s_t$$是最近$$1 / (1-\\gamma)$$个steps的加权平均**，这样学习率就不会一直降低\n\n\n\n\n\n# 6 AdaDelta算法\n\n- AdaDelta同样是针对AdaGrad的问题进行了改进，值得注意的是**AdaDelta没有学习率这一超参**\n\n\n\n### 6.1 算法流程\n\n- AdaDelta和RMSProp同样使用了指数加权移动平均，以减少梯度惩罚致使学习率过小的影响，给定超参数$$\\rho$$（即前面中的$$\\gamma$$），首先和RMSProp一样，计算：\n\n$$\n\\boldsymbol{s}_{t} \\leftarrow \\rho \\boldsymbol{s}_{t-1}+(1-\\rho) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t}\n$$\n\n- 另外，他还维护另一个变量$$\\Delta x_t$$：\n\n$$\n\\boldsymbol{g}_{t}^{\\prime} \\leftarrow \\sqrt{\\frac{\\Delta \\boldsymbol{x}_{t-1}+\\epsilon}{\\boldsymbol{s}_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t} \\\\\n\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{g}_{t}^{\\prime}\n$$\n\n- $$\\Delta x_t$$同样是使用指数加权移动平均：\n\n$$\n\\Delta \\boldsymbol{x}_{t} \\leftarrow \\rho \\Delta \\boldsymbol{x}_{t-1}+(1-\\rho) \\boldsymbol{g}_{t}^{\\prime} \\odot \\boldsymbol{g}_{t}^{\\prime}\n$$\n\n\n\n### 6.2 算法原理\n\n- 首先是牛顿法中的更新公式：\n\n$$\nx_t = x_{t-1} - H^{-1}g\n$$\n\n其中$$H^{-1}$$为黑塞矩阵的逆\n\n- 以及梯度下降中的更新公式：\n\n$$\nx_t = x_{t-1} - \\eta g\n$$\n\n其中$$\\eta$$为学习率\n\n- 从上面的两个式子可以得出：**我们并需要显式的设置学习率，而可以直接用$$H^{-1}$$来代替学习率**\n\n- 而由于黑塞矩阵的计算复杂度过高，所以我们需要进行一些迭代逼近，由于：\n\n$$\nx_t - x_{t-1} = \\Delta x = -H^{-1}g\n$$\n\n所以：\n$$\nH^{-1} = -\\frac{\\Delta x}{g}\n$$\n我们就使用这个式子来进行迭代逼近，其中$$\\Delta x$$和$$g$$使用前面时间步的指数加权平均来取近似\n\n- 所以就得到了：\n\n$$\n\\Delta x_{t}=-\\frac{\\operatorname{RMS}[\\Delta x]_{t-1}}{\\operatorname{RMS}[g]_{t}} g_{t}\n$$\n\n其中$$\\operatorname{RMS}[g]_{t}=\\sqrt{E\\left[g^{2}\\right]_{t}+\\epsilon}$$，$$E[g^2]_t$$为最近t个时间步的$$g^2$$的指数加权移动平均，$$\\operatorname{RMS}[\\Delta x]_{t-1}$$同理\n\n\n\n### 6.3 二阶导和一阶导\n\n- AdaDelta中使用了黑塞矩阵，涉及到二阶导，**而作者认为使用二阶导是优于一阶导的**\n\n- 一阶导可以得到一组近似：（RMSProp举例）\n\n$$\n\\Delta x \\propto g \\propto \\frac{\\partial f}{\\partial x} \\propto \\frac{1}{x}\n$$\n\n- 而二阶导又可以得到另一组近似：（牛顿法举例）\n\n$$\n\\Delta x \\propto H^{-1}g \\propto \\frac{\\frac{\\partial f}{\\partial x}}{\\frac{\\partial^2 f}{\\partial x^2}} \\propto x\n$$\n\n- 由上面两个式子可以得出：**一阶方法最终正比于1/x，即与参数逆相关：参数逐渐变大的时候，更新值反而变小；而二阶方法最终正比于x，即与参数正相关：参数逐渐变大的时候，更新值也会变大。因此，作者称Hessian方法得到了Correct Units(正确的更新单元)。**\n\n\n\n\n\n# 7 Adam算法\n\n- Adam其实就是**动量法和RMSProp算法的结合**\n- 其实就是在RMSProp有惩罚项的基础上，对梯度也做了指数加权平均：\n\n$$\n\\boldsymbol{v}_{t} \\leftarrow \\beta_{1} \\boldsymbol{v}_{t-1}+\\left(1-\\beta_{1}\\right) \\boldsymbol{g}_{t} \\\\\n\\boldsymbol{s}_{t} \\leftarrow \\beta_{2} \\boldsymbol{s}_{t-1}+\\left(1-\\beta_{2}\\right) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t}\n$$\n\n其中超参建议值：$$\\beta_1 = 0.9, \\beta_2 = 0.999$$\n\n- 将$$v_t$$展开：\n\n$$\n\\boldsymbol{v}_{t}=\\left(1-\\beta_{1}\\right) \\sum_{i=1}^{t} \\beta_{1}^{t-i} \\boldsymbol{g}_{i \\circ}\n$$\n\n将所有权值相加得到和为$$1 - \\beta_1^t$$，所以在t较小时，其权值和是不等于1的，所以进行了一个**偏差修正**：\n$$\n\\begin{array}{l}\n\\hat{\\boldsymbol{v}}_{t} \\leftarrow \\frac{\\boldsymbol{v}_{t}}{1-\\beta_{1}^{t}} \\\\\n\\hat{\\boldsymbol{s}}_{t} \\leftarrow \\frac{\\boldsymbol{s}_{t}}{1-\\beta_{2}^{t}}\n\\end{array}\n$$\n这样权重值相加就等于1了\n\n- 然后使用偏差修正后的变量进行更新：\n\n$$\n\\begin{array}{l}\n\\boldsymbol{g}_{t}^{\\prime} \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{v}}_{t}}{\\sqrt{\\hat{\\boldsymbol{s}}_{t}}+\\epsilon} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{g}_{t}^{\\prime}\n\\end{array}\n$$\n\n","slug":"优化算法","published":1,"updated":"2022-12-20T06:26:43.798Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1l000j7cszfce351y5","content":"<h1 id=\"1-沿梯度方向函数值一定下降\"><a href=\"#1-沿梯度方向函数值一定下降\" class=\"headerlink\" title=\"1 沿梯度方向函数值一定下降\"></a>1 沿梯度方向函数值一定下降</h1><ul>\n<li>证明上述的这个结论（这里只证明了一维输入的时候），首先通过泰勒公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n !}(x-a)^{n}</script><ul>\n<li>将上式的x和a替换为<script type=\"math/tex\">x + \\epsilon</script>和x，得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x+\\epsilon) \\approx f(x)+f^{\\prime}(x) \\epsilon+\\mathcal{O}\\left(\\epsilon^{2}\\right)</script><ul>\n<li>当<script type=\"math/tex\">\\epsilon</script>足够小时，<script type=\"math/tex\">\\mathcal{O}(\\epsilon^2)</script>可以忽略不计：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x+\\epsilon) \\approx f(x)+f^{\\prime}(x) \\epsilon</script><ul>\n<li>如果存在<script type=\"math/tex\">\\eta > 0</script>，使得<script type=\"math/tex\">|\\eta f'(x)|</script>足够小，那么：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf\\left(x-\\eta f^{\\prime}(x)\\right) \\approx f(x)-\\eta f^{\\prime}(x)^{2} \\lesssim f(x)</script><ul>\n<li>对于多维输入时的证明，可以看<a href=\"https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#4-1-%E7%89%9B%E9%A1%BF%E6%B3%95\">牛顿法的原理</a></li>\n</ul>\n<h1 id=\"2-梯度下降法\"><a href=\"#2-梯度下降法\" class=\"headerlink\" title=\"2 梯度下降法\"></a>2 梯度下降法</h1><ul>\n<li>梯度下降法即使用所有样本或部分样本关于loss的梯度和，作为整体的梯度：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nf(\\boldsymbol{x})=\\frac{1}{n} \\sum_{i=1}^{n} f_{i}(\\boldsymbol{x}) . \\\\\n\\nabla f(\\boldsymbol{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\boldsymbol{x}) .\n\\end{array}</script><ul>\n<li>不同batch-size的梯度下降有一定的区别，具体可看<a href=\"https://zlkqz.site/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/\">不同batch size梯度下降的影响</a></li>\n</ul>\n<h1 id=\"3-动量法（Momentum）\"><a href=\"#3-动量法（Momentum）\" class=\"headerlink\" title=\"3 动量法（Momentum）\"></a>3 动量法（Momentum）</h1><h3 id=\"3-1-梯度下降法的问题\"><a href=\"#3-1-梯度下降法的问题\" class=\"headerlink\" title=\"3.1 梯度下降法的问题\"></a>3.1 梯度下降法的问题</h3><ul>\n<li>举个栗子，考虑函数<script type=\"math/tex\">f(x) = 0.1x_1^2 + 2x_2^2</script>，<strong>输入中不同维度变量的梯度相差较大</strong>，假设使用较小的学习率，其迭代路线如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221104163804950.png\" alt=\"image-20221104163804950\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><strong>可以看到，由于使用了较小的学习率，所以梯度较小的维度收敛得很慢，但是学习率又不能的过大，因为有较大的梯度的维度会无法收敛，</strong></li>\n</ul>\n<h3 id=\"3-2-指数加权移动平均\"><a href=\"#3-2-指数加权移动平均\" class=\"headerlink\" title=\"3.2 指数加权移动平均\"></a>3.2 指数加权移动平均</h3><ul>\n<li>定义如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ny_{t}=\\gamma y_{t-1}+(1-\\gamma) x_{t} \\\\\ny_0 = 0</script><ul>\n<li>是可以对<script type=\"math/tex\">y_t</script>进行展开的：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny_{t} &=(1-\\gamma) x_{t}+\\gamma y_{t-1} \\\\\n&=(1-\\gamma) x_{t}+(1-\\gamma) \\cdot \\gamma x_{t-1}+\\gamma^{2} y_{t-2} \\\\\n&=(1-\\gamma) x_{t}+(1-\\gamma) \\cdot \\gamma x_{t-1}+(1-\\gamma) \\cdot \\gamma^{2} x_{t-2}+\\gamma^{3} y_{t-3} \\\\\n&= \\quad ......\n\\end{aligned}</script><ul>\n<li>从上面可以看出，<strong>可以将<script type=\"math/tex\">y_t</script>当作前面时间步的<script type=\"math/tex\">x_i</script>的加权平均</strong>。但是我们需要一些近似，由于越往前的<script type=\"math/tex\">x_i</script>涉及到的连乘越多，越接近0，<strong>所以对于权值为<script type=\"math/tex\">\\mathcal{O}(\\gamma^{1 / (1-\\gamma)})</script>的项进行忽略，即把<script type=\"math/tex\">y_t</script>看作是对最近<script type=\"math/tex\">1/(1 − \\gamma)</script>个时间步的<script type=\"math/tex\">x_i</script>值的加权平均，且越接近当前时间步，权重越大</strong></li>\n</ul>\n<h3 id=\"3-3-算法流程\"><a href=\"#3-3-算法流程\" class=\"headerlink\" title=\"3.3 算法流程\"></a>3.3 算法流程</h3><ul>\n<li>算法流程如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\boldsymbol{v}_{t} \\leftarrow \\gamma \\boldsymbol{v}_{t-1}+\\eta \\boldsymbol{g}_{t} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{v}_{t}\n\\end{array}</script><p>其中<script type=\"math/tex\">g_t</script>为现在step的小批量梯度，最开始<script type=\"math/tex\">v_0 = 0</script>，动量超参数<script type=\"math/tex\">\\gamma</script>满⾜<script type=\"math/tex\">0 \\le \\gamma < 1</script>。当<script type=\"math/tex\">\\gamma = 0</script>时，动量法等价于小批量随机梯度下降</p>\n<h3 id=\"3-4-算法原理\"><a href=\"#3-4-算法原理\" class=\"headerlink\" title=\"3.4 算法原理\"></a>3.4 算法原理</h3><ul>\n<li>动量法其实就是对指数加权移动平均中<script type=\"math/tex\">x_t</script>做了一个变形：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{v}_{t} \\leftarrow \\gamma \\boldsymbol{v}_{t-1}+(1-\\gamma)\\left(\\frac{\\eta}{1-\\gamma} g_{t}\\right)</script><ul>\n<li><p>由上式可知，动量法其实就是对最近<script type=\"math/tex\">1/(1 − \\gamma)</script>个时间步的梯度（只是梯度进行了一个放缩）进行加权平均</p>\n</li>\n<li><p><strong>所以，在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决于当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致</strong>。如果梯度不一致，则代表梯度过大，那么动量法就会在加权平均时减去相反的梯度，以达到减少梯度的目的</p>\n</li>\n</ul>\n<h1 id=\"4-AdaGrad算法\"><a href=\"#4-AdaGrad算法\" class=\"headerlink\" title=\"4 AdaGrad算法\"></a>4 AdaGrad算法</h1><ul>\n<li>在前面梯度下降的问题中，造成这种现象的原因是<strong>不同维度使用了相同的学习率</strong>。而AdaGrad则自变量在每个维度的梯度值的大小来<strong>调整各个维度上的学习率</strong>，从而避免统⼀的学习率难以适应所有维度的问题</li>\n</ul>\n<h3 id=\"4-1-算法流程\"><a href=\"#4-1-算法流程\" class=\"headerlink\" title=\"4.1 算法流程\"></a>4.1 算法流程</h3><ul>\n<li>算法流程如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{s}_{t} \\leftarrow \\boldsymbol{s}_{t-1}+g_{t} \\odot g_{t} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\frac{\\eta}{\\sqrt{\\boldsymbol{s}_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t},</script><p>其中<script type=\"math/tex\">\\epsilon</script>是为了防止除以0，一般为<script type=\"math/tex\">10^{-6}</script>，<script type=\"math/tex\">\\odot</script>为按元素相乘，在最开始时<script type=\"math/tex\">s_0 = 0</script></p>\n<h3 id=\"4-2-算法原理\"><a href=\"#4-2-算法原理\" class=\"headerlink\" title=\"4.2 算法原理\"></a>4.2 算法原理</h3><ul>\n<li><p>小批量随机梯度按元素平方的累加变量<script type=\"math/tex\">s_t</script>出现在学习率的分母项中。<strong>因此，如果 目标函数有关目变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；反之， 如果目标函数有关目变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢</strong></p>\n</li>\n<li><p>但是AdaGrad算法也有缺点，由于<script type=\"math/tex\">s_t</script>⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的<strong>学习率在迭代过程中⼀直在降低</strong>。所以，<strong>当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有用的解</strong></p>\n</li>\n</ul>\n<h1 id=\"5-RMSProp算法\"><a href=\"#5-RMSProp算法\" class=\"headerlink\" title=\"5 RMSProp算法\"></a>5 RMSProp算法</h1><ul>\n<li>针对上述AdaGrad的问题，学习率会一直下降，而RMSProp对AdaGrad做了一点微小的改动，<strong>使用了指数加权移动平均：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\boldsymbol{s}_{t} \\leftarrow \\gamma \\boldsymbol{s}_{t-1}+(1-\\gamma) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t} . \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\frac{\\eta}{\\sqrt{s_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t}\n\\end{array}</script><ul>\n<li>可以看到，<strong>AdaGrad中的<script type=\"math/tex\">s_t</script>是所有steps中梯度的平方和，而RMSProp中的<script type=\"math/tex\">s_t</script>是最近<script type=\"math/tex\">1 / (1-\\gamma)</script>个steps的加权平均</strong>，这样学习率就不会一直降低</li>\n</ul>\n<h1 id=\"6-AdaDelta算法\"><a href=\"#6-AdaDelta算法\" class=\"headerlink\" title=\"6 AdaDelta算法\"></a>6 AdaDelta算法</h1><ul>\n<li>AdaDelta同样是针对AdaGrad的问题进行了改进，值得注意的是<strong>AdaDelta没有学习率这一超参</strong></li>\n</ul>\n<h3 id=\"6-1-算法流程\"><a href=\"#6-1-算法流程\" class=\"headerlink\" title=\"6.1 算法流程\"></a>6.1 算法流程</h3><ul>\n<li>AdaDelta和RMSProp同样使用了指数加权移动平均，以减少梯度惩罚致使学习率过小的影响，给定超参数<script type=\"math/tex\">\\rho</script>（即前面中的<script type=\"math/tex\">\\gamma</script>），首先和RMSProp一样，计算：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{s}_{t} \\leftarrow \\rho \\boldsymbol{s}_{t-1}+(1-\\rho) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t}</script><ul>\n<li>另外，他还维护另一个变量<script type=\"math/tex\">\\Delta x_t</script>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{g}_{t}^{\\prime} \\leftarrow \\sqrt{\\frac{\\Delta \\boldsymbol{x}_{t-1}+\\epsilon}{\\boldsymbol{s}_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t} \\\\\n\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{g}_{t}^{\\prime}</script><ul>\n<li><script type=\"math/tex\">\\Delta x_t</script>同样是使用指数加权移动平均：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\Delta \\boldsymbol{x}_{t} \\leftarrow \\rho \\Delta \\boldsymbol{x}_{t-1}+(1-\\rho) \\boldsymbol{g}_{t}^{\\prime} \\odot \\boldsymbol{g}_{t}^{\\prime}</script><h3 id=\"6-2-算法原理\"><a href=\"#6-2-算法原理\" class=\"headerlink\" title=\"6.2 算法原理\"></a>6.2 算法原理</h3><ul>\n<li>首先是牛顿法中的更新公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nx_t = x_{t-1} - H^{-1}g</script><p>其中<script type=\"math/tex\">H^{-1}</script>为黑塞矩阵的逆</p>\n<ul>\n<li>以及梯度下降中的更新公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nx_t = x_{t-1} - \\eta g</script><p>其中<script type=\"math/tex\">\\eta</script>为学习率</p>\n<ul>\n<li><p>从上面的两个式子可以得出：<strong>我们并需要显式的设置学习率，而可以直接用<script type=\"math/tex\">H^{-1}</script>来代替学习率</strong></p>\n</li>\n<li><p>而由于黑塞矩阵的计算复杂度过高，所以我们需要进行一些迭代逼近，由于：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nx_t - x_{t-1} = \\Delta x = -H^{-1}g</script><p>所以：</p>\n<script type=\"math/tex; mode=display\">\nH^{-1} = -\\frac{\\Delta x}{g}</script><p>我们就使用这个式子来进行迭代逼近，其中<script type=\"math/tex\">\\Delta x</script>和<script type=\"math/tex\">g</script>使用前面时间步的指数加权平均来取近似</p>\n<ul>\n<li>所以就得到了：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\Delta x_{t}=-\\frac{\\operatorname{RMS}[\\Delta x]_{t-1}}{\\operatorname{RMS}[g]_{t}} g_{t}</script><p>其中<script type=\"math/tex\">\\operatorname{RMS}[g]_{t}=\\sqrt{E\\left[g^{2}\\right]_{t}+\\epsilon}</script>，<script type=\"math/tex\">E[g^2]_t</script>为最近t个时间步的<script type=\"math/tex\">g^2</script>的指数加权移动平均，<script type=\"math/tex\">\\operatorname{RMS}[\\Delta x]_{t-1}</script>同理</p>\n<h3 id=\"6-3-二阶导和一阶导\"><a href=\"#6-3-二阶导和一阶导\" class=\"headerlink\" title=\"6.3 二阶导和一阶导\"></a>6.3 二阶导和一阶导</h3><ul>\n<li><p>AdaDelta中使用了黑塞矩阵，涉及到二阶导，<strong>而作者认为使用二阶导是优于一阶导的</strong></p>\n</li>\n<li><p>一阶导可以得到一组近似：（RMSProp举例）</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\Delta x \\propto g \\propto \\frac{\\partial f}{\\partial x} \\propto \\frac{1}{x}</script><ul>\n<li>而二阶导又可以得到另一组近似：（牛顿法举例）</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\Delta x \\propto H^{-1}g \\propto \\frac{\\frac{\\partial f}{\\partial x}}{\\frac{\\partial^2 f}{\\partial x^2}} \\propto x</script><ul>\n<li>由上面两个式子可以得出：<strong>一阶方法最终正比于1/x，即与参数逆相关：参数逐渐变大的时候，更新值反而变小；而二阶方法最终正比于x，即与参数正相关：参数逐渐变大的时候，更新值也会变大。因此，作者称Hessian方法得到了Correct Units(正确的更新单元)。</strong></li>\n</ul>\n<h1 id=\"7-Adam算法\"><a href=\"#7-Adam算法\" class=\"headerlink\" title=\"7 Adam算法\"></a>7 Adam算法</h1><ul>\n<li>Adam其实就是<strong>动量法和RMSProp算法的结合</strong></li>\n<li>其实就是在RMSProp有惩罚项的基础上，对梯度也做了指数加权平均：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{v}_{t} \\leftarrow \\beta_{1} \\boldsymbol{v}_{t-1}+\\left(1-\\beta_{1}\\right) \\boldsymbol{g}_{t} \\\\\n\\boldsymbol{s}_{t} \\leftarrow \\beta_{2} \\boldsymbol{s}_{t-1}+\\left(1-\\beta_{2}\\right) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t}</script><p>其中超参建议值：<script type=\"math/tex\">\\beta_1 = 0.9, \\beta_2 = 0.999</script></p>\n<ul>\n<li>将<script type=\"math/tex\">v_t</script>展开：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{v}_{t}=\\left(1-\\beta_{1}\\right) \\sum_{i=1}^{t} \\beta_{1}^{t-i} \\boldsymbol{g}_{i \\circ}</script><p>将所有权值相加得到和为<script type=\"math/tex\">1 - \\beta_1^t</script>，所以在t较小时，其权值和是不等于1的，所以进行了一个<strong>偏差修正</strong>：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\hat{\\boldsymbol{v}}_{t} \\leftarrow \\frac{\\boldsymbol{v}_{t}}{1-\\beta_{1}^{t}} \\\\\n\\hat{\\boldsymbol{s}}_{t} \\leftarrow \\frac{\\boldsymbol{s}_{t}}{1-\\beta_{2}^{t}}\n\\end{array}</script><p>这样权重值相加就等于1了</p>\n<ul>\n<li>然后使用偏差修正后的变量进行更新：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\boldsymbol{g}_{t}^{\\prime} \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{v}}_{t}}{\\sqrt{\\hat{\\boldsymbol{s}}_{t}}+\\epsilon} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{g}_{t}^{\\prime}\n\\end{array}</script>","site":{"data":{}},"wordcount":5336,"excerpt":"","more":"<h1 id=\"1-沿梯度方向函数值一定下降\"><a href=\"#1-沿梯度方向函数值一定下降\" class=\"headerlink\" title=\"1 沿梯度方向函数值一定下降\"></a>1 沿梯度方向函数值一定下降</h1><ul>\n<li>证明上述的这个结论（这里只证明了一维输入的时候），首先通过泰勒公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n !}(x-a)^{n}</script><ul>\n<li>将上式的x和a替换为<script type=\"math/tex\">x + \\epsilon</script>和x，得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x+\\epsilon) \\approx f(x)+f^{\\prime}(x) \\epsilon+\\mathcal{O}\\left(\\epsilon^{2}\\right)</script><ul>\n<li>当<script type=\"math/tex\">\\epsilon</script>足够小时，<script type=\"math/tex\">\\mathcal{O}(\\epsilon^2)</script>可以忽略不计：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x+\\epsilon) \\approx f(x)+f^{\\prime}(x) \\epsilon</script><ul>\n<li>如果存在<script type=\"math/tex\">\\eta > 0</script>，使得<script type=\"math/tex\">|\\eta f'(x)|</script>足够小，那么：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf\\left(x-\\eta f^{\\prime}(x)\\right) \\approx f(x)-\\eta f^{\\prime}(x)^{2} \\lesssim f(x)</script><ul>\n<li>对于多维输入时的证明，可以看<a href=\"https://zlkqz.site/2022/10/27/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#4-1-%E7%89%9B%E9%A1%BF%E6%B3%95\">牛顿法的原理</a></li>\n</ul>\n<h1 id=\"2-梯度下降法\"><a href=\"#2-梯度下降法\" class=\"headerlink\" title=\"2 梯度下降法\"></a>2 梯度下降法</h1><ul>\n<li>梯度下降法即使用所有样本或部分样本关于loss的梯度和，作为整体的梯度：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nf(\\boldsymbol{x})=\\frac{1}{n} \\sum_{i=1}^{n} f_{i}(\\boldsymbol{x}) . \\\\\n\\nabla f(\\boldsymbol{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\boldsymbol{x}) .\n\\end{array}</script><ul>\n<li>不同batch-size的梯度下降有一定的区别，具体可看<a href=\"https://zlkqz.site/2021/11/11/%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95/\">不同batch size梯度下降的影响</a></li>\n</ul>\n<h1 id=\"3-动量法（Momentum）\"><a href=\"#3-动量法（Momentum）\" class=\"headerlink\" title=\"3 动量法（Momentum）\"></a>3 动量法（Momentum）</h1><h3 id=\"3-1-梯度下降法的问题\"><a href=\"#3-1-梯度下降法的问题\" class=\"headerlink\" title=\"3.1 梯度下降法的问题\"></a>3.1 梯度下降法的问题</h3><ul>\n<li>举个栗子，考虑函数<script type=\"math/tex\">f(x) = 0.1x_1^2 + 2x_2^2</script>，<strong>输入中不同维度变量的梯度相差较大</strong>，假设使用较小的学习率，其迭代路线如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221104163804950.png\" alt=\"image-20221104163804950\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><strong>可以看到，由于使用了较小的学习率，所以梯度较小的维度收敛得很慢，但是学习率又不能的过大，因为有较大的梯度的维度会无法收敛，</strong></li>\n</ul>\n<h3 id=\"3-2-指数加权移动平均\"><a href=\"#3-2-指数加权移动平均\" class=\"headerlink\" title=\"3.2 指数加权移动平均\"></a>3.2 指数加权移动平均</h3><ul>\n<li>定义如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ny_{t}=\\gamma y_{t-1}+(1-\\gamma) x_{t} \\\\\ny_0 = 0</script><ul>\n<li>是可以对<script type=\"math/tex\">y_t</script>进行展开的：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny_{t} &=(1-\\gamma) x_{t}+\\gamma y_{t-1} \\\\\n&=(1-\\gamma) x_{t}+(1-\\gamma) \\cdot \\gamma x_{t-1}+\\gamma^{2} y_{t-2} \\\\\n&=(1-\\gamma) x_{t}+(1-\\gamma) \\cdot \\gamma x_{t-1}+(1-\\gamma) \\cdot \\gamma^{2} x_{t-2}+\\gamma^{3} y_{t-3} \\\\\n&= \\quad ......\n\\end{aligned}</script><ul>\n<li>从上面可以看出，<strong>可以将<script type=\"math/tex\">y_t</script>当作前面时间步的<script type=\"math/tex\">x_i</script>的加权平均</strong>。但是我们需要一些近似，由于越往前的<script type=\"math/tex\">x_i</script>涉及到的连乘越多，越接近0，<strong>所以对于权值为<script type=\"math/tex\">\\mathcal{O}(\\gamma^{1 / (1-\\gamma)})</script>的项进行忽略，即把<script type=\"math/tex\">y_t</script>看作是对最近<script type=\"math/tex\">1/(1 − \\gamma)</script>个时间步的<script type=\"math/tex\">x_i</script>值的加权平均，且越接近当前时间步，权重越大</strong></li>\n</ul>\n<h3 id=\"3-3-算法流程\"><a href=\"#3-3-算法流程\" class=\"headerlink\" title=\"3.3 算法流程\"></a>3.3 算法流程</h3><ul>\n<li>算法流程如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\boldsymbol{v}_{t} \\leftarrow \\gamma \\boldsymbol{v}_{t-1}+\\eta \\boldsymbol{g}_{t} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{v}_{t}\n\\end{array}</script><p>其中<script type=\"math/tex\">g_t</script>为现在step的小批量梯度，最开始<script type=\"math/tex\">v_0 = 0</script>，动量超参数<script type=\"math/tex\">\\gamma</script>满⾜<script type=\"math/tex\">0 \\le \\gamma < 1</script>。当<script type=\"math/tex\">\\gamma = 0</script>时，动量法等价于小批量随机梯度下降</p>\n<h3 id=\"3-4-算法原理\"><a href=\"#3-4-算法原理\" class=\"headerlink\" title=\"3.4 算法原理\"></a>3.4 算法原理</h3><ul>\n<li>动量法其实就是对指数加权移动平均中<script type=\"math/tex\">x_t</script>做了一个变形：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{v}_{t} \\leftarrow \\gamma \\boldsymbol{v}_{t-1}+(1-\\gamma)\\left(\\frac{\\eta}{1-\\gamma} g_{t}\\right)</script><ul>\n<li><p>由上式可知，动量法其实就是对最近<script type=\"math/tex\">1/(1 − \\gamma)</script>个时间步的梯度（只是梯度进行了一个放缩）进行加权平均</p>\n</li>\n<li><p><strong>所以，在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决于当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致</strong>。如果梯度不一致，则代表梯度过大，那么动量法就会在加权平均时减去相反的梯度，以达到减少梯度的目的</p>\n</li>\n</ul>\n<h1 id=\"4-AdaGrad算法\"><a href=\"#4-AdaGrad算法\" class=\"headerlink\" title=\"4 AdaGrad算法\"></a>4 AdaGrad算法</h1><ul>\n<li>在前面梯度下降的问题中，造成这种现象的原因是<strong>不同维度使用了相同的学习率</strong>。而AdaGrad则自变量在每个维度的梯度值的大小来<strong>调整各个维度上的学习率</strong>，从而避免统⼀的学习率难以适应所有维度的问题</li>\n</ul>\n<h3 id=\"4-1-算法流程\"><a href=\"#4-1-算法流程\" class=\"headerlink\" title=\"4.1 算法流程\"></a>4.1 算法流程</h3><ul>\n<li>算法流程如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{s}_{t} \\leftarrow \\boldsymbol{s}_{t-1}+g_{t} \\odot g_{t} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\frac{\\eta}{\\sqrt{\\boldsymbol{s}_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t},</script><p>其中<script type=\"math/tex\">\\epsilon</script>是为了防止除以0，一般为<script type=\"math/tex\">10^{-6}</script>，<script type=\"math/tex\">\\odot</script>为按元素相乘，在最开始时<script type=\"math/tex\">s_0 = 0</script></p>\n<h3 id=\"4-2-算法原理\"><a href=\"#4-2-算法原理\" class=\"headerlink\" title=\"4.2 算法原理\"></a>4.2 算法原理</h3><ul>\n<li><p>小批量随机梯度按元素平方的累加变量<script type=\"math/tex\">s_t</script>出现在学习率的分母项中。<strong>因此，如果 目标函数有关目变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；反之， 如果目标函数有关目变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢</strong></p>\n</li>\n<li><p>但是AdaGrad算法也有缺点，由于<script type=\"math/tex\">s_t</script>⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的<strong>学习率在迭代过程中⼀直在降低</strong>。所以，<strong>当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有用的解</strong></p>\n</li>\n</ul>\n<h1 id=\"5-RMSProp算法\"><a href=\"#5-RMSProp算法\" class=\"headerlink\" title=\"5 RMSProp算法\"></a>5 RMSProp算法</h1><ul>\n<li>针对上述AdaGrad的问题，学习率会一直下降，而RMSProp对AdaGrad做了一点微小的改动，<strong>使用了指数加权移动平均：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\boldsymbol{s}_{t} \\leftarrow \\gamma \\boldsymbol{s}_{t-1}+(1-\\gamma) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t} . \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\frac{\\eta}{\\sqrt{s_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t}\n\\end{array}</script><ul>\n<li>可以看到，<strong>AdaGrad中的<script type=\"math/tex\">s_t</script>是所有steps中梯度的平方和，而RMSProp中的<script type=\"math/tex\">s_t</script>是最近<script type=\"math/tex\">1 / (1-\\gamma)</script>个steps的加权平均</strong>，这样学习率就不会一直降低</li>\n</ul>\n<h1 id=\"6-AdaDelta算法\"><a href=\"#6-AdaDelta算法\" class=\"headerlink\" title=\"6 AdaDelta算法\"></a>6 AdaDelta算法</h1><ul>\n<li>AdaDelta同样是针对AdaGrad的问题进行了改进，值得注意的是<strong>AdaDelta没有学习率这一超参</strong></li>\n</ul>\n<h3 id=\"6-1-算法流程\"><a href=\"#6-1-算法流程\" class=\"headerlink\" title=\"6.1 算法流程\"></a>6.1 算法流程</h3><ul>\n<li>AdaDelta和RMSProp同样使用了指数加权移动平均，以减少梯度惩罚致使学习率过小的影响，给定超参数<script type=\"math/tex\">\\rho</script>（即前面中的<script type=\"math/tex\">\\gamma</script>），首先和RMSProp一样，计算：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{s}_{t} \\leftarrow \\rho \\boldsymbol{s}_{t-1}+(1-\\rho) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t}</script><ul>\n<li>另外，他还维护另一个变量<script type=\"math/tex\">\\Delta x_t</script>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{g}_{t}^{\\prime} \\leftarrow \\sqrt{\\frac{\\Delta \\boldsymbol{x}_{t-1}+\\epsilon}{\\boldsymbol{s}_{t}+\\epsilon}} \\odot \\boldsymbol{g}_{t} \\\\\n\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{g}_{t}^{\\prime}</script><ul>\n<li><script type=\"math/tex\">\\Delta x_t</script>同样是使用指数加权移动平均：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\Delta \\boldsymbol{x}_{t} \\leftarrow \\rho \\Delta \\boldsymbol{x}_{t-1}+(1-\\rho) \\boldsymbol{g}_{t}^{\\prime} \\odot \\boldsymbol{g}_{t}^{\\prime}</script><h3 id=\"6-2-算法原理\"><a href=\"#6-2-算法原理\" class=\"headerlink\" title=\"6.2 算法原理\"></a>6.2 算法原理</h3><ul>\n<li>首先是牛顿法中的更新公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nx_t = x_{t-1} - H^{-1}g</script><p>其中<script type=\"math/tex\">H^{-1}</script>为黑塞矩阵的逆</p>\n<ul>\n<li>以及梯度下降中的更新公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nx_t = x_{t-1} - \\eta g</script><p>其中<script type=\"math/tex\">\\eta</script>为学习率</p>\n<ul>\n<li><p>从上面的两个式子可以得出：<strong>我们并需要显式的设置学习率，而可以直接用<script type=\"math/tex\">H^{-1}</script>来代替学习率</strong></p>\n</li>\n<li><p>而由于黑塞矩阵的计算复杂度过高，所以我们需要进行一些迭代逼近，由于：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nx_t - x_{t-1} = \\Delta x = -H^{-1}g</script><p>所以：</p>\n<script type=\"math/tex; mode=display\">\nH^{-1} = -\\frac{\\Delta x}{g}</script><p>我们就使用这个式子来进行迭代逼近，其中<script type=\"math/tex\">\\Delta x</script>和<script type=\"math/tex\">g</script>使用前面时间步的指数加权平均来取近似</p>\n<ul>\n<li>所以就得到了：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\Delta x_{t}=-\\frac{\\operatorname{RMS}[\\Delta x]_{t-1}}{\\operatorname{RMS}[g]_{t}} g_{t}</script><p>其中<script type=\"math/tex\">\\operatorname{RMS}[g]_{t}=\\sqrt{E\\left[g^{2}\\right]_{t}+\\epsilon}</script>，<script type=\"math/tex\">E[g^2]_t</script>为最近t个时间步的<script type=\"math/tex\">g^2</script>的指数加权移动平均，<script type=\"math/tex\">\\operatorname{RMS}[\\Delta x]_{t-1}</script>同理</p>\n<h3 id=\"6-3-二阶导和一阶导\"><a href=\"#6-3-二阶导和一阶导\" class=\"headerlink\" title=\"6.3 二阶导和一阶导\"></a>6.3 二阶导和一阶导</h3><ul>\n<li><p>AdaDelta中使用了黑塞矩阵，涉及到二阶导，<strong>而作者认为使用二阶导是优于一阶导的</strong></p>\n</li>\n<li><p>一阶导可以得到一组近似：（RMSProp举例）</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\Delta x \\propto g \\propto \\frac{\\partial f}{\\partial x} \\propto \\frac{1}{x}</script><ul>\n<li>而二阶导又可以得到另一组近似：（牛顿法举例）</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\Delta x \\propto H^{-1}g \\propto \\frac{\\frac{\\partial f}{\\partial x}}{\\frac{\\partial^2 f}{\\partial x^2}} \\propto x</script><ul>\n<li>由上面两个式子可以得出：<strong>一阶方法最终正比于1/x，即与参数逆相关：参数逐渐变大的时候，更新值反而变小；而二阶方法最终正比于x，即与参数正相关：参数逐渐变大的时候，更新值也会变大。因此，作者称Hessian方法得到了Correct Units(正确的更新单元)。</strong></li>\n</ul>\n<h1 id=\"7-Adam算法\"><a href=\"#7-Adam算法\" class=\"headerlink\" title=\"7 Adam算法\"></a>7 Adam算法</h1><ul>\n<li>Adam其实就是<strong>动量法和RMSProp算法的结合</strong></li>\n<li>其实就是在RMSProp有惩罚项的基础上，对梯度也做了指数加权平均：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{v}_{t} \\leftarrow \\beta_{1} \\boldsymbol{v}_{t-1}+\\left(1-\\beta_{1}\\right) \\boldsymbol{g}_{t} \\\\\n\\boldsymbol{s}_{t} \\leftarrow \\beta_{2} \\boldsymbol{s}_{t-1}+\\left(1-\\beta_{2}\\right) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t}</script><p>其中超参建议值：<script type=\"math/tex\">\\beta_1 = 0.9, \\beta_2 = 0.999</script></p>\n<ul>\n<li>将<script type=\"math/tex\">v_t</script>展开：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\boldsymbol{v}_{t}=\\left(1-\\beta_{1}\\right) \\sum_{i=1}^{t} \\beta_{1}^{t-i} \\boldsymbol{g}_{i \\circ}</script><p>将所有权值相加得到和为<script type=\"math/tex\">1 - \\beta_1^t</script>，所以在t较小时，其权值和是不等于1的，所以进行了一个<strong>偏差修正</strong>：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\hat{\\boldsymbol{v}}_{t} \\leftarrow \\frac{\\boldsymbol{v}_{t}}{1-\\beta_{1}^{t}} \\\\\n\\hat{\\boldsymbol{s}}_{t} \\leftarrow \\frac{\\boldsymbol{s}_{t}}{1-\\beta_{2}^{t}}\n\\end{array}</script><p>这样权重值相加就等于1了</p>\n<ul>\n<li>然后使用偏差修正后的变量进行更新：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\boldsymbol{g}_{t}^{\\prime} \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{v}}_{t}}{\\sqrt{\\hat{\\boldsymbol{s}}_{t}}+\\epsilon} \\\\\n\\boldsymbol{x}_{t} \\leftarrow \\boldsymbol{x}_{t-1}-\\boldsymbol{g}_{t}^{\\prime}\n\\end{array}</script>"},{"title":"常用损失函数和评估模型的指标","math":true,"date":"2021-11-12T16:00:00.000Z","_content":"\n\n\n# 1 常用损失函数\n\n### 1.1 0-1损失函数\n\n$$\nL(y, \\hat{y}) = \\begin{cases}\n1 & y\\neq \\hat{y}\\\\\n0 & y = \\hat{y}\n\\end{cases}\n$$\n\n- 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用\n- 感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足$|y - \\hat{y}| < T$时认为相等，即：\n\n$$\nL(y, \\hat{y}) = \\begin{cases}\n1 & |y - \\hat{y}| \\geq T\\\\\n0 & |y - \\hat{y}| < T\n\\end{cases}\n$$\n\n\n\n### 1.2 均方差损失函数（MSE）\n\n$$\nJ_{MSE} = \\frac{1}{N} \\sum_{i = 1}^N(y_i - \\hat{y_i})^2\n$$\n\n- 也称L2 Loss\n\n\n\n##### 1.2.1 证明\n\n假设预测值和真实值的误差$\\epsilon$服从标准正态分布，则给定一个$x_i$输出真实值$y_i$的概率为：\n$$\np(\\hat y_i = y_i|x_i) = p(\\hat y_i = f(x_i) + \\epsilon) | x_i) = p(\\epsilon) = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{(y_i - \\hat{y_i})^2}{2})\n$$\n其实就是极大似然估计，我们要寻找一组参数，使$p(y_i|x_i)$最大\n\n进一步对所有样本，由于他们相互独立，所以所有样本都正好取到真实值$y$的概率为：\n$$\nL(x, y) = \\prod_{i = 1}^N\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{(y_i - \\hat{y_i})^2}{2})\n$$\n现在我们就要使$L(x, y)$最大，为了方便计算，我们取对数：\n$$\nLL(x, y) = log(L(x, y)) = -\\frac{N}{2}log2\\pi - \\frac{1}{2}\\sum_{i = 1}^N(y_i - \\hat{y_i})^2\n$$\n把第一项无关项去掉，再取负：\n$$\nNLL(x, y) = \\frac{1}{2}\\sum_{i = 1}^N(y_i - \\hat{y_i})^2\n$$\n即得到均方差形式\n\n\n\n##### 1.2.2 为什么可以用极大似然\n\n**在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，拟合数据最好的情况就是所有的$y_i = \\hat{y_i}$，即每个样本的$p(y_i|x_i)$取最大，即$L(x, y)$取最大，由于对数运算不改变单调性，并且最后取了个负值，所以即$NLL(x, y)$取最小**\n\n\n\n### 1.3 平均绝对误差损失（MAE）\n\n$$\nJ_{MAE} = \\frac{1}{N}\\sum_{i = 1}^N|y_i - \\hat{y_i}|\n$$\n\n- 也称L1 Loss\n\n\n\n##### 1.3.1 拉普拉斯分布\n\n$$\nf(x|\\mu, b) = \\frac{1}{2b}exp(-\\frac{|x - \\mu|}{b})\n$$\n\n期望值：$\\mu$             方差：$2b^2$\n\n<img src=\"https://i.loli.net/2021/11/08/yWY3hI4ioKpADRF.png\" alt=\"Laplace_distribution_pdf\" style=\"zoom: 25%;\" />\n\n\n\n##### 1.3.2 证明\n\n假设预测值和真实值的误差服从拉普拉斯分布（$\\mu = 0, b = 1$）\n$$\np(y_i | x_i) = \\frac{1}{2}exp(-{|y_i - \\hat{y_i}|})\n$$\n剩余证明和上述MSE证明过程一样\n\n\n\n##### 1.3.3 MSE和MAE的区别：\n\n- **MSE 损失相比 MAE 通常可以更快地收敛**\n\n关于$\\hat{y_i}$求导时，MSE为$-(y_i - \\hat{y_i})$，MAE为$\\pm1$，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的（当然也可以通过调整学习率缓解这个问题，但总体来说还是MSE更快）。\n\n- **MAE对于离群值更加健壮，即更加不易受到离群值影响**\n\n1. 由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失\n2. MSE 假设了误差服从正态分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于离群值更加健壮\n\n\n\n##### 1.3.4 MSE和MAE的收敛\n\n- MSE收敛于均值\n\n将$\\hat{y_i}$设为变量$t$：\n$$\nJ_{MSE} = \\frac{1}{N}\\sum_{i = 1}^N(t - y_i)^2\n$$\n关于$t$求导：\n$$\n\\frac{\\partial J}{\\partial t} = \\frac{2}{N}\\sum_{i = 1}^N(t - y_i) = 0\n$$\n求得：\n$$\nt = \\frac{1}{N}\\sum_{i = 1}^Ny_i = E(y)\n$$\n\n- MAE收敛于中值\n\n将$\\hat{y_i}$设为变量$t$：\n$$\nJ_{MAE} = \\frac{1}{N}\\sum_{i = 1}^N|t - y_i|\n$$\n关于$t$求导：\n$$\n\\frac{\\partial J}{\\partial t} = \\frac{1}{N}\\sum_{i = 1}^Nsgn(t - y_i) = 0\n$$\n显然在该种情况下应该取$t$为中值\n\n\n\n### 1.4 Huber Loss\n\n- 上面介绍了MSE和MAE，他们各有各的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，而Huber Loss则是将两者结合起来，原理很简单，就是误差接近 0 时使用 MSE，误差较大时使用 MAE：\n\n$$\nJ_{huber} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{\\left|y_{i}-\\hat{y}_{i}\\right| \\leq \\delta} \\frac{\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{2}+\\mathbb{I}_{\\left|y_{i}-\\hat{y}_{i}\\right|>\\delta}\\left(\\delta\\left|y_{i}-\\hat{y}_{i}\\right|-\\frac{1}{2} \\delta^{2}\\right)\n$$\n\n- 前半部分是MSE部分，后半部分是MAE部分，超参数$\\delta$为两个部分的连接处\n- MAE部分为$\\delta |y_i - \\hat{y_i}| - \\frac{1}{2}\\delta ^2$是为了在$|y_i - \\hat{y_i}| = \\delta$ 端点处连续可导\n\n![超参数为1的Huber Loss](https://i.loli.net/2021/11/09/e6FQMBY42PDGxtI.png)\n\n- Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个$\\delta$超参数。\n\n\n\n### 1.5 分位数损失（Quantile Loss）\n\n$$\nJ_{\\text {quant }}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{\\hat{y}_{i} \\geq y_{i}}(1-r)\\left|y_{i}-\\hat{y}_{i}\\right|+\\mathbb{I}_{\\hat{y}_{i}<y_{i}} r\\left|y_{i}-\\hat{y}_{i}\\right|\n$$\n\n- 这是一个分段函数，这个损失函数是一个分段的函数 ，将$\\hat{y_i} \\geq y_i$（高估） 和$\\hat{y_i} < y_i$（低估) 两种情况分开来，并分别给予不同的系数\n- 分位数损失实现了分别用不同的系数（r和1-r）控制高估和低估的损失\n- 特别地，当$r = 0.5$时分位数损失退化为 MAE 损失\n\n![Quantile Loss](https://i.loli.net/2021/11/09/BvpTeXqWFYRKoMU.png)\n\n\n\n### 1.6 交叉熵损失（Cross Entropy Loss）\n\n- 上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 \n\n##### 1.6.1 二分类\n\n$$\nJ_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)\n$$\n\n![二分类的交叉熵](https://i.loli.net/2021/11/09/hPt1nITJ624Llfp.png)\n\n- 证明：\n\n在二分类中我们通常将输出结果用sigmoid映射到区间$(0, 1)$，并将其作为该类的概率，由于只有两类，所以给定$x_i$求出类别为1或0的概率分别为：\n$$\np(y_i = 1|x_i) = \\hat{y_i} \\\\\np(y_i = 0|x_i) = 1 - \\hat{y_i}\n$$\n合并成一个式子：\n$$\np(y_i|x_i) = (\\hat{y_i})^{y_i}(1 - \\hat{y_i})^{1 - y_i}\n$$\n由于各数据点独立同分布，则似然可以表示为：\n$$\nL(x, y) = \\prod_{i=1}^{N}\\left(\\hat{y}_{i}\\right)^{y_{i}}\\left(1-\\hat{y}_{i}\\right)^{1-y_{i}}\n$$\n取负对数：\n$$\nN L L(x, y)=J_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)\n$$\n\n\n##### 1.6.2 多分类\n\n在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值$y_i$现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 Softmax 函数\n$$\nJ_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(\\hat{y}_{i}^{k}\\right)\n$$\n因为$y_i$是一个One-hot向量，所以还可以写为：\n$$\nJ_{C E}=-\\sum_{i=1}^{N} y_{i}^{c_{i}} \\log \\left(\\hat{y}_{i}^{c_{i}}\\right)\n$$\n其中$c_i$为样本$x_i$的目标类\n\n- 证明：\n\n对于一个样本，分类正确的概率为：\n$$\np(y_i|x_i) = \\prod_{k=1}^{K}\\left(\\hat{y}_{i}^{k}\\right)^{y_{i}^{k}}\n$$\n（其中$y_i^k和\\hat{y_i}^k$为该向量的第k维）\n\n因为所有样本相互，所有相乘再取负对数即可得到：\n$$\nN L L(x, y)=J_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(\\hat{y}_{i}^{k}\\right)\n$$\n\n\n### 1.7 合页损失（Hinge Loss）\n\n- Hinge Loss也是一种二分类损失函数\n\n$$\nJ_{\\text {hinge }}=\\sum_{i=1}^{N} \\max \\left(0,1-\\operatorname{sgn}\\left(y_{i}\\right) \\hat{y}_{i}\\right)\n$$\n\n下图是$y$为正类， 即$sgn(y) = 1$时，不同输出的合页损失示意图：\n\n![image-20211109225642368](https://i.loli.net/2021/11/09/KngeQOwp54JZRMf.png)\n\n- 可以看到当$y$为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在$(0, 1)$区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。**使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类**\n- **Hinge Loss常用在支持向量机（SVM）中，在SVM的软间隔中替换数学性质不好的0/1损失**\n\n- **Hinge Loss变种：**有些时候我们关注的并不是单个样本的分类结果，而是两个样本之间的相似性，所以会使用：\n\n$$\n\\ell= \\max(0, m + score(pos\\_pair) - score(neg\\_pair))\n$$\n\n其中两个score分别为正负样本对的得分，m是间隔参数margin，目的是**希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。这样能够拉开正负样本间的差距，更好区分正负样本**\n\n\n\n# 2 评估模型的指标\n\n### 2.1 基本概念\n\n![image-20211109233859657](https://i.loli.net/2021/11/09/OCNnaMs2ceBQIJz.png)\n\n\n\n### 2.2 查准率和查全率\n\n$$\n查准率 P（Precision） = \\frac{TP}{TP + FP}\n$$\n\n\n$$\n查全率 R（Recall） = \\frac{TP}{TP + FN}\n$$\n\n- 查准率可以直观理解为所有预测为正项的样本中有多少是真正的正项\n- 查全率可以直观理解为所有label是正项的样本中有多少被成功预测出来了\n- 理想情况下，查准率和查全率两者都越高越好。**然而事实上这两者在某些情况下是矛盾的，一般来说，查准率高时，查全率低；查确率低时，查全率高**\n\n\n\n### 2.3 准确率和错误率\n\n准确率：\n$$\naccuracy = \\frac{TP + TF}{TP + TN + FP + FN}\n$$\n\n- 即有多少样本被分类正确\n\n而错误率：\n$$\nerrorrate = 1 - accuracy\n$$\n\n\n### 2.4 P-R曲线\n\n<img src=\"https://i.loli.net/2021/11/10/zV1Sj4HyYfD3G5e.png\" alt=\"image-20211110000609300\" style=\"zoom: 80%;\" />\n\n- P-R曲线直观地显示出学习器在样本总体上地查全率、查准率，在进行比较时，**若一个学习器的P-R曲线被另一个学习器曲线“完全包住”，则可以断言后者的性能一定优于前者，如上图的B性能优于C，而A、B不一定**\n- 平衡点（BEP）时$P = R$时的取值，如上图A的BEP为0.8，而如果基于BEP比较，可知A优于B\n\n\n\n### 2.5 F函数\n\nBEP还是过于简化了些，更常用得是F1度量，我们可以取P和R的调和平均：\n$$\n\\frac{2}{F_1} = \\frac{1}{P} + \\frac{1}{R}\n$$\n求得：\n$$\nF1 = \\frac{2PR}{P + R}\n$$\n但是在许多应用中我们对查准率和查全率得重视程度不同，所以可以给予P和R不同的权重，取P和R的加权调和平均：\n$$\n\\frac{1}{F_{\\beta}} = \\frac{1}{1 + \\beta ^2}(\\frac{1}{P} + \\frac{\\beta ^2}{R})\n$$\n求得：\n$$\nF_{\\beta} = \\frac{(1 + \\beta ^2)PR}{\\beta ^2P + R}\n$$\n\n- $\\beta > 0$度量了查全率和查准率的相对重要性，$\\beta = 1$退化为标准的F1，$\\beta > 1$查全率有更大影响，$\\beta < 1$查准率有更大影响\n\n\n\n### 2.6 ROC与AUC\n\n##### 2.6.1 基本概念\n\n- 大多二分类问题是将输出的预测值与一个**分类阈值（threshold）**进行比较，若预测值大于阈值则为正类，反之则为负类\n\n- 根据预测值，我们可将测试样本进行排序，根据预测值由大到小，“最可能”是正例的排在前面，“最不可能”是正例的排到后面。**这样，分类过程就相当于以中间某个截断点（也就是分类阈值）将样本分为两部分，前一部分判定为正例，后一部分判定为反例**\n\n- 在不同的任务中，我们我们可以根据任务需求采取不同的截断点。例如如更重视查准率，可以将截断点设置靠前，更注重查全率，可以将截断点靠后\n- 而我们根据预测值进行排序后，按顺序将样本逐个作为正例进行预测，每次计算出**真正例率（TPR）**和**假正例率（FPR）**，以他们为横纵坐标就得到了**ROC曲线**\n\n\n\n##### 2.6.2 ROC曲线\n\n- 首先介绍真正例率和假正例率：\n\n$$\nTPR = \\frac{TP}{TP + FN}, \\\\\nFPR = \\frac{FP}{TN + FP}\n$$\n\n- ROC曲线：\n\n首先将分类阈值设最大，则所有类都被分类为负类，TPR和FPR都为0，然后每次将分类阈值设为下一个样本点的预测值（按预测值由大到小进行排序），记录每次的TPR和FPR，组成ROC曲线\n\n![image-20211110140633061](https://i.loli.net/2021/11/10/TUMHuC3N9rX18iz.png)\n\n但是现实中我们是基于有限个样本画的图，所以不会产生这么平滑的曲线，更多情况应该像下图：\n\n![image-20211110140853013](https://i.loli.net/2021/11/10/xG1TrPNfyimzq7Q.png)\n\n- 基于ROC的比较方法\n\n>如果一个学习器的ROC曲线完全被另一个学习器的”包住“，则后者性能优于前者\n>\n>若两者的曲线交叉，则可以通过ROC曲线所包裹的面积进行判断，即**AUC**\n\n\n\n##### 2.6.3 AUC\n\n- **AUC就是ROC曲线下的面积**，假定ROC曲线是由坐标为$$\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right),\\left(x_{3}, y_{3}\\right), \\cdots,\\left(x_{m}, y_{m}\\right)$$的点按序连接而形成，则AUC为：\n\n$$\nA U C=\\frac{1}{2} \\sum_{i=1}^{m-1}\\left(x_{i+1}-x_{i}\\right)\\left(y_{i}+y_{i+1}\\right)\n$$\n\n- 从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC， 即**AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性**\n\n- **所以AUC反应的是分类器对样本的排序能力**\n\n证明：\n\n设所有正类的集合$X = \\{ \\hat{X_1}, \\hat{X_2}, ..., \\hat{X_m}\\}$和负类的集合$Y = \\{ \\hat{Y_1}, \\hat{Y_2}, ..., \\hat{Y_n}\\}$，其中是每个样本对应的预测值，设分类阈值为c，$F_X(x)$和$F_Y(y)$分别为X和Y的分布函数，则$TPR(c) = 1 - F_X(c)$，$FPR(c) = 1 - F_Y(c)$\n\n设$t = FPR(c)$， 则$c = F_Y^{-1}(1 - t)$，则$ROC(t) = 1 - F_X(F_Y^{-1}(1 - t))$\n\n则：\n$$\nAUC = \\int_0^1ROC(t)dt \\\\\n=\\int_0^1 [1 - F_X(F_Y^{-1}(1 - t))] dt \\\\\n=\\int_0^1 [1 - F_X(F_Y^{-1}(t))] dt \\\\\n=\\int_{-\\infty}^{+\\infty} [1 - F_X(y)] dF_Y(y) \\\\\n=\\int_{-\\infty}^{+\\infty}P(X > y)f_Y(y)dy \\\\\n=\\int_{-\\infty}^{+\\infty}P(X > y, Y = y)dy \\\\\n=P(X > Y)\n$$\n\n\n##### 2.6.4 使用ROC和AUC的优点\n\n- **AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡（正负类样本不相同）的情况下，依然能够对分类器做出合理的评价**\n\n$$\nTPR = P(\\hat{Y} = 1 | Y = 1), \\\\\nFPR = P(\\hat{Y} = 1 | Y = 0)\n$$\n\n**由上式可得：无论Y的真实概率是多少， 都不会影响TPR和FPR**\n\n而PR曲线更关注正例\n\n- ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响，有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的查全率就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少\n\n\n\n- 上面几种评估方法都是**用于分类**的评估方法，而在**回归问题**当中，这些一般是不适用的，回归问题中我们比较常用的评估方法有一下两种\n\n\n\n### 2.7 平方根误差（RMSE）\n\n$$\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n(y_i - \\hat y_i)^2}{n}}\n$$\n\n- 其实RMSE就是MSE开了个根，但是我们做这样的处理能让**误差和结果值在同一个数量级上，这样能更直观有效的反应拟合程度**\n- 但是RMSE有着和MSE一样的缺点，那就是**对离群值十分敏感，健壮性很差**\n- 比如在实际应用中，有可能在对于预测某些剧集的流量时，以便进行广告投放，在95%的区间内的预测误差都十分低，比如小于1%，这是相当不错的预测结果。但是在总体上，无论运用何种模型，RMSE可能都一直居高不下。**原因是可能在剩余的5%区间里有非常严重的离群点，比如某些冷门剧、新上映的剧**\n- 对此我们可以选择对数据进行处理，或者换一种模型指标\n\n\n\n### 2.8 平均绝对百分比误差（MAPE）\n\n$$\nMAPE = \\sum_{i = 1}^n |\\frac{y_i - \\hat{y}_i}{y_i}| \\times \\frac{100}{n}\n$$\n\n- 相比RMSE， MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的影响\n\n","source":"_posts/常用损失函数和评估模型的指标.md","raw":"---\ntitle: 常用损失函数和评估模型的指标\nmath: true\ndate: 2021-11-13\n---\n\n\n\n# 1 常用损失函数\n\n### 1.1 0-1损失函数\n\n$$\nL(y, \\hat{y}) = \\begin{cases}\n1 & y\\neq \\hat{y}\\\\\n0 & y = \\hat{y}\n\\end{cases}\n$$\n\n- 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用\n- 感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足$|y - \\hat{y}| < T$时认为相等，即：\n\n$$\nL(y, \\hat{y}) = \\begin{cases}\n1 & |y - \\hat{y}| \\geq T\\\\\n0 & |y - \\hat{y}| < T\n\\end{cases}\n$$\n\n\n\n### 1.2 均方差损失函数（MSE）\n\n$$\nJ_{MSE} = \\frac{1}{N} \\sum_{i = 1}^N(y_i - \\hat{y_i})^2\n$$\n\n- 也称L2 Loss\n\n\n\n##### 1.2.1 证明\n\n假设预测值和真实值的误差$\\epsilon$服从标准正态分布，则给定一个$x_i$输出真实值$y_i$的概率为：\n$$\np(\\hat y_i = y_i|x_i) = p(\\hat y_i = f(x_i) + \\epsilon) | x_i) = p(\\epsilon) = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{(y_i - \\hat{y_i})^2}{2})\n$$\n其实就是极大似然估计，我们要寻找一组参数，使$p(y_i|x_i)$最大\n\n进一步对所有样本，由于他们相互独立，所以所有样本都正好取到真实值$y$的概率为：\n$$\nL(x, y) = \\prod_{i = 1}^N\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{(y_i - \\hat{y_i})^2}{2})\n$$\n现在我们就要使$L(x, y)$最大，为了方便计算，我们取对数：\n$$\nLL(x, y) = log(L(x, y)) = -\\frac{N}{2}log2\\pi - \\frac{1}{2}\\sum_{i = 1}^N(y_i - \\hat{y_i})^2\n$$\n把第一项无关项去掉，再取负：\n$$\nNLL(x, y) = \\frac{1}{2}\\sum_{i = 1}^N(y_i - \\hat{y_i})^2\n$$\n即得到均方差形式\n\n\n\n##### 1.2.2 为什么可以用极大似然\n\n**在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，拟合数据最好的情况就是所有的$y_i = \\hat{y_i}$，即每个样本的$p(y_i|x_i)$取最大，即$L(x, y)$取最大，由于对数运算不改变单调性，并且最后取了个负值，所以即$NLL(x, y)$取最小**\n\n\n\n### 1.3 平均绝对误差损失（MAE）\n\n$$\nJ_{MAE} = \\frac{1}{N}\\sum_{i = 1}^N|y_i - \\hat{y_i}|\n$$\n\n- 也称L1 Loss\n\n\n\n##### 1.3.1 拉普拉斯分布\n\n$$\nf(x|\\mu, b) = \\frac{1}{2b}exp(-\\frac{|x - \\mu|}{b})\n$$\n\n期望值：$\\mu$             方差：$2b^2$\n\n<img src=\"https://i.loli.net/2021/11/08/yWY3hI4ioKpADRF.png\" alt=\"Laplace_distribution_pdf\" style=\"zoom: 25%;\" />\n\n\n\n##### 1.3.2 证明\n\n假设预测值和真实值的误差服从拉普拉斯分布（$\\mu = 0, b = 1$）\n$$\np(y_i | x_i) = \\frac{1}{2}exp(-{|y_i - \\hat{y_i}|})\n$$\n剩余证明和上述MSE证明过程一样\n\n\n\n##### 1.3.3 MSE和MAE的区别：\n\n- **MSE 损失相比 MAE 通常可以更快地收敛**\n\n关于$\\hat{y_i}$求导时，MSE为$-(y_i - \\hat{y_i})$，MAE为$\\pm1$，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的（当然也可以通过调整学习率缓解这个问题，但总体来说还是MSE更快）。\n\n- **MAE对于离群值更加健壮，即更加不易受到离群值影响**\n\n1. 由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失\n2. MSE 假设了误差服从正态分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于离群值更加健壮\n\n\n\n##### 1.3.4 MSE和MAE的收敛\n\n- MSE收敛于均值\n\n将$\\hat{y_i}$设为变量$t$：\n$$\nJ_{MSE} = \\frac{1}{N}\\sum_{i = 1}^N(t - y_i)^2\n$$\n关于$t$求导：\n$$\n\\frac{\\partial J}{\\partial t} = \\frac{2}{N}\\sum_{i = 1}^N(t - y_i) = 0\n$$\n求得：\n$$\nt = \\frac{1}{N}\\sum_{i = 1}^Ny_i = E(y)\n$$\n\n- MAE收敛于中值\n\n将$\\hat{y_i}$设为变量$t$：\n$$\nJ_{MAE} = \\frac{1}{N}\\sum_{i = 1}^N|t - y_i|\n$$\n关于$t$求导：\n$$\n\\frac{\\partial J}{\\partial t} = \\frac{1}{N}\\sum_{i = 1}^Nsgn(t - y_i) = 0\n$$\n显然在该种情况下应该取$t$为中值\n\n\n\n### 1.4 Huber Loss\n\n- 上面介绍了MSE和MAE，他们各有各的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，而Huber Loss则是将两者结合起来，原理很简单，就是误差接近 0 时使用 MSE，误差较大时使用 MAE：\n\n$$\nJ_{huber} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{\\left|y_{i}-\\hat{y}_{i}\\right| \\leq \\delta} \\frac{\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{2}+\\mathbb{I}_{\\left|y_{i}-\\hat{y}_{i}\\right|>\\delta}\\left(\\delta\\left|y_{i}-\\hat{y}_{i}\\right|-\\frac{1}{2} \\delta^{2}\\right)\n$$\n\n- 前半部分是MSE部分，后半部分是MAE部分，超参数$\\delta$为两个部分的连接处\n- MAE部分为$\\delta |y_i - \\hat{y_i}| - \\frac{1}{2}\\delta ^2$是为了在$|y_i - \\hat{y_i}| = \\delta$ 端点处连续可导\n\n![超参数为1的Huber Loss](https://i.loli.net/2021/11/09/e6FQMBY42PDGxtI.png)\n\n- Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个$\\delta$超参数。\n\n\n\n### 1.5 分位数损失（Quantile Loss）\n\n$$\nJ_{\\text {quant }}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{\\hat{y}_{i} \\geq y_{i}}(1-r)\\left|y_{i}-\\hat{y}_{i}\\right|+\\mathbb{I}_{\\hat{y}_{i}<y_{i}} r\\left|y_{i}-\\hat{y}_{i}\\right|\n$$\n\n- 这是一个分段函数，这个损失函数是一个分段的函数 ，将$\\hat{y_i} \\geq y_i$（高估） 和$\\hat{y_i} < y_i$（低估) 两种情况分开来，并分别给予不同的系数\n- 分位数损失实现了分别用不同的系数（r和1-r）控制高估和低估的损失\n- 特别地，当$r = 0.5$时分位数损失退化为 MAE 损失\n\n![Quantile Loss](https://i.loli.net/2021/11/09/BvpTeXqWFYRKoMU.png)\n\n\n\n### 1.6 交叉熵损失（Cross Entropy Loss）\n\n- 上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 \n\n##### 1.6.1 二分类\n\n$$\nJ_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)\n$$\n\n![二分类的交叉熵](https://i.loli.net/2021/11/09/hPt1nITJ624Llfp.png)\n\n- 证明：\n\n在二分类中我们通常将输出结果用sigmoid映射到区间$(0, 1)$，并将其作为该类的概率，由于只有两类，所以给定$x_i$求出类别为1或0的概率分别为：\n$$\np(y_i = 1|x_i) = \\hat{y_i} \\\\\np(y_i = 0|x_i) = 1 - \\hat{y_i}\n$$\n合并成一个式子：\n$$\np(y_i|x_i) = (\\hat{y_i})^{y_i}(1 - \\hat{y_i})^{1 - y_i}\n$$\n由于各数据点独立同分布，则似然可以表示为：\n$$\nL(x, y) = \\prod_{i=1}^{N}\\left(\\hat{y}_{i}\\right)^{y_{i}}\\left(1-\\hat{y}_{i}\\right)^{1-y_{i}}\n$$\n取负对数：\n$$\nN L L(x, y)=J_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)\n$$\n\n\n##### 1.6.2 多分类\n\n在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值$y_i$现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 Softmax 函数\n$$\nJ_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(\\hat{y}_{i}^{k}\\right)\n$$\n因为$y_i$是一个One-hot向量，所以还可以写为：\n$$\nJ_{C E}=-\\sum_{i=1}^{N} y_{i}^{c_{i}} \\log \\left(\\hat{y}_{i}^{c_{i}}\\right)\n$$\n其中$c_i$为样本$x_i$的目标类\n\n- 证明：\n\n对于一个样本，分类正确的概率为：\n$$\np(y_i|x_i) = \\prod_{k=1}^{K}\\left(\\hat{y}_{i}^{k}\\right)^{y_{i}^{k}}\n$$\n（其中$y_i^k和\\hat{y_i}^k$为该向量的第k维）\n\n因为所有样本相互，所有相乘再取负对数即可得到：\n$$\nN L L(x, y)=J_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(\\hat{y}_{i}^{k}\\right)\n$$\n\n\n### 1.7 合页损失（Hinge Loss）\n\n- Hinge Loss也是一种二分类损失函数\n\n$$\nJ_{\\text {hinge }}=\\sum_{i=1}^{N} \\max \\left(0,1-\\operatorname{sgn}\\left(y_{i}\\right) \\hat{y}_{i}\\right)\n$$\n\n下图是$y$为正类， 即$sgn(y) = 1$时，不同输出的合页损失示意图：\n\n![image-20211109225642368](https://i.loli.net/2021/11/09/KngeQOwp54JZRMf.png)\n\n- 可以看到当$y$为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在$(0, 1)$区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。**使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类**\n- **Hinge Loss常用在支持向量机（SVM）中，在SVM的软间隔中替换数学性质不好的0/1损失**\n\n- **Hinge Loss变种：**有些时候我们关注的并不是单个样本的分类结果，而是两个样本之间的相似性，所以会使用：\n\n$$\n\\ell= \\max(0, m + score(pos\\_pair) - score(neg\\_pair))\n$$\n\n其中两个score分别为正负样本对的得分，m是间隔参数margin，目的是**希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。这样能够拉开正负样本间的差距，更好区分正负样本**\n\n\n\n# 2 评估模型的指标\n\n### 2.1 基本概念\n\n![image-20211109233859657](https://i.loli.net/2021/11/09/OCNnaMs2ceBQIJz.png)\n\n\n\n### 2.2 查准率和查全率\n\n$$\n查准率 P（Precision） = \\frac{TP}{TP + FP}\n$$\n\n\n$$\n查全率 R（Recall） = \\frac{TP}{TP + FN}\n$$\n\n- 查准率可以直观理解为所有预测为正项的样本中有多少是真正的正项\n- 查全率可以直观理解为所有label是正项的样本中有多少被成功预测出来了\n- 理想情况下，查准率和查全率两者都越高越好。**然而事实上这两者在某些情况下是矛盾的，一般来说，查准率高时，查全率低；查确率低时，查全率高**\n\n\n\n### 2.3 准确率和错误率\n\n准确率：\n$$\naccuracy = \\frac{TP + TF}{TP + TN + FP + FN}\n$$\n\n- 即有多少样本被分类正确\n\n而错误率：\n$$\nerrorrate = 1 - accuracy\n$$\n\n\n### 2.4 P-R曲线\n\n<img src=\"https://i.loli.net/2021/11/10/zV1Sj4HyYfD3G5e.png\" alt=\"image-20211110000609300\" style=\"zoom: 80%;\" />\n\n- P-R曲线直观地显示出学习器在样本总体上地查全率、查准率，在进行比较时，**若一个学习器的P-R曲线被另一个学习器曲线“完全包住”，则可以断言后者的性能一定优于前者，如上图的B性能优于C，而A、B不一定**\n- 平衡点（BEP）时$P = R$时的取值，如上图A的BEP为0.8，而如果基于BEP比较，可知A优于B\n\n\n\n### 2.5 F函数\n\nBEP还是过于简化了些，更常用得是F1度量，我们可以取P和R的调和平均：\n$$\n\\frac{2}{F_1} = \\frac{1}{P} + \\frac{1}{R}\n$$\n求得：\n$$\nF1 = \\frac{2PR}{P + R}\n$$\n但是在许多应用中我们对查准率和查全率得重视程度不同，所以可以给予P和R不同的权重，取P和R的加权调和平均：\n$$\n\\frac{1}{F_{\\beta}} = \\frac{1}{1 + \\beta ^2}(\\frac{1}{P} + \\frac{\\beta ^2}{R})\n$$\n求得：\n$$\nF_{\\beta} = \\frac{(1 + \\beta ^2)PR}{\\beta ^2P + R}\n$$\n\n- $\\beta > 0$度量了查全率和查准率的相对重要性，$\\beta = 1$退化为标准的F1，$\\beta > 1$查全率有更大影响，$\\beta < 1$查准率有更大影响\n\n\n\n### 2.6 ROC与AUC\n\n##### 2.6.1 基本概念\n\n- 大多二分类问题是将输出的预测值与一个**分类阈值（threshold）**进行比较，若预测值大于阈值则为正类，反之则为负类\n\n- 根据预测值，我们可将测试样本进行排序，根据预测值由大到小，“最可能”是正例的排在前面，“最不可能”是正例的排到后面。**这样，分类过程就相当于以中间某个截断点（也就是分类阈值）将样本分为两部分，前一部分判定为正例，后一部分判定为反例**\n\n- 在不同的任务中，我们我们可以根据任务需求采取不同的截断点。例如如更重视查准率，可以将截断点设置靠前，更注重查全率，可以将截断点靠后\n- 而我们根据预测值进行排序后，按顺序将样本逐个作为正例进行预测，每次计算出**真正例率（TPR）**和**假正例率（FPR）**，以他们为横纵坐标就得到了**ROC曲线**\n\n\n\n##### 2.6.2 ROC曲线\n\n- 首先介绍真正例率和假正例率：\n\n$$\nTPR = \\frac{TP}{TP + FN}, \\\\\nFPR = \\frac{FP}{TN + FP}\n$$\n\n- ROC曲线：\n\n首先将分类阈值设最大，则所有类都被分类为负类，TPR和FPR都为0，然后每次将分类阈值设为下一个样本点的预测值（按预测值由大到小进行排序），记录每次的TPR和FPR，组成ROC曲线\n\n![image-20211110140633061](https://i.loli.net/2021/11/10/TUMHuC3N9rX18iz.png)\n\n但是现实中我们是基于有限个样本画的图，所以不会产生这么平滑的曲线，更多情况应该像下图：\n\n![image-20211110140853013](https://i.loli.net/2021/11/10/xG1TrPNfyimzq7Q.png)\n\n- 基于ROC的比较方法\n\n>如果一个学习器的ROC曲线完全被另一个学习器的”包住“，则后者性能优于前者\n>\n>若两者的曲线交叉，则可以通过ROC曲线所包裹的面积进行判断，即**AUC**\n\n\n\n##### 2.6.3 AUC\n\n- **AUC就是ROC曲线下的面积**，假定ROC曲线是由坐标为$$\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right),\\left(x_{3}, y_{3}\\right), \\cdots,\\left(x_{m}, y_{m}\\right)$$的点按序连接而形成，则AUC为：\n\n$$\nA U C=\\frac{1}{2} \\sum_{i=1}^{m-1}\\left(x_{i+1}-x_{i}\\right)\\left(y_{i}+y_{i+1}\\right)\n$$\n\n- 从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC， 即**AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性**\n\n- **所以AUC反应的是分类器对样本的排序能力**\n\n证明：\n\n设所有正类的集合$X = \\{ \\hat{X_1}, \\hat{X_2}, ..., \\hat{X_m}\\}$和负类的集合$Y = \\{ \\hat{Y_1}, \\hat{Y_2}, ..., \\hat{Y_n}\\}$，其中是每个样本对应的预测值，设分类阈值为c，$F_X(x)$和$F_Y(y)$分别为X和Y的分布函数，则$TPR(c) = 1 - F_X(c)$，$FPR(c) = 1 - F_Y(c)$\n\n设$t = FPR(c)$， 则$c = F_Y^{-1}(1 - t)$，则$ROC(t) = 1 - F_X(F_Y^{-1}(1 - t))$\n\n则：\n$$\nAUC = \\int_0^1ROC(t)dt \\\\\n=\\int_0^1 [1 - F_X(F_Y^{-1}(1 - t))] dt \\\\\n=\\int_0^1 [1 - F_X(F_Y^{-1}(t))] dt \\\\\n=\\int_{-\\infty}^{+\\infty} [1 - F_X(y)] dF_Y(y) \\\\\n=\\int_{-\\infty}^{+\\infty}P(X > y)f_Y(y)dy \\\\\n=\\int_{-\\infty}^{+\\infty}P(X > y, Y = y)dy \\\\\n=P(X > Y)\n$$\n\n\n##### 2.6.4 使用ROC和AUC的优点\n\n- **AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡（正负类样本不相同）的情况下，依然能够对分类器做出合理的评价**\n\n$$\nTPR = P(\\hat{Y} = 1 | Y = 1), \\\\\nFPR = P(\\hat{Y} = 1 | Y = 0)\n$$\n\n**由上式可得：无论Y的真实概率是多少， 都不会影响TPR和FPR**\n\n而PR曲线更关注正例\n\n- ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响，有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的查全率就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少\n\n\n\n- 上面几种评估方法都是**用于分类**的评估方法，而在**回归问题**当中，这些一般是不适用的，回归问题中我们比较常用的评估方法有一下两种\n\n\n\n### 2.7 平方根误差（RMSE）\n\n$$\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n(y_i - \\hat y_i)^2}{n}}\n$$\n\n- 其实RMSE就是MSE开了个根，但是我们做这样的处理能让**误差和结果值在同一个数量级上，这样能更直观有效的反应拟合程度**\n- 但是RMSE有着和MSE一样的缺点，那就是**对离群值十分敏感，健壮性很差**\n- 比如在实际应用中，有可能在对于预测某些剧集的流量时，以便进行广告投放，在95%的区间内的预测误差都十分低，比如小于1%，这是相当不错的预测结果。但是在总体上，无论运用何种模型，RMSE可能都一直居高不下。**原因是可能在剩余的5%区间里有非常严重的离群点，比如某些冷门剧、新上映的剧**\n- 对此我们可以选择对数据进行处理，或者换一种模型指标\n\n\n\n### 2.8 平均绝对百分比误差（MAPE）\n\n$$\nMAPE = \\sum_{i = 1}^n |\\frac{y_i - \\hat{y}_i}{y_i}| \\times \\frac{100}{n}\n$$\n\n- 相比RMSE， MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的影响\n\n","slug":"常用损失函数和评估模型的指标","published":1,"updated":"2023-01-08T06:26:05.500Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1m000k7cszgl4u2e1p","content":"<h1 id=\"1-常用损失函数\"><a href=\"#1-常用损失函数\" class=\"headerlink\" title=\"1 常用损失函数\"></a>1 常用损失函数</h1><h3 id=\"1-1-0-1损失函数\"><a href=\"#1-1-0-1损失函数\" class=\"headerlink\" title=\"1.1 0-1损失函数\"></a>1.1 0-1损失函数</h3><script type=\"math/tex; mode=display\">\nL(y, \\hat{y}) = \\begin{cases}\n1 & y\\neq \\hat{y}\\\\\n0 & y = \\hat{y}\n\\end{cases}</script><ul>\n<li>0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用</li>\n<li>感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足$|y - \\hat{y}| &lt; T$时认为相等，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y, \\hat{y}) = \\begin{cases}\n1 & |y - \\hat{y}| \\geq T\\\\\n0 & |y - \\hat{y}| < T\n\\end{cases}</script><h3 id=\"1-2-均方差损失函数（MSE）\"><a href=\"#1-2-均方差损失函数（MSE）\" class=\"headerlink\" title=\"1.2 均方差损失函数（MSE）\"></a>1.2 均方差损失函数（MSE）</h3><script type=\"math/tex; mode=display\">\nJ_{MSE} = \\frac{1}{N} \\sum_{i = 1}^N(y_i - \\hat{y_i})^2</script><ul>\n<li>也称L2 Loss</li>\n</ul>\n<h5 id=\"1-2-1-证明\"><a href=\"#1-2-1-证明\" class=\"headerlink\" title=\"1.2.1 证明\"></a>1.2.1 证明</h5><p>假设预测值和真实值的误差$\\epsilon$服从标准正态分布，则给定一个$x_i$输出真实值$y_i$的概率为：</p>\n<script type=\"math/tex; mode=display\">\np(\\hat y_i = y_i|x_i) = p(\\hat y_i = f(x_i) + \\epsilon) | x_i) = p(\\epsilon) = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{(y_i - \\hat{y_i})^2}{2})</script><p>其实就是极大似然估计，我们要寻找一组参数，使$p(y_i|x_i)$最大</p>\n<p>进一步对所有样本，由于他们相互独立，所以所有样本都正好取到真实值$y$的概率为：</p>\n<script type=\"math/tex; mode=display\">\nL(x, y) = \\prod_{i = 1}^N\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{(y_i - \\hat{y_i})^2}{2})</script><p>现在我们就要使$L(x, y)$最大，为了方便计算，我们取对数：</p>\n<script type=\"math/tex; mode=display\">\nLL(x, y) = log(L(x, y)) = -\\frac{N}{2}log2\\pi - \\frac{1}{2}\\sum_{i = 1}^N(y_i - \\hat{y_i})^2</script><p>把第一项无关项去掉，再取负：</p>\n<script type=\"math/tex; mode=display\">\nNLL(x, y) = \\frac{1}{2}\\sum_{i = 1}^N(y_i - \\hat{y_i})^2</script><p>即得到均方差形式</p>\n<h5 id=\"1-2-2-为什么可以用极大似然\"><a href=\"#1-2-2-为什么可以用极大似然\" class=\"headerlink\" title=\"1.2.2 为什么可以用极大似然\"></a>1.2.2 为什么可以用极大似然</h5><p><strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，拟合数据最好的情况就是所有的$y_i = \\hat{y_i}$，即每个样本的$p(y_i|x_i)$取最大，即$L(x, y)$取最大，由于对数运算不改变单调性，并且最后取了个负值，所以即$NLL(x, y)$取最小</strong></p>\n<h3 id=\"1-3-平均绝对误差损失（MAE）\"><a href=\"#1-3-平均绝对误差损失（MAE）\" class=\"headerlink\" title=\"1.3 平均绝对误差损失（MAE）\"></a>1.3 平均绝对误差损失（MAE）</h3><script type=\"math/tex; mode=display\">\nJ_{MAE} = \\frac{1}{N}\\sum_{i = 1}^N|y_i - \\hat{y_i}|</script><ul>\n<li>也称L1 Loss</li>\n</ul>\n<h5 id=\"1-3-1-拉普拉斯分布\"><a href=\"#1-3-1-拉普拉斯分布\" class=\"headerlink\" title=\"1.3.1 拉普拉斯分布\"></a>1.3.1 拉普拉斯分布</h5><script type=\"math/tex; mode=display\">\nf(x|\\mu, b) = \\frac{1}{2b}exp(-\\frac{|x - \\mu|}{b})</script><p>期望值：$\\mu$             方差：$2b^2$</p>\n<p><img src=\"https://i.loli.net/2021/11/08/yWY3hI4ioKpADRF.png\" alt=\"Laplace_distribution_pdf\" style=\"zoom: 25%;\" /></p>\n<h5 id=\"1-3-2-证明\"><a href=\"#1-3-2-证明\" class=\"headerlink\" title=\"1.3.2 证明\"></a>1.3.2 证明</h5><p>假设预测值和真实值的误差服从拉普拉斯分布（$\\mu = 0, b = 1$）</p>\n<script type=\"math/tex; mode=display\">\np(y_i | x_i) = \\frac{1}{2}exp(-{|y_i - \\hat{y_i}|})</script><p>剩余证明和上述MSE证明过程一样</p>\n<h5 id=\"1-3-3-MSE和MAE的区别：\"><a href=\"#1-3-3-MSE和MAE的区别：\" class=\"headerlink\" title=\"1.3.3 MSE和MAE的区别：\"></a>1.3.3 MSE和MAE的区别：</h5><ul>\n<li><strong>MSE 损失相比 MAE 通常可以更快地收敛</strong></li>\n</ul>\n<p>关于$\\hat{y_i}$求导时，MSE为$-(y_i - \\hat{y_i})$，MAE为$\\pm1$，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的（当然也可以通过调整学习率缓解这个问题，但总体来说还是MSE更快）。</p>\n<ul>\n<li><strong>MAE对于离群值更加健壮，即更加不易受到离群值影响</strong></li>\n</ul>\n<ol>\n<li>由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失</li>\n<li>MSE 假设了误差服从正态分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于离群值更加健壮</li>\n</ol>\n<h5 id=\"1-3-4-MSE和MAE的收敛\"><a href=\"#1-3-4-MSE和MAE的收敛\" class=\"headerlink\" title=\"1.3.4 MSE和MAE的收敛\"></a>1.3.4 MSE和MAE的收敛</h5><ul>\n<li>MSE收敛于均值</li>\n</ul>\n<p>将$\\hat{y_i}$设为变量$t$：</p>\n<script type=\"math/tex; mode=display\">\nJ_{MSE} = \\frac{1}{N}\\sum_{i = 1}^N(t - y_i)^2</script><p>关于$t$求导：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial t} = \\frac{2}{N}\\sum_{i = 1}^N(t - y_i) = 0</script><p>求得：</p>\n<script type=\"math/tex; mode=display\">\nt = \\frac{1}{N}\\sum_{i = 1}^Ny_i = E(y)</script><ul>\n<li>MAE收敛于中值</li>\n</ul>\n<p>将$\\hat{y_i}$设为变量$t$：</p>\n<script type=\"math/tex; mode=display\">\nJ_{MAE} = \\frac{1}{N}\\sum_{i = 1}^N|t - y_i|</script><p>关于$t$求导：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial t} = \\frac{1}{N}\\sum_{i = 1}^Nsgn(t - y_i) = 0</script><p>显然在该种情况下应该取$t$为中值</p>\n<h3 id=\"1-4-Huber-Loss\"><a href=\"#1-4-Huber-Loss\" class=\"headerlink\" title=\"1.4 Huber Loss\"></a>1.4 Huber Loss</h3><ul>\n<li>上面介绍了MSE和MAE，他们各有各的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，而Huber Loss则是将两者结合起来，原理很简单，就是误差接近 0 时使用 MSE，误差较大时使用 MAE：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ_{huber} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{\\left|y_{i}-\\hat{y}_{i}\\right| \\leq \\delta} \\frac{\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{2}+\\mathbb{I}_{\\left|y_{i}-\\hat{y}_{i}\\right|>\\delta}\\left(\\delta\\left|y_{i}-\\hat{y}_{i}\\right|-\\frac{1}{2} \\delta^{2}\\right)</script><ul>\n<li>前半部分是MSE部分，后半部分是MAE部分，超参数$\\delta$为两个部分的连接处</li>\n<li>MAE部分为$\\delta |y_i - \\hat{y_i}| - \\frac{1}{2}\\delta ^2$是为了在$|y_i - \\hat{y_i}| = \\delta$ 端点处连续可导</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/09/e6FQMBY42PDGxtI.png\" alt=\"超参数为1的Huber Loss\"></p>\n<ul>\n<li>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个$\\delta$超参数。</li>\n</ul>\n<h3 id=\"1-5-分位数损失（Quantile-Loss）\"><a href=\"#1-5-分位数损失（Quantile-Loss）\" class=\"headerlink\" title=\"1.5 分位数损失（Quantile Loss）\"></a>1.5 分位数损失（Quantile Loss）</h3><script type=\"math/tex; mode=display\">\nJ_{\\text {quant }}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{\\hat{y}_{i} \\geq y_{i}}(1-r)\\left|y_{i}-\\hat{y}_{i}\\right|+\\mathbb{I}_{\\hat{y}_{i}<y_{i}} r\\left|y_{i}-\\hat{y}_{i}\\right|</script><ul>\n<li>这是一个分段函数，这个损失函数是一个分段的函数 ，将$\\hat{y_i} \\geq y_i$（高估） 和$\\hat{y_i} &lt; y_i$（低估) 两种情况分开来，并分别给予不同的系数</li>\n<li>分位数损失实现了分别用不同的系数（r和1-r）控制高估和低估的损失</li>\n<li>特别地，当$r = 0.5$时分位数损失退化为 MAE 损失</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/09/BvpTeXqWFYRKoMU.png\" alt=\"Quantile Loss\"></p>\n<h3 id=\"1-6-交叉熵损失（Cross-Entropy-Loss）\"><a href=\"#1-6-交叉熵损失（Cross-Entropy-Loss）\" class=\"headerlink\" title=\"1.6 交叉熵损失（Cross Entropy Loss）\"></a>1.6 交叉熵损失（Cross Entropy Loss）</h3><ul>\n<li>上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 </li>\n</ul>\n<h5 id=\"1-6-1-二分类\"><a href=\"#1-6-1-二分类\" class=\"headerlink\" title=\"1.6.1 二分类\"></a>1.6.1 二分类</h5><script type=\"math/tex; mode=display\">\nJ_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)</script><p><img src=\"https://i.loli.net/2021/11/09/hPt1nITJ624Llfp.png\" alt=\"二分类的交叉熵\"></p>\n<ul>\n<li>证明：</li>\n</ul>\n<p>在二分类中我们通常将输出结果用sigmoid映射到区间$(0, 1)$，并将其作为该类的概率，由于只有两类，所以给定$x_i$求出类别为1或0的概率分别为：</p>\n<script type=\"math/tex; mode=display\">\np(y_i = 1|x_i) = \\hat{y_i} \\\\\np(y_i = 0|x_i) = 1 - \\hat{y_i}</script><p>合并成一个式子：</p>\n<script type=\"math/tex; mode=display\">\np(y_i|x_i) = (\\hat{y_i})^{y_i}(1 - \\hat{y_i})^{1 - y_i}</script><p>由于各数据点独立同分布，则似然可以表示为：</p>\n<script type=\"math/tex; mode=display\">\nL(x, y) = \\prod_{i=1}^{N}\\left(\\hat{y}_{i}\\right)^{y_{i}}\\left(1-\\hat{y}_{i}\\right)^{1-y_{i}}</script><p>取负对数：</p>\n<script type=\"math/tex; mode=display\">\nN L L(x, y)=J_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)</script><h5 id=\"1-6-2-多分类\"><a href=\"#1-6-2-多分类\" class=\"headerlink\" title=\"1.6.2 多分类\"></a>1.6.2 多分类</h5><p>在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值$y_i$现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 Softmax 函数</p>\n<script type=\"math/tex; mode=display\">\nJ_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(\\hat{y}_{i}^{k}\\right)</script><p>因为$y_i$是一个One-hot向量，所以还可以写为：</p>\n<script type=\"math/tex; mode=display\">\nJ_{C E}=-\\sum_{i=1}^{N} y_{i}^{c_{i}} \\log \\left(\\hat{y}_{i}^{c_{i}}\\right)</script><p>其中$c_i$为样本$x_i$的目标类</p>\n<ul>\n<li>证明：</li>\n</ul>\n<p>对于一个样本，分类正确的概率为：</p>\n<script type=\"math/tex; mode=display\">\np(y_i|x_i) = \\prod_{k=1}^{K}\\left(\\hat{y}_{i}^{k}\\right)^{y_{i}^{k}}</script><p>（其中$y_i^k和\\hat{y_i}^k$为该向量的第k维）</p>\n<p>因为所有样本相互，所有相乘再取负对数即可得到：</p>\n<script type=\"math/tex; mode=display\">\nN L L(x, y)=J_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(\\hat{y}_{i}^{k}\\right)</script><h3 id=\"1-7-合页损失（Hinge-Loss）\"><a href=\"#1-7-合页损失（Hinge-Loss）\" class=\"headerlink\" title=\"1.7 合页损失（Hinge Loss）\"></a>1.7 合页损失（Hinge Loss）</h3><ul>\n<li>Hinge Loss也是一种二分类损失函数</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ_{\\text {hinge }}=\\sum_{i=1}^{N} \\max \\left(0,1-\\operatorname{sgn}\\left(y_{i}\\right) \\hat{y}_{i}\\right)</script><p>下图是$y$为正类， 即$sgn(y) = 1$时，不同输出的合页损失示意图：</p>\n<p><img src=\"https://i.loli.net/2021/11/09/KngeQOwp54JZRMf.png\" alt=\"image-20211109225642368\"></p>\n<ul>\n<li>可以看到当$y$为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在$(0, 1)$区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。<strong>使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类</strong></li>\n<li><p><strong>Hinge Loss常用在支持向量机（SVM）中，在SVM的软间隔中替换数学性质不好的0/1损失</strong></p>\n</li>\n<li><p><strong>Hinge Loss变种：</strong>有些时候我们关注的并不是单个样本的分类结果，而是两个样本之间的相似性，所以会使用：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell= \\max(0, m + score(pos\\_pair) - score(neg\\_pair))</script><p>其中两个score分别为正负样本对的得分，m是间隔参数margin，目的是<strong>希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。这样能够拉开正负样本间的差距，更好区分正负样本</strong></p>\n<h1 id=\"2-评估模型的指标\"><a href=\"#2-评估模型的指标\" class=\"headerlink\" title=\"2 评估模型的指标\"></a>2 评估模型的指标</h1><h3 id=\"2-1-基本概念\"><a href=\"#2-1-基本概念\" class=\"headerlink\" title=\"2.1 基本概念\"></a>2.1 基本概念</h3><p><img src=\"https://i.loli.net/2021/11/09/OCNnaMs2ceBQIJz.png\" alt=\"image-20211109233859657\"></p>\n<h3 id=\"2-2-查准率和查全率\"><a href=\"#2-2-查准率和查全率\" class=\"headerlink\" title=\"2.2 查准率和查全率\"></a>2.2 查准率和查全率</h3><script type=\"math/tex; mode=display\">\n查准率 P（Precision） = \\frac{TP}{TP + FP}</script><script type=\"math/tex; mode=display\">\n查全率 R（Recall） = \\frac{TP}{TP + FN}</script><ul>\n<li>查准率可以直观理解为所有预测为正项的样本中有多少是真正的正项</li>\n<li>查全率可以直观理解为所有label是正项的样本中有多少被成功预测出来了</li>\n<li>理想情况下，查准率和查全率两者都越高越好。<strong>然而事实上这两者在某些情况下是矛盾的，一般来说，查准率高时，查全率低；查确率低时，查全率高</strong></li>\n</ul>\n<h3 id=\"2-3-准确率和错误率\"><a href=\"#2-3-准确率和错误率\" class=\"headerlink\" title=\"2.3 准确率和错误率\"></a>2.3 准确率和错误率</h3><p>准确率：</p>\n<script type=\"math/tex; mode=display\">\naccuracy = \\frac{TP + TF}{TP + TN + FP + FN}</script><ul>\n<li>即有多少样本被分类正确</li>\n</ul>\n<p>而错误率：</p>\n<script type=\"math/tex; mode=display\">\nerrorrate = 1 - accuracy</script><h3 id=\"2-4-P-R曲线\"><a href=\"#2-4-P-R曲线\" class=\"headerlink\" title=\"2.4 P-R曲线\"></a>2.4 P-R曲线</h3><p><img src=\"https://i.loli.net/2021/11/10/zV1Sj4HyYfD3G5e.png\" alt=\"image-20211110000609300\" style=\"zoom: 80%;\" /></p>\n<ul>\n<li>P-R曲线直观地显示出学习器在样本总体上地查全率、查准率，在进行比较时，<strong>若一个学习器的P-R曲线被另一个学习器曲线“完全包住”，则可以断言后者的性能一定优于前者，如上图的B性能优于C，而A、B不一定</strong></li>\n<li>平衡点（BEP）时$P = R$时的取值，如上图A的BEP为0.8，而如果基于BEP比较，可知A优于B</li>\n</ul>\n<h3 id=\"2-5-F函数\"><a href=\"#2-5-F函数\" class=\"headerlink\" title=\"2.5 F函数\"></a>2.5 F函数</h3><p>BEP还是过于简化了些，更常用得是F1度量，我们可以取P和R的调和平均：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{2}{F_1} = \\frac{1}{P} + \\frac{1}{R}</script><p>求得：</p>\n<script type=\"math/tex; mode=display\">\nF1 = \\frac{2PR}{P + R}</script><p>但是在许多应用中我们对查准率和查全率得重视程度不同，所以可以给予P和R不同的权重，取P和R的加权调和平均：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{F_{\\beta}} = \\frac{1}{1 + \\beta ^2}(\\frac{1}{P} + \\frac{\\beta ^2}{R})</script><p>求得：</p>\n<script type=\"math/tex; mode=display\">\nF_{\\beta} = \\frac{(1 + \\beta ^2)PR}{\\beta ^2P + R}</script><ul>\n<li>$\\beta &gt; 0$度量了查全率和查准率的相对重要性，$\\beta = 1$退化为标准的F1，$\\beta &gt; 1$查全率有更大影响，$\\beta &lt; 1$查准率有更大影响</li>\n</ul>\n<h3 id=\"2-6-ROC与AUC\"><a href=\"#2-6-ROC与AUC\" class=\"headerlink\" title=\"2.6 ROC与AUC\"></a>2.6 ROC与AUC</h3><h5 id=\"2-6-1-基本概念\"><a href=\"#2-6-1-基本概念\" class=\"headerlink\" title=\"2.6.1 基本概念\"></a>2.6.1 基本概念</h5><ul>\n<li><p>大多二分类问题是将输出的预测值与一个<strong>分类阈值（threshold）</strong>进行比较，若预测值大于阈值则为正类，反之则为负类</p>\n</li>\n<li><p>根据预测值，我们可将测试样本进行排序，根据预测值由大到小，“最可能”是正例的排在前面，“最不可能”是正例的排到后面。<strong>这样，分类过程就相当于以中间某个截断点（也就是分类阈值）将样本分为两部分，前一部分判定为正例，后一部分判定为反例</strong></p>\n</li>\n<li><p>在不同的任务中，我们我们可以根据任务需求采取不同的截断点。例如如更重视查准率，可以将截断点设置靠前，更注重查全率，可以将截断点靠后</p>\n</li>\n<li>而我们根据预测值进行排序后，按顺序将样本逐个作为正例进行预测，每次计算出<strong>真正例率（TPR）</strong>和<strong>假正例率（FPR）</strong>，以他们为横纵坐标就得到了<strong>ROC曲线</strong></li>\n</ul>\n<h5 id=\"2-6-2-ROC曲线\"><a href=\"#2-6-2-ROC曲线\" class=\"headerlink\" title=\"2.6.2 ROC曲线\"></a>2.6.2 ROC曲线</h5><ul>\n<li>首先介绍真正例率和假正例率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nTPR = \\frac{TP}{TP + FN}, \\\\\nFPR = \\frac{FP}{TN + FP}</script><ul>\n<li>ROC曲线：</li>\n</ul>\n<p>首先将分类阈值设最大，则所有类都被分类为负类，TPR和FPR都为0，然后每次将分类阈值设为下一个样本点的预测值（按预测值由大到小进行排序），记录每次的TPR和FPR，组成ROC曲线</p>\n<p><img src=\"https://i.loli.net/2021/11/10/TUMHuC3N9rX18iz.png\" alt=\"image-20211110140633061\"></p>\n<p>但是现实中我们是基于有限个样本画的图，所以不会产生这么平滑的曲线，更多情况应该像下图：</p>\n<p><img src=\"https://i.loli.net/2021/11/10/xG1TrPNfyimzq7Q.png\" alt=\"image-20211110140853013\"></p>\n<ul>\n<li>基于ROC的比较方法</li>\n</ul>\n<blockquote>\n<p>如果一个学习器的ROC曲线完全被另一个学习器的”包住“，则后者性能优于前者</p>\n<p>若两者的曲线交叉，则可以通过ROC曲线所包裹的面积进行判断，即<strong>AUC</strong></p>\n</blockquote>\n<h5 id=\"2-6-3-AUC\"><a href=\"#2-6-3-AUC\" class=\"headerlink\" title=\"2.6.3 AUC\"></a>2.6.3 AUC</h5><ul>\n<li><strong>AUC就是ROC曲线下的面积</strong>，假定ROC曲线是由坐标为<script type=\"math/tex\">\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right),\\left(x_{3}, y_{3}\\right), \\cdots,\\left(x_{m}, y_{m}\\right)</script>的点按序连接而形成，则AUC为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nA U C=\\frac{1}{2} \\sum_{i=1}^{m-1}\\left(x_{i+1}-x_{i}\\right)\\left(y_{i}+y_{i+1}\\right)</script><ul>\n<li><p>从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1&gt;p0的概率就等于AUC， 即<strong>AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性</strong></p>\n</li>\n<li><p><strong>所以AUC反应的是分类器对样本的排序能力</strong></p>\n</li>\n</ul>\n<p>证明：</p>\n<p>设所有正类的集合$X = { \\hat{X_1}, \\hat{X_2}, …, \\hat{X_m}}$和负类的集合$Y = { \\hat{Y_1}, \\hat{Y_2}, …, \\hat{Y_n}}$，其中是每个样本对应的预测值，设分类阈值为c，$F_X(x)$和$F_Y(y)$分别为X和Y的分布函数，则$TPR(c) = 1 - F_X(c)$，$FPR(c) = 1 - F_Y(c)$</p>\n<p>设$t = FPR(c)$， 则$c = F_Y^{-1}(1 - t)$，则$ROC(t) = 1 - F_X(F_Y^{-1}(1 - t))$</p>\n<p>则：</p>\n<script type=\"math/tex; mode=display\">\nAUC = \\int_0^1ROC(t)dt \\\\\n=\\int_0^1 [1 - F_X(F_Y^{-1}(1 - t))] dt \\\\\n=\\int_0^1 [1 - F_X(F_Y^{-1}(t))] dt \\\\\n=\\int_{-\\infty}^{+\\infty} [1 - F_X(y)] dF_Y(y) \\\\\n=\\int_{-\\infty}^{+\\infty}P(X > y)f_Y(y)dy \\\\\n=\\int_{-\\infty}^{+\\infty}P(X > y, Y = y)dy \\\\\n=P(X > Y)</script><h5 id=\"2-6-4-使用ROC和AUC的优点\"><a href=\"#2-6-4-使用ROC和AUC的优点\" class=\"headerlink\" title=\"2.6.4 使用ROC和AUC的优点\"></a>2.6.4 使用ROC和AUC的优点</h5><ul>\n<li><strong>AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡（正负类样本不相同）的情况下，依然能够对分类器做出合理的评价</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nTPR = P(\\hat{Y} = 1 | Y = 1), \\\\\nFPR = P(\\hat{Y} = 1 | Y = 0)</script><p><strong>由上式可得：无论Y的真实概率是多少， 都不会影响TPR和FPR</strong></p>\n<p>而PR曲线更关注正例</p>\n<ul>\n<li>ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响，有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的查全率就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少</li>\n</ul>\n<ul>\n<li>上面几种评估方法都是<strong>用于分类</strong>的评估方法，而在<strong>回归问题</strong>当中，这些一般是不适用的，回归问题中我们比较常用的评估方法有一下两种</li>\n</ul>\n<h3 id=\"2-7-平方根误差（RMSE）\"><a href=\"#2-7-平方根误差（RMSE）\" class=\"headerlink\" title=\"2.7 平方根误差（RMSE）\"></a>2.7 平方根误差（RMSE）</h3><script type=\"math/tex; mode=display\">\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n(y_i - \\hat y_i)^2}{n}}</script><ul>\n<li>其实RMSE就是MSE开了个根，但是我们做这样的处理能让<strong>误差和结果值在同一个数量级上，这样能更直观有效的反应拟合程度</strong></li>\n<li>但是RMSE有着和MSE一样的缺点，那就是<strong>对离群值十分敏感，健壮性很差</strong></li>\n<li>比如在实际应用中，有可能在对于预测某些剧集的流量时，以便进行广告投放，在95%的区间内的预测误差都十分低，比如小于1%，这是相当不错的预测结果。但是在总体上，无论运用何种模型，RMSE可能都一直居高不下。<strong>原因是可能在剩余的5%区间里有非常严重的离群点，比如某些冷门剧、新上映的剧</strong></li>\n<li>对此我们可以选择对数据进行处理，或者换一种模型指标</li>\n</ul>\n<h3 id=\"2-8-平均绝对百分比误差（MAPE）\"><a href=\"#2-8-平均绝对百分比误差（MAPE）\" class=\"headerlink\" title=\"2.8 平均绝对百分比误差（MAPE）\"></a>2.8 平均绝对百分比误差（MAPE）</h3><script type=\"math/tex; mode=display\">\nMAPE = \\sum_{i = 1}^n |\\frac{y_i - \\hat{y}_i}{y_i}| \\times \\frac{100}{n}</script><ul>\n<li>相比RMSE， MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的影响</li>\n</ul>\n","site":{"data":{}},"wordcount":3540,"excerpt":"","more":"<h1 id=\"1-常用损失函数\"><a href=\"#1-常用损失函数\" class=\"headerlink\" title=\"1 常用损失函数\"></a>1 常用损失函数</h1><h3 id=\"1-1-0-1损失函数\"><a href=\"#1-1-0-1损失函数\" class=\"headerlink\" title=\"1.1 0-1损失函数\"></a>1.1 0-1损失函数</h3><script type=\"math/tex; mode=display\">\nL(y, \\hat{y}) = \\begin{cases}\n1 & y\\neq \\hat{y}\\\\\n0 & y = \\hat{y}\n\\end{cases}</script><ul>\n<li>0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用</li>\n<li>感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足$|y - \\hat{y}| &lt; T$时认为相等，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y, \\hat{y}) = \\begin{cases}\n1 & |y - \\hat{y}| \\geq T\\\\\n0 & |y - \\hat{y}| < T\n\\end{cases}</script><h3 id=\"1-2-均方差损失函数（MSE）\"><a href=\"#1-2-均方差损失函数（MSE）\" class=\"headerlink\" title=\"1.2 均方差损失函数（MSE）\"></a>1.2 均方差损失函数（MSE）</h3><script type=\"math/tex; mode=display\">\nJ_{MSE} = \\frac{1}{N} \\sum_{i = 1}^N(y_i - \\hat{y_i})^2</script><ul>\n<li>也称L2 Loss</li>\n</ul>\n<h5 id=\"1-2-1-证明\"><a href=\"#1-2-1-证明\" class=\"headerlink\" title=\"1.2.1 证明\"></a>1.2.1 证明</h5><p>假设预测值和真实值的误差$\\epsilon$服从标准正态分布，则给定一个$x_i$输出真实值$y_i$的概率为：</p>\n<script type=\"math/tex; mode=display\">\np(\\hat y_i = y_i|x_i) = p(\\hat y_i = f(x_i) + \\epsilon) | x_i) = p(\\epsilon) = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{(y_i - \\hat{y_i})^2}{2})</script><p>其实就是极大似然估计，我们要寻找一组参数，使$p(y_i|x_i)$最大</p>\n<p>进一步对所有样本，由于他们相互独立，所以所有样本都正好取到真实值$y$的概率为：</p>\n<script type=\"math/tex; mode=display\">\nL(x, y) = \\prod_{i = 1}^N\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{(y_i - \\hat{y_i})^2}{2})</script><p>现在我们就要使$L(x, y)$最大，为了方便计算，我们取对数：</p>\n<script type=\"math/tex; mode=display\">\nLL(x, y) = log(L(x, y)) = -\\frac{N}{2}log2\\pi - \\frac{1}{2}\\sum_{i = 1}^N(y_i - \\hat{y_i})^2</script><p>把第一项无关项去掉，再取负：</p>\n<script type=\"math/tex; mode=display\">\nNLL(x, y) = \\frac{1}{2}\\sum_{i = 1}^N(y_i - \\hat{y_i})^2</script><p>即得到均方差形式</p>\n<h5 id=\"1-2-2-为什么可以用极大似然\"><a href=\"#1-2-2-为什么可以用极大似然\" class=\"headerlink\" title=\"1.2.2 为什么可以用极大似然\"></a>1.2.2 为什么可以用极大似然</h5><p><strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，拟合数据最好的情况就是所有的$y_i = \\hat{y_i}$，即每个样本的$p(y_i|x_i)$取最大，即$L(x, y)$取最大，由于对数运算不改变单调性，并且最后取了个负值，所以即$NLL(x, y)$取最小</strong></p>\n<h3 id=\"1-3-平均绝对误差损失（MAE）\"><a href=\"#1-3-平均绝对误差损失（MAE）\" class=\"headerlink\" title=\"1.3 平均绝对误差损失（MAE）\"></a>1.3 平均绝对误差损失（MAE）</h3><script type=\"math/tex; mode=display\">\nJ_{MAE} = \\frac{1}{N}\\sum_{i = 1}^N|y_i - \\hat{y_i}|</script><ul>\n<li>也称L1 Loss</li>\n</ul>\n<h5 id=\"1-3-1-拉普拉斯分布\"><a href=\"#1-3-1-拉普拉斯分布\" class=\"headerlink\" title=\"1.3.1 拉普拉斯分布\"></a>1.3.1 拉普拉斯分布</h5><script type=\"math/tex; mode=display\">\nf(x|\\mu, b) = \\frac{1}{2b}exp(-\\frac{|x - \\mu|}{b})</script><p>期望值：$\\mu$             方差：$2b^2$</p>\n<p><img src=\"https://i.loli.net/2021/11/08/yWY3hI4ioKpADRF.png\" alt=\"Laplace_distribution_pdf\" style=\"zoom: 25%;\" /></p>\n<h5 id=\"1-3-2-证明\"><a href=\"#1-3-2-证明\" class=\"headerlink\" title=\"1.3.2 证明\"></a>1.3.2 证明</h5><p>假设预测值和真实值的误差服从拉普拉斯分布（$\\mu = 0, b = 1$）</p>\n<script type=\"math/tex; mode=display\">\np(y_i | x_i) = \\frac{1}{2}exp(-{|y_i - \\hat{y_i}|})</script><p>剩余证明和上述MSE证明过程一样</p>\n<h5 id=\"1-3-3-MSE和MAE的区别：\"><a href=\"#1-3-3-MSE和MAE的区别：\" class=\"headerlink\" title=\"1.3.3 MSE和MAE的区别：\"></a>1.3.3 MSE和MAE的区别：</h5><ul>\n<li><strong>MSE 损失相比 MAE 通常可以更快地收敛</strong></li>\n</ul>\n<p>关于$\\hat{y_i}$求导时，MSE为$-(y_i - \\hat{y_i})$，MAE为$\\pm1$，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的（当然也可以通过调整学习率缓解这个问题，但总体来说还是MSE更快）。</p>\n<ul>\n<li><strong>MAE对于离群值更加健壮，即更加不易受到离群值影响</strong></li>\n</ul>\n<ol>\n<li>由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失</li>\n<li>MSE 假设了误差服从正态分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于离群值更加健壮</li>\n</ol>\n<h5 id=\"1-3-4-MSE和MAE的收敛\"><a href=\"#1-3-4-MSE和MAE的收敛\" class=\"headerlink\" title=\"1.3.4 MSE和MAE的收敛\"></a>1.3.4 MSE和MAE的收敛</h5><ul>\n<li>MSE收敛于均值</li>\n</ul>\n<p>将$\\hat{y_i}$设为变量$t$：</p>\n<script type=\"math/tex; mode=display\">\nJ_{MSE} = \\frac{1}{N}\\sum_{i = 1}^N(t - y_i)^2</script><p>关于$t$求导：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial t} = \\frac{2}{N}\\sum_{i = 1}^N(t - y_i) = 0</script><p>求得：</p>\n<script type=\"math/tex; mode=display\">\nt = \\frac{1}{N}\\sum_{i = 1}^Ny_i = E(y)</script><ul>\n<li>MAE收敛于中值</li>\n</ul>\n<p>将$\\hat{y_i}$设为变量$t$：</p>\n<script type=\"math/tex; mode=display\">\nJ_{MAE} = \\frac{1}{N}\\sum_{i = 1}^N|t - y_i|</script><p>关于$t$求导：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial t} = \\frac{1}{N}\\sum_{i = 1}^Nsgn(t - y_i) = 0</script><p>显然在该种情况下应该取$t$为中值</p>\n<h3 id=\"1-4-Huber-Loss\"><a href=\"#1-4-Huber-Loss\" class=\"headerlink\" title=\"1.4 Huber Loss\"></a>1.4 Huber Loss</h3><ul>\n<li>上面介绍了MSE和MAE，他们各有各的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，而Huber Loss则是将两者结合起来，原理很简单，就是误差接近 0 时使用 MSE，误差较大时使用 MAE：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ_{huber} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{\\left|y_{i}-\\hat{y}_{i}\\right| \\leq \\delta} \\frac{\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{2}+\\mathbb{I}_{\\left|y_{i}-\\hat{y}_{i}\\right|>\\delta}\\left(\\delta\\left|y_{i}-\\hat{y}_{i}\\right|-\\frac{1}{2} \\delta^{2}\\right)</script><ul>\n<li>前半部分是MSE部分，后半部分是MAE部分，超参数$\\delta$为两个部分的连接处</li>\n<li>MAE部分为$\\delta |y_i - \\hat{y_i}| - \\frac{1}{2}\\delta ^2$是为了在$|y_i - \\hat{y_i}| = \\delta$ 端点处连续可导</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/09/e6FQMBY42PDGxtI.png\" alt=\"超参数为1的Huber Loss\"></p>\n<ul>\n<li>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个$\\delta$超参数。</li>\n</ul>\n<h3 id=\"1-5-分位数损失（Quantile-Loss）\"><a href=\"#1-5-分位数损失（Quantile-Loss）\" class=\"headerlink\" title=\"1.5 分位数损失（Quantile Loss）\"></a>1.5 分位数损失（Quantile Loss）</h3><script type=\"math/tex; mode=display\">\nJ_{\\text {quant }}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{\\hat{y}_{i} \\geq y_{i}}(1-r)\\left|y_{i}-\\hat{y}_{i}\\right|+\\mathbb{I}_{\\hat{y}_{i}<y_{i}} r\\left|y_{i}-\\hat{y}_{i}\\right|</script><ul>\n<li>这是一个分段函数，这个损失函数是一个分段的函数 ，将$\\hat{y_i} \\geq y_i$（高估） 和$\\hat{y_i} &lt; y_i$（低估) 两种情况分开来，并分别给予不同的系数</li>\n<li>分位数损失实现了分别用不同的系数（r和1-r）控制高估和低估的损失</li>\n<li>特别地，当$r = 0.5$时分位数损失退化为 MAE 损失</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/11/09/BvpTeXqWFYRKoMU.png\" alt=\"Quantile Loss\"></p>\n<h3 id=\"1-6-交叉熵损失（Cross-Entropy-Loss）\"><a href=\"#1-6-交叉熵损失（Cross-Entropy-Loss）\" class=\"headerlink\" title=\"1.6 交叉熵损失（Cross Entropy Loss）\"></a>1.6 交叉熵损失（Cross Entropy Loss）</h3><ul>\n<li>上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 </li>\n</ul>\n<h5 id=\"1-6-1-二分类\"><a href=\"#1-6-1-二分类\" class=\"headerlink\" title=\"1.6.1 二分类\"></a>1.6.1 二分类</h5><script type=\"math/tex; mode=display\">\nJ_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)</script><p><img src=\"https://i.loli.net/2021/11/09/hPt1nITJ624Llfp.png\" alt=\"二分类的交叉熵\"></p>\n<ul>\n<li>证明：</li>\n</ul>\n<p>在二分类中我们通常将输出结果用sigmoid映射到区间$(0, 1)$，并将其作为该类的概率，由于只有两类，所以给定$x_i$求出类别为1或0的概率分别为：</p>\n<script type=\"math/tex; mode=display\">\np(y_i = 1|x_i) = \\hat{y_i} \\\\\np(y_i = 0|x_i) = 1 - \\hat{y_i}</script><p>合并成一个式子：</p>\n<script type=\"math/tex; mode=display\">\np(y_i|x_i) = (\\hat{y_i})^{y_i}(1 - \\hat{y_i})^{1 - y_i}</script><p>由于各数据点独立同分布，则似然可以表示为：</p>\n<script type=\"math/tex; mode=display\">\nL(x, y) = \\prod_{i=1}^{N}\\left(\\hat{y}_{i}\\right)^{y_{i}}\\left(1-\\hat{y}_{i}\\right)^{1-y_{i}}</script><p>取负对数：</p>\n<script type=\"math/tex; mode=display\">\nN L L(x, y)=J_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)</script><h5 id=\"1-6-2-多分类\"><a href=\"#1-6-2-多分类\" class=\"headerlink\" title=\"1.6.2 多分类\"></a>1.6.2 多分类</h5><p>在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值$y_i$现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 Softmax 函数</p>\n<script type=\"math/tex; mode=display\">\nJ_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(\\hat{y}_{i}^{k}\\right)</script><p>因为$y_i$是一个One-hot向量，所以还可以写为：</p>\n<script type=\"math/tex; mode=display\">\nJ_{C E}=-\\sum_{i=1}^{N} y_{i}^{c_{i}} \\log \\left(\\hat{y}_{i}^{c_{i}}\\right)</script><p>其中$c_i$为样本$x_i$的目标类</p>\n<ul>\n<li>证明：</li>\n</ul>\n<p>对于一个样本，分类正确的概率为：</p>\n<script type=\"math/tex; mode=display\">\np(y_i|x_i) = \\prod_{k=1}^{K}\\left(\\hat{y}_{i}^{k}\\right)^{y_{i}^{k}}</script><p>（其中$y_i^k和\\hat{y_i}^k$为该向量的第k维）</p>\n<p>因为所有样本相互，所有相乘再取负对数即可得到：</p>\n<script type=\"math/tex; mode=display\">\nN L L(x, y)=J_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(\\hat{y}_{i}^{k}\\right)</script><h3 id=\"1-7-合页损失（Hinge-Loss）\"><a href=\"#1-7-合页损失（Hinge-Loss）\" class=\"headerlink\" title=\"1.7 合页损失（Hinge Loss）\"></a>1.7 合页损失（Hinge Loss）</h3><ul>\n<li>Hinge Loss也是一种二分类损失函数</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ_{\\text {hinge }}=\\sum_{i=1}^{N} \\max \\left(0,1-\\operatorname{sgn}\\left(y_{i}\\right) \\hat{y}_{i}\\right)</script><p>下图是$y$为正类， 即$sgn(y) = 1$时，不同输出的合页损失示意图：</p>\n<p><img src=\"https://i.loli.net/2021/11/09/KngeQOwp54JZRMf.png\" alt=\"image-20211109225642368\"></p>\n<ul>\n<li>可以看到当$y$为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在$(0, 1)$区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。<strong>使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类</strong></li>\n<li><p><strong>Hinge Loss常用在支持向量机（SVM）中，在SVM的软间隔中替换数学性质不好的0/1损失</strong></p>\n</li>\n<li><p><strong>Hinge Loss变种：</strong>有些时候我们关注的并不是单个样本的分类结果，而是两个样本之间的相似性，所以会使用：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell= \\max(0, m + score(pos\\_pair) - score(neg\\_pair))</script><p>其中两个score分别为正负样本对的得分，m是间隔参数margin，目的是<strong>希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。这样能够拉开正负样本间的差距，更好区分正负样本</strong></p>\n<h1 id=\"2-评估模型的指标\"><a href=\"#2-评估模型的指标\" class=\"headerlink\" title=\"2 评估模型的指标\"></a>2 评估模型的指标</h1><h3 id=\"2-1-基本概念\"><a href=\"#2-1-基本概念\" class=\"headerlink\" title=\"2.1 基本概念\"></a>2.1 基本概念</h3><p><img src=\"https://i.loli.net/2021/11/09/OCNnaMs2ceBQIJz.png\" alt=\"image-20211109233859657\"></p>\n<h3 id=\"2-2-查准率和查全率\"><a href=\"#2-2-查准率和查全率\" class=\"headerlink\" title=\"2.2 查准率和查全率\"></a>2.2 查准率和查全率</h3><script type=\"math/tex; mode=display\">\n查准率 P（Precision） = \\frac{TP}{TP + FP}</script><script type=\"math/tex; mode=display\">\n查全率 R（Recall） = \\frac{TP}{TP + FN}</script><ul>\n<li>查准率可以直观理解为所有预测为正项的样本中有多少是真正的正项</li>\n<li>查全率可以直观理解为所有label是正项的样本中有多少被成功预测出来了</li>\n<li>理想情况下，查准率和查全率两者都越高越好。<strong>然而事实上这两者在某些情况下是矛盾的，一般来说，查准率高时，查全率低；查确率低时，查全率高</strong></li>\n</ul>\n<h3 id=\"2-3-准确率和错误率\"><a href=\"#2-3-准确率和错误率\" class=\"headerlink\" title=\"2.3 准确率和错误率\"></a>2.3 准确率和错误率</h3><p>准确率：</p>\n<script type=\"math/tex; mode=display\">\naccuracy = \\frac{TP + TF}{TP + TN + FP + FN}</script><ul>\n<li>即有多少样本被分类正确</li>\n</ul>\n<p>而错误率：</p>\n<script type=\"math/tex; mode=display\">\nerrorrate = 1 - accuracy</script><h3 id=\"2-4-P-R曲线\"><a href=\"#2-4-P-R曲线\" class=\"headerlink\" title=\"2.4 P-R曲线\"></a>2.4 P-R曲线</h3><p><img src=\"https://i.loli.net/2021/11/10/zV1Sj4HyYfD3G5e.png\" alt=\"image-20211110000609300\" style=\"zoom: 80%;\" /></p>\n<ul>\n<li>P-R曲线直观地显示出学习器在样本总体上地查全率、查准率，在进行比较时，<strong>若一个学习器的P-R曲线被另一个学习器曲线“完全包住”，则可以断言后者的性能一定优于前者，如上图的B性能优于C，而A、B不一定</strong></li>\n<li>平衡点（BEP）时$P = R$时的取值，如上图A的BEP为0.8，而如果基于BEP比较，可知A优于B</li>\n</ul>\n<h3 id=\"2-5-F函数\"><a href=\"#2-5-F函数\" class=\"headerlink\" title=\"2.5 F函数\"></a>2.5 F函数</h3><p>BEP还是过于简化了些，更常用得是F1度量，我们可以取P和R的调和平均：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{2}{F_1} = \\frac{1}{P} + \\frac{1}{R}</script><p>求得：</p>\n<script type=\"math/tex; mode=display\">\nF1 = \\frac{2PR}{P + R}</script><p>但是在许多应用中我们对查准率和查全率得重视程度不同，所以可以给予P和R不同的权重，取P和R的加权调和平均：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{F_{\\beta}} = \\frac{1}{1 + \\beta ^2}(\\frac{1}{P} + \\frac{\\beta ^2}{R})</script><p>求得：</p>\n<script type=\"math/tex; mode=display\">\nF_{\\beta} = \\frac{(1 + \\beta ^2)PR}{\\beta ^2P + R}</script><ul>\n<li>$\\beta &gt; 0$度量了查全率和查准率的相对重要性，$\\beta = 1$退化为标准的F1，$\\beta &gt; 1$查全率有更大影响，$\\beta &lt; 1$查准率有更大影响</li>\n</ul>\n<h3 id=\"2-6-ROC与AUC\"><a href=\"#2-6-ROC与AUC\" class=\"headerlink\" title=\"2.6 ROC与AUC\"></a>2.6 ROC与AUC</h3><h5 id=\"2-6-1-基本概念\"><a href=\"#2-6-1-基本概念\" class=\"headerlink\" title=\"2.6.1 基本概念\"></a>2.6.1 基本概念</h5><ul>\n<li><p>大多二分类问题是将输出的预测值与一个<strong>分类阈值（threshold）</strong>进行比较，若预测值大于阈值则为正类，反之则为负类</p>\n</li>\n<li><p>根据预测值，我们可将测试样本进行排序，根据预测值由大到小，“最可能”是正例的排在前面，“最不可能”是正例的排到后面。<strong>这样，分类过程就相当于以中间某个截断点（也就是分类阈值）将样本分为两部分，前一部分判定为正例，后一部分判定为反例</strong></p>\n</li>\n<li><p>在不同的任务中，我们我们可以根据任务需求采取不同的截断点。例如如更重视查准率，可以将截断点设置靠前，更注重查全率，可以将截断点靠后</p>\n</li>\n<li>而我们根据预测值进行排序后，按顺序将样本逐个作为正例进行预测，每次计算出<strong>真正例率（TPR）</strong>和<strong>假正例率（FPR）</strong>，以他们为横纵坐标就得到了<strong>ROC曲线</strong></li>\n</ul>\n<h5 id=\"2-6-2-ROC曲线\"><a href=\"#2-6-2-ROC曲线\" class=\"headerlink\" title=\"2.6.2 ROC曲线\"></a>2.6.2 ROC曲线</h5><ul>\n<li>首先介绍真正例率和假正例率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nTPR = \\frac{TP}{TP + FN}, \\\\\nFPR = \\frac{FP}{TN + FP}</script><ul>\n<li>ROC曲线：</li>\n</ul>\n<p>首先将分类阈值设最大，则所有类都被分类为负类，TPR和FPR都为0，然后每次将分类阈值设为下一个样本点的预测值（按预测值由大到小进行排序），记录每次的TPR和FPR，组成ROC曲线</p>\n<p><img src=\"https://i.loli.net/2021/11/10/TUMHuC3N9rX18iz.png\" alt=\"image-20211110140633061\"></p>\n<p>但是现实中我们是基于有限个样本画的图，所以不会产生这么平滑的曲线，更多情况应该像下图：</p>\n<p><img src=\"https://i.loli.net/2021/11/10/xG1TrPNfyimzq7Q.png\" alt=\"image-20211110140853013\"></p>\n<ul>\n<li>基于ROC的比较方法</li>\n</ul>\n<blockquote>\n<p>如果一个学习器的ROC曲线完全被另一个学习器的”包住“，则后者性能优于前者</p>\n<p>若两者的曲线交叉，则可以通过ROC曲线所包裹的面积进行判断，即<strong>AUC</strong></p>\n</blockquote>\n<h5 id=\"2-6-3-AUC\"><a href=\"#2-6-3-AUC\" class=\"headerlink\" title=\"2.6.3 AUC\"></a>2.6.3 AUC</h5><ul>\n<li><strong>AUC就是ROC曲线下的面积</strong>，假定ROC曲线是由坐标为<script type=\"math/tex\">\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right),\\left(x_{3}, y_{3}\\right), \\cdots,\\left(x_{m}, y_{m}\\right)</script>的点按序连接而形成，则AUC为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nA U C=\\frac{1}{2} \\sum_{i=1}^{m-1}\\left(x_{i+1}-x_{i}\\right)\\left(y_{i}+y_{i+1}\\right)</script><ul>\n<li><p>从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1&gt;p0的概率就等于AUC， 即<strong>AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性</strong></p>\n</li>\n<li><p><strong>所以AUC反应的是分类器对样本的排序能力</strong></p>\n</li>\n</ul>\n<p>证明：</p>\n<p>设所有正类的集合$X = { \\hat{X_1}, \\hat{X_2}, …, \\hat{X_m}}$和负类的集合$Y = { \\hat{Y_1}, \\hat{Y_2}, …, \\hat{Y_n}}$，其中是每个样本对应的预测值，设分类阈值为c，$F_X(x)$和$F_Y(y)$分别为X和Y的分布函数，则$TPR(c) = 1 - F_X(c)$，$FPR(c) = 1 - F_Y(c)$</p>\n<p>设$t = FPR(c)$， 则$c = F_Y^{-1}(1 - t)$，则$ROC(t) = 1 - F_X(F_Y^{-1}(1 - t))$</p>\n<p>则：</p>\n<script type=\"math/tex; mode=display\">\nAUC = \\int_0^1ROC(t)dt \\\\\n=\\int_0^1 [1 - F_X(F_Y^{-1}(1 - t))] dt \\\\\n=\\int_0^1 [1 - F_X(F_Y^{-1}(t))] dt \\\\\n=\\int_{-\\infty}^{+\\infty} [1 - F_X(y)] dF_Y(y) \\\\\n=\\int_{-\\infty}^{+\\infty}P(X > y)f_Y(y)dy \\\\\n=\\int_{-\\infty}^{+\\infty}P(X > y, Y = y)dy \\\\\n=P(X > Y)</script><h5 id=\"2-6-4-使用ROC和AUC的优点\"><a href=\"#2-6-4-使用ROC和AUC的优点\" class=\"headerlink\" title=\"2.6.4 使用ROC和AUC的优点\"></a>2.6.4 使用ROC和AUC的优点</h5><ul>\n<li><strong>AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡（正负类样本不相同）的情况下，依然能够对分类器做出合理的评价</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nTPR = P(\\hat{Y} = 1 | Y = 1), \\\\\nFPR = P(\\hat{Y} = 1 | Y = 0)</script><p><strong>由上式可得：无论Y的真实概率是多少， 都不会影响TPR和FPR</strong></p>\n<p>而PR曲线更关注正例</p>\n<ul>\n<li>ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响，有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的查全率就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少</li>\n</ul>\n<ul>\n<li>上面几种评估方法都是<strong>用于分类</strong>的评估方法，而在<strong>回归问题</strong>当中，这些一般是不适用的，回归问题中我们比较常用的评估方法有一下两种</li>\n</ul>\n<h3 id=\"2-7-平方根误差（RMSE）\"><a href=\"#2-7-平方根误差（RMSE）\" class=\"headerlink\" title=\"2.7 平方根误差（RMSE）\"></a>2.7 平方根误差（RMSE）</h3><script type=\"math/tex; mode=display\">\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n(y_i - \\hat y_i)^2}{n}}</script><ul>\n<li>其实RMSE就是MSE开了个根，但是我们做这样的处理能让<strong>误差和结果值在同一个数量级上，这样能更直观有效的反应拟合程度</strong></li>\n<li>但是RMSE有着和MSE一样的缺点，那就是<strong>对离群值十分敏感，健壮性很差</strong></li>\n<li>比如在实际应用中，有可能在对于预测某些剧集的流量时，以便进行广告投放，在95%的区间内的预测误差都十分低，比如小于1%，这是相当不错的预测结果。但是在总体上，无论运用何种模型，RMSE可能都一直居高不下。<strong>原因是可能在剩余的5%区间里有非常严重的离群点，比如某些冷门剧、新上映的剧</strong></li>\n<li>对此我们可以选择对数据进行处理，或者换一种模型指标</li>\n</ul>\n<h3 id=\"2-8-平均绝对百分比误差（MAPE）\"><a href=\"#2-8-平均绝对百分比误差（MAPE）\" class=\"headerlink\" title=\"2.8 平均绝对百分比误差（MAPE）\"></a>2.8 平均绝对百分比误差（MAPE）</h3><script type=\"math/tex; mode=display\">\nMAPE = \\sum_{i = 1}^n |\\frac{y_i - \\hat{y}_i}{y_i}| \\times \\frac{100}{n}</script><ul>\n<li>相比RMSE， MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的影响</li>\n</ul>\n"},{"title":"最大熵模型","math":true,"date":"2022-05-25T16:00:00.000Z","_content":"\n\n\n# 1 最大熵模型\n\n### 1.1 最大熵原理\n\n- **最大熵原理：**学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型\n\n- 假设离散变量$$X$$的概率分布为$$P(X)$$，则其熵为：\n\n$$\nH(P) = -\\sum_xP(x)\\log P(x)\n$$\n\n熵的取值范围：$$0 \\le H(P) \\le \\log|X|$$，在$$X$$为均匀分布时熵取得最大\n\n- 所以在没有任何约束时，$$X$$直接取均匀分布，熵即可达到最大\n- **条件熵：**在已知X的情况下，求Y的条件熵：\n\n$$\nH(Y|X) = \\sum_{x}p(x)H(Y|X=x) = -\\sum_xp(x)\\sum_yp(y|x)log(p(y|x))\n$$\n\n\n\n### 1.2 模型约束\n\n- 前面说到最大熵原理是在约束的模型集合中寻找熵最大的，那么介绍一下最大熵模型的约束为什么\n- 假设是一个分类模型$$P(Y|X)$$，首先给定训练集$$T = \\{(x_1, y_1), ...,(x_N, y_N)\\}$$，那么可以由统计得到经验分布：\n\n$$\n\\begin{array}{l}\n\\tilde{P}(X=x, Y=y)=\\frac{v(X=x, Y=y)}{N} \\\\\n\\tilde{P}(X=x)=\\frac{v(X=x)}{N}\n\\end{array}\n$$\n\n- **特征函数：**$$f(x, y)$$用于描述输入x和输出y之间的某个事实：\n\n$$\nf(x, y)=\\left\\{\\begin{array}{ll}\n1, & x \\text { 与 } y \\text { 满足某一事实 } \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.\n$$\n\n特征函数的形式很多样，可以是对数据中字、词的统计信息作为输入，甚至可以直接用某些模型（如BERT）的输出作为输入\n\n- 那么特征函数关于经验分布$$\\tilde{P}(X,Y)$$的期望值为：\n\n$$\nE_{\\tilde{P}}(f) = \\sum_{x,y}\\tilde{P}(x,y)f(x,y)\n$$\n\n- 特征函数关于模型$$P(X|Y)$$和经验分布$$\\tilde{P}(X)$$的期望值为：\n\n$$\nE_P(f) = \\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)\n$$\n\n- **而最大熵模型的假设为这两个期望值相等，即：**\n\n$$\nE_P(f) = E_{\\tilde{P}}(f)\n$$\n\n这就是最大熵模型的约束，有几个特征函数就有几个约束\n\n\n\n### 1.3 模型定义\n\n- 假设满足所有约束条件的模型集合为$$\\mathcal{C} \\equiv\\left\\{P \\in \\mathcal{P} \\mid E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n\\right\\}$$，定义在概率分布$$P(Y|X)$$上的条件熵为：\n\n$$\nH(P)=-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x)\n$$\n\n则模型集合$$\\mathcal{C}$$中$$H(P)$$最大的模型就称为最大熵模型\n\n\n\n\n\n# 2 最大熵模型的学习\n\n- 根据上面所述，现在问题就变为了一个带约束的最优化问题：\n\n$$\n\\begin{array}{ll}\n\\max _{P \\in \\mathrm{C}} & H(P)=-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x) \\\\\n\\text { s.t. } & E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n \\\\\n& \\sum_{y} P(y \\mid x)=1\n\\end{array}\n$$\n\n- 上述问题可使用拉格朗日优化，引入拉格朗如乘子并得到对偶问题进行求解，具体过程可参考[SVM的学习](https://zlkqz.site/2022/09/26/SVM/#2-2-%E5%8E%9F%E9%97%AE%E9%A2%98%E8%BD%AC%E5%8C%96%E5%88%B0%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98)，**首先引入拉格朗日乘子$$w_0, w_1, ...,w_n$$得到拉格朗日函数：**\n\n$$\nL(P, w) \\equiv-H(P)+w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)+\\sum_{i=1}^{n} w_{i}\\left(E_{\\tilde{P}}\\left(f_{i}\\right)-E_{P}\\left(f_{i}\\right)\\right)\n$$\n\n- **然后将原始问题$$\\min _{P \\in \\mathbf{C}} \\max _{w} L(P, w)$$转化为对偶问题$$\\max _{w} \\min _{P \\in \\mathbf{C}} L(P, w)$$，先只考虑里面的最小化问题$$\\min _{P \\in \\mathbf{C}} L(P, w)$$，求偏导$$\\frac{\\partial L(P,w)}{\\partial P(y|x)} = 0$$，最后可以解得：**\n\n$$\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n$$\n\n其中$$w_i$$为每个特征函数对应的权重，也是拉格朗日优化时的拉格朗日乘子。$$Z_w(x)$$为规范化因子：\n$$\nZ_{w}(x)=\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n$$\n\n- 那么接下来的任务变成了外部的极大化问题，即将上述$$P_w(y|x)$$带入$$L(P,w)$$得到$$\\Psi(w)$$，然后寻找：\n\n$$\nw^{*}=\\underset{w}{\\arg \\max } \\Psi(w)\n$$\n\n- **而上述对偶函数的极大化等价于最大熵模型的极大似然估计**\n\n> **证明：**\n>\n> 当概率分布$$P(y|x)$$是最大熵模型时，对数似然函数为：\n> $$\n> \\begin{aligned}\n> L_{\\tilde{P}}\\left(P_{w}\\right) &=\\log\\prod_{x,y}P(y|x)^{\\tilde{P}(x,y)} = \\sum_{x, y} \\tilde{P}(x, y) \\log P(y \\mid x) \\\\\n> &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x, y) \\log Z_{w}(x) \\\\\n> &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n> \\end{aligned}\n> $$\n> 而上述对偶函数$$\\Psi(w)$$中，有一项$$w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)$$，由于$$w$$中并没有包含$$w_0$$，所以这一项可以直接去掉，不影响优化，所以：\n> $$\n> \\begin{aligned}\n> \\Psi(w)=& \\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log P_{w}(y \\mid x) +\\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) f_{i}(x, y)\\right) \\\\\n> =& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)+\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x)\\left(\\log P_{w}(y \\mid x)-\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\\n> =& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log Z_{w}(x) \\\\\n> =& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n> \\end{aligned}\n> $$\n> 由上述两个式子可以得到：\n> $$\n> \\Psi(w)=L_{\\tilde{p}}\\left(P_{w}\\right)\n> $$\n\n\n\n\n\n# 3 优化算法\n\n- 现在我们是要最大化似然函数或者对偶函数，但是**该函数并没有显式的解析解**（有解析解，但是找不到），所以需要运用一些优化算法\n- 值得庆幸的是，对偶函数的目标函数具有很好的性质，**他是光滑的凸函数，因此多种最优化方法都适用，并且保证能找到全局最优解**\n\n\n\n### 3.1 改进的迭代尺度法\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027204416341.png\" alt=\"image-20221027204416341\" style=\"zoom:80%;\" />\n\n- 这一算法的关键是(a)，即求解$$\\delta_i$$，如果$$f\\#(x,y)$$是常数，即对于任意x,y，都有$$f\\#(x,y) = M$$，那么：\n\n$$\n\\delta_{i}=\\frac{1}{M} \\log \\frac{E_{\\tilde{p}}\\left(f_{i}\\right)}{E_{p}\\left(f_{i}\\right)}\n$$\n\n如果$$f\\#(x,y)$$不为常数，最简单有效的是**牛顿法**，将(a)中需要求解的方程表示为$$g(\\delta_i) = 0$$，然后通过牛顿法最小化$$g(\\delta_i)$$，每次迭代的公式为：\n$$\n\\delta_{i}^{(k+1)}=\\delta_{i}^{(k)}-\\frac{g\\left(\\delta_{i}^{(k)}\\right)}{g^{\\prime}\\left(\\delta_{i}^{(k)}\\right)}\n$$\n\n\n### 3.2 拟牛顿法\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027205121849.png\" alt=\"image-20221027205121849\" style=\"zoom:80%;\" />\n\n\n\n\n\n# 4 牛顿法和拟牛顿法\n\n- 上面提到了牛顿法和拟牛顿法，这节就简要介绍以下其原理\n- 他们都是**求解无约束最优化问题**的常用方法，有收敛速度快的优点\n\n\n\n### 4.1 牛顿法\n\n- 考虑最优化：$$\\min_{x \\in R^n}f(x)$$，假设$$f(x)$$具有二阶连续偏导数，若第k次迭代之为$$x^{(k)}$$，则可将$$f(x)$$在$$x^{(k)}$$处进行二阶泰勒展开：\n\n$$\nf(x)=f\\left(x^{(k)}\\right)+g_{k}^{\\mathrm{T}}\\left(x-x^{(k)}\\right)+\\frac{1}{2}\\left(x-x^{(k)}\\right)^{\\mathrm{T}} H\\left(x^{(k)}\\right)\\left(x-x^{(k)}\\right)\n$$\n\n其中$$g_k = g(x^{(k)}) = \\nabla f(x^{(k)})$$是$$f(x)$$的梯度在$$x^{(k)}$$的值，$$H(x^{k})$$是黑塞矩阵$$H(x)=\\left[\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}\\right]_{n \\times n}$$在$$x^{(k)}$$的值\n\n- 函数$$f(x)$$有极值的必要条件是一阶导数为0，即$$\\nabla f(x) = 0$$。特别的当$$H(x)$$为正定矩阵时，该点为极小值点\n- 牛顿法的每次迭代，从$$x^{(k)}$$开始，将目标函数的极小值点作为下次迭代的$$x^{(k+1)}$$，其满足$$\\nabla f(x^{(k+1)}) = 0$$\n\n- 而将上面的泰勒展开求梯度，得到：\n\n$$\n\\nabla f(x) = g_k + H_k(x - x^{(k)}) = 0\n$$\n\n其中$$H_k = H(x^{(k)})$$\n\n- 所以得到：\n\n$$\nx^{(k+1)} = x^{(k)} - H_k^{-1}g_k = x^{(k)} + p_k\n$$\n\n- 将上面的流程总结：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027212238000.png\" alt=\"image-20221027212238000\" style=\"zoom:80%;\" />\n\n\n\n### 4.2 拟牛顿法\n\n- 由于计算黑塞矩阵的逆$$H^{-1}$$比较复杂，所以考虑使用一个n阶矩阵$$G_k$$来代替$$H_k^{-1}$$，这就是拟牛顿法的思想\n- 由上面牛顿法中的$$\\nabla f(x^{(k+1)}) = 0$$的方程可得：\n\n$$\ng_{k+1} - g_k = H_k(x^{(k)} - x^{(k)})\n$$\n\n设$$y_k = g_{k+1} - g_k, \\delta_k = x^{(k)} - x^{(k)}$$，上式可写为：\n$$\ny_k = H_k\\delta_k  \\\\\nH_k^{-1}y_k = \\delta_k\n$$\n上式称为**拟牛顿条件**\n\n- 而上面说过，要保证$$H_k$$是正定矩阵，才能保证驻点一定是极小值点。所以$$H_k$$同时满足拟牛顿条件和正定矩阵两个条件\n\n- **而近似矩阵$$G_k$$同样满足这两个条件，即可将其作为近似的矩阵**\n\n\n\n#### 4.2.1 DFP算法\n\n- 现在重点就变成了如何对$$G_k$$进行更新，才能保证$$G_{k+1}$$同样满足条件\n- 假设G的更新是加上两个附加项构成的，即：\n\n$$\nG_{k+1} = G_k + P_k + Q_k\n$$\n\n- 这时：\n\n$$\nG_{k+1}y_k = G_ky_k + P_ky_k + Q_ky_k\n$$\n\n为使$$G_{k+1}$$满足拟牛顿条件，$$P_k, Q_k$$要满足：\n$$\n\\begin{array}{c}\nP_{k} y_{k}=\\delta_{k} \\\\\nQ_{k} y_{k}=-G_{k} y_{k}\n\\end{array}\n$$\n\n- 这样的矩阵不难找出，例如：\n\n$$\n\\begin{array}{c}\nP_{k}=\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}} \\\\\nQ_{k}=-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}\n\\end{array}\n$$\n\n- 所以更新公式为：\n\n$$\nG_{k+1}=G_{k}+\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}}-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}\n$$\n\n- 总结一下DFP算法的算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214008428.png\" alt=\"image-20221027214008428\" style=\"zoom:80%;\" />\n\n\n\n#### 4.2.2 BFGS算法\n\n- BFGS算法比DFS算法更加常用，后者是用$$G_k$$逼近$$H^{-1}$$，而BFGS是用$$B_k$$逼近$$H$$\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214231809.png\" alt=\"image-20221027214231809\" style=\"zoom:80%;\" />","source":"_posts/最大熵模型.md","raw":"---\ntitle: 最大熵模型\nmath: true\ndate: 2022-5-26\n---\n\n\n\n# 1 最大熵模型\n\n### 1.1 最大熵原理\n\n- **最大熵原理：**学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型\n\n- 假设离散变量$$X$$的概率分布为$$P(X)$$，则其熵为：\n\n$$\nH(P) = -\\sum_xP(x)\\log P(x)\n$$\n\n熵的取值范围：$$0 \\le H(P) \\le \\log|X|$$，在$$X$$为均匀分布时熵取得最大\n\n- 所以在没有任何约束时，$$X$$直接取均匀分布，熵即可达到最大\n- **条件熵：**在已知X的情况下，求Y的条件熵：\n\n$$\nH(Y|X) = \\sum_{x}p(x)H(Y|X=x) = -\\sum_xp(x)\\sum_yp(y|x)log(p(y|x))\n$$\n\n\n\n### 1.2 模型约束\n\n- 前面说到最大熵原理是在约束的模型集合中寻找熵最大的，那么介绍一下最大熵模型的约束为什么\n- 假设是一个分类模型$$P(Y|X)$$，首先给定训练集$$T = \\{(x_1, y_1), ...,(x_N, y_N)\\}$$，那么可以由统计得到经验分布：\n\n$$\n\\begin{array}{l}\n\\tilde{P}(X=x, Y=y)=\\frac{v(X=x, Y=y)}{N} \\\\\n\\tilde{P}(X=x)=\\frac{v(X=x)}{N}\n\\end{array}\n$$\n\n- **特征函数：**$$f(x, y)$$用于描述输入x和输出y之间的某个事实：\n\n$$\nf(x, y)=\\left\\{\\begin{array}{ll}\n1, & x \\text { 与 } y \\text { 满足某一事实 } \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.\n$$\n\n特征函数的形式很多样，可以是对数据中字、词的统计信息作为输入，甚至可以直接用某些模型（如BERT）的输出作为输入\n\n- 那么特征函数关于经验分布$$\\tilde{P}(X,Y)$$的期望值为：\n\n$$\nE_{\\tilde{P}}(f) = \\sum_{x,y}\\tilde{P}(x,y)f(x,y)\n$$\n\n- 特征函数关于模型$$P(X|Y)$$和经验分布$$\\tilde{P}(X)$$的期望值为：\n\n$$\nE_P(f) = \\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)\n$$\n\n- **而最大熵模型的假设为这两个期望值相等，即：**\n\n$$\nE_P(f) = E_{\\tilde{P}}(f)\n$$\n\n这就是最大熵模型的约束，有几个特征函数就有几个约束\n\n\n\n### 1.3 模型定义\n\n- 假设满足所有约束条件的模型集合为$$\\mathcal{C} \\equiv\\left\\{P \\in \\mathcal{P} \\mid E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n\\right\\}$$，定义在概率分布$$P(Y|X)$$上的条件熵为：\n\n$$\nH(P)=-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x)\n$$\n\n则模型集合$$\\mathcal{C}$$中$$H(P)$$最大的模型就称为最大熵模型\n\n\n\n\n\n# 2 最大熵模型的学习\n\n- 根据上面所述，现在问题就变为了一个带约束的最优化问题：\n\n$$\n\\begin{array}{ll}\n\\max _{P \\in \\mathrm{C}} & H(P)=-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x) \\\\\n\\text { s.t. } & E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n \\\\\n& \\sum_{y} P(y \\mid x)=1\n\\end{array}\n$$\n\n- 上述问题可使用拉格朗日优化，引入拉格朗如乘子并得到对偶问题进行求解，具体过程可参考[SVM的学习](https://zlkqz.site/2022/09/26/SVM/#2-2-%E5%8E%9F%E9%97%AE%E9%A2%98%E8%BD%AC%E5%8C%96%E5%88%B0%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98)，**首先引入拉格朗日乘子$$w_0, w_1, ...,w_n$$得到拉格朗日函数：**\n\n$$\nL(P, w) \\equiv-H(P)+w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)+\\sum_{i=1}^{n} w_{i}\\left(E_{\\tilde{P}}\\left(f_{i}\\right)-E_{P}\\left(f_{i}\\right)\\right)\n$$\n\n- **然后将原始问题$$\\min _{P \\in \\mathbf{C}} \\max _{w} L(P, w)$$转化为对偶问题$$\\max _{w} \\min _{P \\in \\mathbf{C}} L(P, w)$$，先只考虑里面的最小化问题$$\\min _{P \\in \\mathbf{C}} L(P, w)$$，求偏导$$\\frac{\\partial L(P,w)}{\\partial P(y|x)} = 0$$，最后可以解得：**\n\n$$\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n$$\n\n其中$$w_i$$为每个特征函数对应的权重，也是拉格朗日优化时的拉格朗日乘子。$$Z_w(x)$$为规范化因子：\n$$\nZ_{w}(x)=\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n$$\n\n- 那么接下来的任务变成了外部的极大化问题，即将上述$$P_w(y|x)$$带入$$L(P,w)$$得到$$\\Psi(w)$$，然后寻找：\n\n$$\nw^{*}=\\underset{w}{\\arg \\max } \\Psi(w)\n$$\n\n- **而上述对偶函数的极大化等价于最大熵模型的极大似然估计**\n\n> **证明：**\n>\n> 当概率分布$$P(y|x)$$是最大熵模型时，对数似然函数为：\n> $$\n> \\begin{aligned}\n> L_{\\tilde{P}}\\left(P_{w}\\right) &=\\log\\prod_{x,y}P(y|x)^{\\tilde{P}(x,y)} = \\sum_{x, y} \\tilde{P}(x, y) \\log P(y \\mid x) \\\\\n> &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x, y) \\log Z_{w}(x) \\\\\n> &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n> \\end{aligned}\n> $$\n> 而上述对偶函数$$\\Psi(w)$$中，有一项$$w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)$$，由于$$w$$中并没有包含$$w_0$$，所以这一项可以直接去掉，不影响优化，所以：\n> $$\n> \\begin{aligned}\n> \\Psi(w)=& \\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log P_{w}(y \\mid x) +\\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) f_{i}(x, y)\\right) \\\\\n> =& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)+\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x)\\left(\\log P_{w}(y \\mid x)-\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\\n> =& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log Z_{w}(x) \\\\\n> =& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n> \\end{aligned}\n> $$\n> 由上述两个式子可以得到：\n> $$\n> \\Psi(w)=L_{\\tilde{p}}\\left(P_{w}\\right)\n> $$\n\n\n\n\n\n# 3 优化算法\n\n- 现在我们是要最大化似然函数或者对偶函数，但是**该函数并没有显式的解析解**（有解析解，但是找不到），所以需要运用一些优化算法\n- 值得庆幸的是，对偶函数的目标函数具有很好的性质，**他是光滑的凸函数，因此多种最优化方法都适用，并且保证能找到全局最优解**\n\n\n\n### 3.1 改进的迭代尺度法\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027204416341.png\" alt=\"image-20221027204416341\" style=\"zoom:80%;\" />\n\n- 这一算法的关键是(a)，即求解$$\\delta_i$$，如果$$f\\#(x,y)$$是常数，即对于任意x,y，都有$$f\\#(x,y) = M$$，那么：\n\n$$\n\\delta_{i}=\\frac{1}{M} \\log \\frac{E_{\\tilde{p}}\\left(f_{i}\\right)}{E_{p}\\left(f_{i}\\right)}\n$$\n\n如果$$f\\#(x,y)$$不为常数，最简单有效的是**牛顿法**，将(a)中需要求解的方程表示为$$g(\\delta_i) = 0$$，然后通过牛顿法最小化$$g(\\delta_i)$$，每次迭代的公式为：\n$$\n\\delta_{i}^{(k+1)}=\\delta_{i}^{(k)}-\\frac{g\\left(\\delta_{i}^{(k)}\\right)}{g^{\\prime}\\left(\\delta_{i}^{(k)}\\right)}\n$$\n\n\n### 3.2 拟牛顿法\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027205121849.png\" alt=\"image-20221027205121849\" style=\"zoom:80%;\" />\n\n\n\n\n\n# 4 牛顿法和拟牛顿法\n\n- 上面提到了牛顿法和拟牛顿法，这节就简要介绍以下其原理\n- 他们都是**求解无约束最优化问题**的常用方法，有收敛速度快的优点\n\n\n\n### 4.1 牛顿法\n\n- 考虑最优化：$$\\min_{x \\in R^n}f(x)$$，假设$$f(x)$$具有二阶连续偏导数，若第k次迭代之为$$x^{(k)}$$，则可将$$f(x)$$在$$x^{(k)}$$处进行二阶泰勒展开：\n\n$$\nf(x)=f\\left(x^{(k)}\\right)+g_{k}^{\\mathrm{T}}\\left(x-x^{(k)}\\right)+\\frac{1}{2}\\left(x-x^{(k)}\\right)^{\\mathrm{T}} H\\left(x^{(k)}\\right)\\left(x-x^{(k)}\\right)\n$$\n\n其中$$g_k = g(x^{(k)}) = \\nabla f(x^{(k)})$$是$$f(x)$$的梯度在$$x^{(k)}$$的值，$$H(x^{k})$$是黑塞矩阵$$H(x)=\\left[\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}\\right]_{n \\times n}$$在$$x^{(k)}$$的值\n\n- 函数$$f(x)$$有极值的必要条件是一阶导数为0，即$$\\nabla f(x) = 0$$。特别的当$$H(x)$$为正定矩阵时，该点为极小值点\n- 牛顿法的每次迭代，从$$x^{(k)}$$开始，将目标函数的极小值点作为下次迭代的$$x^{(k+1)}$$，其满足$$\\nabla f(x^{(k+1)}) = 0$$\n\n- 而将上面的泰勒展开求梯度，得到：\n\n$$\n\\nabla f(x) = g_k + H_k(x - x^{(k)}) = 0\n$$\n\n其中$$H_k = H(x^{(k)})$$\n\n- 所以得到：\n\n$$\nx^{(k+1)} = x^{(k)} - H_k^{-1}g_k = x^{(k)} + p_k\n$$\n\n- 将上面的流程总结：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027212238000.png\" alt=\"image-20221027212238000\" style=\"zoom:80%;\" />\n\n\n\n### 4.2 拟牛顿法\n\n- 由于计算黑塞矩阵的逆$$H^{-1}$$比较复杂，所以考虑使用一个n阶矩阵$$G_k$$来代替$$H_k^{-1}$$，这就是拟牛顿法的思想\n- 由上面牛顿法中的$$\\nabla f(x^{(k+1)}) = 0$$的方程可得：\n\n$$\ng_{k+1} - g_k = H_k(x^{(k)} - x^{(k)})\n$$\n\n设$$y_k = g_{k+1} - g_k, \\delta_k = x^{(k)} - x^{(k)}$$，上式可写为：\n$$\ny_k = H_k\\delta_k  \\\\\nH_k^{-1}y_k = \\delta_k\n$$\n上式称为**拟牛顿条件**\n\n- 而上面说过，要保证$$H_k$$是正定矩阵，才能保证驻点一定是极小值点。所以$$H_k$$同时满足拟牛顿条件和正定矩阵两个条件\n\n- **而近似矩阵$$G_k$$同样满足这两个条件，即可将其作为近似的矩阵**\n\n\n\n#### 4.2.1 DFP算法\n\n- 现在重点就变成了如何对$$G_k$$进行更新，才能保证$$G_{k+1}$$同样满足条件\n- 假设G的更新是加上两个附加项构成的，即：\n\n$$\nG_{k+1} = G_k + P_k + Q_k\n$$\n\n- 这时：\n\n$$\nG_{k+1}y_k = G_ky_k + P_ky_k + Q_ky_k\n$$\n\n为使$$G_{k+1}$$满足拟牛顿条件，$$P_k, Q_k$$要满足：\n$$\n\\begin{array}{c}\nP_{k} y_{k}=\\delta_{k} \\\\\nQ_{k} y_{k}=-G_{k} y_{k}\n\\end{array}\n$$\n\n- 这样的矩阵不难找出，例如：\n\n$$\n\\begin{array}{c}\nP_{k}=\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}} \\\\\nQ_{k}=-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}\n\\end{array}\n$$\n\n- 所以更新公式为：\n\n$$\nG_{k+1}=G_{k}+\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}}-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}\n$$\n\n- 总结一下DFP算法的算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214008428.png\" alt=\"image-20221027214008428\" style=\"zoom:80%;\" />\n\n\n\n#### 4.2.2 BFGS算法\n\n- BFGS算法比DFS算法更加常用，后者是用$$G_k$$逼近$$H^{-1}$$，而BFGS是用$$B_k$$逼近$$H$$\n\n- 算法流程：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214231809.png\" alt=\"image-20221027214231809\" style=\"zoom:80%;\" />","slug":"最大熵模型","published":1,"updated":"2022-12-20T06:22:16.792Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1m000l7cszcvrs9bl3","content":"<h1 id=\"1-最大熵模型\"><a href=\"#1-最大熵模型\" class=\"headerlink\" title=\"1 最大熵模型\"></a>1 最大熵模型</h1><h3 id=\"1-1-最大熵原理\"><a href=\"#1-1-最大熵原理\" class=\"headerlink\" title=\"1.1 最大熵原理\"></a>1.1 最大熵原理</h3><ul>\n<li><p><strong>最大熵原理：</strong>学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型</p>\n</li>\n<li><p>假设离散变量<script type=\"math/tex\">X</script>的概率分布为<script type=\"math/tex\">P(X)</script>，则其熵为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(P) = -\\sum_xP(x)\\log P(x)</script><p>熵的取值范围：<script type=\"math/tex\">0 \\le H(P) \\le \\log|X|</script>，在<script type=\"math/tex\">X</script>为均匀分布时熵取得最大</p>\n<ul>\n<li>所以在没有任何约束时，<script type=\"math/tex\">X</script>直接取均匀分布，熵即可达到最大</li>\n<li><strong>条件熵：</strong>在已知X的情况下，求Y的条件熵：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(Y|X) = \\sum_{x}p(x)H(Y|X=x) = -\\sum_xp(x)\\sum_yp(y|x)log(p(y|x))</script><h3 id=\"1-2-模型约束\"><a href=\"#1-2-模型约束\" class=\"headerlink\" title=\"1.2 模型约束\"></a>1.2 模型约束</h3><ul>\n<li>前面说到最大熵原理是在约束的模型集合中寻找熵最大的，那么介绍一下最大熵模型的约束为什么</li>\n<li>假设是一个分类模型<script type=\"math/tex\">P(Y|X)</script>，首先给定训练集<script type=\"math/tex\">T = \\{(x_1, y_1), ...,(x_N, y_N)\\}</script>，那么可以由统计得到经验分布：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\tilde{P}(X=x, Y=y)=\\frac{v(X=x, Y=y)}{N} \\\\\n\\tilde{P}(X=x)=\\frac{v(X=x)}{N}\n\\end{array}</script><ul>\n<li><strong>特征函数：</strong><script type=\"math/tex\">f(x, y)</script>用于描述输入x和输出y之间的某个事实：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x, y)=\\left\\{\\begin{array}{ll}\n1, & x \\text { 与 } y \\text { 满足某一事实 } \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.</script><p>特征函数的形式很多样，可以是对数据中字、词的统计信息作为输入，甚至可以直接用某些模型（如BERT）的输出作为输入</p>\n<ul>\n<li>那么特征函数关于经验分布<script type=\"math/tex\">\\tilde{P}(X,Y)</script>的期望值为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE_{\\tilde{P}}(f) = \\sum_{x,y}\\tilde{P}(x,y)f(x,y)</script><ul>\n<li>特征函数关于模型<script type=\"math/tex\">P(X|Y)</script>和经验分布<script type=\"math/tex\">\\tilde{P}(X)</script>的期望值为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE_P(f) = \\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)</script><ul>\n<li><strong>而最大熵模型的假设为这两个期望值相等，即：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE_P(f) = E_{\\tilde{P}}(f)</script><p>这就是最大熵模型的约束，有几个特征函数就有几个约束</p>\n<h3 id=\"1-3-模型定义\"><a href=\"#1-3-模型定义\" class=\"headerlink\" title=\"1.3 模型定义\"></a>1.3 模型定义</h3><ul>\n<li>假设满足所有约束条件的模型集合为<script type=\"math/tex\">\\mathcal{C} \\equiv\\left\\{P \\in \\mathcal{P} \\mid E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n\\right\\}</script>，定义在概率分布<script type=\"math/tex\">P(Y|X)</script>上的条件熵为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(P)=-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x)</script><p>则模型集合<script type=\"math/tex\">\\mathcal{C}</script>中<script type=\"math/tex\">H(P)</script>最大的模型就称为最大熵模型</p>\n<h1 id=\"2-最大熵模型的学习\"><a href=\"#2-最大熵模型的学习\" class=\"headerlink\" title=\"2 最大熵模型的学习\"></a>2 最大熵模型的学习</h1><ul>\n<li>根据上面所述，现在问题就变为了一个带约束的最优化问题：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{ll}\n\\max _{P \\in \\mathrm{C}} & H(P)=-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x) \\\\\n\\text { s.t. } & E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n \\\\\n& \\sum_{y} P(y \\mid x)=1\n\\end{array}</script><ul>\n<li>上述问题可使用拉格朗日优化，引入拉格朗如乘子并得到对偶问题进行求解，具体过程可参考<a href=\"https://zlkqz.site/2022/09/26/SVM/#2-2-%E5%8E%9F%E9%97%AE%E9%A2%98%E8%BD%AC%E5%8C%96%E5%88%B0%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98\">SVM的学习</a>，<strong>首先引入拉格朗日乘子<script type=\"math/tex\">w_0, w_1, ...,w_n</script>得到拉格朗日函数：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(P, w) \\equiv-H(P)+w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)+\\sum_{i=1}^{n} w_{i}\\left(E_{\\tilde{P}}\\left(f_{i}\\right)-E_{P}\\left(f_{i}\\right)\\right)</script><ul>\n<li><strong>然后将原始问题<script type=\"math/tex\">\\min _{P \\in \\mathbf{C}} \\max _{w} L(P, w)</script>转化为对偶问题<script type=\"math/tex\">\\max _{w} \\min _{P \\in \\mathbf{C}} L(P, w)</script>，先只考虑里面的最小化问题<script type=\"math/tex\">\\min _{P \\in \\mathbf{C}} L(P, w)</script>，求偏导<script type=\"math/tex\">\\frac{\\partial L(P,w)}{\\partial P(y|x)} = 0</script>，最后可以解得：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)</script><p>其中<script type=\"math/tex\">w_i</script>为每个特征函数对应的权重，也是拉格朗日优化时的拉格朗日乘子。<script type=\"math/tex\">Z_w(x)</script>为规范化因子：</p>\n<script type=\"math/tex; mode=display\">\nZ_{w}(x)=\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)</script><ul>\n<li>那么接下来的任务变成了外部的极大化问题，即将上述<script type=\"math/tex\">P_w(y|x)</script>带入<script type=\"math/tex\">L(P,w)</script>得到<script type=\"math/tex\">\\Psi(w)</script>，然后寻找：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nw^{*}=\\underset{w}{\\arg \\max } \\Psi(w)</script><ul>\n<li><strong>而上述对偶函数的极大化等价于最大熵模型的极大似然估计</strong></li>\n</ul>\n<blockquote>\n<p><strong>证明：</strong></p>\n<p>当概率分布<script type=\"math/tex\">P(y|x)</script>是最大熵模型时，对数似然函数为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nL_{\\tilde{P}}\\left(P_{w}\\right) &=\\log\\prod_{x,y}P(y|x)^{\\tilde{P}(x,y)} = \\sum_{x, y} \\tilde{P}(x, y) \\log P(y \\mid x) \\\\\n&=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x, y) \\log Z_{w}(x) \\\\\n&=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n\\end{aligned}</script><p>而上述对偶函数<script type=\"math/tex\">\\Psi(w)</script>中，有一项<script type=\"math/tex\">w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)</script>，由于<script type=\"math/tex\">w</script>中并没有包含<script type=\"math/tex\">w_0</script>，所以这一项可以直接去掉，不影响优化，所以：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Psi(w)=& \\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log P_{w}(y \\mid x) +\\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) f_{i}(x, y)\\right) \\\\\n=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)+\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x)\\left(\\log P_{w}(y \\mid x)-\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\\n=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log Z_{w}(x) \\\\\n=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n\\end{aligned}</script><p>由上述两个式子可以得到：</p>\n<script type=\"math/tex; mode=display\">\n\\Psi(w)=L_{\\tilde{p}}\\left(P_{w}\\right)</script></blockquote>\n<h1 id=\"3-优化算法\"><a href=\"#3-优化算法\" class=\"headerlink\" title=\"3 优化算法\"></a>3 优化算法</h1><ul>\n<li>现在我们是要最大化似然函数或者对偶函数，但是<strong>该函数并没有显式的解析解</strong>（有解析解，但是找不到），所以需要运用一些优化算法</li>\n<li>值得庆幸的是，对偶函数的目标函数具有很好的性质，<strong>他是光滑的凸函数，因此多种最优化方法都适用，并且保证能找到全局最优解</strong></li>\n</ul>\n<h3 id=\"3-1-改进的迭代尺度法\"><a href=\"#3-1-改进的迭代尺度法\" class=\"headerlink\" title=\"3.1 改进的迭代尺度法\"></a>3.1 改进的迭代尺度法</h3><ul>\n<li>算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027204416341.png\" alt=\"image-20221027204416341\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>这一算法的关键是(a)，即求解<script type=\"math/tex\">\\delta_i</script>，如果<script type=\"math/tex\">f\\#(x,y)</script>是常数，即对于任意x,y，都有<script type=\"math/tex\">f\\#(x,y) = M</script>，那么：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\delta_{i}=\\frac{1}{M} \\log \\frac{E_{\\tilde{p}}\\left(f_{i}\\right)}{E_{p}\\left(f_{i}\\right)}</script><p>如果<script type=\"math/tex\">f\\#(x,y)</script>不为常数，最简单有效的是<strong>牛顿法</strong>，将(a)中需要求解的方程表示为<script type=\"math/tex\">g(\\delta_i) = 0</script>，然后通过牛顿法最小化<script type=\"math/tex\">g(\\delta_i)</script>，每次迭代的公式为：</p>\n<script type=\"math/tex; mode=display\">\n\\delta_{i}^{(k+1)}=\\delta_{i}^{(k)}-\\frac{g\\left(\\delta_{i}^{(k)}\\right)}{g^{\\prime}\\left(\\delta_{i}^{(k)}\\right)}</script><h3 id=\"3-2-拟牛顿法\"><a href=\"#3-2-拟牛顿法\" class=\"headerlink\" title=\"3.2 拟牛顿法\"></a>3.2 拟牛顿法</h3><ul>\n<li>算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027205121849.png\" alt=\"image-20221027205121849\" style=\"zoom:80%;\" /></p>\n<h1 id=\"4-牛顿法和拟牛顿法\"><a href=\"#4-牛顿法和拟牛顿法\" class=\"headerlink\" title=\"4 牛顿法和拟牛顿法\"></a>4 牛顿法和拟牛顿法</h1><ul>\n<li>上面提到了牛顿法和拟牛顿法，这节就简要介绍以下其原理</li>\n<li>他们都是<strong>求解无约束最优化问题</strong>的常用方法，有收敛速度快的优点</li>\n</ul>\n<h3 id=\"4-1-牛顿法\"><a href=\"#4-1-牛顿法\" class=\"headerlink\" title=\"4.1 牛顿法\"></a>4.1 牛顿法</h3><ul>\n<li>考虑最优化：<script type=\"math/tex\">\\min_{x \\in R^n}f(x)</script>，假设<script type=\"math/tex\">f(x)</script>具有二阶连续偏导数，若第k次迭代之为<script type=\"math/tex\">x^{(k)}</script>，则可将<script type=\"math/tex\">f(x)</script>在<script type=\"math/tex\">x^{(k)}</script>处进行二阶泰勒展开：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x)=f\\left(x^{(k)}\\right)+g_{k}^{\\mathrm{T}}\\left(x-x^{(k)}\\right)+\\frac{1}{2}\\left(x-x^{(k)}\\right)^{\\mathrm{T}} H\\left(x^{(k)}\\right)\\left(x-x^{(k)}\\right)</script><p>其中<script type=\"math/tex\">g_k = g(x^{(k)}) = \\nabla f(x^{(k)})</script>是<script type=\"math/tex\">f(x)</script>的梯度在<script type=\"math/tex\">x^{(k)}</script>的值，<script type=\"math/tex\">H(x^{k})</script>是黑塞矩阵<script type=\"math/tex\">H(x)=\\left[\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}\\right]_{n \\times n}</script>在<script type=\"math/tex\">x^{(k)}</script>的值</p>\n<ul>\n<li>函数<script type=\"math/tex\">f(x)</script>有极值的必要条件是一阶导数为0，即<script type=\"math/tex\">\\nabla f(x) = 0</script>。特别的当<script type=\"math/tex\">H(x)</script>为正定矩阵时，该点为极小值点</li>\n<li><p>牛顿法的每次迭代，从<script type=\"math/tex\">x^{(k)}</script>开始，将目标函数的极小值点作为下次迭代的<script type=\"math/tex\">x^{(k+1)}</script>，其满足<script type=\"math/tex\">\\nabla f(x^{(k+1)}) = 0</script></p>\n</li>\n<li><p>而将上面的泰勒展开求梯度，得到：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\nabla f(x) = g_k + H_k(x - x^{(k)}) = 0</script><p>其中<script type=\"math/tex\">H_k = H(x^{(k)})</script></p>\n<ul>\n<li>所以得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nx^{(k+1)} = x^{(k)} - H_k^{-1}g_k = x^{(k)} + p_k</script><ul>\n<li>将上面的流程总结：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027212238000.png\" alt=\"image-20221027212238000\" style=\"zoom:80%;\" /></p>\n<h3 id=\"4-2-拟牛顿法\"><a href=\"#4-2-拟牛顿法\" class=\"headerlink\" title=\"4.2 拟牛顿法\"></a>4.2 拟牛顿法</h3><ul>\n<li>由于计算黑塞矩阵的逆<script type=\"math/tex\">H^{-1}</script>比较复杂，所以考虑使用一个n阶矩阵<script type=\"math/tex\">G_k</script>来代替<script type=\"math/tex\">H_k^{-1}</script>，这就是拟牛顿法的思想</li>\n<li>由上面牛顿法中的<script type=\"math/tex\">\\nabla f(x^{(k+1)}) = 0</script>的方程可得：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ng_{k+1} - g_k = H_k(x^{(k)} - x^{(k)})</script><p>设<script type=\"math/tex\">y_k = g_{k+1} - g_k, \\delta_k = x^{(k)} - x^{(k)}</script>，上式可写为：</p>\n<script type=\"math/tex; mode=display\">\ny_k = H_k\\delta_k  \\\\\nH_k^{-1}y_k = \\delta_k</script><p>上式称为<strong>拟牛顿条件</strong></p>\n<ul>\n<li><p>而上面说过，要保证<script type=\"math/tex\">H_k</script>是正定矩阵，才能保证驻点一定是极小值点。所以<script type=\"math/tex\">H_k</script>同时满足拟牛顿条件和正定矩阵两个条件</p>\n</li>\n<li><p><strong>而近似矩阵<script type=\"math/tex\">G_k</script>同样满足这两个条件，即可将其作为近似的矩阵</strong></p>\n</li>\n</ul>\n<h4 id=\"4-2-1-DFP算法\"><a href=\"#4-2-1-DFP算法\" class=\"headerlink\" title=\"4.2.1 DFP算法\"></a>4.2.1 DFP算法</h4><ul>\n<li>现在重点就变成了如何对<script type=\"math/tex\">G_k</script>进行更新，才能保证<script type=\"math/tex\">G_{k+1}</script>同样满足条件</li>\n<li>假设G的更新是加上两个附加项构成的，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nG_{k+1} = G_k + P_k + Q_k</script><ul>\n<li>这时：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nG_{k+1}y_k = G_ky_k + P_ky_k + Q_ky_k</script><p>为使<script type=\"math/tex\">G_{k+1}</script>满足拟牛顿条件，<script type=\"math/tex\">P_k, Q_k</script>要满足：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nP_{k} y_{k}=\\delta_{k} \\\\\nQ_{k} y_{k}=-G_{k} y_{k}\n\\end{array}</script><ul>\n<li>这样的矩阵不难找出，例如：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nP_{k}=\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}} \\\\\nQ_{k}=-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}\n\\end{array}</script><ul>\n<li>所以更新公式为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nG_{k+1}=G_{k}+\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}}-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}</script><ul>\n<li>总结一下DFP算法的算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214008428.png\" alt=\"image-20221027214008428\" style=\"zoom:80%;\" /></p>\n<h4 id=\"4-2-2-BFGS算法\"><a href=\"#4-2-2-BFGS算法\" class=\"headerlink\" title=\"4.2.2 BFGS算法\"></a>4.2.2 BFGS算法</h4><ul>\n<li><p>BFGS算法比DFS算法更加常用，后者是用<script type=\"math/tex\">G_k</script>逼近<script type=\"math/tex\">H^{-1}</script>，而BFGS是用<script type=\"math/tex\">B_k</script>逼近<script type=\"math/tex\">H</script></p>\n</li>\n<li><p>算法流程：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214231809.png\" alt=\"image-20221027214231809\" style=\"zoom:80%;\" /></p>\n","site":{"data":{}},"wordcount":5342,"excerpt":"","more":"<h1 id=\"1-最大熵模型\"><a href=\"#1-最大熵模型\" class=\"headerlink\" title=\"1 最大熵模型\"></a>1 最大熵模型</h1><h3 id=\"1-1-最大熵原理\"><a href=\"#1-1-最大熵原理\" class=\"headerlink\" title=\"1.1 最大熵原理\"></a>1.1 最大熵原理</h3><ul>\n<li><p><strong>最大熵原理：</strong>学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型</p>\n</li>\n<li><p>假设离散变量<script type=\"math/tex\">X</script>的概率分布为<script type=\"math/tex\">P(X)</script>，则其熵为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(P) = -\\sum_xP(x)\\log P(x)</script><p>熵的取值范围：<script type=\"math/tex\">0 \\le H(P) \\le \\log|X|</script>，在<script type=\"math/tex\">X</script>为均匀分布时熵取得最大</p>\n<ul>\n<li>所以在没有任何约束时，<script type=\"math/tex\">X</script>直接取均匀分布，熵即可达到最大</li>\n<li><strong>条件熵：</strong>在已知X的情况下，求Y的条件熵：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(Y|X) = \\sum_{x}p(x)H(Y|X=x) = -\\sum_xp(x)\\sum_yp(y|x)log(p(y|x))</script><h3 id=\"1-2-模型约束\"><a href=\"#1-2-模型约束\" class=\"headerlink\" title=\"1.2 模型约束\"></a>1.2 模型约束</h3><ul>\n<li>前面说到最大熵原理是在约束的模型集合中寻找熵最大的，那么介绍一下最大熵模型的约束为什么</li>\n<li>假设是一个分类模型<script type=\"math/tex\">P(Y|X)</script>，首先给定训练集<script type=\"math/tex\">T = \\{(x_1, y_1), ...,(x_N, y_N)\\}</script>，那么可以由统计得到经验分布：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\tilde{P}(X=x, Y=y)=\\frac{v(X=x, Y=y)}{N} \\\\\n\\tilde{P}(X=x)=\\frac{v(X=x)}{N}\n\\end{array}</script><ul>\n<li><strong>特征函数：</strong><script type=\"math/tex\">f(x, y)</script>用于描述输入x和输出y之间的某个事实：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x, y)=\\left\\{\\begin{array}{ll}\n1, & x \\text { 与 } y \\text { 满足某一事实 } \\\\\n0, & \\text { 否则 }\n\\end{array}\\right.</script><p>特征函数的形式很多样，可以是对数据中字、词的统计信息作为输入，甚至可以直接用某些模型（如BERT）的输出作为输入</p>\n<ul>\n<li>那么特征函数关于经验分布<script type=\"math/tex\">\\tilde{P}(X,Y)</script>的期望值为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE_{\\tilde{P}}(f) = \\sum_{x,y}\\tilde{P}(x,y)f(x,y)</script><ul>\n<li>特征函数关于模型<script type=\"math/tex\">P(X|Y)</script>和经验分布<script type=\"math/tex\">\\tilde{P}(X)</script>的期望值为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE_P(f) = \\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)</script><ul>\n<li><strong>而最大熵模型的假设为这两个期望值相等，即：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE_P(f) = E_{\\tilde{P}}(f)</script><p>这就是最大熵模型的约束，有几个特征函数就有几个约束</p>\n<h3 id=\"1-3-模型定义\"><a href=\"#1-3-模型定义\" class=\"headerlink\" title=\"1.3 模型定义\"></a>1.3 模型定义</h3><ul>\n<li>假设满足所有约束条件的模型集合为<script type=\"math/tex\">\\mathcal{C} \\equiv\\left\\{P \\in \\mathcal{P} \\mid E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n\\right\\}</script>，定义在概率分布<script type=\"math/tex\">P(Y|X)</script>上的条件熵为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(P)=-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x)</script><p>则模型集合<script type=\"math/tex\">\\mathcal{C}</script>中<script type=\"math/tex\">H(P)</script>最大的模型就称为最大熵模型</p>\n<h1 id=\"2-最大熵模型的学习\"><a href=\"#2-最大熵模型的学习\" class=\"headerlink\" title=\"2 最大熵模型的学习\"></a>2 最大熵模型的学习</h1><ul>\n<li>根据上面所述，现在问题就变为了一个带约束的最优化问题：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{ll}\n\\max _{P \\in \\mathrm{C}} & H(P)=-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x) \\\\\n\\text { s.t. } & E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n \\\\\n& \\sum_{y} P(y \\mid x)=1\n\\end{array}</script><ul>\n<li>上述问题可使用拉格朗日优化，引入拉格朗如乘子并得到对偶问题进行求解，具体过程可参考<a href=\"https://zlkqz.site/2022/09/26/SVM/#2-2-%E5%8E%9F%E9%97%AE%E9%A2%98%E8%BD%AC%E5%8C%96%E5%88%B0%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98\">SVM的学习</a>，<strong>首先引入拉格朗日乘子<script type=\"math/tex\">w_0, w_1, ...,w_n</script>得到拉格朗日函数：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(P, w) \\equiv-H(P)+w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)+\\sum_{i=1}^{n} w_{i}\\left(E_{\\tilde{P}}\\left(f_{i}\\right)-E_{P}\\left(f_{i}\\right)\\right)</script><ul>\n<li><strong>然后将原始问题<script type=\"math/tex\">\\min _{P \\in \\mathbf{C}} \\max _{w} L(P, w)</script>转化为对偶问题<script type=\"math/tex\">\\max _{w} \\min _{P \\in \\mathbf{C}} L(P, w)</script>，先只考虑里面的最小化问题<script type=\"math/tex\">\\min _{P \\in \\mathbf{C}} L(P, w)</script>，求偏导<script type=\"math/tex\">\\frac{\\partial L(P,w)}{\\partial P(y|x)} = 0</script>，最后可以解得：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)</script><p>其中<script type=\"math/tex\">w_i</script>为每个特征函数对应的权重，也是拉格朗日优化时的拉格朗日乘子。<script type=\"math/tex\">Z_w(x)</script>为规范化因子：</p>\n<script type=\"math/tex; mode=display\">\nZ_{w}(x)=\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)</script><ul>\n<li>那么接下来的任务变成了外部的极大化问题，即将上述<script type=\"math/tex\">P_w(y|x)</script>带入<script type=\"math/tex\">L(P,w)</script>得到<script type=\"math/tex\">\\Psi(w)</script>，然后寻找：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nw^{*}=\\underset{w}{\\arg \\max } \\Psi(w)</script><ul>\n<li><strong>而上述对偶函数的极大化等价于最大熵模型的极大似然估计</strong></li>\n</ul>\n<blockquote>\n<p><strong>证明：</strong></p>\n<p>当概率分布<script type=\"math/tex\">P(y|x)</script>是最大熵模型时，对数似然函数为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nL_{\\tilde{P}}\\left(P_{w}\\right) &=\\log\\prod_{x,y}P(y|x)^{\\tilde{P}(x,y)} = \\sum_{x, y} \\tilde{P}(x, y) \\log P(y \\mid x) \\\\\n&=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x, y) \\log Z_{w}(x) \\\\\n&=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n\\end{aligned}</script><p>而上述对偶函数<script type=\"math/tex\">\\Psi(w)</script>中，有一项<script type=\"math/tex\">w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)</script>，由于<script type=\"math/tex\">w</script>中并没有包含<script type=\"math/tex\">w_0</script>，所以这一项可以直接去掉，不影响优化，所以：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Psi(w)=& \\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log P_{w}(y \\mid x) +\\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) f_{i}(x, y)\\right) \\\\\n=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)+\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x)\\left(\\log P_{w}(y \\mid x)-\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\\n=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log Z_{w}(x) \\\\\n=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n\\end{aligned}</script><p>由上述两个式子可以得到：</p>\n<script type=\"math/tex; mode=display\">\n\\Psi(w)=L_{\\tilde{p}}\\left(P_{w}\\right)</script></blockquote>\n<h1 id=\"3-优化算法\"><a href=\"#3-优化算法\" class=\"headerlink\" title=\"3 优化算法\"></a>3 优化算法</h1><ul>\n<li>现在我们是要最大化似然函数或者对偶函数，但是<strong>该函数并没有显式的解析解</strong>（有解析解，但是找不到），所以需要运用一些优化算法</li>\n<li>值得庆幸的是，对偶函数的目标函数具有很好的性质，<strong>他是光滑的凸函数，因此多种最优化方法都适用，并且保证能找到全局最优解</strong></li>\n</ul>\n<h3 id=\"3-1-改进的迭代尺度法\"><a href=\"#3-1-改进的迭代尺度法\" class=\"headerlink\" title=\"3.1 改进的迭代尺度法\"></a>3.1 改进的迭代尺度法</h3><ul>\n<li>算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027204416341.png\" alt=\"image-20221027204416341\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>这一算法的关键是(a)，即求解<script type=\"math/tex\">\\delta_i</script>，如果<script type=\"math/tex\">f\\#(x,y)</script>是常数，即对于任意x,y，都有<script type=\"math/tex\">f\\#(x,y) = M</script>，那么：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\delta_{i}=\\frac{1}{M} \\log \\frac{E_{\\tilde{p}}\\left(f_{i}\\right)}{E_{p}\\left(f_{i}\\right)}</script><p>如果<script type=\"math/tex\">f\\#(x,y)</script>不为常数，最简单有效的是<strong>牛顿法</strong>，将(a)中需要求解的方程表示为<script type=\"math/tex\">g(\\delta_i) = 0</script>，然后通过牛顿法最小化<script type=\"math/tex\">g(\\delta_i)</script>，每次迭代的公式为：</p>\n<script type=\"math/tex; mode=display\">\n\\delta_{i}^{(k+1)}=\\delta_{i}^{(k)}-\\frac{g\\left(\\delta_{i}^{(k)}\\right)}{g^{\\prime}\\left(\\delta_{i}^{(k)}\\right)}</script><h3 id=\"3-2-拟牛顿法\"><a href=\"#3-2-拟牛顿法\" class=\"headerlink\" title=\"3.2 拟牛顿法\"></a>3.2 拟牛顿法</h3><ul>\n<li>算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027205121849.png\" alt=\"image-20221027205121849\" style=\"zoom:80%;\" /></p>\n<h1 id=\"4-牛顿法和拟牛顿法\"><a href=\"#4-牛顿法和拟牛顿法\" class=\"headerlink\" title=\"4 牛顿法和拟牛顿法\"></a>4 牛顿法和拟牛顿法</h1><ul>\n<li>上面提到了牛顿法和拟牛顿法，这节就简要介绍以下其原理</li>\n<li>他们都是<strong>求解无约束最优化问题</strong>的常用方法，有收敛速度快的优点</li>\n</ul>\n<h3 id=\"4-1-牛顿法\"><a href=\"#4-1-牛顿法\" class=\"headerlink\" title=\"4.1 牛顿法\"></a>4.1 牛顿法</h3><ul>\n<li>考虑最优化：<script type=\"math/tex\">\\min_{x \\in R^n}f(x)</script>，假设<script type=\"math/tex\">f(x)</script>具有二阶连续偏导数，若第k次迭代之为<script type=\"math/tex\">x^{(k)}</script>，则可将<script type=\"math/tex\">f(x)</script>在<script type=\"math/tex\">x^{(k)}</script>处进行二阶泰勒展开：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(x)=f\\left(x^{(k)}\\right)+g_{k}^{\\mathrm{T}}\\left(x-x^{(k)}\\right)+\\frac{1}{2}\\left(x-x^{(k)}\\right)^{\\mathrm{T}} H\\left(x^{(k)}\\right)\\left(x-x^{(k)}\\right)</script><p>其中<script type=\"math/tex\">g_k = g(x^{(k)}) = \\nabla f(x^{(k)})</script>是<script type=\"math/tex\">f(x)</script>的梯度在<script type=\"math/tex\">x^{(k)}</script>的值，<script type=\"math/tex\">H(x^{k})</script>是黑塞矩阵<script type=\"math/tex\">H(x)=\\left[\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}\\right]_{n \\times n}</script>在<script type=\"math/tex\">x^{(k)}</script>的值</p>\n<ul>\n<li>函数<script type=\"math/tex\">f(x)</script>有极值的必要条件是一阶导数为0，即<script type=\"math/tex\">\\nabla f(x) = 0</script>。特别的当<script type=\"math/tex\">H(x)</script>为正定矩阵时，该点为极小值点</li>\n<li><p>牛顿法的每次迭代，从<script type=\"math/tex\">x^{(k)}</script>开始，将目标函数的极小值点作为下次迭代的<script type=\"math/tex\">x^{(k+1)}</script>，其满足<script type=\"math/tex\">\\nabla f(x^{(k+1)}) = 0</script></p>\n</li>\n<li><p>而将上面的泰勒展开求梯度，得到：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\nabla f(x) = g_k + H_k(x - x^{(k)}) = 0</script><p>其中<script type=\"math/tex\">H_k = H(x^{(k)})</script></p>\n<ul>\n<li>所以得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nx^{(k+1)} = x^{(k)} - H_k^{-1}g_k = x^{(k)} + p_k</script><ul>\n<li>将上面的流程总结：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027212238000.png\" alt=\"image-20221027212238000\" style=\"zoom:80%;\" /></p>\n<h3 id=\"4-2-拟牛顿法\"><a href=\"#4-2-拟牛顿法\" class=\"headerlink\" title=\"4.2 拟牛顿法\"></a>4.2 拟牛顿法</h3><ul>\n<li>由于计算黑塞矩阵的逆<script type=\"math/tex\">H^{-1}</script>比较复杂，所以考虑使用一个n阶矩阵<script type=\"math/tex\">G_k</script>来代替<script type=\"math/tex\">H_k^{-1}</script>，这就是拟牛顿法的思想</li>\n<li>由上面牛顿法中的<script type=\"math/tex\">\\nabla f(x^{(k+1)}) = 0</script>的方程可得：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ng_{k+1} - g_k = H_k(x^{(k)} - x^{(k)})</script><p>设<script type=\"math/tex\">y_k = g_{k+1} - g_k, \\delta_k = x^{(k)} - x^{(k)}</script>，上式可写为：</p>\n<script type=\"math/tex; mode=display\">\ny_k = H_k\\delta_k  \\\\\nH_k^{-1}y_k = \\delta_k</script><p>上式称为<strong>拟牛顿条件</strong></p>\n<ul>\n<li><p>而上面说过，要保证<script type=\"math/tex\">H_k</script>是正定矩阵，才能保证驻点一定是极小值点。所以<script type=\"math/tex\">H_k</script>同时满足拟牛顿条件和正定矩阵两个条件</p>\n</li>\n<li><p><strong>而近似矩阵<script type=\"math/tex\">G_k</script>同样满足这两个条件，即可将其作为近似的矩阵</strong></p>\n</li>\n</ul>\n<h4 id=\"4-2-1-DFP算法\"><a href=\"#4-2-1-DFP算法\" class=\"headerlink\" title=\"4.2.1 DFP算法\"></a>4.2.1 DFP算法</h4><ul>\n<li>现在重点就变成了如何对<script type=\"math/tex\">G_k</script>进行更新，才能保证<script type=\"math/tex\">G_{k+1}</script>同样满足条件</li>\n<li>假设G的更新是加上两个附加项构成的，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nG_{k+1} = G_k + P_k + Q_k</script><ul>\n<li>这时：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nG_{k+1}y_k = G_ky_k + P_ky_k + Q_ky_k</script><p>为使<script type=\"math/tex\">G_{k+1}</script>满足拟牛顿条件，<script type=\"math/tex\">P_k, Q_k</script>要满足：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nP_{k} y_{k}=\\delta_{k} \\\\\nQ_{k} y_{k}=-G_{k} y_{k}\n\\end{array}</script><ul>\n<li>这样的矩阵不难找出，例如：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\nP_{k}=\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}} \\\\\nQ_{k}=-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}\n\\end{array}</script><ul>\n<li>所以更新公式为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nG_{k+1}=G_{k}+\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}}-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}</script><ul>\n<li>总结一下DFP算法的算法流程：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214008428.png\" alt=\"image-20221027214008428\" style=\"zoom:80%;\" /></p>\n<h4 id=\"4-2-2-BFGS算法\"><a href=\"#4-2-2-BFGS算法\" class=\"headerlink\" title=\"4.2.2 BFGS算法\"></a>4.2.2 BFGS算法</h4><ul>\n<li><p>BFGS算法比DFS算法更加常用，后者是用<script type=\"math/tex\">G_k</script>逼近<script type=\"math/tex\">H^{-1}</script>，而BFGS是用<script type=\"math/tex\">B_k</script>逼近<script type=\"math/tex\">H</script></p>\n</li>\n<li><p>算法流程：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221027214231809.png\" alt=\"image-20221027214231809\" style=\"zoom:80%;\" /></p>\n"},{"title":"深度学习基础总结","math":true,"date":"2021-10-14T16:00:00.000Z","_content":"\n\n\n# 1 行业知识体系\n\n### 1.1 机器学习算法\n\n![qq_pic_merged_1635482805011](https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg)\n\n\n\n### 1.2 机器学习分类\n\n![qq_pic_merged_1635482964705](https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg)\n\n\n\n\n\n# 2 线性回归\n\n### 2.1 基本概念\n\n![image-20211029131504886](https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png)\n\n- 线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析\n\n- 线性回归输出是⼀个连续值，因此适⽤于回归问题\n\n\n\n### 2.2 损失函数\n\n- **均方差（mse）：**\n\n$$\n\\ell^{(i)}(W, b) = \\frac{(\\hat{y}^{(i)} - y^{(i)})^2}{2}\n$$\n\n其中$\\hat{y}$为计算出的预测值，$y$为真实值（label）\n\n\n\n\n\n# 3  全连接层（稠密层）\n\n- 输出层中的神经元和输⼊ 层中各个输⼊完全连接\n- 计算完全依赖于输入层。\n\n\n\n\n\n# 4  小批量随机梯度下降\n\n- 在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\\eta$）作为模型参数在本次迭代的减小量。\n\n- 每次mini-batch梯度下降计算过程：\n\n$$\nw_1 = w_1 - \\frac{\\eta}{|\\mathbb{B}|}\\sum_{i \\in \\mathbb{B}}\\frac{\\partial{\\ell(...)}}{\\partial{w_1}}\n$$\n\n- 注意学习率要取正数\n\n\n\n\n\n# 5  Softmax分类\n\n### 5.1 基本概念\n\n- softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数\n- 由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为该类的概率，具体计算如下：\n\n假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:\n$$\n\\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = softmax(o_1, o_2, o_3)\n$$\n其中\n$$\n\\hat{y}_i = \\frac{e^{o_i}}{\\sum_{j = 1}^{3}e^{o_j}}\n$$\n\n- 容易看出$\\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）\n- 通常，我们把预测概率最⼤的类别作为输出类别\n\n\n\n### 5.2 损失函数\n\n- 这里我们运用交叉熵损失函数（cross entropy）：\n\n$$\nH(y^{(i)}, \\hat{y}^{(i)}) = -\\sum_{j = 1}^qy_j^{(i)}\\log{\\hat{y}^{(i)}}\n$$\n\nq为输出元个数$\\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值\n\n\n\n\n\n# 6 多层感知机（MLP）\n\n- 多层感知机有一到多个隐藏层\n- 多层感知机中的隐藏层和输出层都是全连接层\n- 多层感知机具有激活函数，下面介绍常用激活函数\n\n\n\n\n\n# 7 激活函数\n\n### 7.1 Sigmoid函数\n\n$$\nsigmoid(x) = \\frac{1}{1 + e^{-x}}\n$$\n\n$$\nsigmoid'(x) = sigmoid(x)(1 - sigmoid(x))\n$$\n\n![image-20211030004640247](https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png)\n\n- 一般用在二分类的输出层中\n\n\n\n### 7.2 tanh函数\n\n$$\ntanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}\n$$\n\n$$\ntanh'(x) = 1 - tanh^2(x)\n$$\n\n![image-20211030005138300](https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png)\n\n- tanh和sigmoid比较相似，但是性能一般要优于sigmoid\n\n\n\n### 7.3 ReLU函数\n\n$$\nReLU(x) = max(0, x)\n$$\n\n![image-20211030005121490](https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png)\n\n- 十分常用\n\n\n\n### 7.4 Leaky ReLU函数\n\n$$\nLeaky\\_Relu(x) = max(0.1 * x, x)\n$$\n\n![image-20211030005818507](https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png)\n\n\n\n\n\n# 8 训练误差和泛化误差\n\n### 8.1 基本概念\n\n- **训练误差：**指模型在训练数据集上表现出的误差\n- **泛化误差：**指模型在任意⼀个测试数据样本上表现出的误差的期望\n\n\n\n\n### 8.2 k折交叉验证\n\n- 当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法\n- 即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个子数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。\n\n\n\n### 8.3 欠拟合和过拟合\n\n- **欠拟合：**训练误差过高\n\n- **过拟合：**泛化误差过高\n\n- 我们知道进行反向传播的目的是让学习器去拟合数据，而当拟合程度不够时就称为**欠拟合**，拟合程度过于高时则称为**过拟合**，用下图可以很好的解释：\n\n  <img src=\"https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png\" alt=\"image-20211111215618245\"  />\n\n- 欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了，**可以使用early stop来防止训练时间不足导致的欠拟合**。但是如果真的还是存在的话，可以通过**增加网络复杂度**或者在模型中**增加特征**，这些都是很好解决欠拟合的方法。而一般我们更加关注的是如何如何防止过拟合\n\n\n\n### 8.4 造成过拟合的原因\n\n- **训练数据集样本单一，样本不足**。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型\n- **训练数据中噪声干扰过大**。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系\n- **模型过于复杂。**模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。\n> - 模型太复杂是过拟合的重要因素，下面提到的正则化方法和Dropout方法都是基于减少模型复杂度来进行的。\n> - 另外，在模型设计方面有一个根本设计原则**奥卡姆剃刀法则：优先选择拟合数据的最简单的假设。 简单的模型才是最好的**\n> - **当模型复杂度过高的时候，拟合函数的系数往往非常大，需要顾忌每一个点，最终形成的拟合函数波动很大**，如上图过拟合情况。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的梯度非常大，所以才需要惩罚项来降低\n> - **模型复杂度也可以解释为：模型对某些非必要的特征过于看重（即给予了较大的权值）**\n\n\n\n### 8.5 对应过拟合手段\n\n#### 8.5.1 增加数据\n\n- 由于**数据量不足是造成过拟合的根本原因**，所以对应过拟合最有效的手段肯定是增大数据集，但是这种方法成本过高\n- 在数据层面，还可以进行**数据增强**，创造一些假数据，还可以提升模型的泛化能力\n- 在CV方面常用的数据增强有旋转图像、缩放图像、随机裁剪等，而在NLP方面还有同义词替换、随机删除词、随机颠倒句子顺序等方法\n\n\n\n#### 8.5.2 正则化\n\n- L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积\n- 损失函数添加一个L2惩罚项：\n\n$$\n\\ell(...) + \\frac{\\lambda}{2}\\begin{Vmatrix} W \\end{Vmatrix}^2\n$$\n\n求导后：（每个mini-batch）\n$$\nw1 = (1 - \\eta\\lambda)w1 - \\frac{\\eta}{|\\mathbb{B}|}\\sum_{i \\in \\mathbb{B}}\\frac{\\partial\\ell^{(i)}(...)}{\\partial{w1}}\n$$\n其中超参数$\\lambda$ > 0。当权重参数均为0时，惩罚项最小。当$\\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\\lambda$设为0时，惩罚项完全不起作⽤。\n\n- 除此之外还有L1正则化，使用L1范数作为惩罚项：\n\n$$\n\\ell(...) + \\frac{\\lambda}{2}\\begin{Vmatrix} W \\end{Vmatrix}\n$$\n\n- 一般训练集中的损失函数要加惩罚项，测试集中不用\n- 正则化的原理可以从**拉格朗日乘子法和最大后验概率估计**两方面解释\n- 首先是拉格朗日乘子法：\n\n> - 设模型本来的优化目标为：\n>\n> $$\n> \\min _{w} J(w ; X, y)\n> $$\n>\n> - 而过拟合的原因在于模型复杂度过高，而一般模型复杂度和参数$$w$$的稀疏度相关，即**参数越少或参数中0的个数越多，模型复杂度越小**\n> - 那么0的个数可以让我们联想到**w的L0范数（代表w中非零元素的个数）**，那么可以**添加一个约束，使得优化目标变为：**\n>\n> $$\n> \\begin{array}{c}\n> \\min _{w} J(w ; X, y) \\\\\n> s \\cdot t .\\|w\\|_{0} \\leq C\n> \\end{array}\n> $$\n>\n> - 但是**使用L0范数太过困难，主要是L0范数无法进行求导来反向传播，所以我们退而求其次，要求权重w向量中某些维度的非零参数尽可能接近于0，尽可能的小，所以就可以使用L1、L2范数：**\n>\n> $$\n> \\begin{array}{c}\n> \\min _{w} J(w ; X, y) \\\\\n> s \\cdot t .\\|w\\|_{1} \\leq C 或 s \\cdot t .\\|w\\|_{2} \\leq C \n> \\end{array}\n> $$\n>\n> - 以L2正则化为例，采用拉格朗日乘子法，其拉格朗日函数为：\n>\n> $$\n> \\min _{w} J(w ; X, y)+\\alpha^{*}\\|w\\|_{2}^{2}\n> $$\n>\n> - 这就是现在的优化目标，而后面的一项恰好就是惩罚项\n\n- 还可以用最大后验概率估计来解释：\n\n> - 最大后验估计的优化目标是：\n>\n> $$\n> M A P=\\log P(y \\mid X, w) P(w)=\\log P(y \\mid X, w)+\\log P(w)\n> $$\n>\n> - 上式左边的一项即最大化似然的优化目标，也就是本来的不加惩罚项的优化目标；**而右边的一项仅关于参数w的先验分布**\n> - 我们对w的先验分布进行不同的假设，即可得到不同的惩罚项\n> - **在L2正则化中，假设$$w \\sim N(0, \\sigma^2)$$，那么有：**\n>\n> $$\n> \\begin{array}{l}\n> \\log P(w)=\\log \\prod_{j} P\\left(w_{j}\\right)= \\\\\n> \\log \\prod_{j}\\left[\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(w_{j}\\right)^{2}}{2 \\sigma^{2}}}\\right] \\\\\n> =-\\frac{1}{2 \\sigma^{2}} \\sum_{j} w_{j}^{2}+C\n> \\end{array}\n> $$\n>\n> 那么就得到了L2范数作为惩罚项\n>\n> - **L1正则化也一样，假设$$w$$服从均值为0参数为a的拉普拉斯分布，那么有：**\n>\n> $$\n> \\begin{array}{l}\n> \\log P(w)=\\log \\prod_{j} P\\left(w_{j}\\right)= \\\\\n> \\log \\prod_{j}\\left[\\frac{1}{\\sqrt{2 a} \\sigma} e^{-\\frac{w_{j}}{a}}\\right] \\\\\n> =-\\frac{1}{2 a} \\sum_{j}\\left|w_{j}\\right|+C\n> \\end{array}\n> $$\n\n\n\n#### 8.5.2 丢弃法（Dropout）\n\n- 对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：\n\n$$\nh_i' = \\frac{\\xi_i}{1 - p}h_i\n$$\n\n$\\xi_i$为0和1的概率分别为p和1 - p\n\n分母中的1- p是为了不改变其输⼊的期望值：\n$$\n由于E(\\xi_i) = 1 - p \\\\\n所以E(h_i') = \\frac{E(\\xi_i)}{1 - p}h_i = h_i\n$$\n\n- 测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法\n\n- 进行了Dropout的多层感知机：\n\n![image-20211030141251677](https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png)\n\n可以看到隐藏层中的$h_2$和$h_5$消失了\n\n- 每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点\n\n- **Dropout的原理：**\n\n> 1. **集成学习：**每次随机置0都可以得到一个新模型，而Dropout可以当作对这些所有新得到的模型的集成\n> 2. **正则化：**因为Dropout导致两个神经元不一定每次都在一个dropout网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 ，迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。正因为这样，网络由于不知道浅层的哪些神经元会失活，导致网络不敢赋予浅层神经元太大的权重，这样就减轻了网络对某些局部特征的依赖，换句话说网络不会对一些特定的线索片段太过敏感，即使丢失特定的线索，它也可以从众多其它线索中学习一些共同的特征。从这个角度看Dropout就类似于L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。\n\n\n\n# 9 正向传播和反向传播\n\n### 9.1 正向传播\n\n- 正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）\n- 中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等\n- 正向传播的计算图：\n\n![image-20211030142019713](https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png)\n\n其中：\n\n1. 左下⻆是输⼊，右上⻆是输出\n2. ⽅框代表变量，圆圈代表运算符\n3. $J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项\n\n\n\n### 9.2 反向传播\n\n- 反向传播最重要的是通过链式法则求导，从后往前进行计算\n- 反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的\n\n\n\n### 9.3 衰减和爆炸\n\n- 如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：\n\n假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积\n\n\n\n### 9.4 随机初始化参数\n\n- 考虑三种参数的极端情况：\n\n> 1. **参数很稀疏，那么输入的特征会在经过前向计算后逐渐消失（计算出来都接近0）**\n> 2. **参数很大，输出值容易进入激活函数的饱和区，比如Sigmoid函数**\n> 3. 假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。**在这种情况下，无论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作用**\n\n- 可以看出，我们**希望每一层的输出方差能够固定**，因为这样能够防止我们的信号变得很大/直接消失。换句话来说，**我们需要一种权重初始化的手段，使得输入、输出的方差保持不变，**这就是Xavier初始化做的事。\n\n\n\n### 9.5 Xavier随机初始化\n\n- 假设某层的参数，前一层神经元个数为a，后一层神经元个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：\n\n$$\nU(-\\sqrt{\\frac{6}{a + b}}, \\sqrt{\\frac{6}{a + b}})\n$$\n\n- 该设计主要为了**在前向计算的时候，经过此层的参数后得到的输出方差不变（即输入输出方差相等）；并且在反向计算的时候，后一层的梯度和前一层的梯度方差也相等**\n\n- **Xavier初始化在sigmoid和tanh激活函数上有很好的表现，但是在Relu激活函数上表现很差。**\n\n- 现在开始推导，这里只展示前向计算时，首先是有3个假设：\n\n> - 激活函数为$$f(x) = x$$\n> - 偏置项b初始化为0\n> - 输入和参数的期望都为0\n\n- 设某一层值为$$x$$，参数为$$w$$，对应输入为$$y = x^Tw$$（注意$$x,w$$为向量，$$y$$为标量），那么我们的目的是：\n\n$$\nVar[y] = Var[x] \\\\\nVar[x^Tw] = Var[x]\n$$\n\n- 引入公式：\n\n$$\nVar[XY] = E[X^2]E[Y^2] - E^2[X]E^2[Y] \\\\\n在E[X] = E[Y]时，可以得出： Var[XY] = Var[X]Var[Y]\n$$\n\n- 而由于$$E[w] = E[x] = 0$$，所以：\n\n$$\nVar[x] = Var[x^Tw] = Var[\\sum^{N}_ix_iw_i] = N* Var[x]Var[w]\n$$\n\n其中$$N = dim(x) = dim(w)$$，即当前层的神经元个数\n\n- 由上式可以得出：\n\n$$\nN * Var[w] = 1 \\\\\nVar[w] = \\frac{1}{N}\n$$\n\n- 而在反向传播的时候，我们同样可以推出$$Var[w] = \\frac{1}{N'}$$，其中$$N'$$为下一层神经元个数\n- 而由于方差取值无法同时满足前后向，所以采用两者的平均数：\n\n$$\nVar[w] = \\frac{2}{N + N'}\n$$\n\n- 我们可以通过$$N(0, \\frac{2}{N+N'})$$对w取样，也可以使用均匀分布采样，采用均匀函数的时候需要进行一定的放缩（Var[w]的分子不一定为2）","source":"_posts/深度学习基础总结.md","raw":"---\ntitle: 深度学习基础总结\nmath: true\ndate: 2021-10-15\n---\n\n\n\n# 1 行业知识体系\n\n### 1.1 机器学习算法\n\n![qq_pic_merged_1635482805011](https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg)\n\n\n\n### 1.2 机器学习分类\n\n![qq_pic_merged_1635482964705](https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg)\n\n\n\n\n\n# 2 线性回归\n\n### 2.1 基本概念\n\n![image-20211029131504886](https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png)\n\n- 线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析\n\n- 线性回归输出是⼀个连续值，因此适⽤于回归问题\n\n\n\n### 2.2 损失函数\n\n- **均方差（mse）：**\n\n$$\n\\ell^{(i)}(W, b) = \\frac{(\\hat{y}^{(i)} - y^{(i)})^2}{2}\n$$\n\n其中$\\hat{y}$为计算出的预测值，$y$为真实值（label）\n\n\n\n\n\n# 3  全连接层（稠密层）\n\n- 输出层中的神经元和输⼊ 层中各个输⼊完全连接\n- 计算完全依赖于输入层。\n\n\n\n\n\n# 4  小批量随机梯度下降\n\n- 在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\\eta$）作为模型参数在本次迭代的减小量。\n\n- 每次mini-batch梯度下降计算过程：\n\n$$\nw_1 = w_1 - \\frac{\\eta}{|\\mathbb{B}|}\\sum_{i \\in \\mathbb{B}}\\frac{\\partial{\\ell(...)}}{\\partial{w_1}}\n$$\n\n- 注意学习率要取正数\n\n\n\n\n\n# 5  Softmax分类\n\n### 5.1 基本概念\n\n- softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数\n- 由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为该类的概率，具体计算如下：\n\n假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:\n$$\n\\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = softmax(o_1, o_2, o_3)\n$$\n其中\n$$\n\\hat{y}_i = \\frac{e^{o_i}}{\\sum_{j = 1}^{3}e^{o_j}}\n$$\n\n- 容易看出$\\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）\n- 通常，我们把预测概率最⼤的类别作为输出类别\n\n\n\n### 5.2 损失函数\n\n- 这里我们运用交叉熵损失函数（cross entropy）：\n\n$$\nH(y^{(i)}, \\hat{y}^{(i)}) = -\\sum_{j = 1}^qy_j^{(i)}\\log{\\hat{y}^{(i)}}\n$$\n\nq为输出元个数$\\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值\n\n\n\n\n\n# 6 多层感知机（MLP）\n\n- 多层感知机有一到多个隐藏层\n- 多层感知机中的隐藏层和输出层都是全连接层\n- 多层感知机具有激活函数，下面介绍常用激活函数\n\n\n\n\n\n# 7 激活函数\n\n### 7.1 Sigmoid函数\n\n$$\nsigmoid(x) = \\frac{1}{1 + e^{-x}}\n$$\n\n$$\nsigmoid'(x) = sigmoid(x)(1 - sigmoid(x))\n$$\n\n![image-20211030004640247](https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png)\n\n- 一般用在二分类的输出层中\n\n\n\n### 7.2 tanh函数\n\n$$\ntanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}\n$$\n\n$$\ntanh'(x) = 1 - tanh^2(x)\n$$\n\n![image-20211030005138300](https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png)\n\n- tanh和sigmoid比较相似，但是性能一般要优于sigmoid\n\n\n\n### 7.3 ReLU函数\n\n$$\nReLU(x) = max(0, x)\n$$\n\n![image-20211030005121490](https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png)\n\n- 十分常用\n\n\n\n### 7.4 Leaky ReLU函数\n\n$$\nLeaky\\_Relu(x) = max(0.1 * x, x)\n$$\n\n![image-20211030005818507](https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png)\n\n\n\n\n\n# 8 训练误差和泛化误差\n\n### 8.1 基本概念\n\n- **训练误差：**指模型在训练数据集上表现出的误差\n- **泛化误差：**指模型在任意⼀个测试数据样本上表现出的误差的期望\n\n\n\n\n### 8.2 k折交叉验证\n\n- 当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法\n- 即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个子数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。\n\n\n\n### 8.3 欠拟合和过拟合\n\n- **欠拟合：**训练误差过高\n\n- **过拟合：**泛化误差过高\n\n- 我们知道进行反向传播的目的是让学习器去拟合数据，而当拟合程度不够时就称为**欠拟合**，拟合程度过于高时则称为**过拟合**，用下图可以很好的解释：\n\n  <img src=\"https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png\" alt=\"image-20211111215618245\"  />\n\n- 欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了，**可以使用early stop来防止训练时间不足导致的欠拟合**。但是如果真的还是存在的话，可以通过**增加网络复杂度**或者在模型中**增加特征**，这些都是很好解决欠拟合的方法。而一般我们更加关注的是如何如何防止过拟合\n\n\n\n### 8.4 造成过拟合的原因\n\n- **训练数据集样本单一，样本不足**。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型\n- **训练数据中噪声干扰过大**。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系\n- **模型过于复杂。**模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。\n> - 模型太复杂是过拟合的重要因素，下面提到的正则化方法和Dropout方法都是基于减少模型复杂度来进行的。\n> - 另外，在模型设计方面有一个根本设计原则**奥卡姆剃刀法则：优先选择拟合数据的最简单的假设。 简单的模型才是最好的**\n> - **当模型复杂度过高的时候，拟合函数的系数往往非常大，需要顾忌每一个点，最终形成的拟合函数波动很大**，如上图过拟合情况。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的梯度非常大，所以才需要惩罚项来降低\n> - **模型复杂度也可以解释为：模型对某些非必要的特征过于看重（即给予了较大的权值）**\n\n\n\n### 8.5 对应过拟合手段\n\n#### 8.5.1 增加数据\n\n- 由于**数据量不足是造成过拟合的根本原因**，所以对应过拟合最有效的手段肯定是增大数据集，但是这种方法成本过高\n- 在数据层面，还可以进行**数据增强**，创造一些假数据，还可以提升模型的泛化能力\n- 在CV方面常用的数据增强有旋转图像、缩放图像、随机裁剪等，而在NLP方面还有同义词替换、随机删除词、随机颠倒句子顺序等方法\n\n\n\n#### 8.5.2 正则化\n\n- L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积\n- 损失函数添加一个L2惩罚项：\n\n$$\n\\ell(...) + \\frac{\\lambda}{2}\\begin{Vmatrix} W \\end{Vmatrix}^2\n$$\n\n求导后：（每个mini-batch）\n$$\nw1 = (1 - \\eta\\lambda)w1 - \\frac{\\eta}{|\\mathbb{B}|}\\sum_{i \\in \\mathbb{B}}\\frac{\\partial\\ell^{(i)}(...)}{\\partial{w1}}\n$$\n其中超参数$\\lambda$ > 0。当权重参数均为0时，惩罚项最小。当$\\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\\lambda$设为0时，惩罚项完全不起作⽤。\n\n- 除此之外还有L1正则化，使用L1范数作为惩罚项：\n\n$$\n\\ell(...) + \\frac{\\lambda}{2}\\begin{Vmatrix} W \\end{Vmatrix}\n$$\n\n- 一般训练集中的损失函数要加惩罚项，测试集中不用\n- 正则化的原理可以从**拉格朗日乘子法和最大后验概率估计**两方面解释\n- 首先是拉格朗日乘子法：\n\n> - 设模型本来的优化目标为：\n>\n> $$\n> \\min _{w} J(w ; X, y)\n> $$\n>\n> - 而过拟合的原因在于模型复杂度过高，而一般模型复杂度和参数$$w$$的稀疏度相关，即**参数越少或参数中0的个数越多，模型复杂度越小**\n> - 那么0的个数可以让我们联想到**w的L0范数（代表w中非零元素的个数）**，那么可以**添加一个约束，使得优化目标变为：**\n>\n> $$\n> \\begin{array}{c}\n> \\min _{w} J(w ; X, y) \\\\\n> s \\cdot t .\\|w\\|_{0} \\leq C\n> \\end{array}\n> $$\n>\n> - 但是**使用L0范数太过困难，主要是L0范数无法进行求导来反向传播，所以我们退而求其次，要求权重w向量中某些维度的非零参数尽可能接近于0，尽可能的小，所以就可以使用L1、L2范数：**\n>\n> $$\n> \\begin{array}{c}\n> \\min _{w} J(w ; X, y) \\\\\n> s \\cdot t .\\|w\\|_{1} \\leq C 或 s \\cdot t .\\|w\\|_{2} \\leq C \n> \\end{array}\n> $$\n>\n> - 以L2正则化为例，采用拉格朗日乘子法，其拉格朗日函数为：\n>\n> $$\n> \\min _{w} J(w ; X, y)+\\alpha^{*}\\|w\\|_{2}^{2}\n> $$\n>\n> - 这就是现在的优化目标，而后面的一项恰好就是惩罚项\n\n- 还可以用最大后验概率估计来解释：\n\n> - 最大后验估计的优化目标是：\n>\n> $$\n> M A P=\\log P(y \\mid X, w) P(w)=\\log P(y \\mid X, w)+\\log P(w)\n> $$\n>\n> - 上式左边的一项即最大化似然的优化目标，也就是本来的不加惩罚项的优化目标；**而右边的一项仅关于参数w的先验分布**\n> - 我们对w的先验分布进行不同的假设，即可得到不同的惩罚项\n> - **在L2正则化中，假设$$w \\sim N(0, \\sigma^2)$$，那么有：**\n>\n> $$\n> \\begin{array}{l}\n> \\log P(w)=\\log \\prod_{j} P\\left(w_{j}\\right)= \\\\\n> \\log \\prod_{j}\\left[\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(w_{j}\\right)^{2}}{2 \\sigma^{2}}}\\right] \\\\\n> =-\\frac{1}{2 \\sigma^{2}} \\sum_{j} w_{j}^{2}+C\n> \\end{array}\n> $$\n>\n> 那么就得到了L2范数作为惩罚项\n>\n> - **L1正则化也一样，假设$$w$$服从均值为0参数为a的拉普拉斯分布，那么有：**\n>\n> $$\n> \\begin{array}{l}\n> \\log P(w)=\\log \\prod_{j} P\\left(w_{j}\\right)= \\\\\n> \\log \\prod_{j}\\left[\\frac{1}{\\sqrt{2 a} \\sigma} e^{-\\frac{w_{j}}{a}}\\right] \\\\\n> =-\\frac{1}{2 a} \\sum_{j}\\left|w_{j}\\right|+C\n> \\end{array}\n> $$\n\n\n\n#### 8.5.2 丢弃法（Dropout）\n\n- 对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：\n\n$$\nh_i' = \\frac{\\xi_i}{1 - p}h_i\n$$\n\n$\\xi_i$为0和1的概率分别为p和1 - p\n\n分母中的1- p是为了不改变其输⼊的期望值：\n$$\n由于E(\\xi_i) = 1 - p \\\\\n所以E(h_i') = \\frac{E(\\xi_i)}{1 - p}h_i = h_i\n$$\n\n- 测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法\n\n- 进行了Dropout的多层感知机：\n\n![image-20211030141251677](https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png)\n\n可以看到隐藏层中的$h_2$和$h_5$消失了\n\n- 每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点\n\n- **Dropout的原理：**\n\n> 1. **集成学习：**每次随机置0都可以得到一个新模型，而Dropout可以当作对这些所有新得到的模型的集成\n> 2. **正则化：**因为Dropout导致两个神经元不一定每次都在一个dropout网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 ，迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。正因为这样，网络由于不知道浅层的哪些神经元会失活，导致网络不敢赋予浅层神经元太大的权重，这样就减轻了网络对某些局部特征的依赖，换句话说网络不会对一些特定的线索片段太过敏感，即使丢失特定的线索，它也可以从众多其它线索中学习一些共同的特征。从这个角度看Dropout就类似于L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。\n\n\n\n# 9 正向传播和反向传播\n\n### 9.1 正向传播\n\n- 正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）\n- 中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等\n- 正向传播的计算图：\n\n![image-20211030142019713](https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png)\n\n其中：\n\n1. 左下⻆是输⼊，右上⻆是输出\n2. ⽅框代表变量，圆圈代表运算符\n3. $J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项\n\n\n\n### 9.2 反向传播\n\n- 反向传播最重要的是通过链式法则求导，从后往前进行计算\n- 反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的\n\n\n\n### 9.3 衰减和爆炸\n\n- 如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：\n\n假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积\n\n\n\n### 9.4 随机初始化参数\n\n- 考虑三种参数的极端情况：\n\n> 1. **参数很稀疏，那么输入的特征会在经过前向计算后逐渐消失（计算出来都接近0）**\n> 2. **参数很大，输出值容易进入激活函数的饱和区，比如Sigmoid函数**\n> 3. 假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。**在这种情况下，无论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作用**\n\n- 可以看出，我们**希望每一层的输出方差能够固定**，因为这样能够防止我们的信号变得很大/直接消失。换句话来说，**我们需要一种权重初始化的手段，使得输入、输出的方差保持不变，**这就是Xavier初始化做的事。\n\n\n\n### 9.5 Xavier随机初始化\n\n- 假设某层的参数，前一层神经元个数为a，后一层神经元个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：\n\n$$\nU(-\\sqrt{\\frac{6}{a + b}}, \\sqrt{\\frac{6}{a + b}})\n$$\n\n- 该设计主要为了**在前向计算的时候，经过此层的参数后得到的输出方差不变（即输入输出方差相等）；并且在反向计算的时候，后一层的梯度和前一层的梯度方差也相等**\n\n- **Xavier初始化在sigmoid和tanh激活函数上有很好的表现，但是在Relu激活函数上表现很差。**\n\n- 现在开始推导，这里只展示前向计算时，首先是有3个假设：\n\n> - 激活函数为$$f(x) = x$$\n> - 偏置项b初始化为0\n> - 输入和参数的期望都为0\n\n- 设某一层值为$$x$$，参数为$$w$$，对应输入为$$y = x^Tw$$（注意$$x,w$$为向量，$$y$$为标量），那么我们的目的是：\n\n$$\nVar[y] = Var[x] \\\\\nVar[x^Tw] = Var[x]\n$$\n\n- 引入公式：\n\n$$\nVar[XY] = E[X^2]E[Y^2] - E^2[X]E^2[Y] \\\\\n在E[X] = E[Y]时，可以得出： Var[XY] = Var[X]Var[Y]\n$$\n\n- 而由于$$E[w] = E[x] = 0$$，所以：\n\n$$\nVar[x] = Var[x^Tw] = Var[\\sum^{N}_ix_iw_i] = N* Var[x]Var[w]\n$$\n\n其中$$N = dim(x) = dim(w)$$，即当前层的神经元个数\n\n- 由上式可以得出：\n\n$$\nN * Var[w] = 1 \\\\\nVar[w] = \\frac{1}{N}\n$$\n\n- 而在反向传播的时候，我们同样可以推出$$Var[w] = \\frac{1}{N'}$$，其中$$N'$$为下一层神经元个数\n- 而由于方差取值无法同时满足前后向，所以采用两者的平均数：\n\n$$\nVar[w] = \\frac{2}{N + N'}\n$$\n\n- 我们可以通过$$N(0, \\frac{2}{N+N'})$$对w取样，也可以使用均匀分布采样，采用均匀函数的时候需要进行一定的放缩（Var[w]的分子不一定为2）","slug":"深度学习基础总结","published":1,"updated":"2022-12-20T06:14:53.043Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1n000m7cszg6ahai5w","content":"<h1 id=\"1-行业知识体系\"><a href=\"#1-行业知识体系\" class=\"headerlink\" title=\"1 行业知识体系\"></a>1 行业知识体系</h1><h3 id=\"1-1-机器学习算法\"><a href=\"#1-1-机器学习算法\" class=\"headerlink\" title=\"1.1 机器学习算法\"></a>1.1 机器学习算法</h3><p><img src=\"https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg\" alt=\"qq_pic_merged_1635482805011\"></p>\n<h3 id=\"1-2-机器学习分类\"><a href=\"#1-2-机器学习分类\" class=\"headerlink\" title=\"1.2 机器学习分类\"></a>1.2 机器学习分类</h3><p><img src=\"https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg\" alt=\"qq_pic_merged_1635482964705\"></p>\n<h1 id=\"2-线性回归\"><a href=\"#2-线性回归\" class=\"headerlink\" title=\"2 线性回归\"></a>2 线性回归</h1><h3 id=\"2-1-基本概念\"><a href=\"#2-1-基本概念\" class=\"headerlink\" title=\"2.1 基本概念\"></a>2.1 基本概念</h3><p><img src=\"https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png\" alt=\"image-20211029131504886\"></p>\n<ul>\n<li><p>线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</p>\n</li>\n<li><p>线性回归输出是⼀个连续值，因此适⽤于回归问题</p>\n</li>\n</ul>\n<h3 id=\"2-2-损失函数\"><a href=\"#2-2-损失函数\" class=\"headerlink\" title=\"2.2 损失函数\"></a>2.2 损失函数</h3><ul>\n<li><strong>均方差（mse）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell^{(i)}(W, b) = \\frac{(\\hat{y}^{(i)} - y^{(i)})^2}{2}</script><p>其中$\\hat{y}$为计算出的预测值，$y$为真实值（label）</p>\n<h1 id=\"3-全连接层（稠密层）\"><a href=\"#3-全连接层（稠密层）\" class=\"headerlink\" title=\"3  全连接层（稠密层）\"></a>3  全连接层（稠密层）</h1><ul>\n<li>输出层中的神经元和输⼊ 层中各个输⼊完全连接</li>\n<li>计算完全依赖于输入层。</li>\n</ul>\n<h1 id=\"4-小批量随机梯度下降\"><a href=\"#4-小批量随机梯度下降\" class=\"headerlink\" title=\"4  小批量随机梯度下降\"></a>4  小批量随机梯度下降</h1><ul>\n<li><p>在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\\eta$）作为模型参数在本次迭代的减小量。</p>\n</li>\n<li><p>每次mini-batch梯度下降计算过程：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nw_1 = w_1 - \\frac{\\eta}{|\\mathbb{B}|}\\sum_{i \\in \\mathbb{B}}\\frac{\\partial{\\ell(...)}}{\\partial{w_1}}</script><ul>\n<li>注意学习率要取正数</li>\n</ul>\n<h1 id=\"5-Softmax分类\"><a href=\"#5-Softmax分类\" class=\"headerlink\" title=\"5  Softmax分类\"></a>5  Softmax分类</h1><h3 id=\"5-1-基本概念\"><a href=\"#5-1-基本概念\" class=\"headerlink\" title=\"5.1 基本概念\"></a>5.1 基本概念</h3><ul>\n<li>softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数</li>\n<li>由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为该类的概率，具体计算如下：</li>\n</ul>\n<p>假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:</p>\n<script type=\"math/tex; mode=display\">\n\\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = softmax(o_1, o_2, o_3)</script><p>其中</p>\n<script type=\"math/tex; mode=display\">\n\\hat{y}_i = \\frac{e^{o_i}}{\\sum_{j = 1}^{3}e^{o_j}}</script><ul>\n<li>容易看出$\\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）</li>\n<li>通常，我们把预测概率最⼤的类别作为输出类别</li>\n</ul>\n<h3 id=\"5-2-损失函数\"><a href=\"#5-2-损失函数\" class=\"headerlink\" title=\"5.2 损失函数\"></a>5.2 损失函数</h3><ul>\n<li>这里我们运用交叉熵损失函数（cross entropy）：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(y^{(i)}, \\hat{y}^{(i)}) = -\\sum_{j = 1}^qy_j^{(i)}\\log{\\hat{y}^{(i)}}</script><p>q为输出元个数$\\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值</p>\n<h1 id=\"6-多层感知机（MLP）\"><a href=\"#6-多层感知机（MLP）\" class=\"headerlink\" title=\"6 多层感知机（MLP）\"></a>6 多层感知机（MLP）</h1><ul>\n<li>多层感知机有一到多个隐藏层</li>\n<li>多层感知机中的隐藏层和输出层都是全连接层</li>\n<li>多层感知机具有激活函数，下面介绍常用激活函数</li>\n</ul>\n<h1 id=\"7-激活函数\"><a href=\"#7-激活函数\" class=\"headerlink\" title=\"7 激活函数\"></a>7 激活函数</h1><h3 id=\"7-1-Sigmoid函数\"><a href=\"#7-1-Sigmoid函数\" class=\"headerlink\" title=\"7.1 Sigmoid函数\"></a>7.1 Sigmoid函数</h3><script type=\"math/tex; mode=display\">\nsigmoid(x) = \\frac{1}{1 + e^{-x}}</script><script type=\"math/tex; mode=display\">\nsigmoid'(x) = sigmoid(x)(1 - sigmoid(x))</script><p><img src=\"https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png\" alt=\"image-20211030004640247\"></p>\n<ul>\n<li>一般用在二分类的输出层中</li>\n</ul>\n<h3 id=\"7-2-tanh函数\"><a href=\"#7-2-tanh函数\" class=\"headerlink\" title=\"7.2 tanh函数\"></a>7.2 tanh函数</h3><script type=\"math/tex; mode=display\">\ntanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}</script><script type=\"math/tex; mode=display\">\ntanh'(x) = 1 - tanh^2(x)</script><p><img src=\"https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png\" alt=\"image-20211030005138300\"></p>\n<ul>\n<li>tanh和sigmoid比较相似，但是性能一般要优于sigmoid</li>\n</ul>\n<h3 id=\"7-3-ReLU函数\"><a href=\"#7-3-ReLU函数\" class=\"headerlink\" title=\"7.3 ReLU函数\"></a>7.3 ReLU函数</h3><script type=\"math/tex; mode=display\">\nReLU(x) = max(0, x)</script><p><img src=\"https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png\" alt=\"image-20211030005121490\"></p>\n<ul>\n<li>十分常用</li>\n</ul>\n<h3 id=\"7-4-Leaky-ReLU函数\"><a href=\"#7-4-Leaky-ReLU函数\" class=\"headerlink\" title=\"7.4 Leaky ReLU函数\"></a>7.4 Leaky ReLU函数</h3><script type=\"math/tex; mode=display\">\nLeaky\\_Relu(x) = max(0.1 * x, x)</script><p><img src=\"https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png\" alt=\"image-20211030005818507\"></p>\n<h1 id=\"8-训练误差和泛化误差\"><a href=\"#8-训练误差和泛化误差\" class=\"headerlink\" title=\"8 训练误差和泛化误差\"></a>8 训练误差和泛化误差</h1><h3 id=\"8-1-基本概念\"><a href=\"#8-1-基本概念\" class=\"headerlink\" title=\"8.1 基本概念\"></a>8.1 基本概念</h3><ul>\n<li><strong>训练误差：</strong>指模型在训练数据集上表现出的误差</li>\n<li><strong>泛化误差：</strong>指模型在任意⼀个测试数据样本上表现出的误差的期望</li>\n</ul>\n<h3 id=\"8-2-k折交叉验证\"><a href=\"#8-2-k折交叉验证\" class=\"headerlink\" title=\"8.2 k折交叉验证\"></a>8.2 k折交叉验证</h3><ul>\n<li>当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法</li>\n<li>即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个子数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。</li>\n</ul>\n<h3 id=\"8-3-欠拟合和过拟合\"><a href=\"#8-3-欠拟合和过拟合\" class=\"headerlink\" title=\"8.3 欠拟合和过拟合\"></a>8.3 欠拟合和过拟合</h3><ul>\n<li><p><strong>欠拟合：</strong>训练误差过高</p>\n</li>\n<li><p><strong>过拟合：</strong>泛化误差过高</p>\n</li>\n<li><p>我们知道进行反向传播的目的是让学习器去拟合数据，而当拟合程度不够时就称为<strong>欠拟合</strong>，拟合程度过于高时则称为<strong>过拟合</strong>，用下图可以很好的解释：</p>\n<p><img src=\"https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png\" alt=\"image-20211111215618245\"  /></p>\n</li>\n<li><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了，<strong>可以使用early stop来防止训练时间不足导致的欠拟合</strong>。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法。而一般我们更加关注的是如何如何防止过拟合</p>\n</li>\n</ul>\n<h3 id=\"8-4-造成过拟合的原因\"><a href=\"#8-4-造成过拟合的原因\" class=\"headerlink\" title=\"8.4 造成过拟合的原因\"></a>8.4 造成过拟合的原因</h3><ul>\n<li><strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型</li>\n<li><strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系</li>\n<li><strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。<blockquote>\n<ul>\n<li>模型太复杂是过拟合的重要因素，下面提到的正则化方法和Dropout方法都是基于减少模型复杂度来进行的。</li>\n<li>另外，在模型设计方面有一个根本设计原则<strong>奥卡姆剃刀法则：优先选择拟合数据的最简单的假设。 简单的模型才是最好的</strong></li>\n<li><strong>当模型复杂度过高的时候，拟合函数的系数往往非常大，需要顾忌每一个点，最终形成的拟合函数波动很大</strong>，如上图过拟合情况。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的梯度非常大，所以才需要惩罚项来降低</li>\n<li><strong>模型复杂度也可以解释为：模型对某些非必要的特征过于看重（即给予了较大的权值）</strong></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"8-5-对应过拟合手段\"><a href=\"#8-5-对应过拟合手段\" class=\"headerlink\" title=\"8.5 对应过拟合手段\"></a>8.5 对应过拟合手段</h3><h4 id=\"8-5-1-增加数据\"><a href=\"#8-5-1-增加数据\" class=\"headerlink\" title=\"8.5.1 增加数据\"></a>8.5.1 增加数据</h4><ul>\n<li>由于<strong>数据量不足是造成过拟合的根本原因</strong>，所以对应过拟合最有效的手段肯定是增大数据集，但是这种方法成本过高</li>\n<li>在数据层面，还可以进行<strong>数据增强</strong>，创造一些假数据，还可以提升模型的泛化能力</li>\n<li>在CV方面常用的数据增强有旋转图像、缩放图像、随机裁剪等，而在NLP方面还有同义词替换、随机删除词、随机颠倒句子顺序等方法</li>\n</ul>\n<h4 id=\"8-5-2-正则化\"><a href=\"#8-5-2-正则化\" class=\"headerlink\" title=\"8.5.2 正则化\"></a>8.5.2 正则化</h4><ul>\n<li>L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积</li>\n<li>损失函数添加一个L2惩罚项：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell(...) + \\frac{\\lambda}{2}\\begin{Vmatrix} W \\end{Vmatrix}^2</script><p>求导后：（每个mini-batch）</p>\n<script type=\"math/tex; mode=display\">\nw1 = (1 - \\eta\\lambda)w1 - \\frac{\\eta}{|\\mathbb{B}|}\\sum_{i \\in \\mathbb{B}}\\frac{\\partial\\ell^{(i)}(...)}{\\partial{w1}}</script><p>其中超参数$\\lambda$ &gt; 0。当权重参数均为0时，惩罚项最小。当$\\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\\lambda$设为0时，惩罚项完全不起作⽤。</p>\n<ul>\n<li>除此之外还有L1正则化，使用L1范数作为惩罚项：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell(...) + \\frac{\\lambda}{2}\\begin{Vmatrix} W \\end{Vmatrix}</script><ul>\n<li>一般训练集中的损失函数要加惩罚项，测试集中不用</li>\n<li>正则化的原理可以从<strong>拉格朗日乘子法和最大后验概率估计</strong>两方面解释</li>\n<li>首先是拉格朗日乘子法：</li>\n</ul>\n<blockquote>\n<ul>\n<li>设模型本来的优化目标为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{w} J(w ; X, y)</script><ul>\n<li>而过拟合的原因在于模型复杂度过高，而一般模型复杂度和参数<script type=\"math/tex\">w</script>的稀疏度相关，即<strong>参数越少或参数中0的个数越多，模型复杂度越小</strong></li>\n<li>那么0的个数可以让我们联想到<strong>w的L0范数（代表w中非零元素的个数）</strong>，那么可以<strong>添加一个约束，使得优化目标变为：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\min _{w} J(w ; X, y) \\\\\ns \\cdot t .\\|w\\|_{0} \\leq C\n\\end{array}</script><ul>\n<li>但是<strong>使用L0范数太过困难，主要是L0范数无法进行求导来反向传播，所以我们退而求其次，要求权重w向量中某些维度的非零参数尽可能接近于0，尽可能的小，所以就可以使用L1、L2范数：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\min _{w} J(w ; X, y) \\\\\ns \\cdot t .\\|w\\|_{1} \\leq C 或 s \\cdot t .\\|w\\|_{2} \\leq C \n\\end{array}</script><ul>\n<li>以L2正则化为例，采用拉格朗日乘子法，其拉格朗日函数为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{w} J(w ; X, y)+\\alpha^{*}\\|w\\|_{2}^{2}</script><ul>\n<li>这就是现在的优化目标，而后面的一项恰好就是惩罚项</li>\n</ul>\n</blockquote>\n<ul>\n<li>还可以用最大后验概率估计来解释：</li>\n</ul>\n<blockquote>\n<ul>\n<li>最大后验估计的优化目标是：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nM A P=\\log P(y \\mid X, w) P(w)=\\log P(y \\mid X, w)+\\log P(w)</script><ul>\n<li>上式左边的一项即最大化似然的优化目标，也就是本来的不加惩罚项的优化目标；<strong>而右边的一项仅关于参数w的先验分布</strong></li>\n<li>我们对w的先验分布进行不同的假设，即可得到不同的惩罚项</li>\n<li><strong>在L2正则化中，假设<script type=\"math/tex\">w \\sim N(0, \\sigma^2)</script>，那么有：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\log P(w)=\\log \\prod_{j} P\\left(w_{j}\\right)= \\\\\n\\log \\prod_{j}\\left[\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(w_{j}\\right)^{2}}{2 \\sigma^{2}}}\\right] \\\\\n=-\\frac{1}{2 \\sigma^{2}} \\sum_{j} w_{j}^{2}+C\n\\end{array}</script><p>那么就得到了L2范数作为惩罚项</p>\n<ul>\n<li><strong>L1正则化也一样，假设<script type=\"math/tex\">w</script>服从均值为0参数为a的拉普拉斯分布，那么有：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\log P(w)=\\log \\prod_{j} P\\left(w_{j}\\right)= \\\\\n\\log \\prod_{j}\\left[\\frac{1}{\\sqrt{2 a} \\sigma} e^{-\\frac{w_{j}}{a}}\\right] \\\\\n=-\\frac{1}{2 a} \\sum_{j}\\left|w_{j}\\right|+C\n\\end{array}</script></blockquote>\n<h4 id=\"8-5-2-丢弃法（Dropout）\"><a href=\"#8-5-2-丢弃法（Dropout）\" class=\"headerlink\" title=\"8.5.2 丢弃法（Dropout）\"></a>8.5.2 丢弃法（Dropout）</h4><ul>\n<li>对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh_i' = \\frac{\\xi_i}{1 - p}h_i</script><p>$\\xi_i$为0和1的概率分别为p和1 - p</p>\n<p>分母中的1- p是为了不改变其输⼊的期望值：</p>\n<script type=\"math/tex; mode=display\">\n由于E(\\xi_i) = 1 - p \\\\\n所以E(h_i') = \\frac{E(\\xi_i)}{1 - p}h_i = h_i</script><ul>\n<li><p>测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法</p>\n</li>\n<li><p>进行了Dropout的多层感知机：</p>\n</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png\" alt=\"image-20211030141251677\"></p>\n<p>可以看到隐藏层中的$h_2$和$h_5$消失了</p>\n<ul>\n<li><p>每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点</p>\n</li>\n<li><p><strong>Dropout的原理：</strong></p>\n</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>集成学习：</strong>每次随机置0都可以得到一个新模型，而Dropout可以当作对这些所有新得到的模型的集成</li>\n<li><strong>正则化：</strong>因为Dropout导致两个神经元不一定每次都在一个dropout网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 ，迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。正因为这样，网络由于不知道浅层的哪些神经元会失活，导致网络不敢赋予浅层神经元太大的权重，这样就减轻了网络对某些局部特征的依赖，换句话说网络不会对一些特定的线索片段太过敏感，即使丢失特定的线索，它也可以从众多其它线索中学习一些共同的特征。从这个角度看Dropout就类似于L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</li>\n</ol>\n</blockquote>\n<h1 id=\"9-正向传播和反向传播\"><a href=\"#9-正向传播和反向传播\" class=\"headerlink\" title=\"9 正向传播和反向传播\"></a>9 正向传播和反向传播</h1><h3 id=\"9-1-正向传播\"><a href=\"#9-1-正向传播\" class=\"headerlink\" title=\"9.1 正向传播\"></a>9.1 正向传播</h3><ul>\n<li>正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）</li>\n<li>中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等</li>\n<li>正向传播的计算图：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png\" alt=\"image-20211030142019713\"></p>\n<p>其中：</p>\n<ol>\n<li>左下⻆是输⼊，右上⻆是输出</li>\n<li>⽅框代表变量，圆圈代表运算符</li>\n<li>$J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项</li>\n</ol>\n<h3 id=\"9-2-反向传播\"><a href=\"#9-2-反向传播\" class=\"headerlink\" title=\"9.2 反向传播\"></a>9.2 反向传播</h3><ul>\n<li>反向传播最重要的是通过链式法则求导，从后往前进行计算</li>\n<li>反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的</li>\n</ul>\n<h3 id=\"9-3-衰减和爆炸\"><a href=\"#9-3-衰减和爆炸\" class=\"headerlink\" title=\"9.3 衰减和爆炸\"></a>9.3 衰减和爆炸</h3><ul>\n<li>如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：</li>\n</ul>\n<p>假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积</p>\n<h3 id=\"9-4-随机初始化参数\"><a href=\"#9-4-随机初始化参数\" class=\"headerlink\" title=\"9.4 随机初始化参数\"></a>9.4 随机初始化参数</h3><ul>\n<li>考虑三种参数的极端情况：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>参数很稀疏，那么输入的特征会在经过前向计算后逐渐消失（计算出来都接近0）</strong></li>\n<li><strong>参数很大，输出值容易进入激活函数的饱和区，比如Sigmoid函数</strong></li>\n<li>假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。<strong>在这种情况下，无论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作用</strong></li>\n</ol>\n</blockquote>\n<ul>\n<li>可以看出，我们<strong>希望每一层的输出方差能够固定</strong>，因为这样能够防止我们的信号变得很大/直接消失。换句话来说，<strong>我们需要一种权重初始化的手段，使得输入、输出的方差保持不变，</strong>这就是Xavier初始化做的事。</li>\n</ul>\n<h3 id=\"9-5-Xavier随机初始化\"><a href=\"#9-5-Xavier随机初始化\" class=\"headerlink\" title=\"9.5 Xavier随机初始化\"></a>9.5 Xavier随机初始化</h3><ul>\n<li>假设某层的参数，前一层神经元个数为a，后一层神经元个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nU(-\\sqrt{\\frac{6}{a + b}}, \\sqrt{\\frac{6}{a + b}})</script><ul>\n<li><p>该设计主要为了<strong>在前向计算的时候，经过此层的参数后得到的输出方差不变（即输入输出方差相等）；并且在反向计算的时候，后一层的梯度和前一层的梯度方差也相等</strong></p>\n</li>\n<li><p><strong>Xavier初始化在sigmoid和tanh激活函数上有很好的表现，但是在Relu激活函数上表现很差。</strong></p>\n</li>\n<li><p>现在开始推导，这里只展示前向计算时，首先是有3个假设：</p>\n</li>\n</ul>\n<blockquote>\n<ul>\n<li>激活函数为<script type=\"math/tex\">f(x) = x</script></li>\n<li>偏置项b初始化为0</li>\n<li>输入和参数的期望都为0</li>\n</ul>\n</blockquote>\n<ul>\n<li>设某一层值为<script type=\"math/tex\">x</script>，参数为<script type=\"math/tex\">w</script>，对应输入为<script type=\"math/tex\">y = x^Tw</script>（注意<script type=\"math/tex\">x,w</script>为向量，<script type=\"math/tex\">y</script>为标量），那么我们的目的是：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nVar[y] = Var[x] \\\\\nVar[x^Tw] = Var[x]</script><ul>\n<li>引入公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nVar[XY] = E[X^2]E[Y^2] - E^2[X]E^2[Y] \\\\\n在E[X] = E[Y]时，可以得出： Var[XY] = Var[X]Var[Y]</script><ul>\n<li>而由于<script type=\"math/tex\">E[w] = E[x] = 0</script>，所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nVar[x] = Var[x^Tw] = Var[\\sum^{N}_ix_iw_i] = N* Var[x]Var[w]</script><p>其中<script type=\"math/tex\">N = dim(x) = dim(w)</script>，即当前层的神经元个数</p>\n<ul>\n<li>由上式可以得出：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nN * Var[w] = 1 \\\\\nVar[w] = \\frac{1}{N}</script><ul>\n<li>而在反向传播的时候，我们同样可以推出<script type=\"math/tex\">Var[w] = \\frac{1}{N'}</script>，其中<script type=\"math/tex\">N'</script>为下一层神经元个数</li>\n<li>而由于方差取值无法同时满足前后向，所以采用两者的平均数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nVar[w] = \\frac{2}{N + N'}</script><ul>\n<li>我们可以通过<script type=\"math/tex\">N(0, \\frac{2}{N+N'})</script>对w取样，也可以使用均匀分布采样，采用均匀函数的时候需要进行一定的放缩（Var[w]的分子不一定为2）</li>\n</ul>\n","site":{"data":{}},"wordcount":6151,"excerpt":"","more":"<h1 id=\"1-行业知识体系\"><a href=\"#1-行业知识体系\" class=\"headerlink\" title=\"1 行业知识体系\"></a>1 行业知识体系</h1><h3 id=\"1-1-机器学习算法\"><a href=\"#1-1-机器学习算法\" class=\"headerlink\" title=\"1.1 机器学习算法\"></a>1.1 机器学习算法</h3><p><img src=\"https://i.loli.net/2021/10/31/romyfLZ5KjP9a3B.jpg\" alt=\"qq_pic_merged_1635482805011\"></p>\n<h3 id=\"1-2-机器学习分类\"><a href=\"#1-2-机器学习分类\" class=\"headerlink\" title=\"1.2 机器学习分类\"></a>1.2 机器学习分类</h3><p><img src=\"https://i.loli.net/2021/10/31/7Lvd8NIsezYm1Qy.jpg\" alt=\"qq_pic_merged_1635482964705\"></p>\n<h1 id=\"2-线性回归\"><a href=\"#2-线性回归\" class=\"headerlink\" title=\"2 线性回归\"></a>2 线性回归</h1><h3 id=\"2-1-基本概念\"><a href=\"#2-1-基本概念\" class=\"headerlink\" title=\"2.1 基本概念\"></a>2.1 基本概念</h3><p><img src=\"https://i.loli.net/2021/10/31/ChVyXvJetFWN5Y8.png\" alt=\"image-20211029131504886\"></p>\n<ul>\n<li><p>线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</p>\n</li>\n<li><p>线性回归输出是⼀个连续值，因此适⽤于回归问题</p>\n</li>\n</ul>\n<h3 id=\"2-2-损失函数\"><a href=\"#2-2-损失函数\" class=\"headerlink\" title=\"2.2 损失函数\"></a>2.2 损失函数</h3><ul>\n<li><strong>均方差（mse）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell^{(i)}(W, b) = \\frac{(\\hat{y}^{(i)} - y^{(i)})^2}{2}</script><p>其中$\\hat{y}$为计算出的预测值，$y$为真实值（label）</p>\n<h1 id=\"3-全连接层（稠密层）\"><a href=\"#3-全连接层（稠密层）\" class=\"headerlink\" title=\"3  全连接层（稠密层）\"></a>3  全连接层（稠密层）</h1><ul>\n<li>输出层中的神经元和输⼊ 层中各个输⼊完全连接</li>\n<li>计算完全依赖于输入层。</li>\n</ul>\n<h1 id=\"4-小批量随机梯度下降\"><a href=\"#4-小批量随机梯度下降\" class=\"headerlink\" title=\"4  小批量随机梯度下降\"></a>4  小批量随机梯度下降</h1><ul>\n<li><p>在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\\mathbb{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积（学习率$\\eta$）作为模型参数在本次迭代的减小量。</p>\n</li>\n<li><p>每次mini-batch梯度下降计算过程：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nw_1 = w_1 - \\frac{\\eta}{|\\mathbb{B}|}\\sum_{i \\in \\mathbb{B}}\\frac{\\partial{\\ell(...)}}{\\partial{w_1}}</script><ul>\n<li>注意学习率要取正数</li>\n</ul>\n<h1 id=\"5-Softmax分类\"><a href=\"#5-Softmax分类\" class=\"headerlink\" title=\"5  Softmax分类\"></a>5  Softmax分类</h1><h3 id=\"5-1-基本概念\"><a href=\"#5-1-基本概念\" class=\"headerlink\" title=\"5.1 基本概念\"></a>5.1 基本概念</h3><ul>\n<li>softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于， softmax回归的输出值个数等于标签⾥的类别数</li>\n<li>由于输出元有多个，所以我们要讲输出做softmax计算，得到的结果作为该类的概率，具体计算如下：</li>\n</ul>\n<p>假设输出元有3个，输出值分别为$o_1, o_2, o_3$， 则:</p>\n<script type=\"math/tex; mode=display\">\n\\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = softmax(o_1, o_2, o_3)</script><p>其中</p>\n<script type=\"math/tex; mode=display\">\n\\hat{y}_i = \\frac{e^{o_i}}{\\sum_{j = 1}^{3}e^{o_j}}</script><ul>\n<li>容易看出$\\sum_{i = 1}^{3}y_i = 1$（即所有概率加起来等于1）</li>\n<li>通常，我们把预测概率最⼤的类别作为输出类别</li>\n</ul>\n<h3 id=\"5-2-损失函数\"><a href=\"#5-2-损失函数\" class=\"headerlink\" title=\"5.2 损失函数\"></a>5.2 损失函数</h3><ul>\n<li>这里我们运用交叉熵损失函数（cross entropy）：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(y^{(i)}, \\hat{y}^{(i)}) = -\\sum_{j = 1}^qy_j^{(i)}\\log{\\hat{y}^{(i)}}</script><p>q为输出元个数$\\times$样本个数，即所有样本的所有输出元都要参与运算，最后求平均值</p>\n<h1 id=\"6-多层感知机（MLP）\"><a href=\"#6-多层感知机（MLP）\" class=\"headerlink\" title=\"6 多层感知机（MLP）\"></a>6 多层感知机（MLP）</h1><ul>\n<li>多层感知机有一到多个隐藏层</li>\n<li>多层感知机中的隐藏层和输出层都是全连接层</li>\n<li>多层感知机具有激活函数，下面介绍常用激活函数</li>\n</ul>\n<h1 id=\"7-激活函数\"><a href=\"#7-激活函数\" class=\"headerlink\" title=\"7 激活函数\"></a>7 激活函数</h1><h3 id=\"7-1-Sigmoid函数\"><a href=\"#7-1-Sigmoid函数\" class=\"headerlink\" title=\"7.1 Sigmoid函数\"></a>7.1 Sigmoid函数</h3><script type=\"math/tex; mode=display\">\nsigmoid(x) = \\frac{1}{1 + e^{-x}}</script><script type=\"math/tex; mode=display\">\nsigmoid'(x) = sigmoid(x)(1 - sigmoid(x))</script><p><img src=\"https://i.loli.net/2021/10/31/2JsDXo9mQtI6rzM.png\" alt=\"image-20211030004640247\"></p>\n<ul>\n<li>一般用在二分类的输出层中</li>\n</ul>\n<h3 id=\"7-2-tanh函数\"><a href=\"#7-2-tanh函数\" class=\"headerlink\" title=\"7.2 tanh函数\"></a>7.2 tanh函数</h3><script type=\"math/tex; mode=display\">\ntanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}</script><script type=\"math/tex; mode=display\">\ntanh'(x) = 1 - tanh^2(x)</script><p><img src=\"https://i.loli.net/2021/10/31/rdx3sEtVFGlHink.png\" alt=\"image-20211030005138300\"></p>\n<ul>\n<li>tanh和sigmoid比较相似，但是性能一般要优于sigmoid</li>\n</ul>\n<h3 id=\"7-3-ReLU函数\"><a href=\"#7-3-ReLU函数\" class=\"headerlink\" title=\"7.3 ReLU函数\"></a>7.3 ReLU函数</h3><script type=\"math/tex; mode=display\">\nReLU(x) = max(0, x)</script><p><img src=\"https://i.loli.net/2021/10/31/6Ioqb3OBldsCuS8.png\" alt=\"image-20211030005121490\"></p>\n<ul>\n<li>十分常用</li>\n</ul>\n<h3 id=\"7-4-Leaky-ReLU函数\"><a href=\"#7-4-Leaky-ReLU函数\" class=\"headerlink\" title=\"7.4 Leaky ReLU函数\"></a>7.4 Leaky ReLU函数</h3><script type=\"math/tex; mode=display\">\nLeaky\\_Relu(x) = max(0.1 * x, x)</script><p><img src=\"https://i.loli.net/2021/10/31/qA4w6a7Irj3WEJg.png\" alt=\"image-20211030005818507\"></p>\n<h1 id=\"8-训练误差和泛化误差\"><a href=\"#8-训练误差和泛化误差\" class=\"headerlink\" title=\"8 训练误差和泛化误差\"></a>8 训练误差和泛化误差</h1><h3 id=\"8-1-基本概念\"><a href=\"#8-1-基本概念\" class=\"headerlink\" title=\"8.1 基本概念\"></a>8.1 基本概念</h3><ul>\n<li><strong>训练误差：</strong>指模型在训练数据集上表现出的误差</li>\n<li><strong>泛化误差：</strong>指模型在任意⼀个测试数据样本上表现出的误差的期望</li>\n</ul>\n<h3 id=\"8-2-k折交叉验证\"><a href=\"#8-2-k折交叉验证\" class=\"headerlink\" title=\"8.2 k折交叉验证\"></a>8.2 k折交叉验证</h3><ul>\n<li>当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈，这时候就可以运用k折交叉验证法</li>\n<li>即我们把原始训练数据 集分割成k个不重合的⼦数据集，然后我们做k次模型训练和验证。每⼀次，我们使⽤⼀个子数据集验证模型，并使⽤其他k − 1个⼦数据集来训练模型，最后，我们对这k次训练误差和验证误差分别求平均。</li>\n</ul>\n<h3 id=\"8-3-欠拟合和过拟合\"><a href=\"#8-3-欠拟合和过拟合\" class=\"headerlink\" title=\"8.3 欠拟合和过拟合\"></a>8.3 欠拟合和过拟合</h3><ul>\n<li><p><strong>欠拟合：</strong>训练误差过高</p>\n</li>\n<li><p><strong>过拟合：</strong>泛化误差过高</p>\n</li>\n<li><p>我们知道进行反向传播的目的是让学习器去拟合数据，而当拟合程度不够时就称为<strong>欠拟合</strong>，拟合程度过于高时则称为<strong>过拟合</strong>，用下图可以很好的解释：</p>\n<p><img src=\"https://i.loli.net/2021/11/11/npR3aNjrLOISQC8.png\" alt=\"image-20211111215618245\"  /></p>\n</li>\n<li><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了，<strong>可以使用early stop来防止训练时间不足导致的欠拟合</strong>。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法。而一般我们更加关注的是如何如何防止过拟合</p>\n</li>\n</ul>\n<h3 id=\"8-4-造成过拟合的原因\"><a href=\"#8-4-造成过拟合的原因\" class=\"headerlink\" title=\"8.4 造成过拟合的原因\"></a>8.4 造成过拟合的原因</h3><ul>\n<li><strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型</li>\n<li><strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系</li>\n<li><strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。<blockquote>\n<ul>\n<li>模型太复杂是过拟合的重要因素，下面提到的正则化方法和Dropout方法都是基于减少模型复杂度来进行的。</li>\n<li>另外，在模型设计方面有一个根本设计原则<strong>奥卡姆剃刀法则：优先选择拟合数据的最简单的假设。 简单的模型才是最好的</strong></li>\n<li><strong>当模型复杂度过高的时候，拟合函数的系数往往非常大，需要顾忌每一个点，最终形成的拟合函数波动很大</strong>，如上图过拟合情况。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的梯度非常大，所以才需要惩罚项来降低</li>\n<li><strong>模型复杂度也可以解释为：模型对某些非必要的特征过于看重（即给予了较大的权值）</strong></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"8-5-对应过拟合手段\"><a href=\"#8-5-对应过拟合手段\" class=\"headerlink\" title=\"8.5 对应过拟合手段\"></a>8.5 对应过拟合手段</h3><h4 id=\"8-5-1-增加数据\"><a href=\"#8-5-1-增加数据\" class=\"headerlink\" title=\"8.5.1 增加数据\"></a>8.5.1 增加数据</h4><ul>\n<li>由于<strong>数据量不足是造成过拟合的根本原因</strong>，所以对应过拟合最有效的手段肯定是增大数据集，但是这种方法成本过高</li>\n<li>在数据层面，还可以进行<strong>数据增强</strong>，创造一些假数据，还可以提升模型的泛化能力</li>\n<li>在CV方面常用的数据增强有旋转图像、缩放图像、随机裁剪等，而在NLP方面还有同义词替换、随机删除词、随机颠倒句子顺序等方法</li>\n</ul>\n<h4 id=\"8-5-2-正则化\"><a href=\"#8-5-2-正则化\" class=\"headerlink\" title=\"8.5.2 正则化\"></a>8.5.2 正则化</h4><ul>\n<li>L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积</li>\n<li>损失函数添加一个L2惩罚项：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell(...) + \\frac{\\lambda}{2}\\begin{Vmatrix} W \\end{Vmatrix}^2</script><p>求导后：（每个mini-batch）</p>\n<script type=\"math/tex; mode=display\">\nw1 = (1 - \\eta\\lambda)w1 - \\frac{\\eta}{|\\mathbb{B}|}\\sum_{i \\in \\mathbb{B}}\\frac{\\partial\\ell^{(i)}(...)}{\\partial{w1}}</script><p>其中超参数$\\lambda$ &gt; 0。当权重参数均为0时，惩罚项最小。当$\\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\\lambda$设为0时，惩罚项完全不起作⽤。</p>\n<ul>\n<li>除此之外还有L1正则化，使用L1范数作为惩罚项：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\ell(...) + \\frac{\\lambda}{2}\\begin{Vmatrix} W \\end{Vmatrix}</script><ul>\n<li>一般训练集中的损失函数要加惩罚项，测试集中不用</li>\n<li>正则化的原理可以从<strong>拉格朗日乘子法和最大后验概率估计</strong>两方面解释</li>\n<li>首先是拉格朗日乘子法：</li>\n</ul>\n<blockquote>\n<ul>\n<li>设模型本来的优化目标为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{w} J(w ; X, y)</script><ul>\n<li>而过拟合的原因在于模型复杂度过高，而一般模型复杂度和参数<script type=\"math/tex\">w</script>的稀疏度相关，即<strong>参数越少或参数中0的个数越多，模型复杂度越小</strong></li>\n<li>那么0的个数可以让我们联想到<strong>w的L0范数（代表w中非零元素的个数）</strong>，那么可以<strong>添加一个约束，使得优化目标变为：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\min _{w} J(w ; X, y) \\\\\ns \\cdot t .\\|w\\|_{0} \\leq C\n\\end{array}</script><ul>\n<li>但是<strong>使用L0范数太过困难，主要是L0范数无法进行求导来反向传播，所以我们退而求其次，要求权重w向量中某些维度的非零参数尽可能接近于0，尽可能的小，所以就可以使用L1、L2范数：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{c}\n\\min _{w} J(w ; X, y) \\\\\ns \\cdot t .\\|w\\|_{1} \\leq C 或 s \\cdot t .\\|w\\|_{2} \\leq C \n\\end{array}</script><ul>\n<li>以L2正则化为例，采用拉格朗日乘子法，其拉格朗日函数为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\min _{w} J(w ; X, y)+\\alpha^{*}\\|w\\|_{2}^{2}</script><ul>\n<li>这就是现在的优化目标，而后面的一项恰好就是惩罚项</li>\n</ul>\n</blockquote>\n<ul>\n<li>还可以用最大后验概率估计来解释：</li>\n</ul>\n<blockquote>\n<ul>\n<li>最大后验估计的优化目标是：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nM A P=\\log P(y \\mid X, w) P(w)=\\log P(y \\mid X, w)+\\log P(w)</script><ul>\n<li>上式左边的一项即最大化似然的优化目标，也就是本来的不加惩罚项的优化目标；<strong>而右边的一项仅关于参数w的先验分布</strong></li>\n<li>我们对w的先验分布进行不同的假设，即可得到不同的惩罚项</li>\n<li><strong>在L2正则化中，假设<script type=\"math/tex\">w \\sim N(0, \\sigma^2)</script>，那么有：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\log P(w)=\\log \\prod_{j} P\\left(w_{j}\\right)= \\\\\n\\log \\prod_{j}\\left[\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(w_{j}\\right)^{2}}{2 \\sigma^{2}}}\\right] \\\\\n=-\\frac{1}{2 \\sigma^{2}} \\sum_{j} w_{j}^{2}+C\n\\end{array}</script><p>那么就得到了L2范数作为惩罚项</p>\n<ul>\n<li><strong>L1正则化也一样，假设<script type=\"math/tex\">w</script>服从均值为0参数为a的拉普拉斯分布，那么有：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\log P(w)=\\log \\prod_{j} P\\left(w_{j}\\right)= \\\\\n\\log \\prod_{j}\\left[\\frac{1}{\\sqrt{2 a} \\sigma} e^{-\\frac{w_{j}}{a}}\\right] \\\\\n=-\\frac{1}{2 a} \\sum_{j}\\left|w_{j}\\right|+C\n\\end{array}</script></blockquote>\n<h4 id=\"8-5-2-丢弃法（Dropout）\"><a href=\"#8-5-2-丢弃法（Dropout）\" class=\"headerlink\" title=\"8.5.2 丢弃法（Dropout）\"></a>8.5.2 丢弃法（Dropout）</h4><ul>\n<li>对于每一层的神经元，有一定的概率被丢弃掉，设丢弃概率为p，计算新的单元：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh_i' = \\frac{\\xi_i}{1 - p}h_i</script><p>$\\xi_i$为0和1的概率分别为p和1 - p</p>\n<p>分母中的1- p是为了不改变其输⼊的期望值：</p>\n<script type=\"math/tex; mode=display\">\n由于E(\\xi_i) = 1 - p \\\\\n所以E(h_i') = \\frac{E(\\xi_i)}{1 - p}h_i = h_i</script><ul>\n<li><p>测试模型时，我们为了得到更加确定性的结果，⼀般不使⽤丢弃法</p>\n</li>\n<li><p>进行了Dropout的多层感知机：</p>\n</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/10/31/CnUgmA4dVb8vFR2.png\" alt=\"image-20211030141251677\"></p>\n<p>可以看到隐藏层中的$h_2$和$h_5$消失了</p>\n<ul>\n<li><p>每一层的丢弃概率可以不同，通常把靠近输⼊层的丢弃概率设得小⼀点</p>\n</li>\n<li><p><strong>Dropout的原理：</strong></p>\n</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>集成学习：</strong>每次随机置0都可以得到一个新模型，而Dropout可以当作对这些所有新得到的模型的集成</li>\n<li><strong>正则化：</strong>因为Dropout导致两个神经元不一定每次都在一个dropout网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 ，迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。正因为这样，网络由于不知道浅层的哪些神经元会失活，导致网络不敢赋予浅层神经元太大的权重，这样就减轻了网络对某些局部特征的依赖，换句话说网络不会对一些特定的线索片段太过敏感，即使丢失特定的线索，它也可以从众多其它线索中学习一些共同的特征。从这个角度看Dropout就类似于L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</li>\n</ol>\n</blockquote>\n<h1 id=\"9-正向传播和反向传播\"><a href=\"#9-正向传播和反向传播\" class=\"headerlink\" title=\"9 正向传播和反向传播\"></a>9 正向传播和反向传播</h1><h3 id=\"9-1-正向传播\"><a href=\"#9-1-正向传播\" class=\"headerlink\" title=\"9.1 正向传播\"></a>9.1 正向传播</h3><ul>\n<li>正向传播（forward propagation）是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）</li>\n<li>中间变量称为缓存，在反向传播中会用到，避免重复计算，如每层的输出值等</li>\n<li>正向传播的计算图：</li>\n</ul>\n<p><img src=\"https://i.loli.net/2021/10/31/4LnFKkrSQHsUYNv.png\" alt=\"image-20211030142019713\"></p>\n<p>其中：</p>\n<ol>\n<li>左下⻆是输⼊，右上⻆是输出</li>\n<li>⽅框代表变量，圆圈代表运算符</li>\n<li>$J = L + s$，$J$称为目标函数，$L$为损失函数，$s$为惩罚项</li>\n</ol>\n<h3 id=\"9-2-反向传播\"><a href=\"#9-2-反向传播\" class=\"headerlink\" title=\"9.2 反向传播\"></a>9.2 反向传播</h3><ul>\n<li>反向传播最重要的是通过链式法则求导，从后往前进行计算</li>\n<li>反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的</li>\n</ul>\n<h3 id=\"9-3-衰减和爆炸\"><a href=\"#9-3-衰减和爆炸\" class=\"headerlink\" title=\"9.3 衰减和爆炸\"></a>9.3 衰减和爆炸</h3><ul>\n<li>如果隐藏层的层数很大，$H(l)$的计算可能会出现衰减或爆炸，如：</li>\n</ul>\n<p>假设输⼊和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知 机的第30层输出为输⼊X分别与0.2 30 ≈ 1 × 10−21（衰减）和5 30 ≈ 9 × 1020（爆炸）的乘积</p>\n<h3 id=\"9-4-随机初始化参数\"><a href=\"#9-4-随机初始化参数\" class=\"headerlink\" title=\"9.4 随机初始化参数\"></a>9.4 随机初始化参数</h3><ul>\n<li>考虑三种参数的极端情况：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>参数很稀疏，那么输入的特征会在经过前向计算后逐渐消失（计算出来都接近0）</strong></li>\n<li><strong>参数很大，输出值容易进入激活函数的饱和区，比如Sigmoid函数</strong></li>\n<li>假设将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值， 并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使⽤基于梯度的优化算法迭代后值依然相等。<strong>在这种情况下，无论隐藏单元有多少， 隐藏层本质上只有1个隐藏单元在发挥作用</strong></li>\n</ol>\n</blockquote>\n<ul>\n<li>可以看出，我们<strong>希望每一层的输出方差能够固定</strong>，因为这样能够防止我们的信号变得很大/直接消失。换句话来说，<strong>我们需要一种权重初始化的手段，使得输入、输出的方差保持不变，</strong>这就是Xavier初始化做的事。</li>\n</ul>\n<h3 id=\"9-5-Xavier随机初始化\"><a href=\"#9-5-Xavier随机初始化\" class=\"headerlink\" title=\"9.5 Xavier随机初始化\"></a>9.5 Xavier随机初始化</h3><ul>\n<li>假设某层的参数，前一层神经元个数为a，后一层神经元个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nU(-\\sqrt{\\frac{6}{a + b}}, \\sqrt{\\frac{6}{a + b}})</script><ul>\n<li><p>该设计主要为了<strong>在前向计算的时候，经过此层的参数后得到的输出方差不变（即输入输出方差相等）；并且在反向计算的时候，后一层的梯度和前一层的梯度方差也相等</strong></p>\n</li>\n<li><p><strong>Xavier初始化在sigmoid和tanh激活函数上有很好的表现，但是在Relu激活函数上表现很差。</strong></p>\n</li>\n<li><p>现在开始推导，这里只展示前向计算时，首先是有3个假设：</p>\n</li>\n</ul>\n<blockquote>\n<ul>\n<li>激活函数为<script type=\"math/tex\">f(x) = x</script></li>\n<li>偏置项b初始化为0</li>\n<li>输入和参数的期望都为0</li>\n</ul>\n</blockquote>\n<ul>\n<li>设某一层值为<script type=\"math/tex\">x</script>，参数为<script type=\"math/tex\">w</script>，对应输入为<script type=\"math/tex\">y = x^Tw</script>（注意<script type=\"math/tex\">x,w</script>为向量，<script type=\"math/tex\">y</script>为标量），那么我们的目的是：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nVar[y] = Var[x] \\\\\nVar[x^Tw] = Var[x]</script><ul>\n<li>引入公式：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nVar[XY] = E[X^2]E[Y^2] - E^2[X]E^2[Y] \\\\\n在E[X] = E[Y]时，可以得出： Var[XY] = Var[X]Var[Y]</script><ul>\n<li>而由于<script type=\"math/tex\">E[w] = E[x] = 0</script>，所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nVar[x] = Var[x^Tw] = Var[\\sum^{N}_ix_iw_i] = N* Var[x]Var[w]</script><p>其中<script type=\"math/tex\">N = dim(x) = dim(w)</script>，即当前层的神经元个数</p>\n<ul>\n<li>由上式可以得出：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nN * Var[w] = 1 \\\\\nVar[w] = \\frac{1}{N}</script><ul>\n<li>而在反向传播的时候，我们同样可以推出<script type=\"math/tex\">Var[w] = \\frac{1}{N'}</script>，其中<script type=\"math/tex\">N'</script>为下一层神经元个数</li>\n<li>而由于方差取值无法同时满足前后向，所以采用两者的平均数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nVar[w] = \\frac{2}{N + N'}</script><ul>\n<li>我们可以通过<script type=\"math/tex\">N(0, \\frac{2}{N+N'})</script>对w取样，也可以使用均匀分布采样，采用均匀函数的时候需要进行一定的放缩（Var[w]的分子不一定为2）</li>\n</ul>\n"},{"title":"知识图谱-基本概念","math":true,"date":"2022-12-29T16:00:00.000Z","_content":"\n\n\n# 1 知识图谱的构成\n\n- 作为一种只是表示形式，知识图谱是一种大规模语义网络，包含实体（Entity）、概念（Concept）及其之间的各种语义关系，如下图：\n\n![image-20230108190012895](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190012895.png)\n\n- 而语义网络是以图形化的形式通过点和边表达知识的方式，其中点包括：\n\n> 1. **实体：**实体是属性赖以存在的基础，并且必须是自在的，即独立的、不依附于其他东西而存在的。比如身高，仅仅说身高是没有意义的，说“哲学家”这个类别的身高也是没有意义的，而必须说某个具体的哲学家的身高，这才是有明确所指且有意义的。\n> 2. **概念：**是指一类实体，比如“哲学家”，不是指某个特定的哲学家，而是指一类人\n> 3. **值：**每个实体都有一定的属性值，一般是数值、日期或文本\n>\n> ![image-20230108190222968](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190222968.png)\n\n- 知识图谱中的边可以分为**属性（Property）**和**关系（Relation）**两类。属性描述实体某方面的特性，而关系则可以认为是一类特殊的属性，当实体的某个属性值也是一个实体时，这个属性就是关系\n\n- 知识图谱和传统语义网络很像，但是两者的根本区别在于**前者是大规模自动化知识获取，而后者过于依赖于专家知识，导致规模等受限**\n\n\n\n\n\n\n# 2 知识图谱中的分类\n\n- 可以根据所包含的不同知识对知识图谱进行分类：\n\n> 1. **事实知识（Factual Knowledge）：**关于某个特定实体的基本事实。如（柏拉图，出生地，雅典）\n> 2. **概念知识（Taxonomy Knowledge）：**概念知识分为两类，一类是实体与概念之间的类属关系（isA关系），如（柏拉图，isA，哲学家），另一类是子概念与父概念之间的子类关系（subclassOf ），如（唯心主义哲学家 subclassOf 哲学家）\n> 3. **词汇知识（Lexical Knowledge）：**主要包括实体与词汇之间的关系（比如，实体的命名、称谓、英文名等）以及词汇之间的关系(包括同义关系、反义关系、缩略词关系、上下位词关系等)。例如﹐(“Plato”，中文名，柏拉图)、(赵匡胤，谥号，宋太祖)、(妻子，同义，老婆）\n> 4. **常识知识（Commonsense Knowledge）：**是人类通过身体与世界交互而积累的经验与知识，是人们在交流时无需言明就能理解的知识。如我们都知道鸟有翅膀，能飞，但是这种信息很少出现在文本里，所以常识知识的提取是十分困难的\n\n- 知识图谱还可以通过其他方式来分类，总结一下：\n\n![image-20230108194127845](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194127845.png)\n\n\n\n\n\n# 3 知识图谱的数值表示\n\n- 一个三元组包括：主题（Subject）、谓词（Predicate）以及客体（Object）。而一个知识图谱可以视作许多个三元组的集合，如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194532703.png\" alt=\"image-20230108194532703\" style=\"zoom:80%;\" />\n\n- 知识图谱的表示学习旨在将知识图谱中的元素（包括实体、属性、概念等）表示为低维稠密实值向量。而其关键是合理定义知识图谱中**关于事实（即三元组$$<h, r, t>$$）的损失函数$$f_r(h,t)$$，当事实$$<h, r, t>$$成立时，我们期望$$f_r(h,t)$$越小。考虑整个知识图谱的事实，就可以通过最小化$$\\sum_{\\langle h, r, t\\rangle \\in O} f_{r}(\\boldsymbol{h}, \\boldsymbol{t})$$来学习向量化表示**\n\n- 那么现在问题就变成了如何定义$$f_r(h,t)$$，一般思路有基于距离和基于翻译两种\n\n\n\n### 3.1 SE模型\n\n- 基于距离的代表模型，基本思想是当两个实体属于同一个三元组$$<h,r,t >$$时，它们的向量表示在投影后的空间中也应该彼此靠近\n- 因此定义损失函数为向量投影后的距离：\n\n$$\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\boldsymbol{W}_{r, 1} \\boldsymbol{h}-\\boldsymbol{W}_{r, 2} \\boldsymbol{t}\\right\\|_{l_{1}}\n$$\n\n其中的两个矩阵$$W_{r, 1}, W_{r, 2}$$分别用于头实体向量$$h$$和尾实体向量$$t$$的投影操作，**但SE难以捕捉实体和关系之间的语义相关性**\n\n\n\n### 3.2 TransE模型\n\n- 除了基于距离，还有基于翻译的模型，如TransE认为在知识库中，三元组$$<h,r,t >$$可以堪称头实体h到尾实体t利用关系r所进行的翻译。\n- 比如，对于三元组<柏拉图，老师，苏格拉底>来说，头实体“柏拉图”的向量加上关系“老师”的向量，应该尽可能和尾实体“苏格拉底”的向量接近，**也就是$$h+r \\approx t$$**\n- 基于这一思想可以得到损失函数：\n\n$$\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\|\\boldsymbol{h}+\\boldsymbol{r}-\\boldsymbol{t}\\|_{l_{1} / l_{2}}\n$$\n\n- 在实际应用中，**为了使习得的表示更有区分度，使用Hinge Loss目标函数，使得正负例尽可能分开：**\n\n$$\nL=\\sum_{(h, r, t) \\in S} \\sum_{\\left(h^{\\prime}, r, t^{\\prime}\\right) \\in S^{\\prime}}\\left[\\gamma+f_{r}(\\boldsymbol{h}, \\boldsymbol{t})-f_{r}\\left(\\boldsymbol{h}^{\\prime}, \\boldsymbol{t}^{\\prime}\\right)\\right]_{+}\n$$\n\n其中，$$\\gamma$$是间隔参数，$$S$$是正例集合（知识库中已有的三元组)，$$S'$$是负例集合（知识库中不存在的三元组，通常通过对关系r的三元组头尾实体进行随机替换来构造)。$$[x]_+ = \\max(0, x)$$\n\n\n\n### 3.3 TransH模型\n\n- **TransE模型中的$$h+r \\approx t$$假设太强**，导致在自反一对多、多对一等关系下实体向量学习的错误。\n- 比如，对于自反关系r，$$<h,r,t >$$和$$<t,r,h >$$同时成立，导致h =t。对于多对一关系。又比如<柏拉图，性别，男>、<特朗普，性别，男>两个三元组有着相同的关系和尾实体，导致柏拉图和特朗普向量接近。但是柏拉图与特朗普除了在性别上相同，在其他方面显然完全不同。\n- 为解决上述问题，TransH放宽了假设，**只要求头尾实体在关系r相对应的超平面上的投影彼此接近即可**。设r所对应的超平面的法向量为$$W_r$$，那么$$h, t$$映射到超平面上为：\n\n$$\n\\begin{aligned}\n\\boldsymbol{h}_{\\perp} & =\\boldsymbol{h}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{h} \\boldsymbol{W}_{r} \\\\\n\\boldsymbol{t}_{\\perp} & =\\boldsymbol{t}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{t} \\boldsymbol{W}_{r}\n\\end{aligned}\n$$\n\n- 那么损失函数为：\n\n$$\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\left(\\boldsymbol{h}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{h} \\boldsymbol{W}_{r}\\right)+r-\\left(\\boldsymbol{t}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{t} \\boldsymbol{W}_{r}\\right)\\right\\|_{l_{1} / l_{2}}\n$$\n\n\n\n### 3.4 TransR模型\n\n- 在TransE模型和TransH模型中，实体和关系都在相同的空间中进行表示。这种做法无法区分两个语义相近的实体在某些特定方面（关系）上的不同\n- 而TransR模型基本思想和TransH相似，**但是要求将头尾实体映射到关系r所对应的向量空间中，并且彼此接近**\n- 每个关系r维护一个映射矩阵$$M_r$$，那么头尾实体映射到该向量空间为：\n\n$$\n\\begin{aligned}\nh_r = M_rh \\\\\nt_r = M_rt\n\\end{aligned}\n$$\n\n- 那么损失函数为：\n\n$$\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\boldsymbol{h}_{r}+\\boldsymbol{r}-\\boldsymbol{t}_{r}\\right\\|_{l_{1} / l_{2}}\n$$\n\n\n\n### 3.5 TransD模型\n\n- 在TransR中，一个关系r对应一个映射矩阵$$M_r$$，**该矩阵是和头尾实体无关的**。对于一个三元组，头尾实体可能不是一类实体，比如<柏拉图，出生地，希腊>，**头尾实体不是一类实体，但是却使用了相同的映射矩阵$$M_r$$**，明显不合理\n\n- **所以TransD令映射矩阵和实体、关系同时相关**。在TransD中，**每个实体或关系都拥有两个向量**，对于三元组$$<h,r,t >$$，需要用6个向量$$h,h_p,t,t_p \\in R^n, r, r_p \\in R^m$$，其中**没有下标p的向量是用来捕捉语义信息的，而有下标p的向量是用于构造映射矩阵$$M_{rh}, M_{rt}$$的**，映射函数为：\n\n$$\n\\begin{aligned}\n\\boldsymbol{M}_{r h}=r_{p} \\boldsymbol{h}_{p}^{\\mathrm{T}}+\\boldsymbol{I}^{m \\times n}\\\\\\quad \\boldsymbol{M}_{r t}=r_{p} \\boldsymbol{t}_{p}^{\\mathrm{T}}+\\boldsymbol{I}^{m \\times n}\n\\end{aligned}\n$$\n\n> - 为什么构造映射矩阵的时候要加单位矩阵？\n>\n> 每次构造映射矩阵时，是先将矩阵初始化为单位矩阵，然后再通过向量内积来改变这个矩阵。**个人猜测是为了不要让映射后的空间和原空间相差过大**\n\n- 得到映射矩阵后，进行空间变换$$\\boldsymbol{h}_{\\perp}=\\boldsymbol{M}_{r h} \\boldsymbol{h}, \\quad \\boldsymbol{t}_{\\perp}=\\boldsymbol{M}_{r t} \\boldsymbol{t}$$，那么损失函数变为：\n\n$$\nf_{r}(\\boldsymbol{h}+\\boldsymbol{t})=\\left\\|\\boldsymbol{h}_{\\perp}+\\boldsymbol{r}-\\boldsymbol{t}_{\\perp}\\right\\|_{l_{1} / l_{2}}\n$$\n\n\n\n\n\n# 4 知识表示形式\n\n- 最常见的知识表示形式即前面的三元组形式，但是也有其他的表示形式，如谓词逻辑、产生式规则、框架（Frame）、树形知识表示、概率图等：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215032003.png\" alt=\"image-20230108215032003\" style=\"zoom:80%;\" />\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215042336.png\" alt=\"image-20230108215042336\" style=\"zoom:80%;\" />\n\n- 这里主要介绍一下用马尔可夫随机场（MRF）和马尔可夫逻辑网（MLN）进行表示\n- **马尔可夫逻辑网是通过软化一阶逻辑实现的**。传统的一阶逻辑知识库（由一阶逻辑命题所组成的知识库）被视作在一系列可能世界 （Possible World）上所施加的一组硬约束（Hard Constraint)，**但是这样的约束太过生硬，有时观察到的规则可能和知识库中的规则冲突**。而MLN旨在软化这些约束，**每条规则都与一个反应其约束强度的权重关联，权重越高，满足和不满足此规则的对数概率差就越大，即这条规则的置信度就越高**\n\n![image-20230108215936859](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215936859.png)\n\n其中，$$Fr(x, y)$$表示x，y是朋友，$$Sm(x)$$表示x抽烟。综上所述，**MLN就是一个（规则，权重）对的集合**\n\n- 而MLN可以视作定义具体的MRF（马尔可夫随机场）的模板。给定一个MLN（记作$$L=\\{F_i, w_i\\}$$）以及一个常量集合（如$$C=\\{Anna, Bob\\}$$），就可以定义一个相应的MRF（记作$$M_{L, C}$$）。构筑规则如下：\n\n> - **MRF的每个节点对应将MLN规则中经过给定常量实例化（Grounding）而得到的一个谓词**。比如规则$$\\forall x \\forall y \\forall z \\operatorname{Fr}(x, y) \\wedge \\operatorname{Fr}(y, z) \\Rightarrow \\operatorname{Fr}(x, z)$$中包含$$Fr(x, y)$$，由于给定了$$C=\\{Anna, Bob\\}$$，所以可以将其实例化为$$Fr(Anna, Bob)$$，这就是一个谓词实例，也是一个二元随机变量（要么为真要么为假），其就对应了MRF中的一个节点\n> - 现在要进一步需要明确MRF中的边：**两个谓词实例之间存在一条边，当且仅当它们至少在一个规则中同时出现。一条规则中的谓词之间形成了马尔可夫随机场中的一个团（不一定是最大团）**\n\n- 通过上图的最后两条规则，并给定$$C=\\{Anna, Bob\\}$$，可以构建出如下的MRF：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108221652749.png\" alt=\"image-20230108221652749\"  />\n\n- 根据得到的$$M_{L,C}$$可以进行概率推理，推理所要回答的问题形式是：**一直一条规则成立，则另一条规则成立的概率是多少**。比如已知$$F_2$$为$$Fr(Anna, Bob) \\vee Sm(Anna)$$，求$$F_1 = Sm(Bob)$$的概率：\n\n$$\n\\begin{aligned}\nP(F_1 \\mid F_2, M_{L, C}) & =\\frac{P\\left(F_{1} \\wedge F_{2} \\mid M_{L, C}\\right)}{P\\left(F_{2} \\mid M_{L, C}\\right)} \\\\\n& =\\frac{\\sum_{x \\in \\chi_{F_{1}} \\cap \\chi_{F_{2}}} P\\left(X=x \\mid M_{L, C}\\right)}{\\sum_{x \\in \\chi_{F_{2}}} P\\left(X=x \\mid M_{L, C}\\right)}\n\\end{aligned}\n$$\n\n其中$$\\chi_{F_i}$$表示$$F_i$$成立的世界的集合\n\n","source":"_posts/知识图谱-基本概念.md","raw":"---\ntitle: 知识图谱-基本概念\nmath: true\ndate: 2022-12-30\n---\n\n\n\n# 1 知识图谱的构成\n\n- 作为一种只是表示形式，知识图谱是一种大规模语义网络，包含实体（Entity）、概念（Concept）及其之间的各种语义关系，如下图：\n\n![image-20230108190012895](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190012895.png)\n\n- 而语义网络是以图形化的形式通过点和边表达知识的方式，其中点包括：\n\n> 1. **实体：**实体是属性赖以存在的基础，并且必须是自在的，即独立的、不依附于其他东西而存在的。比如身高，仅仅说身高是没有意义的，说“哲学家”这个类别的身高也是没有意义的，而必须说某个具体的哲学家的身高，这才是有明确所指且有意义的。\n> 2. **概念：**是指一类实体，比如“哲学家”，不是指某个特定的哲学家，而是指一类人\n> 3. **值：**每个实体都有一定的属性值，一般是数值、日期或文本\n>\n> ![image-20230108190222968](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190222968.png)\n\n- 知识图谱中的边可以分为**属性（Property）**和**关系（Relation）**两类。属性描述实体某方面的特性，而关系则可以认为是一类特殊的属性，当实体的某个属性值也是一个实体时，这个属性就是关系\n\n- 知识图谱和传统语义网络很像，但是两者的根本区别在于**前者是大规模自动化知识获取，而后者过于依赖于专家知识，导致规模等受限**\n\n\n\n\n\n\n# 2 知识图谱中的分类\n\n- 可以根据所包含的不同知识对知识图谱进行分类：\n\n> 1. **事实知识（Factual Knowledge）：**关于某个特定实体的基本事实。如（柏拉图，出生地，雅典）\n> 2. **概念知识（Taxonomy Knowledge）：**概念知识分为两类，一类是实体与概念之间的类属关系（isA关系），如（柏拉图，isA，哲学家），另一类是子概念与父概念之间的子类关系（subclassOf ），如（唯心主义哲学家 subclassOf 哲学家）\n> 3. **词汇知识（Lexical Knowledge）：**主要包括实体与词汇之间的关系（比如，实体的命名、称谓、英文名等）以及词汇之间的关系(包括同义关系、反义关系、缩略词关系、上下位词关系等)。例如﹐(“Plato”，中文名，柏拉图)、(赵匡胤，谥号，宋太祖)、(妻子，同义，老婆）\n> 4. **常识知识（Commonsense Knowledge）：**是人类通过身体与世界交互而积累的经验与知识，是人们在交流时无需言明就能理解的知识。如我们都知道鸟有翅膀，能飞，但是这种信息很少出现在文本里，所以常识知识的提取是十分困难的\n\n- 知识图谱还可以通过其他方式来分类，总结一下：\n\n![image-20230108194127845](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194127845.png)\n\n\n\n\n\n# 3 知识图谱的数值表示\n\n- 一个三元组包括：主题（Subject）、谓词（Predicate）以及客体（Object）。而一个知识图谱可以视作许多个三元组的集合，如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194532703.png\" alt=\"image-20230108194532703\" style=\"zoom:80%;\" />\n\n- 知识图谱的表示学习旨在将知识图谱中的元素（包括实体、属性、概念等）表示为低维稠密实值向量。而其关键是合理定义知识图谱中**关于事实（即三元组$$<h, r, t>$$）的损失函数$$f_r(h,t)$$，当事实$$<h, r, t>$$成立时，我们期望$$f_r(h,t)$$越小。考虑整个知识图谱的事实，就可以通过最小化$$\\sum_{\\langle h, r, t\\rangle \\in O} f_{r}(\\boldsymbol{h}, \\boldsymbol{t})$$来学习向量化表示**\n\n- 那么现在问题就变成了如何定义$$f_r(h,t)$$，一般思路有基于距离和基于翻译两种\n\n\n\n### 3.1 SE模型\n\n- 基于距离的代表模型，基本思想是当两个实体属于同一个三元组$$<h,r,t >$$时，它们的向量表示在投影后的空间中也应该彼此靠近\n- 因此定义损失函数为向量投影后的距离：\n\n$$\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\boldsymbol{W}_{r, 1} \\boldsymbol{h}-\\boldsymbol{W}_{r, 2} \\boldsymbol{t}\\right\\|_{l_{1}}\n$$\n\n其中的两个矩阵$$W_{r, 1}, W_{r, 2}$$分别用于头实体向量$$h$$和尾实体向量$$t$$的投影操作，**但SE难以捕捉实体和关系之间的语义相关性**\n\n\n\n### 3.2 TransE模型\n\n- 除了基于距离，还有基于翻译的模型，如TransE认为在知识库中，三元组$$<h,r,t >$$可以堪称头实体h到尾实体t利用关系r所进行的翻译。\n- 比如，对于三元组<柏拉图，老师，苏格拉底>来说，头实体“柏拉图”的向量加上关系“老师”的向量，应该尽可能和尾实体“苏格拉底”的向量接近，**也就是$$h+r \\approx t$$**\n- 基于这一思想可以得到损失函数：\n\n$$\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\|\\boldsymbol{h}+\\boldsymbol{r}-\\boldsymbol{t}\\|_{l_{1} / l_{2}}\n$$\n\n- 在实际应用中，**为了使习得的表示更有区分度，使用Hinge Loss目标函数，使得正负例尽可能分开：**\n\n$$\nL=\\sum_{(h, r, t) \\in S} \\sum_{\\left(h^{\\prime}, r, t^{\\prime}\\right) \\in S^{\\prime}}\\left[\\gamma+f_{r}(\\boldsymbol{h}, \\boldsymbol{t})-f_{r}\\left(\\boldsymbol{h}^{\\prime}, \\boldsymbol{t}^{\\prime}\\right)\\right]_{+}\n$$\n\n其中，$$\\gamma$$是间隔参数，$$S$$是正例集合（知识库中已有的三元组)，$$S'$$是负例集合（知识库中不存在的三元组，通常通过对关系r的三元组头尾实体进行随机替换来构造)。$$[x]_+ = \\max(0, x)$$\n\n\n\n### 3.3 TransH模型\n\n- **TransE模型中的$$h+r \\approx t$$假设太强**，导致在自反一对多、多对一等关系下实体向量学习的错误。\n- 比如，对于自反关系r，$$<h,r,t >$$和$$<t,r,h >$$同时成立，导致h =t。对于多对一关系。又比如<柏拉图，性别，男>、<特朗普，性别，男>两个三元组有着相同的关系和尾实体，导致柏拉图和特朗普向量接近。但是柏拉图与特朗普除了在性别上相同，在其他方面显然完全不同。\n- 为解决上述问题，TransH放宽了假设，**只要求头尾实体在关系r相对应的超平面上的投影彼此接近即可**。设r所对应的超平面的法向量为$$W_r$$，那么$$h, t$$映射到超平面上为：\n\n$$\n\\begin{aligned}\n\\boldsymbol{h}_{\\perp} & =\\boldsymbol{h}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{h} \\boldsymbol{W}_{r} \\\\\n\\boldsymbol{t}_{\\perp} & =\\boldsymbol{t}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{t} \\boldsymbol{W}_{r}\n\\end{aligned}\n$$\n\n- 那么损失函数为：\n\n$$\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\left(\\boldsymbol{h}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{h} \\boldsymbol{W}_{r}\\right)+r-\\left(\\boldsymbol{t}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{t} \\boldsymbol{W}_{r}\\right)\\right\\|_{l_{1} / l_{2}}\n$$\n\n\n\n### 3.4 TransR模型\n\n- 在TransE模型和TransH模型中，实体和关系都在相同的空间中进行表示。这种做法无法区分两个语义相近的实体在某些特定方面（关系）上的不同\n- 而TransR模型基本思想和TransH相似，**但是要求将头尾实体映射到关系r所对应的向量空间中，并且彼此接近**\n- 每个关系r维护一个映射矩阵$$M_r$$，那么头尾实体映射到该向量空间为：\n\n$$\n\\begin{aligned}\nh_r = M_rh \\\\\nt_r = M_rt\n\\end{aligned}\n$$\n\n- 那么损失函数为：\n\n$$\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\boldsymbol{h}_{r}+\\boldsymbol{r}-\\boldsymbol{t}_{r}\\right\\|_{l_{1} / l_{2}}\n$$\n\n\n\n### 3.5 TransD模型\n\n- 在TransR中，一个关系r对应一个映射矩阵$$M_r$$，**该矩阵是和头尾实体无关的**。对于一个三元组，头尾实体可能不是一类实体，比如<柏拉图，出生地，希腊>，**头尾实体不是一类实体，但是却使用了相同的映射矩阵$$M_r$$**，明显不合理\n\n- **所以TransD令映射矩阵和实体、关系同时相关**。在TransD中，**每个实体或关系都拥有两个向量**，对于三元组$$<h,r,t >$$，需要用6个向量$$h,h_p,t,t_p \\in R^n, r, r_p \\in R^m$$，其中**没有下标p的向量是用来捕捉语义信息的，而有下标p的向量是用于构造映射矩阵$$M_{rh}, M_{rt}$$的**，映射函数为：\n\n$$\n\\begin{aligned}\n\\boldsymbol{M}_{r h}=r_{p} \\boldsymbol{h}_{p}^{\\mathrm{T}}+\\boldsymbol{I}^{m \\times n}\\\\\\quad \\boldsymbol{M}_{r t}=r_{p} \\boldsymbol{t}_{p}^{\\mathrm{T}}+\\boldsymbol{I}^{m \\times n}\n\\end{aligned}\n$$\n\n> - 为什么构造映射矩阵的时候要加单位矩阵？\n>\n> 每次构造映射矩阵时，是先将矩阵初始化为单位矩阵，然后再通过向量内积来改变这个矩阵。**个人猜测是为了不要让映射后的空间和原空间相差过大**\n\n- 得到映射矩阵后，进行空间变换$$\\boldsymbol{h}_{\\perp}=\\boldsymbol{M}_{r h} \\boldsymbol{h}, \\quad \\boldsymbol{t}_{\\perp}=\\boldsymbol{M}_{r t} \\boldsymbol{t}$$，那么损失函数变为：\n\n$$\nf_{r}(\\boldsymbol{h}+\\boldsymbol{t})=\\left\\|\\boldsymbol{h}_{\\perp}+\\boldsymbol{r}-\\boldsymbol{t}_{\\perp}\\right\\|_{l_{1} / l_{2}}\n$$\n\n\n\n\n\n# 4 知识表示形式\n\n- 最常见的知识表示形式即前面的三元组形式，但是也有其他的表示形式，如谓词逻辑、产生式规则、框架（Frame）、树形知识表示、概率图等：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215032003.png\" alt=\"image-20230108215032003\" style=\"zoom:80%;\" />\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215042336.png\" alt=\"image-20230108215042336\" style=\"zoom:80%;\" />\n\n- 这里主要介绍一下用马尔可夫随机场（MRF）和马尔可夫逻辑网（MLN）进行表示\n- **马尔可夫逻辑网是通过软化一阶逻辑实现的**。传统的一阶逻辑知识库（由一阶逻辑命题所组成的知识库）被视作在一系列可能世界 （Possible World）上所施加的一组硬约束（Hard Constraint)，**但是这样的约束太过生硬，有时观察到的规则可能和知识库中的规则冲突**。而MLN旨在软化这些约束，**每条规则都与一个反应其约束强度的权重关联，权重越高，满足和不满足此规则的对数概率差就越大，即这条规则的置信度就越高**\n\n![image-20230108215936859](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215936859.png)\n\n其中，$$Fr(x, y)$$表示x，y是朋友，$$Sm(x)$$表示x抽烟。综上所述，**MLN就是一个（规则，权重）对的集合**\n\n- 而MLN可以视作定义具体的MRF（马尔可夫随机场）的模板。给定一个MLN（记作$$L=\\{F_i, w_i\\}$$）以及一个常量集合（如$$C=\\{Anna, Bob\\}$$），就可以定义一个相应的MRF（记作$$M_{L, C}$$）。构筑规则如下：\n\n> - **MRF的每个节点对应将MLN规则中经过给定常量实例化（Grounding）而得到的一个谓词**。比如规则$$\\forall x \\forall y \\forall z \\operatorname{Fr}(x, y) \\wedge \\operatorname{Fr}(y, z) \\Rightarrow \\operatorname{Fr}(x, z)$$中包含$$Fr(x, y)$$，由于给定了$$C=\\{Anna, Bob\\}$$，所以可以将其实例化为$$Fr(Anna, Bob)$$，这就是一个谓词实例，也是一个二元随机变量（要么为真要么为假），其就对应了MRF中的一个节点\n> - 现在要进一步需要明确MRF中的边：**两个谓词实例之间存在一条边，当且仅当它们至少在一个规则中同时出现。一条规则中的谓词之间形成了马尔可夫随机场中的一个团（不一定是最大团）**\n\n- 通过上图的最后两条规则，并给定$$C=\\{Anna, Bob\\}$$，可以构建出如下的MRF：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108221652749.png\" alt=\"image-20230108221652749\"  />\n\n- 根据得到的$$M_{L,C}$$可以进行概率推理，推理所要回答的问题形式是：**一直一条规则成立，则另一条规则成立的概率是多少**。比如已知$$F_2$$为$$Fr(Anna, Bob) \\vee Sm(Anna)$$，求$$F_1 = Sm(Bob)$$的概率：\n\n$$\n\\begin{aligned}\nP(F_1 \\mid F_2, M_{L, C}) & =\\frac{P\\left(F_{1} \\wedge F_{2} \\mid M_{L, C}\\right)}{P\\left(F_{2} \\mid M_{L, C}\\right)} \\\\\n& =\\frac{\\sum_{x \\in \\chi_{F_{1}} \\cap \\chi_{F_{2}}} P\\left(X=x \\mid M_{L, C}\\right)}{\\sum_{x \\in \\chi_{F_{2}}} P\\left(X=x \\mid M_{L, C}\\right)}\n\\end{aligned}\n$$\n\n其中$$\\chi_{F_i}$$表示$$F_i$$成立的世界的集合\n\n","slug":"知识图谱-基本概念","published":1,"updated":"2023-01-09T06:01:13.403Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1n000n7csz80x57vdv","content":"<h1 id=\"1-知识图谱的构成\"><a href=\"#1-知识图谱的构成\" class=\"headerlink\" title=\"1 知识图谱的构成\"></a>1 知识图谱的构成</h1><ul>\n<li>作为一种只是表示形式，知识图谱是一种大规模语义网络，包含实体（Entity）、概念（Concept）及其之间的各种语义关系，如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190012895.png\" alt=\"image-20230108190012895\"></p>\n<ul>\n<li>而语义网络是以图形化的形式通过点和边表达知识的方式，其中点包括：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>实体：</strong>实体是属性赖以存在的基础，并且必须是自在的，即独立的、不依附于其他东西而存在的。比如身高，仅仅说身高是没有意义的，说“哲学家”这个类别的身高也是没有意义的，而必须说某个具体的哲学家的身高，这才是有明确所指且有意义的。</li>\n<li><strong>概念：</strong>是指一类实体，比如“哲学家”，不是指某个特定的哲学家，而是指一类人</li>\n<li><strong>值：</strong>每个实体都有一定的属性值，一般是数值、日期或文本</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190222968.png\" alt=\"image-20230108190222968\"></p>\n</blockquote>\n<ul>\n<li><p>知识图谱中的边可以分为<strong>属性（Property）</strong>和<strong>关系（Relation）</strong>两类。属性描述实体某方面的特性，而关系则可以认为是一类特殊的属性，当实体的某个属性值也是一个实体时，这个属性就是关系</p>\n</li>\n<li><p>知识图谱和传统语义网络很像，但是两者的根本区别在于<strong>前者是大规模自动化知识获取，而后者过于依赖于专家知识，导致规模等受限</strong></p>\n</li>\n</ul>\n<h1 id=\"2-知识图谱中的分类\"><a href=\"#2-知识图谱中的分类\" class=\"headerlink\" title=\"2 知识图谱中的分类\"></a>2 知识图谱中的分类</h1><ul>\n<li>可以根据所包含的不同知识对知识图谱进行分类：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>事实知识（Factual Knowledge）：</strong>关于某个特定实体的基本事实。如（柏拉图，出生地，雅典）</li>\n<li><strong>概念知识（Taxonomy Knowledge）：</strong>概念知识分为两类，一类是实体与概念之间的类属关系（isA关系），如（柏拉图，isA，哲学家），另一类是子概念与父概念之间的子类关系（subclassOf ），如（唯心主义哲学家 subclassOf 哲学家）</li>\n<li><strong>词汇知识（Lexical Knowledge）：</strong>主要包括实体与词汇之间的关系（比如，实体的命名、称谓、英文名等）以及词汇之间的关系(包括同义关系、反义关系、缩略词关系、上下位词关系等)。例如﹐(“Plato”，中文名，柏拉图)、(赵匡胤，谥号，宋太祖)、(妻子，同义，老婆）</li>\n<li><strong>常识知识（Commonsense Knowledge）：</strong>是人类通过身体与世界交互而积累的经验与知识，是人们在交流时无需言明就能理解的知识。如我们都知道鸟有翅膀，能飞，但是这种信息很少出现在文本里，所以常识知识的提取是十分困难的</li>\n</ol>\n</blockquote>\n<ul>\n<li>知识图谱还可以通过其他方式来分类，总结一下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194127845.png\" alt=\"image-20230108194127845\"></p>\n<h1 id=\"3-知识图谱的数值表示\"><a href=\"#3-知识图谱的数值表示\" class=\"headerlink\" title=\"3 知识图谱的数值表示\"></a>3 知识图谱的数值表示</h1><ul>\n<li>一个三元组包括：主题（Subject）、谓词（Predicate）以及客体（Object）。而一个知识图谱可以视作许多个三元组的集合，如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194532703.png\" alt=\"image-20230108194532703\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><p>知识图谱的表示学习旨在将知识图谱中的元素（包括实体、属性、概念等）表示为低维稠密实值向量。而其关键是合理定义知识图谱中<strong>关于事实（即三元组<script type=\"math/tex\"><h, r, t></script>）的损失函数<script type=\"math/tex\">f_r(h,t)</script>，当事实<script type=\"math/tex\"><h, r, t></script>成立时，我们期望<script type=\"math/tex\">f_r(h,t)</script>越小。考虑整个知识图谱的事实，就可以通过最小化<script type=\"math/tex\">\\sum_{\\langle h, r, t\\rangle \\in O} f_{r}(\\boldsymbol{h}, \\boldsymbol{t})</script>来学习向量化表示</strong></p>\n</li>\n<li><p>那么现在问题就变成了如何定义<script type=\"math/tex\">f_r(h,t)</script>，一般思路有基于距离和基于翻译两种</p>\n</li>\n</ul>\n<h3 id=\"3-1-SE模型\"><a href=\"#3-1-SE模型\" class=\"headerlink\" title=\"3.1 SE模型\"></a>3.1 SE模型</h3><ul>\n<li>基于距离的代表模型，基本思想是当两个实体属于同一个三元组<script type=\"math/tex\"><h,r,t ></script>时，它们的向量表示在投影后的空间中也应该彼此靠近</li>\n<li>因此定义损失函数为向量投影后的距离：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\boldsymbol{W}_{r, 1} \\boldsymbol{h}-\\boldsymbol{W}_{r, 2} \\boldsymbol{t}\\right\\|_{l_{1}}</script><p>其中的两个矩阵<script type=\"math/tex\">W_{r, 1}, W_{r, 2}</script>分别用于头实体向量<script type=\"math/tex\">h</script>和尾实体向量<script type=\"math/tex\">t</script>的投影操作，<strong>但SE难以捕捉实体和关系之间的语义相关性</strong></p>\n<h3 id=\"3-2-TransE模型\"><a href=\"#3-2-TransE模型\" class=\"headerlink\" title=\"3.2 TransE模型\"></a>3.2 TransE模型</h3><ul>\n<li>除了基于距离，还有基于翻译的模型，如TransE认为在知识库中，三元组<script type=\"math/tex\"><h,r,t ></script>可以堪称头实体h到尾实体t利用关系r所进行的翻译。</li>\n<li>比如，对于三元组&lt;柏拉图，老师，苏格拉底&gt;来说，头实体“柏拉图”的向量加上关系“老师”的向量，应该尽可能和尾实体“苏格拉底”的向量接近，<strong>也就是<script type=\"math/tex\">h+r \\approx t</script></strong></li>\n<li>基于这一思想可以得到损失函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\|\\boldsymbol{h}+\\boldsymbol{r}-\\boldsymbol{t}\\|_{l_{1} / l_{2}}</script><ul>\n<li>在实际应用中，<strong>为了使习得的表示更有区分度，使用Hinge Loss目标函数，使得正负例尽可能分开：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL=\\sum_{(h, r, t) \\in S} \\sum_{\\left(h^{\\prime}, r, t^{\\prime}\\right) \\in S^{\\prime}}\\left[\\gamma+f_{r}(\\boldsymbol{h}, \\boldsymbol{t})-f_{r}\\left(\\boldsymbol{h}^{\\prime}, \\boldsymbol{t}^{\\prime}\\right)\\right]_{+}</script><p>其中，<script type=\"math/tex\">\\gamma</script>是间隔参数，<script type=\"math/tex\">S</script>是正例集合（知识库中已有的三元组)，<script type=\"math/tex\">S'</script>是负例集合（知识库中不存在的三元组，通常通过对关系r的三元组头尾实体进行随机替换来构造)。<script type=\"math/tex\">[x]_+ = \\max(0, x)</script></p>\n<h3 id=\"3-3-TransH模型\"><a href=\"#3-3-TransH模型\" class=\"headerlink\" title=\"3.3 TransH模型\"></a>3.3 TransH模型</h3><ul>\n<li><strong>TransE模型中的<script type=\"math/tex\">h+r \\approx t</script>假设太强</strong>，导致在自反一对多、多对一等关系下实体向量学习的错误。</li>\n<li>比如，对于自反关系r，<script type=\"math/tex\"><h,r,t ></script>和<script type=\"math/tex\"><t,r,h ></script>同时成立，导致h =t。对于多对一关系。又比如&lt;柏拉图，性别，男&gt;、&lt;特朗普，性别，男&gt;两个三元组有着相同的关系和尾实体，导致柏拉图和特朗普向量接近。但是柏拉图与特朗普除了在性别上相同，在其他方面显然完全不同。</li>\n<li>为解决上述问题，TransH放宽了假设，<strong>只要求头尾实体在关系r相对应的超平面上的投影彼此接近即可</strong>。设r所对应的超平面的法向量为<script type=\"math/tex\">W_r</script>，那么<script type=\"math/tex\">h, t</script>映射到超平面上为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\boldsymbol{h}_{\\perp} & =\\boldsymbol{h}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{h} \\boldsymbol{W}_{r} \\\\\n\\boldsymbol{t}_{\\perp} & =\\boldsymbol{t}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{t} \\boldsymbol{W}_{r}\n\\end{aligned}</script><ul>\n<li>那么损失函数为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\left(\\boldsymbol{h}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{h} \\boldsymbol{W}_{r}\\right)+r-\\left(\\boldsymbol{t}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{t} \\boldsymbol{W}_{r}\\right)\\right\\|_{l_{1} / l_{2}}</script><h3 id=\"3-4-TransR模型\"><a href=\"#3-4-TransR模型\" class=\"headerlink\" title=\"3.4 TransR模型\"></a>3.4 TransR模型</h3><ul>\n<li>在TransE模型和TransH模型中，实体和关系都在相同的空间中进行表示。这种做法无法区分两个语义相近的实体在某些特定方面（关系）上的不同</li>\n<li>而TransR模型基本思想和TransH相似，<strong>但是要求将头尾实体映射到关系r所对应的向量空间中，并且彼此接近</strong></li>\n<li>每个关系r维护一个映射矩阵<script type=\"math/tex\">M_r</script>，那么头尾实体映射到该向量空间为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nh_r = M_rh \\\\\nt_r = M_rt\n\\end{aligned}</script><ul>\n<li>那么损失函数为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\boldsymbol{h}_{r}+\\boldsymbol{r}-\\boldsymbol{t}_{r}\\right\\|_{l_{1} / l_{2}}</script><h3 id=\"3-5-TransD模型\"><a href=\"#3-5-TransD模型\" class=\"headerlink\" title=\"3.5 TransD模型\"></a>3.5 TransD模型</h3><ul>\n<li><p>在TransR中，一个关系r对应一个映射矩阵<script type=\"math/tex\">M_r</script>，<strong>该矩阵是和头尾实体无关的</strong>。对于一个三元组，头尾实体可能不是一类实体，比如&lt;柏拉图，出生地，希腊&gt;，<strong>头尾实体不是一类实体，但是却使用了相同的映射矩阵<script type=\"math/tex\">M_r</script></strong>，明显不合理</p>\n</li>\n<li><p><strong>所以TransD令映射矩阵和实体、关系同时相关</strong>。在TransD中，<strong>每个实体或关系都拥有两个向量</strong>，对于三元组<script type=\"math/tex\"><h,r,t ></script>，需要用6个向量<script type=\"math/tex\">h,h_p,t,t_p \\in R^n, r, r_p \\in R^m</script>，其中<strong>没有下标p的向量是用来捕捉语义信息的，而有下标p的向量是用于构造映射矩阵<script type=\"math/tex\">M_{rh}, M_{rt}</script>的</strong>，映射函数为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\boldsymbol{M}_{r h}=r_{p} \\boldsymbol{h}_{p}^{\\mathrm{T}}+\\boldsymbol{I}^{m \\times n}\\\\\\quad \\boldsymbol{M}_{r t}=r_{p} \\boldsymbol{t}_{p}^{\\mathrm{T}}+\\boldsymbol{I}^{m \\times n}\n\\end{aligned}</script><blockquote>\n<ul>\n<li>为什么构造映射矩阵的时候要加单位矩阵？</li>\n</ul>\n<p>每次构造映射矩阵时，是先将矩阵初始化为单位矩阵，然后再通过向量内积来改变这个矩阵。<strong>个人猜测是为了不要让映射后的空间和原空间相差过大</strong></p>\n</blockquote>\n<ul>\n<li>得到映射矩阵后，进行空间变换<script type=\"math/tex\">\\boldsymbol{h}_{\\perp}=\\boldsymbol{M}_{r h} \\boldsymbol{h}, \\quad \\boldsymbol{t}_{\\perp}=\\boldsymbol{M}_{r t} \\boldsymbol{t}</script>，那么损失函数变为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}+\\boldsymbol{t})=\\left\\|\\boldsymbol{h}_{\\perp}+\\boldsymbol{r}-\\boldsymbol{t}_{\\perp}\\right\\|_{l_{1} / l_{2}}</script><h1 id=\"4-知识表示形式\"><a href=\"#4-知识表示形式\" class=\"headerlink\" title=\"4 知识表示形式\"></a>4 知识表示形式</h1><ul>\n<li>最常见的知识表示形式即前面的三元组形式，但是也有其他的表示形式，如谓词逻辑、产生式规则、框架（Frame）、树形知识表示、概率图等：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215032003.png\" alt=\"image-20230108215032003\" style=\"zoom:80%;\" /></p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215042336.png\" alt=\"image-20230108215042336\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>这里主要介绍一下用马尔可夫随机场（MRF）和马尔可夫逻辑网（MLN）进行表示</li>\n<li><strong>马尔可夫逻辑网是通过软化一阶逻辑实现的</strong>。传统的一阶逻辑知识库（由一阶逻辑命题所组成的知识库）被视作在一系列可能世界 （Possible World）上所施加的一组硬约束（Hard Constraint)，<strong>但是这样的约束太过生硬，有时观察到的规则可能和知识库中的规则冲突</strong>。而MLN旨在软化这些约束，<strong>每条规则都与一个反应其约束强度的权重关联，权重越高，满足和不满足此规则的对数概率差就越大，即这条规则的置信度就越高</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215936859.png\" alt=\"image-20230108215936859\"></p>\n<p>其中，<script type=\"math/tex\">Fr(x, y)</script>表示x，y是朋友，<script type=\"math/tex\">Sm(x)</script>表示x抽烟。综上所述，<strong>MLN就是一个（规则，权重）对的集合</strong></p>\n<ul>\n<li>而MLN可以视作定义具体的MRF（马尔可夫随机场）的模板。给定一个MLN（记作<script type=\"math/tex\">L=\\{F_i, w_i\\}</script>）以及一个常量集合（如<script type=\"math/tex\">C=\\{Anna, Bob\\}</script>），就可以定义一个相应的MRF（记作<script type=\"math/tex\">M_{L, C}</script>）。构筑规则如下：</li>\n</ul>\n<blockquote>\n<ul>\n<li><strong>MRF的每个节点对应将MLN规则中经过给定常量实例化（Grounding）而得到的一个谓词</strong>。比如规则<script type=\"math/tex\">\\forall x \\forall y \\forall z \\operatorname{Fr}(x, y) \\wedge \\operatorname{Fr}(y, z) \\Rightarrow \\operatorname{Fr}(x, z)</script>中包含<script type=\"math/tex\">Fr(x, y)</script>，由于给定了<script type=\"math/tex\">C=\\{Anna, Bob\\}</script>，所以可以将其实例化为<script type=\"math/tex\">Fr(Anna, Bob)</script>，这就是一个谓词实例，也是一个二元随机变量（要么为真要么为假），其就对应了MRF中的一个节点</li>\n<li>现在要进一步需要明确MRF中的边：<strong>两个谓词实例之间存在一条边，当且仅当它们至少在一个规则中同时出现。一条规则中的谓词之间形成了马尔可夫随机场中的一个团（不一定是最大团）</strong></li>\n</ul>\n</blockquote>\n<ul>\n<li>通过上图的最后两条规则，并给定<script type=\"math/tex\">C=\\{Anna, Bob\\}</script>，可以构建出如下的MRF：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108221652749.png\" alt=\"image-20230108221652749\"  /></p>\n<ul>\n<li>根据得到的<script type=\"math/tex\">M_{L,C}</script>可以进行概率推理，推理所要回答的问题形式是：<strong>一直一条规则成立，则另一条规则成立的概率是多少</strong>。比如已知<script type=\"math/tex\">F_2</script>为<script type=\"math/tex\">Fr(Anna, Bob) \\vee Sm(Anna)</script>，求<script type=\"math/tex\">F_1 = Sm(Bob)</script>的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(F_1 \\mid F_2, M_{L, C}) & =\\frac{P\\left(F_{1} \\wedge F_{2} \\mid M_{L, C}\\right)}{P\\left(F_{2} \\mid M_{L, C}\\right)} \\\\\n& =\\frac{\\sum_{x \\in \\chi_{F_{1}} \\cap \\chi_{F_{2}}} P\\left(X=x \\mid M_{L, C}\\right)}{\\sum_{x \\in \\chi_{F_{2}}} P\\left(X=x \\mid M_{L, C}\\right)}\n\\end{aligned}</script><p>其中<script type=\"math/tex\">\\chi_{F_i}</script>表示<script type=\"math/tex\">F_i</script>成立的世界的集合</p>\n","site":{"data":{}},"wordcount":5301,"excerpt":"","more":"<h1 id=\"1-知识图谱的构成\"><a href=\"#1-知识图谱的构成\" class=\"headerlink\" title=\"1 知识图谱的构成\"></a>1 知识图谱的构成</h1><ul>\n<li>作为一种只是表示形式，知识图谱是一种大规模语义网络，包含实体（Entity）、概念（Concept）及其之间的各种语义关系，如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190012895.png\" alt=\"image-20230108190012895\"></p>\n<ul>\n<li>而语义网络是以图形化的形式通过点和边表达知识的方式，其中点包括：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>实体：</strong>实体是属性赖以存在的基础，并且必须是自在的，即独立的、不依附于其他东西而存在的。比如身高，仅仅说身高是没有意义的，说“哲学家”这个类别的身高也是没有意义的，而必须说某个具体的哲学家的身高，这才是有明确所指且有意义的。</li>\n<li><strong>概念：</strong>是指一类实体，比如“哲学家”，不是指某个特定的哲学家，而是指一类人</li>\n<li><strong>值：</strong>每个实体都有一定的属性值，一般是数值、日期或文本</li>\n</ol>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108190222968.png\" alt=\"image-20230108190222968\"></p>\n</blockquote>\n<ul>\n<li><p>知识图谱中的边可以分为<strong>属性（Property）</strong>和<strong>关系（Relation）</strong>两类。属性描述实体某方面的特性，而关系则可以认为是一类特殊的属性，当实体的某个属性值也是一个实体时，这个属性就是关系</p>\n</li>\n<li><p>知识图谱和传统语义网络很像，但是两者的根本区别在于<strong>前者是大规模自动化知识获取，而后者过于依赖于专家知识，导致规模等受限</strong></p>\n</li>\n</ul>\n<h1 id=\"2-知识图谱中的分类\"><a href=\"#2-知识图谱中的分类\" class=\"headerlink\" title=\"2 知识图谱中的分类\"></a>2 知识图谱中的分类</h1><ul>\n<li>可以根据所包含的不同知识对知识图谱进行分类：</li>\n</ul>\n<blockquote>\n<ol>\n<li><strong>事实知识（Factual Knowledge）：</strong>关于某个特定实体的基本事实。如（柏拉图，出生地，雅典）</li>\n<li><strong>概念知识（Taxonomy Knowledge）：</strong>概念知识分为两类，一类是实体与概念之间的类属关系（isA关系），如（柏拉图，isA，哲学家），另一类是子概念与父概念之间的子类关系（subclassOf ），如（唯心主义哲学家 subclassOf 哲学家）</li>\n<li><strong>词汇知识（Lexical Knowledge）：</strong>主要包括实体与词汇之间的关系（比如，实体的命名、称谓、英文名等）以及词汇之间的关系(包括同义关系、反义关系、缩略词关系、上下位词关系等)。例如﹐(“Plato”，中文名，柏拉图)、(赵匡胤，谥号，宋太祖)、(妻子，同义，老婆）</li>\n<li><strong>常识知识（Commonsense Knowledge）：</strong>是人类通过身体与世界交互而积累的经验与知识，是人们在交流时无需言明就能理解的知识。如我们都知道鸟有翅膀，能飞，但是这种信息很少出现在文本里，所以常识知识的提取是十分困难的</li>\n</ol>\n</blockquote>\n<ul>\n<li>知识图谱还可以通过其他方式来分类，总结一下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194127845.png\" alt=\"image-20230108194127845\"></p>\n<h1 id=\"3-知识图谱的数值表示\"><a href=\"#3-知识图谱的数值表示\" class=\"headerlink\" title=\"3 知识图谱的数值表示\"></a>3 知识图谱的数值表示</h1><ul>\n<li>一个三元组包括：主题（Subject）、谓词（Predicate）以及客体（Object）。而一个知识图谱可以视作许多个三元组的集合，如下图：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108194532703.png\" alt=\"image-20230108194532703\" style=\"zoom:80%;\" /></p>\n<ul>\n<li><p>知识图谱的表示学习旨在将知识图谱中的元素（包括实体、属性、概念等）表示为低维稠密实值向量。而其关键是合理定义知识图谱中<strong>关于事实（即三元组<script type=\"math/tex\"><h, r, t></script>）的损失函数<script type=\"math/tex\">f_r(h,t)</script>，当事实<script type=\"math/tex\"><h, r, t></script>成立时，我们期望<script type=\"math/tex\">f_r(h,t)</script>越小。考虑整个知识图谱的事实，就可以通过最小化<script type=\"math/tex\">\\sum_{\\langle h, r, t\\rangle \\in O} f_{r}(\\boldsymbol{h}, \\boldsymbol{t})</script>来学习向量化表示</strong></p>\n</li>\n<li><p>那么现在问题就变成了如何定义<script type=\"math/tex\">f_r(h,t)</script>，一般思路有基于距离和基于翻译两种</p>\n</li>\n</ul>\n<h3 id=\"3-1-SE模型\"><a href=\"#3-1-SE模型\" class=\"headerlink\" title=\"3.1 SE模型\"></a>3.1 SE模型</h3><ul>\n<li>基于距离的代表模型，基本思想是当两个实体属于同一个三元组<script type=\"math/tex\"><h,r,t ></script>时，它们的向量表示在投影后的空间中也应该彼此靠近</li>\n<li>因此定义损失函数为向量投影后的距离：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\boldsymbol{W}_{r, 1} \\boldsymbol{h}-\\boldsymbol{W}_{r, 2} \\boldsymbol{t}\\right\\|_{l_{1}}</script><p>其中的两个矩阵<script type=\"math/tex\">W_{r, 1}, W_{r, 2}</script>分别用于头实体向量<script type=\"math/tex\">h</script>和尾实体向量<script type=\"math/tex\">t</script>的投影操作，<strong>但SE难以捕捉实体和关系之间的语义相关性</strong></p>\n<h3 id=\"3-2-TransE模型\"><a href=\"#3-2-TransE模型\" class=\"headerlink\" title=\"3.2 TransE模型\"></a>3.2 TransE模型</h3><ul>\n<li>除了基于距离，还有基于翻译的模型，如TransE认为在知识库中，三元组<script type=\"math/tex\"><h,r,t ></script>可以堪称头实体h到尾实体t利用关系r所进行的翻译。</li>\n<li>比如，对于三元组&lt;柏拉图，老师，苏格拉底&gt;来说，头实体“柏拉图”的向量加上关系“老师”的向量，应该尽可能和尾实体“苏格拉底”的向量接近，<strong>也就是<script type=\"math/tex\">h+r \\approx t</script></strong></li>\n<li>基于这一思想可以得到损失函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\|\\boldsymbol{h}+\\boldsymbol{r}-\\boldsymbol{t}\\|_{l_{1} / l_{2}}</script><ul>\n<li>在实际应用中，<strong>为了使习得的表示更有区分度，使用Hinge Loss目标函数，使得正负例尽可能分开：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL=\\sum_{(h, r, t) \\in S} \\sum_{\\left(h^{\\prime}, r, t^{\\prime}\\right) \\in S^{\\prime}}\\left[\\gamma+f_{r}(\\boldsymbol{h}, \\boldsymbol{t})-f_{r}\\left(\\boldsymbol{h}^{\\prime}, \\boldsymbol{t}^{\\prime}\\right)\\right]_{+}</script><p>其中，<script type=\"math/tex\">\\gamma</script>是间隔参数，<script type=\"math/tex\">S</script>是正例集合（知识库中已有的三元组)，<script type=\"math/tex\">S'</script>是负例集合（知识库中不存在的三元组，通常通过对关系r的三元组头尾实体进行随机替换来构造)。<script type=\"math/tex\">[x]_+ = \\max(0, x)</script></p>\n<h3 id=\"3-3-TransH模型\"><a href=\"#3-3-TransH模型\" class=\"headerlink\" title=\"3.3 TransH模型\"></a>3.3 TransH模型</h3><ul>\n<li><strong>TransE模型中的<script type=\"math/tex\">h+r \\approx t</script>假设太强</strong>，导致在自反一对多、多对一等关系下实体向量学习的错误。</li>\n<li>比如，对于自反关系r，<script type=\"math/tex\"><h,r,t ></script>和<script type=\"math/tex\"><t,r,h ></script>同时成立，导致h =t。对于多对一关系。又比如&lt;柏拉图，性别，男&gt;、&lt;特朗普，性别，男&gt;两个三元组有着相同的关系和尾实体，导致柏拉图和特朗普向量接近。但是柏拉图与特朗普除了在性别上相同，在其他方面显然完全不同。</li>\n<li>为解决上述问题，TransH放宽了假设，<strong>只要求头尾实体在关系r相对应的超平面上的投影彼此接近即可</strong>。设r所对应的超平面的法向量为<script type=\"math/tex\">W_r</script>，那么<script type=\"math/tex\">h, t</script>映射到超平面上为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\boldsymbol{h}_{\\perp} & =\\boldsymbol{h}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{h} \\boldsymbol{W}_{r} \\\\\n\\boldsymbol{t}_{\\perp} & =\\boldsymbol{t}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{t} \\boldsymbol{W}_{r}\n\\end{aligned}</script><ul>\n<li>那么损失函数为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\left(\\boldsymbol{h}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{h} \\boldsymbol{W}_{r}\\right)+r-\\left(\\boldsymbol{t}-\\boldsymbol{W}_{r}^{\\mathrm{T}} \\boldsymbol{t} \\boldsymbol{W}_{r}\\right)\\right\\|_{l_{1} / l_{2}}</script><h3 id=\"3-4-TransR模型\"><a href=\"#3-4-TransR模型\" class=\"headerlink\" title=\"3.4 TransR模型\"></a>3.4 TransR模型</h3><ul>\n<li>在TransE模型和TransH模型中，实体和关系都在相同的空间中进行表示。这种做法无法区分两个语义相近的实体在某些特定方面（关系）上的不同</li>\n<li>而TransR模型基本思想和TransH相似，<strong>但是要求将头尾实体映射到关系r所对应的向量空间中，并且彼此接近</strong></li>\n<li>每个关系r维护一个映射矩阵<script type=\"math/tex\">M_r</script>，那么头尾实体映射到该向量空间为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nh_r = M_rh \\\\\nt_r = M_rt\n\\end{aligned}</script><ul>\n<li>那么损失函数为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}, \\boldsymbol{t})=\\left\\|\\boldsymbol{h}_{r}+\\boldsymbol{r}-\\boldsymbol{t}_{r}\\right\\|_{l_{1} / l_{2}}</script><h3 id=\"3-5-TransD模型\"><a href=\"#3-5-TransD模型\" class=\"headerlink\" title=\"3.5 TransD模型\"></a>3.5 TransD模型</h3><ul>\n<li><p>在TransR中，一个关系r对应一个映射矩阵<script type=\"math/tex\">M_r</script>，<strong>该矩阵是和头尾实体无关的</strong>。对于一个三元组，头尾实体可能不是一类实体，比如&lt;柏拉图，出生地，希腊&gt;，<strong>头尾实体不是一类实体，但是却使用了相同的映射矩阵<script type=\"math/tex\">M_r</script></strong>，明显不合理</p>\n</li>\n<li><p><strong>所以TransD令映射矩阵和实体、关系同时相关</strong>。在TransD中，<strong>每个实体或关系都拥有两个向量</strong>，对于三元组<script type=\"math/tex\"><h,r,t ></script>，需要用6个向量<script type=\"math/tex\">h,h_p,t,t_p \\in R^n, r, r_p \\in R^m</script>，其中<strong>没有下标p的向量是用来捕捉语义信息的，而有下标p的向量是用于构造映射矩阵<script type=\"math/tex\">M_{rh}, M_{rt}</script>的</strong>，映射函数为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\boldsymbol{M}_{r h}=r_{p} \\boldsymbol{h}_{p}^{\\mathrm{T}}+\\boldsymbol{I}^{m \\times n}\\\\\\quad \\boldsymbol{M}_{r t}=r_{p} \\boldsymbol{t}_{p}^{\\mathrm{T}}+\\boldsymbol{I}^{m \\times n}\n\\end{aligned}</script><blockquote>\n<ul>\n<li>为什么构造映射矩阵的时候要加单位矩阵？</li>\n</ul>\n<p>每次构造映射矩阵时，是先将矩阵初始化为单位矩阵，然后再通过向量内积来改变这个矩阵。<strong>个人猜测是为了不要让映射后的空间和原空间相差过大</strong></p>\n</blockquote>\n<ul>\n<li>得到映射矩阵后，进行空间变换<script type=\"math/tex\">\\boldsymbol{h}_{\\perp}=\\boldsymbol{M}_{r h} \\boldsymbol{h}, \\quad \\boldsymbol{t}_{\\perp}=\\boldsymbol{M}_{r t} \\boldsymbol{t}</script>，那么损失函数变为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_{r}(\\boldsymbol{h}+\\boldsymbol{t})=\\left\\|\\boldsymbol{h}_{\\perp}+\\boldsymbol{r}-\\boldsymbol{t}_{\\perp}\\right\\|_{l_{1} / l_{2}}</script><h1 id=\"4-知识表示形式\"><a href=\"#4-知识表示形式\" class=\"headerlink\" title=\"4 知识表示形式\"></a>4 知识表示形式</h1><ul>\n<li>最常见的知识表示形式即前面的三元组形式，但是也有其他的表示形式，如谓词逻辑、产生式规则、框架（Frame）、树形知识表示、概率图等：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215032003.png\" alt=\"image-20230108215032003\" style=\"zoom:80%;\" /></p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215042336.png\" alt=\"image-20230108215042336\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>这里主要介绍一下用马尔可夫随机场（MRF）和马尔可夫逻辑网（MLN）进行表示</li>\n<li><strong>马尔可夫逻辑网是通过软化一阶逻辑实现的</strong>。传统的一阶逻辑知识库（由一阶逻辑命题所组成的知识库）被视作在一系列可能世界 （Possible World）上所施加的一组硬约束（Hard Constraint)，<strong>但是这样的约束太过生硬，有时观察到的规则可能和知识库中的规则冲突</strong>。而MLN旨在软化这些约束，<strong>每条规则都与一个反应其约束强度的权重关联，权重越高，满足和不满足此规则的对数概率差就越大，即这条规则的置信度就越高</strong></li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108215936859.png\" alt=\"image-20230108215936859\"></p>\n<p>其中，<script type=\"math/tex\">Fr(x, y)</script>表示x，y是朋友，<script type=\"math/tex\">Sm(x)</script>表示x抽烟。综上所述，<strong>MLN就是一个（规则，权重）对的集合</strong></p>\n<ul>\n<li>而MLN可以视作定义具体的MRF（马尔可夫随机场）的模板。给定一个MLN（记作<script type=\"math/tex\">L=\\{F_i, w_i\\}</script>）以及一个常量集合（如<script type=\"math/tex\">C=\\{Anna, Bob\\}</script>），就可以定义一个相应的MRF（记作<script type=\"math/tex\">M_{L, C}</script>）。构筑规则如下：</li>\n</ul>\n<blockquote>\n<ul>\n<li><strong>MRF的每个节点对应将MLN规则中经过给定常量实例化（Grounding）而得到的一个谓词</strong>。比如规则<script type=\"math/tex\">\\forall x \\forall y \\forall z \\operatorname{Fr}(x, y) \\wedge \\operatorname{Fr}(y, z) \\Rightarrow \\operatorname{Fr}(x, z)</script>中包含<script type=\"math/tex\">Fr(x, y)</script>，由于给定了<script type=\"math/tex\">C=\\{Anna, Bob\\}</script>，所以可以将其实例化为<script type=\"math/tex\">Fr(Anna, Bob)</script>，这就是一个谓词实例，也是一个二元随机变量（要么为真要么为假），其就对应了MRF中的一个节点</li>\n<li>现在要进一步需要明确MRF中的边：<strong>两个谓词实例之间存在一条边，当且仅当它们至少在一个规则中同时出现。一条规则中的谓词之间形成了马尔可夫随机场中的一个团（不一定是最大团）</strong></li>\n</ul>\n</blockquote>\n<ul>\n<li>通过上图的最后两条规则，并给定<script type=\"math/tex\">C=\\{Anna, Bob\\}</script>，可以构建出如下的MRF：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20230108221652749.png\" alt=\"image-20230108221652749\"  /></p>\n<ul>\n<li>根据得到的<script type=\"math/tex\">M_{L,C}</script>可以进行概率推理，推理所要回答的问题形式是：<strong>一直一条规则成立，则另一条规则成立的概率是多少</strong>。比如已知<script type=\"math/tex\">F_2</script>为<script type=\"math/tex\">Fr(Anna, Bob) \\vee Sm(Anna)</script>，求<script type=\"math/tex\">F_1 = Sm(Bob)</script>的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(F_1 \\mid F_2, M_{L, C}) & =\\frac{P\\left(F_{1} \\wedge F_{2} \\mid M_{L, C}\\right)}{P\\left(F_{2} \\mid M_{L, C}\\right)} \\\\\n& =\\frac{\\sum_{x \\in \\chi_{F_{1}} \\cap \\chi_{F_{2}}} P\\left(X=x \\mid M_{L, C}\\right)}{\\sum_{x \\in \\chi_{F_{2}}} P\\left(X=x \\mid M_{L, C}\\right)}\n\\end{aligned}</script><p>其中<script type=\"math/tex\">\\chi_{F_i}</script>表示<script type=\"math/tex\">F_i</script>成立的世界的集合</p>\n"},{"title":"Markdown & Letax常用语法","math":true,"date":"2021-09-14T16:00:00.000Z","_content":"\n\n\n# 1、标题\n\n```\n1~6级标题\n1~6个#加个空格\n```\n\n\n\n# 2、代码块\n\n```\n​``` + 代码语言\n```\n\n\n\n# 3、字体\n\n```\n加粗：**文本**\n删除线:~~文本~~\n斜体：*文本*\n```\n\n加粗：**文本**\n删除线：~~文本~~\n斜体：*文本*\n\n\n\n# 4、引用\n\n```\n>A\n>>A\n>>>A\n```\n\n>A\n>\n>>A\n>\n>>>A\n\n\n\n# 5、分割线\n\n```\n---\n```\n\n\n\n---\n\n\n\n# 6、图片插入\n\n```\n![名字](本地或网页链接)\n```\n\n\n\n# 7、超链接\n\n```\n[名字](链接)\n```\n\n[百度](www.baidu.com)\n\n\n\n# 8、列表\n\n```\n无序列表\n- (空格) 内容\n有序列表\n数字.(空格)内容\n```\n\n- 无序\n\n1. 有序\n\n\n\n# 9、表格\n\n```\n干脆直接快捷键\n```\n\n\n\n\n\n# 10、Latex\n\n### 希腊字母\n\n| Latex             | 对应形式            | Latex       | 对应形式      |\n| ----------------- | ------------------- | ----------- | ------------- |\n| \\alpha            | $\\alpha$            | \\Alpha      | $\\Alpha$      |\n| \\beta             | $\\beta$             | \\Beta       | $\\Beta$       |\n| \\gamma            | $\\gamma$            | \\Gamma      | $\\Gamma$      |\n| \\delta            | $\\delta$            | \\Delta      | $\\Delta$      |\n| \\epsilon          | $\\epsilon$          | \\Epsilon    | $\\Epsilon$    |\n| \\zeta             | $\\zeta$             | \\Zeta       | $\\Zeta$       |\n| \\eta              | $\\eta$              | \\Eta        | $\\Eta$        |\n| \\theta            | $\\theta$            | \\Theta      | $\\Theta$      |\n| \\lambda           | $\\lambda$           | \\Lambda     | $\\Lambda$     |\n| \\mu               | $\\mu$               | \\Mu         | $\\Mu$         |\n| \\nu               | $\\nu$               | \\Nu         | $\\Mu$         |\n| \\xi               | $\\xi$               | \\Xi         | $\\Xi$         |\n| \\pi               | $\\pi$               | \\Pi         | $\\Pi$         |\n| \\rho              | $\\rho$              | \\Rho        | $\\Rho$        |\n| \\sigma            | $\\sigma$            | \\Sigma      | $\\Sigma$      |\n| \\varphi  //  \\phi | $\\varphi$ // $\\phi$ | \\Phi        | $\\Phi$        |\n| \\chi              | $\\chi$              | \\Chi        | $\\Chi$        |\n| \\psi              | $\\psi$              | \\Psi        | $\\Psi$        |\n| \\omega            | $\\omega$            | \\Omega      | $\\Omega$      |\n| \\ell              | $\\ell$              | \\varepsilon | $\\varepsilon$ |\n\n### 运算符\n\n| 解释                                       | 代码                                   |\n| ------------------------------------------ | -------------------------------------- |\n| 粗体                                       | \\pmb{.....}                            |\n| 单空格                                     | \\quad                                  |\n| 双空格                                     | \\qquad                                 |\n| 换行                                       | \\\\\\                                    |\n| $\\times$                                   | \\times                                 |\n| $\\div$                                     | \\div                                   |\n| $\\pm$                                      | \\pm                                    |\n| 下标                                       | _                                      |\n| 上标                                       | ^                                      |\n| $\\hat{a}$                                  | \\hat{a}                                |\n| $$\\tilde{a}$$                              | \\tilde{a}                              |\n| $\\bar{a}$                                  | \\bar{a}                                |\n| $\\vec{a}$                                  | \\vec{a}                                |\n| $\\overrightarrow{a}$                       | \\overrightarrow{a}                     |\n| $\\overleftarrow{a}$                        | \\overleftarrow{a}                      |\n| $\\log_{32}{xy}$                            | \\log_{32}{xy}                          |\n| { }                                        | 需要转义                               |\n| $\\sum_1^n$                                 | \\sum_1^n                               |\n| $\\prod_{k=1}^nk^2$                         | \\prod_{k=1}^nk^2                       |\n| $\\tilde{H}$                                | \\tilde{H}                              |\n|                                            |                                        |\n| $\\int_a^b$                                 | \\int_a^b                               |\n| $\\iint$                                    | \\iint                                  |\n| $\\iiint$                                   | \\iiint                                 |\n| $\\infty$                                   | \\infty                                 |\n| $\\lim_{x\\to0}$                             | 极限       \\lim_{x\\to0}                |\n| $f^{\\prime}(x)$                            | 导数    f^{\\prime}(x)                  |\n| $$\\int_{\\theta=0}^{2\\pi}    \\int_{r=0}^R$$ | \\int_{\\theta=0}^{2\\pi}    \\int_{r=0}^R |\n| $\\nabla$                                   | \\nabla                                 |\n| $\\partial$                                 | \\partial                               |\n| $\\forall$                                  | \\forall                                |\n| $\\exists$                                  | \\exists                                |\n| $$\\wedge$$                                 | \\wedge                                 |\n| $$\\vee$$                                   | \\vee                                   |\n| $$\\neg$$                                   | \\neg                                   |\n|                                            |                                        |\n| $\\subset$                                  | \\subset                                |\n| $\\subseteq$                                | \\subseteq                              |\n| $\\in$                                      | \\in                                    |\n| $\\notin$                                   | \\notin                                 |\n| $\\emptyset$                                | \\emptyset                              |\n| $\\varnothing$                              | \\varnothing                            |\n| $\\bigcup$                                  | \\bigcup                                |\n| $\\cup$                                     | \\cup                                   |\n| $\\bigcap$                                  | \\bigcap                                |\n| $\\cap$                                     | \\cap                                   |\n|                                            |                                        |\n| $\\frac{a+1}{b+1}$                          | 分数    \\frac{a+1}{b+1}                |\n| $\\sqrt{x^5}$                               | 开方      \\sqrt{x^5}                   |\n| $\\sqrt[3]{xy}$                             | 开方       \\sqrt[3]{xy}                |\n|                                            |                                        |\n| $\\le$                                      | \\le                                    |\n| $\\geq$                                     | \\geq                                   |\n| $\\neq$                                     | \\neq                                   |\n| $\\approx$                                  | \\approx                                |\n| $\\equiv$                                   | \\equiv                                 |\n| $\\pm$                                      | \\pm                                    |\n| $\\mp$                                      | \\mp                                    |\n| $\\mathbb{R}$                               | 将字母改为黑板字体\\mathbb{R}           |\n| $\\mathcal{B}$                              | \\mathcal{B}                            |\n| $\\leftarrow$                               | \\leftarrow                             |\n| $\\rightarrow$                              | \\rightarrow                            |\n| $$\\Leftarrow$$                             | \\Leftarrow                             |\n| $$\\Rightarrow$$                            | \\Rightarrow                            |\n| $$\\sim$$                                   | \\sim                                   |\n| $\\odot$                                    | \\odot                                  |\n| $\\perp$                                    | 独立符号\\perp                          |\n| $$\\propto$$                                | 正比符号\\propto                        |\n\n### 换行\n\n$$\n\\begin{aligned}\nthe \\ first \\ row \\\\\nthe \\ second \\ row\n\\end{aligned}\n$$\n\n```\n$$\n\\begin{aligned}\nthe \\ first \\ row \\\\\nthe \\ second \\ row\n\\end{aligned}\n$$\n```\n\n### 分段函数\n\n$$\nf(x) = \\begin{cases}\n1 & x = 2\\\\\n2 & x > 2\\\\\n3 & x \\leqslant 2\\\\\n\\end{cases}\n$$\n\n```\n$$\nf(x) = \\begin{cases}\n1 & x = 2 \\\\\n2 & x > 2 \\\\\n3 & x \\leqslant 2\\\\\n\\end{cases}\n$$\n```\n\n### 矩阵\n\n$$\n\\begin{matrix}\n1 & 2 \\\\\n3 & 4 \n\\end{matrix}\n$$\n\n```\n$$\n\\begin{matrix}\n1 & 2 \\\\\n3 & 4 \n\\end{matrix}\n$$\n```\n\n\n\n\n$$\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{pmatrix}\n$$\n\n```\n$$\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{pmatrix}\n$$\n```\n\n\n\n\n$$\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{bmatrix}\n$$\n\n```\n$$\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{bmatrix}\n$$\n```\n\n\n\n\n$$\n\\begin{Bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Bmatrix}\n$$\n\n```\n$$\n\\begin{Bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Bmatrix}\n$$\n```\n\n\n\n\n$$\n\\begin{vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{vmatrix}\n$$\n\n```\n\\begin{vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{vmatrix}\n```\n\n\n\n\n$$\n\\begin{Vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Vmatrix}\n$$\n\n```\n\\begin{Vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Vmatrix}\n```\n\n","source":"_posts/语法.md","raw":"---\ntitle: Markdown & Letax常用语法\nmath: true\ndate: 2021-9-15\n---\n\n\n\n# 1、标题\n\n```\n1~6级标题\n1~6个#加个空格\n```\n\n\n\n# 2、代码块\n\n```\n​``` + 代码语言\n```\n\n\n\n# 3、字体\n\n```\n加粗：**文本**\n删除线:~~文本~~\n斜体：*文本*\n```\n\n加粗：**文本**\n删除线：~~文本~~\n斜体：*文本*\n\n\n\n# 4、引用\n\n```\n>A\n>>A\n>>>A\n```\n\n>A\n>\n>>A\n>\n>>>A\n\n\n\n# 5、分割线\n\n```\n---\n```\n\n\n\n---\n\n\n\n# 6、图片插入\n\n```\n![名字](本地或网页链接)\n```\n\n\n\n# 7、超链接\n\n```\n[名字](链接)\n```\n\n[百度](www.baidu.com)\n\n\n\n# 8、列表\n\n```\n无序列表\n- (空格) 内容\n有序列表\n数字.(空格)内容\n```\n\n- 无序\n\n1. 有序\n\n\n\n# 9、表格\n\n```\n干脆直接快捷键\n```\n\n\n\n\n\n# 10、Latex\n\n### 希腊字母\n\n| Latex             | 对应形式            | Latex       | 对应形式      |\n| ----------------- | ------------------- | ----------- | ------------- |\n| \\alpha            | $\\alpha$            | \\Alpha      | $\\Alpha$      |\n| \\beta             | $\\beta$             | \\Beta       | $\\Beta$       |\n| \\gamma            | $\\gamma$            | \\Gamma      | $\\Gamma$      |\n| \\delta            | $\\delta$            | \\Delta      | $\\Delta$      |\n| \\epsilon          | $\\epsilon$          | \\Epsilon    | $\\Epsilon$    |\n| \\zeta             | $\\zeta$             | \\Zeta       | $\\Zeta$       |\n| \\eta              | $\\eta$              | \\Eta        | $\\Eta$        |\n| \\theta            | $\\theta$            | \\Theta      | $\\Theta$      |\n| \\lambda           | $\\lambda$           | \\Lambda     | $\\Lambda$     |\n| \\mu               | $\\mu$               | \\Mu         | $\\Mu$         |\n| \\nu               | $\\nu$               | \\Nu         | $\\Mu$         |\n| \\xi               | $\\xi$               | \\Xi         | $\\Xi$         |\n| \\pi               | $\\pi$               | \\Pi         | $\\Pi$         |\n| \\rho              | $\\rho$              | \\Rho        | $\\Rho$        |\n| \\sigma            | $\\sigma$            | \\Sigma      | $\\Sigma$      |\n| \\varphi  //  \\phi | $\\varphi$ // $\\phi$ | \\Phi        | $\\Phi$        |\n| \\chi              | $\\chi$              | \\Chi        | $\\Chi$        |\n| \\psi              | $\\psi$              | \\Psi        | $\\Psi$        |\n| \\omega            | $\\omega$            | \\Omega      | $\\Omega$      |\n| \\ell              | $\\ell$              | \\varepsilon | $\\varepsilon$ |\n\n### 运算符\n\n| 解释                                       | 代码                                   |\n| ------------------------------------------ | -------------------------------------- |\n| 粗体                                       | \\pmb{.....}                            |\n| 单空格                                     | \\quad                                  |\n| 双空格                                     | \\qquad                                 |\n| 换行                                       | \\\\\\                                    |\n| $\\times$                                   | \\times                                 |\n| $\\div$                                     | \\div                                   |\n| $\\pm$                                      | \\pm                                    |\n| 下标                                       | _                                      |\n| 上标                                       | ^                                      |\n| $\\hat{a}$                                  | \\hat{a}                                |\n| $$\\tilde{a}$$                              | \\tilde{a}                              |\n| $\\bar{a}$                                  | \\bar{a}                                |\n| $\\vec{a}$                                  | \\vec{a}                                |\n| $\\overrightarrow{a}$                       | \\overrightarrow{a}                     |\n| $\\overleftarrow{a}$                        | \\overleftarrow{a}                      |\n| $\\log_{32}{xy}$                            | \\log_{32}{xy}                          |\n| { }                                        | 需要转义                               |\n| $\\sum_1^n$                                 | \\sum_1^n                               |\n| $\\prod_{k=1}^nk^2$                         | \\prod_{k=1}^nk^2                       |\n| $\\tilde{H}$                                | \\tilde{H}                              |\n|                                            |                                        |\n| $\\int_a^b$                                 | \\int_a^b                               |\n| $\\iint$                                    | \\iint                                  |\n| $\\iiint$                                   | \\iiint                                 |\n| $\\infty$                                   | \\infty                                 |\n| $\\lim_{x\\to0}$                             | 极限       \\lim_{x\\to0}                |\n| $f^{\\prime}(x)$                            | 导数    f^{\\prime}(x)                  |\n| $$\\int_{\\theta=0}^{2\\pi}    \\int_{r=0}^R$$ | \\int_{\\theta=0}^{2\\pi}    \\int_{r=0}^R |\n| $\\nabla$                                   | \\nabla                                 |\n| $\\partial$                                 | \\partial                               |\n| $\\forall$                                  | \\forall                                |\n| $\\exists$                                  | \\exists                                |\n| $$\\wedge$$                                 | \\wedge                                 |\n| $$\\vee$$                                   | \\vee                                   |\n| $$\\neg$$                                   | \\neg                                   |\n|                                            |                                        |\n| $\\subset$                                  | \\subset                                |\n| $\\subseteq$                                | \\subseteq                              |\n| $\\in$                                      | \\in                                    |\n| $\\notin$                                   | \\notin                                 |\n| $\\emptyset$                                | \\emptyset                              |\n| $\\varnothing$                              | \\varnothing                            |\n| $\\bigcup$                                  | \\bigcup                                |\n| $\\cup$                                     | \\cup                                   |\n| $\\bigcap$                                  | \\bigcap                                |\n| $\\cap$                                     | \\cap                                   |\n|                                            |                                        |\n| $\\frac{a+1}{b+1}$                          | 分数    \\frac{a+1}{b+1}                |\n| $\\sqrt{x^5}$                               | 开方      \\sqrt{x^5}                   |\n| $\\sqrt[3]{xy}$                             | 开方       \\sqrt[3]{xy}                |\n|                                            |                                        |\n| $\\le$                                      | \\le                                    |\n| $\\geq$                                     | \\geq                                   |\n| $\\neq$                                     | \\neq                                   |\n| $\\approx$                                  | \\approx                                |\n| $\\equiv$                                   | \\equiv                                 |\n| $\\pm$                                      | \\pm                                    |\n| $\\mp$                                      | \\mp                                    |\n| $\\mathbb{R}$                               | 将字母改为黑板字体\\mathbb{R}           |\n| $\\mathcal{B}$                              | \\mathcal{B}                            |\n| $\\leftarrow$                               | \\leftarrow                             |\n| $\\rightarrow$                              | \\rightarrow                            |\n| $$\\Leftarrow$$                             | \\Leftarrow                             |\n| $$\\Rightarrow$$                            | \\Rightarrow                            |\n| $$\\sim$$                                   | \\sim                                   |\n| $\\odot$                                    | \\odot                                  |\n| $\\perp$                                    | 独立符号\\perp                          |\n| $$\\propto$$                                | 正比符号\\propto                        |\n\n### 换行\n\n$$\n\\begin{aligned}\nthe \\ first \\ row \\\\\nthe \\ second \\ row\n\\end{aligned}\n$$\n\n```\n$$\n\\begin{aligned}\nthe \\ first \\ row \\\\\nthe \\ second \\ row\n\\end{aligned}\n$$\n```\n\n### 分段函数\n\n$$\nf(x) = \\begin{cases}\n1 & x = 2\\\\\n2 & x > 2\\\\\n3 & x \\leqslant 2\\\\\n\\end{cases}\n$$\n\n```\n$$\nf(x) = \\begin{cases}\n1 & x = 2 \\\\\n2 & x > 2 \\\\\n3 & x \\leqslant 2\\\\\n\\end{cases}\n$$\n```\n\n### 矩阵\n\n$$\n\\begin{matrix}\n1 & 2 \\\\\n3 & 4 \n\\end{matrix}\n$$\n\n```\n$$\n\\begin{matrix}\n1 & 2 \\\\\n3 & 4 \n\\end{matrix}\n$$\n```\n\n\n\n\n$$\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{pmatrix}\n$$\n\n```\n$$\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{pmatrix}\n$$\n```\n\n\n\n\n$$\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{bmatrix}\n$$\n\n```\n$$\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{bmatrix}\n$$\n```\n\n\n\n\n$$\n\\begin{Bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Bmatrix}\n$$\n\n```\n$$\n\\begin{Bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Bmatrix}\n$$\n```\n\n\n\n\n$$\n\\begin{vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{vmatrix}\n$$\n\n```\n\\begin{vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{vmatrix}\n```\n\n\n\n\n$$\n\\begin{Vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Vmatrix}\n$$\n\n```\n\\begin{Vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Vmatrix}\n```\n\n","slug":"语法","published":1,"updated":"2023-01-08T14:24:10.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1o000o7cszhl7chckm","content":"<h1 id=\"1、标题\"><a href=\"#1、标题\" class=\"headerlink\" title=\"1、标题\"></a>1、标题</h1><figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs apache\"><span class=\"hljs-attribute\">1</span>~<span class=\"hljs-number\">6</span>级标题<br><span class=\"hljs-attribute\">1</span>~<span class=\"hljs-number\">6</span>个#加个空格<br></code></pre></td></tr></table></figure>\n<h1 id=\"2、代码块\"><a href=\"#2、代码块\" class=\"headerlink\" title=\"2、代码块\"></a>2、代码块</h1><figure class=\"highlight autohotkey\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs autohotkey\">​``` + 代码语言<br></code></pre></td></tr></table></figure>\n<h1 id=\"3、字体\"><a href=\"#3、字体\" class=\"headerlink\" title=\"3、字体\"></a>3、字体</h1><figure class=\"highlight asciidoc\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs asciidoc\">加粗：<span class=\"hljs-strong\">**文本**</span><br>删除线:~~文本~~<br>斜体：<span class=\"hljs-strong\">*文本*</span><br></code></pre></td></tr></table></figure>\n<p>加粗：<strong>文本</strong><br>删除线：<del>文本</del><br>斜体：<em>文本</em></p>\n<h1 id=\"4、引用\"><a href=\"#4、引用\" class=\"headerlink\" title=\"4、引用\"></a>4、引用</h1><figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs css\">&gt;<span class=\"hljs-selector-tag\">A</span><br>&gt;&gt;<span class=\"hljs-selector-tag\">A</span><br>&gt;&gt;&gt;<span class=\"hljs-selector-tag\">A</span><br></code></pre></td></tr></table></figure>\n<blockquote>\n<p>A</p>\n<blockquote>\n<p>A</p>\n<blockquote>\n<p>A</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<h1 id=\"5、分割线\"><a href=\"#5、分割线\" class=\"headerlink\" title=\"5、分割线\"></a>5、分割线</h1><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs yaml\"><span class=\"hljs-meta\">---</span><br></code></pre></td></tr></table></figure>\n<hr>\n<h1 id=\"6、图片插入\"><a href=\"#6、图片插入\" class=\"headerlink\" title=\"6、图片插入\"></a>6、图片插入</h1><figure class=\"highlight less\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs less\">!<span class=\"hljs-selector-attr\">[名字]</span>(本地或网页链接)<br></code></pre></td></tr></table></figure>\n<h1 id=\"7、超链接\"><a href=\"#7、超链接\" class=\"headerlink\" title=\"7、超链接\"></a>7、超链接</h1><figure class=\"highlight clojure\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs clojure\">[名字](链接)<br></code></pre></td></tr></table></figure>\n<p><a href=\"www.baidu.com\">百度</a></p>\n<h1 id=\"8、列表\"><a href=\"#8、列表\" class=\"headerlink\" title=\"8、列表\"></a>8、列表</h1><figure class=\"highlight gcode\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gcode\">无序列表<br>- <span class=\"hljs-comment\">(空格)</span> 内容<br>有序列表<br>数字.<span class=\"hljs-comment\">(空格)</span>内容<br></code></pre></td></tr></table></figure>\n<ul>\n<li>无序</li>\n</ul>\n<ol>\n<li>有序</li>\n</ol>\n<h1 id=\"9、表格\"><a href=\"#9、表格\" class=\"headerlink\" title=\"9、表格\"></a>9、表格</h1><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs\">干脆直接快捷键<br></code></pre></td></tr></table></figure>\n<h1 id=\"10、Latex\"><a href=\"#10、Latex\" class=\"headerlink\" title=\"10、Latex\"></a>10、Latex</h1><h3 id=\"希腊字母\"><a href=\"#希腊字母\" class=\"headerlink\" title=\"希腊字母\"></a>希腊字母</h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Latex</th>\n<th>对应形式</th>\n<th>Latex</th>\n<th>对应形式</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\\alpha</td>\n<td>$\\alpha$</td>\n<td>\\Alpha</td>\n<td>$\\Alpha$</td>\n</tr>\n<tr>\n<td>\\beta</td>\n<td>$\\beta$</td>\n<td>\\Beta</td>\n<td>$\\Beta$</td>\n</tr>\n<tr>\n<td>\\gamma</td>\n<td>$\\gamma$</td>\n<td>\\Gamma</td>\n<td>$\\Gamma$</td>\n</tr>\n<tr>\n<td>\\delta</td>\n<td>$\\delta$</td>\n<td>\\Delta</td>\n<td>$\\Delta$</td>\n</tr>\n<tr>\n<td>\\epsilon</td>\n<td>$\\epsilon$</td>\n<td>\\Epsilon</td>\n<td>$\\Epsilon$</td>\n</tr>\n<tr>\n<td>\\zeta</td>\n<td>$\\zeta$</td>\n<td>\\Zeta</td>\n<td>$\\Zeta$</td>\n</tr>\n<tr>\n<td>\\eta</td>\n<td>$\\eta$</td>\n<td>\\Eta</td>\n<td>$\\Eta$</td>\n</tr>\n<tr>\n<td>\\theta</td>\n<td>$\\theta$</td>\n<td>\\Theta</td>\n<td>$\\Theta$</td>\n</tr>\n<tr>\n<td>\\lambda</td>\n<td>$\\lambda$</td>\n<td>\\Lambda</td>\n<td>$\\Lambda$</td>\n</tr>\n<tr>\n<td>\\mu</td>\n<td>$\\mu$</td>\n<td>\\Mu</td>\n<td>$\\Mu$</td>\n</tr>\n<tr>\n<td>\\nu</td>\n<td>$\\nu$</td>\n<td>\\Nu</td>\n<td>$\\Mu$</td>\n</tr>\n<tr>\n<td>\\xi</td>\n<td>$\\xi$</td>\n<td>\\Xi</td>\n<td>$\\Xi$</td>\n</tr>\n<tr>\n<td>\\pi</td>\n<td>$\\pi$</td>\n<td>\\Pi</td>\n<td>$\\Pi$</td>\n</tr>\n<tr>\n<td>\\rho</td>\n<td>$\\rho$</td>\n<td>\\Rho</td>\n<td>$\\Rho$</td>\n</tr>\n<tr>\n<td>\\sigma</td>\n<td>$\\sigma$</td>\n<td>\\Sigma</td>\n<td>$\\Sigma$</td>\n</tr>\n<tr>\n<td>\\varphi  //  \\phi</td>\n<td>$\\varphi$ // $\\phi$</td>\n<td>\\Phi</td>\n<td>$\\Phi$</td>\n</tr>\n<tr>\n<td>\\chi</td>\n<td>$\\chi$</td>\n<td>\\Chi</td>\n<td>$\\Chi$</td>\n</tr>\n<tr>\n<td>\\psi</td>\n<td>$\\psi$</td>\n<td>\\Psi</td>\n<td>$\\Psi$</td>\n</tr>\n<tr>\n<td>\\omega</td>\n<td>$\\omega$</td>\n<td>\\Omega</td>\n<td>$\\Omega$</td>\n</tr>\n<tr>\n<td>\\ell</td>\n<td>$\\ell$</td>\n<td>\\varepsilon</td>\n<td>$\\varepsilon$</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"运算符\"><a href=\"#运算符\" class=\"headerlink\" title=\"运算符\"></a>运算符</h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>解释</th>\n<th>代码</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>粗体</td>\n<td>\\pmb{…..}</td>\n</tr>\n<tr>\n<td>单空格</td>\n<td>\\quad</td>\n</tr>\n<tr>\n<td>双空格</td>\n<td>\\qquad</td>\n</tr>\n<tr>\n<td>换行</td>\n<td>\\\\</td>\n</tr>\n<tr>\n<td>$\\times$</td>\n<td>\\times</td>\n</tr>\n<tr>\n<td>$\\div$</td>\n<td>\\div</td>\n</tr>\n<tr>\n<td>$\\pm$</td>\n<td>\\pm</td>\n</tr>\n<tr>\n<td>下标</td>\n<td>_</td>\n</tr>\n<tr>\n<td>上标</td>\n<td>^</td>\n</tr>\n<tr>\n<td>$\\hat{a}$</td>\n<td>\\hat{a}</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\tilde{a}</script></td>\n<td>\\tilde{a}</td>\n</tr>\n<tr>\n<td>$\\bar{a}$</td>\n<td>\\bar{a}</td>\n</tr>\n<tr>\n<td>$\\vec{a}$</td>\n<td>\\vec{a}</td>\n</tr>\n<tr>\n<td>$\\overrightarrow{a}$</td>\n<td>\\overrightarrow{a}</td>\n</tr>\n<tr>\n<td>$\\overleftarrow{a}$</td>\n<td>\\overleftarrow{a}</td>\n</tr>\n<tr>\n<td>$\\log_{32}{xy}$</td>\n<td>\\log_{32}{xy}</td>\n</tr>\n<tr>\n<td>{ }</td>\n<td>需要转义</td>\n</tr>\n<tr>\n<td>$\\sum_1^n$</td>\n<td>\\sum_1^n</td>\n</tr>\n<tr>\n<td>$\\prod_{k=1}^nk^2$</td>\n<td>\\prod_{k=1}^nk^2</td>\n</tr>\n<tr>\n<td>$\\tilde{H}$</td>\n<td>\\tilde{H}</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$\\int_a^b$</td>\n<td>\\int_a^b</td>\n</tr>\n<tr>\n<td>$\\iint$</td>\n<td>\\iint</td>\n</tr>\n<tr>\n<td>$\\iiint$</td>\n<td>\\iiint</td>\n</tr>\n<tr>\n<td>$\\infty$</td>\n<td>\\infty</td>\n</tr>\n<tr>\n<td>$\\lim_{x\\to0}$</td>\n<td>极限       \\lim_{x\\to0}</td>\n</tr>\n<tr>\n<td>$f^{\\prime}(x)$</td>\n<td>导数    f^{\\prime}(x)</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\int_{\\theta=0}^{2\\pi}    \\int_{r=0}^R</script></td>\n<td>\\int<em>{\\theta=0}^{2\\pi}    \\int</em>{r=0}^R</td>\n</tr>\n<tr>\n<td>$\\nabla$</td>\n<td>\\nabla</td>\n</tr>\n<tr>\n<td>$\\partial$</td>\n<td>\\partial</td>\n</tr>\n<tr>\n<td>$\\forall$</td>\n<td>\\forall</td>\n</tr>\n<tr>\n<td>$\\exists$</td>\n<td>\\exists</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\wedge</script></td>\n<td>\\wedge</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\vee</script></td>\n<td>\\vee</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\neg</script></td>\n<td>\\neg</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$\\subset$</td>\n<td>\\subset</td>\n</tr>\n<tr>\n<td>$\\subseteq$</td>\n<td>\\subseteq</td>\n</tr>\n<tr>\n<td>$\\in$</td>\n<td>\\in</td>\n</tr>\n<tr>\n<td>$\\notin$</td>\n<td>\\notin</td>\n</tr>\n<tr>\n<td>$\\emptyset$</td>\n<td>\\emptyset</td>\n</tr>\n<tr>\n<td>$\\varnothing$</td>\n<td>\\varnothing</td>\n</tr>\n<tr>\n<td>$\\bigcup$</td>\n<td>\\bigcup</td>\n</tr>\n<tr>\n<td>$\\cup$</td>\n<td>\\cup</td>\n</tr>\n<tr>\n<td>$\\bigcap$</td>\n<td>\\bigcap</td>\n</tr>\n<tr>\n<td>$\\cap$</td>\n<td>\\cap</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$\\frac{a+1}{b+1}$</td>\n<td>分数    \\frac{a+1}{b+1}</td>\n</tr>\n<tr>\n<td>$\\sqrt{x^5}$</td>\n<td>开方      \\sqrt{x^5}</td>\n</tr>\n<tr>\n<td>$\\sqrt[3]{xy}$</td>\n<td>开方       \\sqrt[3]{xy}</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$\\le$</td>\n<td>\\le</td>\n</tr>\n<tr>\n<td>$\\geq$</td>\n<td>\\geq</td>\n</tr>\n<tr>\n<td>$\\neq$</td>\n<td>\\neq</td>\n</tr>\n<tr>\n<td>$\\approx$</td>\n<td>\\approx</td>\n</tr>\n<tr>\n<td>$\\equiv$</td>\n<td>\\equiv</td>\n</tr>\n<tr>\n<td>$\\pm$</td>\n<td>\\pm</td>\n</tr>\n<tr>\n<td>$\\mp$</td>\n<td>\\mp</td>\n</tr>\n<tr>\n<td>$\\mathbb{R}$</td>\n<td>将字母改为黑板字体\\mathbb{R}</td>\n</tr>\n<tr>\n<td>$\\mathcal{B}$</td>\n<td>\\mathcal{B}</td>\n</tr>\n<tr>\n<td>$\\leftarrow$</td>\n<td>\\leftarrow</td>\n</tr>\n<tr>\n<td>$\\rightarrow$</td>\n<td>\\rightarrow</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\Leftarrow</script></td>\n<td>\\Leftarrow</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\Rightarrow</script></td>\n<td>\\Rightarrow</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\sim</script></td>\n<td>\\sim</td>\n</tr>\n<tr>\n<td>$\\odot$</td>\n<td>\\odot</td>\n</tr>\n<tr>\n<td>$\\perp$</td>\n<td>独立符号\\perp</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\propto</script></td>\n<td>正比符号\\propto</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"换行\"><a href=\"#换行\" class=\"headerlink\" title=\"换行\"></a>换行</h3><script type=\"math/tex; mode=display\">\n\\begin{aligned}\nthe \\ first \\ row \\\\\nthe \\ second \\ row\n\\end{aligned}</script><figure class=\"highlight livescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs livescript\">$$<br><span class=\"hljs-string\">\\begin&#123;aligned&#125;</span><br>the <span class=\"hljs-string\">\\</span> first <span class=\"hljs-string\">\\</span> row <span class=\"hljs-string\">\\\\</span><br>the <span class=\"hljs-string\">\\</span> second <span class=\"hljs-string\">\\</span> row<br><span class=\"hljs-string\">\\end&#123;aligned&#125;</span><br>$$<br></code></pre></td></tr></table></figure>\n<h3 id=\"分段函数\"><a href=\"#分段函数\" class=\"headerlink\" title=\"分段函数\"></a>分段函数</h3><script type=\"math/tex; mode=display\">\nf(x) = \\begin{cases}\n1 & x = 2\\\\\n2 & x > 2\\\\\n3 & x \\leqslant 2\\\\\n\\end{cases}</script><figure class=\"highlight livescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs livescript\">$$<br>f(x) = <span class=\"hljs-string\">\\begin&#123;cases&#125;</span><br><span class=\"hljs-number\">1</span> &amp; x = <span class=\"hljs-number\">2</span> <span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-number\">2</span> &amp; x &gt; <span class=\"hljs-number\">2</span> <span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-number\">3</span> &amp; x <span class=\"hljs-string\">\\leqslant</span> <span class=\"hljs-number\">2</span><span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-string\">\\end&#123;cases&#125;</span><br>$$<br></code></pre></td></tr></table></figure>\n<h3 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h3><script type=\"math/tex; mode=display\">\n\\begin{matrix}\n1 & 2 \\\\\n3 & 4 \n\\end{matrix}</script><figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs elixir\"><span class=\"hljs-variable\">$$</span><br>\\<span class=\"hljs-keyword\">begin</span>&#123;matrix&#125;<br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> \\\\<br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br>\\<span class=\"hljs-keyword\">end</span>&#123;matrix&#125;<br><span class=\"hljs-variable\">$$</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{pmatrix}</script><figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs elixir\"><span class=\"hljs-variable\">$$</span><br>\\<span class=\"hljs-keyword\">begin</span>&#123;pmatrix&#125;<br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> \\\\<br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br>\\<span class=\"hljs-keyword\">end</span>&#123;pmatrix&#125;<br><span class=\"hljs-variable\">$$</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{bmatrix}</script><figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs elixir\"><span class=\"hljs-variable\">$$</span><br>\\<span class=\"hljs-keyword\">begin</span>&#123;bmatrix&#125;<br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> \\\\<br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br>\\<span class=\"hljs-keyword\">end</span>&#123;bmatrix&#125;<br><span class=\"hljs-variable\">$$</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{Bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Bmatrix}</script><figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs elixir\"><span class=\"hljs-variable\">$$</span><br>\\<span class=\"hljs-keyword\">begin</span>&#123;Bmatrix&#125;<br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> \\\\<br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br>\\<span class=\"hljs-keyword\">end</span>&#123;Bmatrix&#125;<br><span class=\"hljs-variable\">$$</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{vmatrix}</script><figure class=\"highlight livescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs livescript\"><span class=\"hljs-string\">\\begin&#123;vmatrix&#125;</span><br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> <span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br><span class=\"hljs-string\">\\end&#123;vmatrix&#125;</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{Vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Vmatrix}</script><figure class=\"highlight livescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs livescript\"><span class=\"hljs-string\">\\begin&#123;Vmatrix&#125;</span><br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> <span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br><span class=\"hljs-string\">\\end&#123;Vmatrix&#125;</span><br></code></pre></td></tr></table></figure>\n","site":{"data":{}},"wordcount":2814,"excerpt":"","more":"<h1 id=\"1、标题\"><a href=\"#1、标题\" class=\"headerlink\" title=\"1、标题\"></a>1、标题</h1><figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs apache\"><span class=\"hljs-attribute\">1</span>~<span class=\"hljs-number\">6</span>级标题<br><span class=\"hljs-attribute\">1</span>~<span class=\"hljs-number\">6</span>个#加个空格<br></code></pre></td></tr></table></figure>\n<h1 id=\"2、代码块\"><a href=\"#2、代码块\" class=\"headerlink\" title=\"2、代码块\"></a>2、代码块</h1><figure class=\"highlight autohotkey\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs autohotkey\">​``` + 代码语言<br></code></pre></td></tr></table></figure>\n<h1 id=\"3、字体\"><a href=\"#3、字体\" class=\"headerlink\" title=\"3、字体\"></a>3、字体</h1><figure class=\"highlight asciidoc\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs asciidoc\">加粗：<span class=\"hljs-strong\">**文本**</span><br>删除线:~~文本~~<br>斜体：<span class=\"hljs-strong\">*文本*</span><br></code></pre></td></tr></table></figure>\n<p>加粗：<strong>文本</strong><br>删除线：<del>文本</del><br>斜体：<em>文本</em></p>\n<h1 id=\"4、引用\"><a href=\"#4、引用\" class=\"headerlink\" title=\"4、引用\"></a>4、引用</h1><figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs css\">&gt;<span class=\"hljs-selector-tag\">A</span><br>&gt;&gt;<span class=\"hljs-selector-tag\">A</span><br>&gt;&gt;&gt;<span class=\"hljs-selector-tag\">A</span><br></code></pre></td></tr></table></figure>\n<blockquote>\n<p>A</p>\n<blockquote>\n<p>A</p>\n<blockquote>\n<p>A</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<h1 id=\"5、分割线\"><a href=\"#5、分割线\" class=\"headerlink\" title=\"5、分割线\"></a>5、分割线</h1><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs yaml\"><span class=\"hljs-meta\">---</span><br></code></pre></td></tr></table></figure>\n<hr>\n<h1 id=\"6、图片插入\"><a href=\"#6、图片插入\" class=\"headerlink\" title=\"6、图片插入\"></a>6、图片插入</h1><figure class=\"highlight less\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs less\">!<span class=\"hljs-selector-attr\">[名字]</span>(本地或网页链接)<br></code></pre></td></tr></table></figure>\n<h1 id=\"7、超链接\"><a href=\"#7、超链接\" class=\"headerlink\" title=\"7、超链接\"></a>7、超链接</h1><figure class=\"highlight clojure\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs clojure\">[名字](链接)<br></code></pre></td></tr></table></figure>\n<p><a href=\"www.baidu.com\">百度</a></p>\n<h1 id=\"8、列表\"><a href=\"#8、列表\" class=\"headerlink\" title=\"8、列表\"></a>8、列表</h1><figure class=\"highlight gcode\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gcode\">无序列表<br>- <span class=\"hljs-comment\">(空格)</span> 内容<br>有序列表<br>数字.<span class=\"hljs-comment\">(空格)</span>内容<br></code></pre></td></tr></table></figure>\n<ul>\n<li>无序</li>\n</ul>\n<ol>\n<li>有序</li>\n</ol>\n<h1 id=\"9、表格\"><a href=\"#9、表格\" class=\"headerlink\" title=\"9、表格\"></a>9、表格</h1><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs\">干脆直接快捷键<br></code></pre></td></tr></table></figure>\n<h1 id=\"10、Latex\"><a href=\"#10、Latex\" class=\"headerlink\" title=\"10、Latex\"></a>10、Latex</h1><h3 id=\"希腊字母\"><a href=\"#希腊字母\" class=\"headerlink\" title=\"希腊字母\"></a>希腊字母</h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Latex</th>\n<th>对应形式</th>\n<th>Latex</th>\n<th>对应形式</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\\alpha</td>\n<td>$\\alpha$</td>\n<td>\\Alpha</td>\n<td>$\\Alpha$</td>\n</tr>\n<tr>\n<td>\\beta</td>\n<td>$\\beta$</td>\n<td>\\Beta</td>\n<td>$\\Beta$</td>\n</tr>\n<tr>\n<td>\\gamma</td>\n<td>$\\gamma$</td>\n<td>\\Gamma</td>\n<td>$\\Gamma$</td>\n</tr>\n<tr>\n<td>\\delta</td>\n<td>$\\delta$</td>\n<td>\\Delta</td>\n<td>$\\Delta$</td>\n</tr>\n<tr>\n<td>\\epsilon</td>\n<td>$\\epsilon$</td>\n<td>\\Epsilon</td>\n<td>$\\Epsilon$</td>\n</tr>\n<tr>\n<td>\\zeta</td>\n<td>$\\zeta$</td>\n<td>\\Zeta</td>\n<td>$\\Zeta$</td>\n</tr>\n<tr>\n<td>\\eta</td>\n<td>$\\eta$</td>\n<td>\\Eta</td>\n<td>$\\Eta$</td>\n</tr>\n<tr>\n<td>\\theta</td>\n<td>$\\theta$</td>\n<td>\\Theta</td>\n<td>$\\Theta$</td>\n</tr>\n<tr>\n<td>\\lambda</td>\n<td>$\\lambda$</td>\n<td>\\Lambda</td>\n<td>$\\Lambda$</td>\n</tr>\n<tr>\n<td>\\mu</td>\n<td>$\\mu$</td>\n<td>\\Mu</td>\n<td>$\\Mu$</td>\n</tr>\n<tr>\n<td>\\nu</td>\n<td>$\\nu$</td>\n<td>\\Nu</td>\n<td>$\\Mu$</td>\n</tr>\n<tr>\n<td>\\xi</td>\n<td>$\\xi$</td>\n<td>\\Xi</td>\n<td>$\\Xi$</td>\n</tr>\n<tr>\n<td>\\pi</td>\n<td>$\\pi$</td>\n<td>\\Pi</td>\n<td>$\\Pi$</td>\n</tr>\n<tr>\n<td>\\rho</td>\n<td>$\\rho$</td>\n<td>\\Rho</td>\n<td>$\\Rho$</td>\n</tr>\n<tr>\n<td>\\sigma</td>\n<td>$\\sigma$</td>\n<td>\\Sigma</td>\n<td>$\\Sigma$</td>\n</tr>\n<tr>\n<td>\\varphi  //  \\phi</td>\n<td>$\\varphi$ // $\\phi$</td>\n<td>\\Phi</td>\n<td>$\\Phi$</td>\n</tr>\n<tr>\n<td>\\chi</td>\n<td>$\\chi$</td>\n<td>\\Chi</td>\n<td>$\\Chi$</td>\n</tr>\n<tr>\n<td>\\psi</td>\n<td>$\\psi$</td>\n<td>\\Psi</td>\n<td>$\\Psi$</td>\n</tr>\n<tr>\n<td>\\omega</td>\n<td>$\\omega$</td>\n<td>\\Omega</td>\n<td>$\\Omega$</td>\n</tr>\n<tr>\n<td>\\ell</td>\n<td>$\\ell$</td>\n<td>\\varepsilon</td>\n<td>$\\varepsilon$</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"运算符\"><a href=\"#运算符\" class=\"headerlink\" title=\"运算符\"></a>运算符</h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>解释</th>\n<th>代码</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>粗体</td>\n<td>\\pmb{…..}</td>\n</tr>\n<tr>\n<td>单空格</td>\n<td>\\quad</td>\n</tr>\n<tr>\n<td>双空格</td>\n<td>\\qquad</td>\n</tr>\n<tr>\n<td>换行</td>\n<td>\\\\</td>\n</tr>\n<tr>\n<td>$\\times$</td>\n<td>\\times</td>\n</tr>\n<tr>\n<td>$\\div$</td>\n<td>\\div</td>\n</tr>\n<tr>\n<td>$\\pm$</td>\n<td>\\pm</td>\n</tr>\n<tr>\n<td>下标</td>\n<td>_</td>\n</tr>\n<tr>\n<td>上标</td>\n<td>^</td>\n</tr>\n<tr>\n<td>$\\hat{a}$</td>\n<td>\\hat{a}</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\tilde{a}</script></td>\n<td>\\tilde{a}</td>\n</tr>\n<tr>\n<td>$\\bar{a}$</td>\n<td>\\bar{a}</td>\n</tr>\n<tr>\n<td>$\\vec{a}$</td>\n<td>\\vec{a}</td>\n</tr>\n<tr>\n<td>$\\overrightarrow{a}$</td>\n<td>\\overrightarrow{a}</td>\n</tr>\n<tr>\n<td>$\\overleftarrow{a}$</td>\n<td>\\overleftarrow{a}</td>\n</tr>\n<tr>\n<td>$\\log_{32}{xy}$</td>\n<td>\\log_{32}{xy}</td>\n</tr>\n<tr>\n<td>{ }</td>\n<td>需要转义</td>\n</tr>\n<tr>\n<td>$\\sum_1^n$</td>\n<td>\\sum_1^n</td>\n</tr>\n<tr>\n<td>$\\prod_{k=1}^nk^2$</td>\n<td>\\prod_{k=1}^nk^2</td>\n</tr>\n<tr>\n<td>$\\tilde{H}$</td>\n<td>\\tilde{H}</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$\\int_a^b$</td>\n<td>\\int_a^b</td>\n</tr>\n<tr>\n<td>$\\iint$</td>\n<td>\\iint</td>\n</tr>\n<tr>\n<td>$\\iiint$</td>\n<td>\\iiint</td>\n</tr>\n<tr>\n<td>$\\infty$</td>\n<td>\\infty</td>\n</tr>\n<tr>\n<td>$\\lim_{x\\to0}$</td>\n<td>极限       \\lim_{x\\to0}</td>\n</tr>\n<tr>\n<td>$f^{\\prime}(x)$</td>\n<td>导数    f^{\\prime}(x)</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\int_{\\theta=0}^{2\\pi}    \\int_{r=0}^R</script></td>\n<td>\\int<em>{\\theta=0}^{2\\pi}    \\int</em>{r=0}^R</td>\n</tr>\n<tr>\n<td>$\\nabla$</td>\n<td>\\nabla</td>\n</tr>\n<tr>\n<td>$\\partial$</td>\n<td>\\partial</td>\n</tr>\n<tr>\n<td>$\\forall$</td>\n<td>\\forall</td>\n</tr>\n<tr>\n<td>$\\exists$</td>\n<td>\\exists</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\wedge</script></td>\n<td>\\wedge</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\vee</script></td>\n<td>\\vee</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\neg</script></td>\n<td>\\neg</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$\\subset$</td>\n<td>\\subset</td>\n</tr>\n<tr>\n<td>$\\subseteq$</td>\n<td>\\subseteq</td>\n</tr>\n<tr>\n<td>$\\in$</td>\n<td>\\in</td>\n</tr>\n<tr>\n<td>$\\notin$</td>\n<td>\\notin</td>\n</tr>\n<tr>\n<td>$\\emptyset$</td>\n<td>\\emptyset</td>\n</tr>\n<tr>\n<td>$\\varnothing$</td>\n<td>\\varnothing</td>\n</tr>\n<tr>\n<td>$\\bigcup$</td>\n<td>\\bigcup</td>\n</tr>\n<tr>\n<td>$\\cup$</td>\n<td>\\cup</td>\n</tr>\n<tr>\n<td>$\\bigcap$</td>\n<td>\\bigcap</td>\n</tr>\n<tr>\n<td>$\\cap$</td>\n<td>\\cap</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$\\frac{a+1}{b+1}$</td>\n<td>分数    \\frac{a+1}{b+1}</td>\n</tr>\n<tr>\n<td>$\\sqrt{x^5}$</td>\n<td>开方      \\sqrt{x^5}</td>\n</tr>\n<tr>\n<td>$\\sqrt[3]{xy}$</td>\n<td>开方       \\sqrt[3]{xy}</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$\\le$</td>\n<td>\\le</td>\n</tr>\n<tr>\n<td>$\\geq$</td>\n<td>\\geq</td>\n</tr>\n<tr>\n<td>$\\neq$</td>\n<td>\\neq</td>\n</tr>\n<tr>\n<td>$\\approx$</td>\n<td>\\approx</td>\n</tr>\n<tr>\n<td>$\\equiv$</td>\n<td>\\equiv</td>\n</tr>\n<tr>\n<td>$\\pm$</td>\n<td>\\pm</td>\n</tr>\n<tr>\n<td>$\\mp$</td>\n<td>\\mp</td>\n</tr>\n<tr>\n<td>$\\mathbb{R}$</td>\n<td>将字母改为黑板字体\\mathbb{R}</td>\n</tr>\n<tr>\n<td>$\\mathcal{B}$</td>\n<td>\\mathcal{B}</td>\n</tr>\n<tr>\n<td>$\\leftarrow$</td>\n<td>\\leftarrow</td>\n</tr>\n<tr>\n<td>$\\rightarrow$</td>\n<td>\\rightarrow</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\Leftarrow</script></td>\n<td>\\Leftarrow</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\Rightarrow</script></td>\n<td>\\Rightarrow</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\sim</script></td>\n<td>\\sim</td>\n</tr>\n<tr>\n<td>$\\odot$</td>\n<td>\\odot</td>\n</tr>\n<tr>\n<td>$\\perp$</td>\n<td>独立符号\\perp</td>\n</tr>\n<tr>\n<td><script type=\"math/tex\">\\propto</script></td>\n<td>正比符号\\propto</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"换行\"><a href=\"#换行\" class=\"headerlink\" title=\"换行\"></a>换行</h3><script type=\"math/tex; mode=display\">\n\\begin{aligned}\nthe \\ first \\ row \\\\\nthe \\ second \\ row\n\\end{aligned}</script><figure class=\"highlight livescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs livescript\">$$<br><span class=\"hljs-string\">\\begin&#123;aligned&#125;</span><br>the <span class=\"hljs-string\">\\</span> first <span class=\"hljs-string\">\\</span> row <span class=\"hljs-string\">\\\\</span><br>the <span class=\"hljs-string\">\\</span> second <span class=\"hljs-string\">\\</span> row<br><span class=\"hljs-string\">\\end&#123;aligned&#125;</span><br>$$<br></code></pre></td></tr></table></figure>\n<h3 id=\"分段函数\"><a href=\"#分段函数\" class=\"headerlink\" title=\"分段函数\"></a>分段函数</h3><script type=\"math/tex; mode=display\">\nf(x) = \\begin{cases}\n1 & x = 2\\\\\n2 & x > 2\\\\\n3 & x \\leqslant 2\\\\\n\\end{cases}</script><figure class=\"highlight livescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs livescript\">$$<br>f(x) = <span class=\"hljs-string\">\\begin&#123;cases&#125;</span><br><span class=\"hljs-number\">1</span> &amp; x = <span class=\"hljs-number\">2</span> <span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-number\">2</span> &amp; x &gt; <span class=\"hljs-number\">2</span> <span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-number\">3</span> &amp; x <span class=\"hljs-string\">\\leqslant</span> <span class=\"hljs-number\">2</span><span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-string\">\\end&#123;cases&#125;</span><br>$$<br></code></pre></td></tr></table></figure>\n<h3 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h3><script type=\"math/tex; mode=display\">\n\\begin{matrix}\n1 & 2 \\\\\n3 & 4 \n\\end{matrix}</script><figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs elixir\"><span class=\"hljs-variable\">$$</span><br>\\<span class=\"hljs-keyword\">begin</span>&#123;matrix&#125;<br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> \\\\<br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br>\\<span class=\"hljs-keyword\">end</span>&#123;matrix&#125;<br><span class=\"hljs-variable\">$$</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{pmatrix}</script><figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs elixir\"><span class=\"hljs-variable\">$$</span><br>\\<span class=\"hljs-keyword\">begin</span>&#123;pmatrix&#125;<br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> \\\\<br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br>\\<span class=\"hljs-keyword\">end</span>&#123;pmatrix&#125;<br><span class=\"hljs-variable\">$$</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{bmatrix}</script><figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs elixir\"><span class=\"hljs-variable\">$$</span><br>\\<span class=\"hljs-keyword\">begin</span>&#123;bmatrix&#125;<br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> \\\\<br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br>\\<span class=\"hljs-keyword\">end</span>&#123;bmatrix&#125;<br><span class=\"hljs-variable\">$$</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{Bmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Bmatrix}</script><figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs elixir\"><span class=\"hljs-variable\">$$</span><br>\\<span class=\"hljs-keyword\">begin</span>&#123;Bmatrix&#125;<br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> \\\\<br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br>\\<span class=\"hljs-keyword\">end</span>&#123;Bmatrix&#125;<br><span class=\"hljs-variable\">$$</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{vmatrix}</script><figure class=\"highlight livescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs livescript\"><span class=\"hljs-string\">\\begin&#123;vmatrix&#125;</span><br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> <span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br><span class=\"hljs-string\">\\end&#123;vmatrix&#125;</span><br></code></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{Vmatrix}\n1 & 2 \\\\\n3 & 4 \n\\end{Vmatrix}</script><figure class=\"highlight livescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs livescript\"><span class=\"hljs-string\">\\begin&#123;Vmatrix&#125;</span><br><span class=\"hljs-number\">1</span> &amp; <span class=\"hljs-number\">2</span> <span class=\"hljs-string\">\\\\</span><br><span class=\"hljs-number\">3</span> &amp; <span class=\"hljs-number\">4</span> <br><span class=\"hljs-string\">\\end&#123;Vmatrix&#125;</span><br></code></pre></td></tr></table></figure>\n"},{"title":"贝叶斯分类器","math":true,"date":"2022-04-25T16:00:00.000Z","_content":"\n\n\n# 1 贝叶斯决策论\n\n- 假设有N个可能的类别$$\\mathcal{Y} = \\{c_1, ..., c_N\\}$$，$\\lambda_{ij}$是将一个$c_i$类样本误分类为$c_j$类的损失，则在样本x上的条件风险为：\n\n$$\nR\\left(c_{i} \\mid \\boldsymbol{x}\\right)=\\sum_{j=1}^{N} \\lambda_{i j} P\\left(c_{j} \\mid \\boldsymbol{x}\\right)\n$$\n\n- 我们的任务是寻找一个判定准则$$h: \\mathcal{X} \\mapsto \\mathcal{Y}$$以最小化总体风险：\n\n$$\nR(h)=\\mathbb{E}_{\\boldsymbol{x}}[R(h(\\boldsymbol{x}) \\mid \\boldsymbol{x})]\n$$\n\n\n\n- 显然，对每个样本x，若能最小化条件风险$$R(h(x) | x)$$，则总体风险$R(h)$也被最小化。这就产生了**贝叶斯判定准则（Bayes decision rule）**：为最小化总体风险，只需在每个样本上选择那个能使条件风险最小的类别标记，即：\n\n$$\nh^{*}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\min } R(c \\mid \\boldsymbol{x})\n$$\n\n**此时，$h^*$称为贝叶斯最优分类器**\n\n- 具体来说，若目标是最小化分类错误率，则误判损失可以写为0/1损失：\n\n$$\n\\lambda_{ij}  = \\begin{cases}\n0, if \\quad i = j \\\\\n1, otherwise\n\\end {cases}\n$$\n\n- 则此时条件风险为：\n\n$$\nR(c|x) = 1 - P(c|x)\n$$\n\n- 所以贝叶斯最优分类器为：\n\n$$\nh^*(x) = arg\\ max_{c \\in \\mathcal{Y}}P(c|x)\n$$\n\n**即对每个样本x，选择能使后验概率$P(c|x)$最大的类别标记**\n\n- 所以首先要获得后验概率，然而这在现实任务中难以直接获得，所以机器学习的任务是**基于有限的训练样本集尽可能准确地估计出后验概率**，大体有两种策略：\n\n> - **判别式模型（discriminative models）：**直接建模后验概率$P(c|x)$来预测c，决策树、SVM、神经网络等都是判别式模型\n> - **生成式模型（generative models）：**先对联合概率分布$P(x, c)$建模，再由此得到后验概率$P(c|x)$\n\n- 对于生成式模型，必然考虑贝叶斯定理：\n\n$$\nP(c|x) = \\frac{P(x, c)}{P(x)} \\\\ = \\frac{P(c)P(x|c)}{P(x)}\n$$\n\n其中，$P(c)$是先验概率，$P(x|c)$是似然，$P(x)$是用于归一化的证据因子，与类标记无关，所以建模的时候都是把分母$P(x)$直接去掉。所以现在，**估计后验概率$P(c|x)$的任务就会转化为如何基于训练集D来估计先验概率$P(c)$和似然$P(x|c)$ **。在训练集足够大的时候可以直接用样本频率代替$P(c)$。而$P(x|c)$显然是无法通过频率估计的（不同属性的组合结果太多）\n\n> 基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上将会遭遇样本稀疏问题。属性数越多，问题越严重\n\n\n\n\n\n# 2 极大似然估计\n\n- 求解$P(x|c)$的一个方法就是使用极大似然估计（MLE），这需要先假定其**具有一种确定的概率分布形式**，再基于训练样本对概率分布的参数进行估计\n\n- 具体来说，记关于类别c的似然为$P(x|c)$，假设$P(x|c)$具有确定的形式并且被参数向量$\\theta_c$唯一确定，我们的任务就是通过训练集估计参数$\\theta_c$，为明确起见，将$P(x|c)$记为$P(x|\\theta_c)$。令$D_c$为数据集D中第c类样本的集合，假设这些样本独立同分布，则参数$\\theta_c$关于$D_c$的似然是：\n\n$$\nP(D_c|\\theta_c) = \\prod_{x \\in D_c}P(x|\\theta_c)\n$$\n\n- 然后再对上式取负对数得到$LL(\\theta_c)$，最后得到极大估计值$$\\hat{\\theta_c}$$：\n\n$$\n\\hat{\\boldsymbol{\\theta}}_{c}=\\underset{\\boldsymbol{\\theta}_{c}}{\\arg \\max } L L\\left(\\boldsymbol{\\theta}_{c}\\right)\n$$\n\n\n\n\n\n# 3 朴素贝叶斯分类器\n\n### 3.1 基本概念\n\n- 上面已经说过最大的困难在于$P(x|c)$难以从有限的样本中估计而得。而朴素贝叶斯采用了**属性条件独立性假设：对已知类别，假设所有属性相互独立**\n\n- 则生成式模型的目标可以重写为：\n\n$$\nP(c \\mid \\boldsymbol{x})=\\frac{P(c) P(\\boldsymbol{x} \\mid c)}{P(\\boldsymbol{x})}=\\frac{P(c)}{P(\\boldsymbol{x})} \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)\n$$\n\n其中d为属性个数，$x_i$为样本$x$在第$i$个属性上的取值\n\n- 所以朴素贝叶斯分类器的表达式为：\n\n$$\nh_{n b}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\max } P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)\n$$\n\n- 上式的概率都可以通过统计频率获得：\n\n$$\nP(c) = \\frac{|D_c|}{|D|} \\\\ P(x_i|c) = \\frac{|D_{c, x_i}|}{|D_c|}\n$$\n\n其中$$D_c$$为类别为c的样本集，$$D_{c, x_i}$$为类别为c并在第$$i$$个属性上取值$$x_i$$的样本集。若是对于连续属性可考虑概率密度函数，比如$$p\\left(x_{i} \\mid c\\right) \\sim \\mathcal{N}\\left(\\mu_{c, i}, \\sigma_{c, i}^{2}\\right)$$，$$\\mu_{c, i}$$和$$\\sigma^2_{c, i}$$分别是第c类样本在第$$i$$个属性上取值的均值和方差\n\n\n\n### 3.2 引入先验分布\n\n- 在上面计算概率时，若某个属性值在训练集中没有与某个类同时出现过，则会导致0乘，频率估计将会出现问题。所以要进行平滑处理，常用**拉普拉斯修正（Laplacian correction）**\n- 具体来说，令N表示可能的类别数，$N_i$表示第$i$个属性可能的取值数，则上面的概率计算式可以修正为：\n\n$$\n\\begin{aligned}\n\\hat{P}(c) &=\\frac{\\left|D_{c}\\right|+1}{|D|+N} \\\\\n\\hat{P}\\left(x_{i} \\mid c\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{\\left|D_{c}\\right|+N_{i}}\n\\end{aligned}\n$$\n\n- 显然，拉普拉斯是**引入了一个均匀分布的先验分布**，避免了上述的0乘问题，并且在训练集变大时，修正过程索隐入的先验分布的影响也会逐渐变得可忽略，使得估值逐渐趋于实际概率值\n\n\n\n\n\n# 4 半朴素贝叶斯分类器\n\n- 朴素贝叶斯是采用了属性条件独立性假设，但是在现实任务中往往很难成立，于是尝试对这种假设进行一定的放松，由此产生了半朴素贝叶斯分类器。**基本思想是适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系**\n- **独依赖估计（One-Dependent Estimator, OED）**是半朴素贝叶斯分类器最常用的一种策略，就是**假设每个属性在类别之外最多仅依赖一个其他属性**，即：\n\n$$\nP(c \\mid \\boldsymbol{x}) \\propto P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c, p a_{i}\\right)\n$$\n\n其中$pa_i$为$x_i$的父属性，若$pa_i$已知，则可以通过前面的方法计算$$P(x_i|c, pa_i)$$，所以问题就转化为如何确定每个属性的父属性\n\n\n\n### 4.1 SPODE\n\n- SPODE（Super-Parent ODE）方法是**假设所有属性都依赖于同一个属性，成为超父，然后通过交叉验证等模型选择方法来确定超父属性**\n\n\n\n### 4.2 TAN\n\n- TAN（Tree Augmented naive Bayes）是在最大带权生成树的基础上构建的依赖关系，具体步骤如下：\n\n> 1. 计算任意两个属性之间的条件互信息（conditional mutual information）：\n>\n> $$\n> I\\left(x_{i}, x_{j} \\mid y\\right)=\\sum_{x_{i}, x_{j} ; c \\in \\mathcal{Y}} P\\left(x_{i}, x_{j} \\mid c\\right) \\log \\frac{P\\left(x_{i}, x_{j} \\mid c\\right)}{P\\left(x_{i} \\mid c\\right) P\\left(x_{j} \\mid c\\right)}\n> $$\n>\n> 2. 以属性作为结点构建完全图，每两个节点之间边的权重为$$I(x_i, x_j|y)$$\n> 3. 构建此完全图的最大带权生成树，挑选根节点，并将边置为有向\n> 4. 加入类别结点y，增加y到每个属性的有向边\n\n- 以下是朴素贝叶斯（NB）和两种ODE的对比：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220926174528428.png\" alt=\"image-20220926174528428\" style=\"zoom:80%;\" />\n\n\n\n### 4.3 AODE\n\n- AODE（Averaged One-Dependent Estimator）是一种基于集成学习的ODE，与SPODE通过模型选择确定超父属性不同，**AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果**，即：\n\n$$\nP(c \\mid \\boldsymbol{x}) \\propto \\sum_{\\substack{i=1 \\\\\\left|D_{x_{i}}\\right| \\geqslant m^{\\prime}}}^{d} P\\left(c, x_{i}\\right) \\prod_{j=1}^{d} P\\left(x_{j} \\mid c, x_{i}\\right)\n$$\n\n其中$$D_{x_i}$$是在第$i$个属性上取值为$x_i$的样本集合，$m^{\\prime}$为阈值（默认为30）\n\n- 概率统计公式如下：\n\n$$\n\\begin{aligned}\n\\hat{P}\\left(c, x_{i}\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{|D|+ N \\times N_{i}} \\\\\n\\hat{P}\\left(x_{j} \\mid c, x_{i}\\right) &=\\frac{\\left|D_{c, x_{i}, x_{j}}\\right|+1}{\\left|D_{c, x_{i}}\\right|+N_{j}}\n\\end{aligned}\n$$\n\n- 注意：SPODE是假设类别和超父属性相互独立，所以连乘项前面是乘$P(c)$；而AODE则没有假设两者独立，所以连乘项前是乘$P(c|x_i)$\n\n\n\n\n\n# 5 贝叶斯网\n\n- 贝叶斯网亦称信念网，借助有向无环图（DAG）来刻画属性之间的依赖关系，并使用条件概率表（CPT）来描述属性间的联合概率分布\n\n- 具体来说，一个贝叶斯网B由结构G和参数$$\\Theta$$构成，即$$B = <G, \\Theta>$$。G是一个有向无环图，每个节点对应一个属性，若两个属性有直接依赖关系，则由一条边连接起来。$$\\Theta$$定量描述这种依赖关系，假设属性$$x_i$$的父节点集为$$\\pi_i$$，则$$\\Theta$$包含了每个属性的条件概率表$$\\theta_{x_i|\\pi_i} = P_B(x_i|\\pi_i)$$。如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927112128167.png\" alt=\"image-20220927112128167\"  />\n\n\n\n### 5.1 结构\n\n- 贝叶斯网有效的表达了属性间的条件独立性，**给定父节点集，贝叶斯网假设每个属性与他的非后裔属性独立**，那么属性$$x_1, ...,x_d$$的联合概率分布为：\n\n$$\nP_{B}\\left(x_{1}, x_{2}, \\ldots, x_{d}\\right)=\\prod_{i=1}^{d} P_{B}\\left(x_{i} \\mid \\pi_{i}\\right)=\\prod_{i=1}^{d} \\theta_{x_{i} \\mid \\pi_{i}}\n$$\n\n- 贝叶斯网中有3种典型的依赖关系：\n\n\n\n![image-20220927161634626](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927161634626.png)\n\n1、同父结构中，给定父节点$x_1$的值，则$x_3$和$x_4$条件独立\n\n> **证明：**\n>\n> $$\n> P(x_1, x_3, x_4) = P(x_1)P(x_3|x_1)P(x_4|x_1) \\\\\n> P(x_3, x_4| x_1) = P(x_1, x_3, x_4) / P(x_1)\n> $$\n> 联立上述两式：\n> $$\n> P(x_3, x_4| x_1) = P(x_3|x_1)P(x_4|x_1)\n> $$\n\n2、顺序结构中，给定x的值，则y和z条件独立\n\n> **证明：**\n> $$\n> P(x, y, z) = P(z)P(x|z)P(y|x) = P(x)P(z|x)P(y|x) \\\\\n> P(z, y | x) = P(x, y, z) / P(x)\n> $$\n> 联立上述两式：\n> $$\n> P(z, y | x) = P(z|x)P(y|x)\n> $$\n\n3、V型结构中，给定$x_4$的取值，则$x_1$和$x_2$必不独立；但是若$x_4$取值完全未知，则$x_1$和$x_2$却是相互独立的（**边际独立性**）\n\n> **证明：**\n>\n> $$\n> P(x_1, x_2) = \\sum_{x_4}P(x_1, x_2, x_4) = \\sum_{x_4}P(x_1)P(x_2)P(x_4|x_1, x_2) = P(x_1)P(x_2)\n> $$\n\n\n\n### 5.2 有向分离\n\n- 可以使用**有向分离（D-separation）**分析有向图中变量间的条件独立性\n- 首先要把有向图转化为无向图，由此产生的无向图称为道德图：\n\n> 1. 找出有向图中的所有V型结构，在V型结构的两个父节点之间加上一条无向边\n> 2. 然后将所有有向边改为无向边\n\n- 基于道德图能直观迅速地找到变量间地条件独立性。假定道德图中有变量$x,y$和变量集合$$z = \\{z_i\\}$$。若变量x和y能在图上被z分开，即从道德图中将变量集合z去除后，x和y分属两个连通分支，则称x和y被有向分离，$$x \\perp y | z$$成立\n\n\n\n### 5.3 学习\n\n- 贝叶斯网学习的首要任务是根据训练数据来找出结构最恰当的贝叶斯网。**评分搜索**是求解的常用方法，具体来说，先定义一个评分函数，以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网\n\n- 常用评分函数通常基于信息论准则，此类准则将学习问题看作一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时**编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需的字节长度**。对贝叶斯网学习而言,模型就是一个贝叶斯网\n- 每个贝叶斯网描述了一个在训练数据上的概率分布，**自有一套编码机制能使那些经常出现的样本有更短的编码**。于是应**选择那个综合编码长度(包括描述网络和编码数据)最短的贝叶斯网**，这就是**最小描述长度(Minimal Description Length,MDL)准则**\n- 若给定训练集$$D = \\{x_1, ..., x_m\\}$$（每个样本向量中是包含了类别的），则贝叶斯网$$B = <G, \\Theta>$$在D上的评分函数可以写为：\n\n$$\ns(B \\mid D)=f(\\theta)|B|-L L(B \\mid D)\n$$\n\n其中|B|是贝叶斯网络的参数个数，$f(\\theta)$表示描述每个参数$\\theta$所需的编码位数；而第二项$$L L(B \\mid D)=\\sum_{i=1}^{m} \\log P_{B}\\left(x_{i}\\right)$$是贝叶斯网B的对数似然。**显然第一项是计算编码贝叶斯网B所需的编码位数，第二项是计算B所对应的概率分布$P_B$对D描述的有多好**\n\n> - 若$f(\\theta)=1$，则得到AIC评分函数：\n>\n> $$\n> \\operatorname{AIC}(B \\mid D)=|B|-L L(B \\mid D)\n> $$\n>\n> - 若$f(\\theta) = \\frac{1}{2}\\log m$，则得到BIC评分函数：\n>\n> $$\n> \\operatorname{BIC}(B \\mid D)=\\frac{\\log m}{2}|B|-L L(B \\mid D)\n> $$\n\n- 若贝叶斯网B的网络结构G固定，则评分函数第一项为常数，那么最小化$s(B|D)$等价于对参数$\\Theta$的极大似然估计，而此时每个参数$$\\theta_{x_i|\\pi_i}$$可以直接从D中通过频率统计获得。**所以，要最小化评分函数，只需对网络每种结构进行搜索，而候选结构的最优参数可直接在训练数据D上计算得到**\n\n> 但是搜索所有可能的结构是一个NP难问题。但是可以采用一些策略求得近似解，比如：\n>\n> 1. 贪心法，从某个网络结构出发，每次调整一条边（增加、删除、调整方向），直到评分函数不再降低\n> 2. 添加约束，比如将网络结构限定为树形结构（比如TAN）\n\n\n\n### 5.4 推断\n\n- 贝叶斯网训练好之后就能用来回答“查询”（query），即通过一些属性变量的观测值来推测其他属性变量的取值（类别也算作一个变量）。例如在西瓜问题中，若我们观测到西瓜色泽青绿、敲声浊响、根蒂蜷缩，想知道它是否成熟、甜度如何。**这样通过已知变量观测值来推测待查询变量的过程称为“推断”（inference），已知变量观测值称为“证据”（evidence）**\n\n- 理想情况下是直接通过贝叶斯网定义的联合概率分布来计算后验概率，但是在节点多、连接稠密时，难以进行这样的精确推断，这时需借助**近似推断**，尝试用**吉布斯采样（Gibbs sampling）**\n\n- 具体来说，$$Q = \\{Q_1, ..., Q_n\\}$$表示带查询变量，$$E = \\{E_1, ..., E_k\\}$$表示证据变量，其取值为$$e = \\{e_1, ..., e_k\\}$$。我们的任务是计算后验概率$$P(Q=q|E=e)$$，其中$$q = \\{q_1, ..., q_n\\}$$代表查询变量的一组取值\n\n- 吉布斯采样步骤如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927213559671.png\" alt=\"image-20220927213559671\" style=\"zoom:80%;\" />\n\n> 一开始先产生一个与证据$E = e$一致的样本$q^0$作为初始点，然后经过T次迭代，每次迭代都对非证据变量（即Z）逐个采样，然后由贝叶斯的概率分布推断其取值。若经过T次采样得到的与q一致的样本共有$n_q$个，则可近似估算出后验概率：\n> $$\n> P(\\mathbf{Q}=\\mathbf{q} \\mid \\mathbf{E}=\\mathbf{e}) \\simeq \\frac{n_{q}}{T}\n> $$\n\n\n\n\n\n# 6 EM算法\n\n- 上面的讨论中，都是认定训练样本是完整的，但是现实应用中往往有的属性值未知，这些未观测变量称为**隐变量（latent variable）**，在这种存在隐变量的情况下进行参数估计，可使用EM算法\n\n\n\n### 6.1 基本思想\n\n- EM 算法的核心思想非常简单，分为两步：Expectation-Step 和 Maximization-Step。E-Step 主要通过观察数据和现有模型来估计参数，然后用这个估计的参数值来计算似然函数的期望值；而 M-Step 是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后似然函数都会增加，所以函数最终会收敛\n- 于是以随机初始值$\\Theta^0$为起点，执行以下步骤直至收敛：\n\n> 1. 基于$\\Theta^t$推断隐变量Z的期望，记为$Z^t$\n> 2. 基于已观测变量X和$Z^t$对参数$\\Theta$做最大似然估计，记为$$\\Theta^{t+1}$$\n\n\n\n### 6.2 举个栗子\n\n- 有两枚硬币A、B，随机抛掷结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-4e19d89b47e21cf284644b0576e9af0f_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" />\n\n很容易估计出两枚硬币抛掷正面的概率：\n$$\n\\begin{array}{l}\n\\theta_{A}=24 / 30=0.8 \\\\\n\\theta_{B}=9 / 20=0.45\n\\end{array}\n$$\n\n- 现在加入隐变量，抹去每次投掷的硬币标记，即不知道这次投的是A还是B：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-caa896173185a8f527c037c122122258_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" />\n\n这种情况又如何估计$\\theta_A, \\theta_B$呢。我们多出了一个隐变量$$Z = \\{z_1, ..., z_5\\}$$，代表每次投掷的硬币类型。我们需要Z才能估计参数$\\theta_A, \\theta_B$，而又需要$\\theta_A, \\theta_B$才能估计Z。其解决方法就是先随机初始化$\\theta_A, \\theta_B$ ，然后用去估计 Z， 然后基于 Z 按照最大似然概率去估计新的$\\theta_A, \\theta_B$，循环至收敛。\n\n- 现在随机初始化$$\\theta_A = 0.6, \\theta_B = 0.5$$，以第一轮投掷来说，硬币A投出5H5T结果的概率是$$C_{10}^5 0.6^5 * 0.4^5$$，而B投出5H5T的概率为$$C_{10}^5 0.5^5 * 0.5^5$$，由此可以算出本次使用A或B硬币的概率：\n\n$$\n\\begin{array}{l}\nP_{A}=\\frac{0.6^{5} * 0.4^{5}}{\\left(0.6^{5} * 0.4^{5}\\right)+\\left(0.5^{5} * 0.5^{5}\\right)}=0.45 \\\\\nP_{B}=\\frac{0.5^{5} * 0.5^{5}}{\\left(0.6^{5} * 0.4^{5}\\right)+\\left(0.5^{5} * 0.5^{5}\\right)}=0.55\n\\end{array}\n$$\n\n对其他轮进行同样的操作，得到：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-b325de65a5bcac196fc0939f346410d7_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" />\n\n**这一步我们实际上是估计出了 Z 的概率分布，这部就是 E-Step**\n\n- 结合硬币 A 的概率和上一张投掷结果，我们利用期望可以求出硬币 A 和硬币 B 的贡献。以第二轮硬币 A 为例子，计算方式为：\n\n$$\n\\begin{array}{l}\nH: 0.80 * 9=7.2 \\\\\nT: 0.80 * 1=0.8\n\\end{array}\n$$\n\n对其他轮和硬币B进行同样的操作，得到：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-9b6e8c50c0761c6ac19909c26e0a71d4_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" />\n\n- 然后用极大似然估计来估计新的$\\theta_A, \\theta_B$：\n\n$$\n\\begin{aligned}\n\\theta_{A} &=\\frac{21.3}{21.3+8.6}=0.71 \\\\\n\\theta_{B} &=\\frac{11.7}{11.7+8.4}=0.58\n\\end{aligned}\n$$\n\n**这步就对应了 M-Step，重新估计出了参数值。**\n\n- 如此反复迭代，我们就可以算出最终的参数值。\n\n\n\n### 6.3 算法流程\n\n- 给定观测变量Y、隐变量Z，模型参数为$$\\theta$$\n\n> 1. 首先选定参数的初始值$$\\theta^{(0)}$$，开始迭代\n> 2. E步：在第$$i+1$$次迭代时，已知$$\\theta^{(i)}$$，计算：\n>\n> $$\n> \\begin{aligned}\n> Q\\left(\\theta, \\theta^{(i)}\\right) &=E_{Z}\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] \\\\\n> &=\\sum_{Z} \\log P(Y, Z \\mid \\theta) P\\left(Z \\mid Y, \\theta^{(i)}\\right)\n> \\end{aligned}\n> $$\n>\n> 3. M步：求得使$$Q(\\theta, \\theta^{(i)})$$最大化的$$\\theta$$，作为$$\\theta^{(i+1)}$$：\n>\n> $$\n> \\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)\n> $$\n>\n> 4. 重复上述的E步和M步，直至收敛\n\n- **注意：**初始值是可以随机选择的，但是**EM算法对初值敏感**，EM有可能收敛到局部最优点\n\n\n\n### 6.4 算法推导\n\n- 面对一个含隐变量的概率模型，目标是最大化：\n\n$$\n\\begin{aligned}\nL(\\theta) &=\\log P(Y \\mid \\theta)=\\log \\sum_{Z} P(Y, Z \\mid \\theta) \\\\\n&=\\log \\left(\\sum_{Z} P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)\n\\end{aligned}\n$$\n\n- 在第$$i+1$$次迭代时，我们是希望有所提升，即得到的$$\\theta$$的似然$$L(\\theta)$$要大于当前的似然$$L(\\theta^{(i)})$$，所以将两者相减：\n\n$$\n\\begin{aligned}\nL(\\theta)-L\\left(\\theta^{(i)}\\right) &= \\log \\left(\\sum_{Z} P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right)\\\\\n& =\\log \\left(\\sum_{Z} P\\left(Y \\mid Z, \\theta^{(i)}\\right) \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Y \\mid Z, \\theta^{(i)}\\right)}\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\n& \\geqslant \\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right)}-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\n&=\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\n\\end{aligned}\n$$\n\n上述放缩用到了**Jensen不等式**\n\n- 令：\n\n$$\nB\\left(\\theta, \\theta^{(i)}\\right) \\hat{=} L\\left(\\theta^{(i)}\\right)+\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\n$$\n\n那么：\n$$\nL(\\theta) \\geqslant B\\left(\\theta, \\theta^{(i)}\\right)\n$$\n即$$B(\\theta, \\theta^{(i)})$$为$$L(\\theta)$$的下界，并且在$$\\theta=\\theta^{(i)}$$时取等号：\n$$\nL\\left(\\theta^{(i)}\\right)=B\\left(\\theta^{(i)}, \\theta^{(i)}\\right)\n$$\n\n- 所以，增大下界$$B(\\theta, \\theta^{(i)})$$，同样可以使得$$L(\\theta)$$增大，而为了$$L(\\theta)$$增大得最多，选择$$\\theta^{(i+1)}$$使得$$B(\\theta, \\theta^{(i)})$$达到极大：\n\n$$\n\\theta^{(i+1)}=\\arg \\max _{\\theta} B\\left(\\theta, \\theta^{(i)}\\right)\n$$\n\n- 由上式就可以推出$$Q(\\theta, \\theta^{(i)})$$函数：\n\n$$\n\\begin{aligned}\n\\theta^{(i+1)} &=\\arg \\max _{\\theta}\\left(L\\left(\\theta^{(i)}\\right)+\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\\right) \\\\\n&=\\arg \\max _{\\theta}\\left(\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log (P(Y \\mid Z, \\theta) P(Z \\mid \\theta))\\right) \\\\\n&=\\arg \\max _{\\theta}\\left(\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log P(Y, Z \\mid \\theta)\\right) \\\\\n&=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)\n\\end{aligned}\n$$\n\n\n\n### 6.5 直观解释\n\n- 上方曲线为$$L(\\theta)$$，下方曲线为$$B(\\theta, \\theta^{(i)})$$，两者在$$\\theta=\\theta^{(i)}$$处相等，此时执行M步：找到$$\\theta^{(i+1)}=\\arg \\max _{\\theta} B\\left(\\theta, \\theta^{(i)}\\right)$$。函数$$B(\\theta, \\theta^{(i)})$$的增加同时也造成了$$L(\\theta)$$的增加。得到$$\\theta^{(i+1)}$$后再执行E步：在$$\\theta = \\theta^{(i+1)}$$点重新计算Q函数，然后进行下一次迭代\n\n![image-20221021194004500](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221021194004500.png)","source":"_posts/贝叶斯分类器.md","raw":"---\ntitle: 贝叶斯分类器\nmath: true\ndate: 2022-4-26\n---\n\n\n\n# 1 贝叶斯决策论\n\n- 假设有N个可能的类别$$\\mathcal{Y} = \\{c_1, ..., c_N\\}$$，$\\lambda_{ij}$是将一个$c_i$类样本误分类为$c_j$类的损失，则在样本x上的条件风险为：\n\n$$\nR\\left(c_{i} \\mid \\boldsymbol{x}\\right)=\\sum_{j=1}^{N} \\lambda_{i j} P\\left(c_{j} \\mid \\boldsymbol{x}\\right)\n$$\n\n- 我们的任务是寻找一个判定准则$$h: \\mathcal{X} \\mapsto \\mathcal{Y}$$以最小化总体风险：\n\n$$\nR(h)=\\mathbb{E}_{\\boldsymbol{x}}[R(h(\\boldsymbol{x}) \\mid \\boldsymbol{x})]\n$$\n\n\n\n- 显然，对每个样本x，若能最小化条件风险$$R(h(x) | x)$$，则总体风险$R(h)$也被最小化。这就产生了**贝叶斯判定准则（Bayes decision rule）**：为最小化总体风险，只需在每个样本上选择那个能使条件风险最小的类别标记，即：\n\n$$\nh^{*}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\min } R(c \\mid \\boldsymbol{x})\n$$\n\n**此时，$h^*$称为贝叶斯最优分类器**\n\n- 具体来说，若目标是最小化分类错误率，则误判损失可以写为0/1损失：\n\n$$\n\\lambda_{ij}  = \\begin{cases}\n0, if \\quad i = j \\\\\n1, otherwise\n\\end {cases}\n$$\n\n- 则此时条件风险为：\n\n$$\nR(c|x) = 1 - P(c|x)\n$$\n\n- 所以贝叶斯最优分类器为：\n\n$$\nh^*(x) = arg\\ max_{c \\in \\mathcal{Y}}P(c|x)\n$$\n\n**即对每个样本x，选择能使后验概率$P(c|x)$最大的类别标记**\n\n- 所以首先要获得后验概率，然而这在现实任务中难以直接获得，所以机器学习的任务是**基于有限的训练样本集尽可能准确地估计出后验概率**，大体有两种策略：\n\n> - **判别式模型（discriminative models）：**直接建模后验概率$P(c|x)$来预测c，决策树、SVM、神经网络等都是判别式模型\n> - **生成式模型（generative models）：**先对联合概率分布$P(x, c)$建模，再由此得到后验概率$P(c|x)$\n\n- 对于生成式模型，必然考虑贝叶斯定理：\n\n$$\nP(c|x) = \\frac{P(x, c)}{P(x)} \\\\ = \\frac{P(c)P(x|c)}{P(x)}\n$$\n\n其中，$P(c)$是先验概率，$P(x|c)$是似然，$P(x)$是用于归一化的证据因子，与类标记无关，所以建模的时候都是把分母$P(x)$直接去掉。所以现在，**估计后验概率$P(c|x)$的任务就会转化为如何基于训练集D来估计先验概率$P(c)$和似然$P(x|c)$ **。在训练集足够大的时候可以直接用样本频率代替$P(c)$。而$P(x|c)$显然是无法通过频率估计的（不同属性的组合结果太多）\n\n> 基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上将会遭遇样本稀疏问题。属性数越多，问题越严重\n\n\n\n\n\n# 2 极大似然估计\n\n- 求解$P(x|c)$的一个方法就是使用极大似然估计（MLE），这需要先假定其**具有一种确定的概率分布形式**，再基于训练样本对概率分布的参数进行估计\n\n- 具体来说，记关于类别c的似然为$P(x|c)$，假设$P(x|c)$具有确定的形式并且被参数向量$\\theta_c$唯一确定，我们的任务就是通过训练集估计参数$\\theta_c$，为明确起见，将$P(x|c)$记为$P(x|\\theta_c)$。令$D_c$为数据集D中第c类样本的集合，假设这些样本独立同分布，则参数$\\theta_c$关于$D_c$的似然是：\n\n$$\nP(D_c|\\theta_c) = \\prod_{x \\in D_c}P(x|\\theta_c)\n$$\n\n- 然后再对上式取负对数得到$LL(\\theta_c)$，最后得到极大估计值$$\\hat{\\theta_c}$$：\n\n$$\n\\hat{\\boldsymbol{\\theta}}_{c}=\\underset{\\boldsymbol{\\theta}_{c}}{\\arg \\max } L L\\left(\\boldsymbol{\\theta}_{c}\\right)\n$$\n\n\n\n\n\n# 3 朴素贝叶斯分类器\n\n### 3.1 基本概念\n\n- 上面已经说过最大的困难在于$P(x|c)$难以从有限的样本中估计而得。而朴素贝叶斯采用了**属性条件独立性假设：对已知类别，假设所有属性相互独立**\n\n- 则生成式模型的目标可以重写为：\n\n$$\nP(c \\mid \\boldsymbol{x})=\\frac{P(c) P(\\boldsymbol{x} \\mid c)}{P(\\boldsymbol{x})}=\\frac{P(c)}{P(\\boldsymbol{x})} \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)\n$$\n\n其中d为属性个数，$x_i$为样本$x$在第$i$个属性上的取值\n\n- 所以朴素贝叶斯分类器的表达式为：\n\n$$\nh_{n b}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\max } P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)\n$$\n\n- 上式的概率都可以通过统计频率获得：\n\n$$\nP(c) = \\frac{|D_c|}{|D|} \\\\ P(x_i|c) = \\frac{|D_{c, x_i}|}{|D_c|}\n$$\n\n其中$$D_c$$为类别为c的样本集，$$D_{c, x_i}$$为类别为c并在第$$i$$个属性上取值$$x_i$$的样本集。若是对于连续属性可考虑概率密度函数，比如$$p\\left(x_{i} \\mid c\\right) \\sim \\mathcal{N}\\left(\\mu_{c, i}, \\sigma_{c, i}^{2}\\right)$$，$$\\mu_{c, i}$$和$$\\sigma^2_{c, i}$$分别是第c类样本在第$$i$$个属性上取值的均值和方差\n\n\n\n### 3.2 引入先验分布\n\n- 在上面计算概率时，若某个属性值在训练集中没有与某个类同时出现过，则会导致0乘，频率估计将会出现问题。所以要进行平滑处理，常用**拉普拉斯修正（Laplacian correction）**\n- 具体来说，令N表示可能的类别数，$N_i$表示第$i$个属性可能的取值数，则上面的概率计算式可以修正为：\n\n$$\n\\begin{aligned}\n\\hat{P}(c) &=\\frac{\\left|D_{c}\\right|+1}{|D|+N} \\\\\n\\hat{P}\\left(x_{i} \\mid c\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{\\left|D_{c}\\right|+N_{i}}\n\\end{aligned}\n$$\n\n- 显然，拉普拉斯是**引入了一个均匀分布的先验分布**，避免了上述的0乘问题，并且在训练集变大时，修正过程索隐入的先验分布的影响也会逐渐变得可忽略，使得估值逐渐趋于实际概率值\n\n\n\n\n\n# 4 半朴素贝叶斯分类器\n\n- 朴素贝叶斯是采用了属性条件独立性假设，但是在现实任务中往往很难成立，于是尝试对这种假设进行一定的放松，由此产生了半朴素贝叶斯分类器。**基本思想是适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系**\n- **独依赖估计（One-Dependent Estimator, OED）**是半朴素贝叶斯分类器最常用的一种策略，就是**假设每个属性在类别之外最多仅依赖一个其他属性**，即：\n\n$$\nP(c \\mid \\boldsymbol{x}) \\propto P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c, p a_{i}\\right)\n$$\n\n其中$pa_i$为$x_i$的父属性，若$pa_i$已知，则可以通过前面的方法计算$$P(x_i|c, pa_i)$$，所以问题就转化为如何确定每个属性的父属性\n\n\n\n### 4.1 SPODE\n\n- SPODE（Super-Parent ODE）方法是**假设所有属性都依赖于同一个属性，成为超父，然后通过交叉验证等模型选择方法来确定超父属性**\n\n\n\n### 4.2 TAN\n\n- TAN（Tree Augmented naive Bayes）是在最大带权生成树的基础上构建的依赖关系，具体步骤如下：\n\n> 1. 计算任意两个属性之间的条件互信息（conditional mutual information）：\n>\n> $$\n> I\\left(x_{i}, x_{j} \\mid y\\right)=\\sum_{x_{i}, x_{j} ; c \\in \\mathcal{Y}} P\\left(x_{i}, x_{j} \\mid c\\right) \\log \\frac{P\\left(x_{i}, x_{j} \\mid c\\right)}{P\\left(x_{i} \\mid c\\right) P\\left(x_{j} \\mid c\\right)}\n> $$\n>\n> 2. 以属性作为结点构建完全图，每两个节点之间边的权重为$$I(x_i, x_j|y)$$\n> 3. 构建此完全图的最大带权生成树，挑选根节点，并将边置为有向\n> 4. 加入类别结点y，增加y到每个属性的有向边\n\n- 以下是朴素贝叶斯（NB）和两种ODE的对比：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220926174528428.png\" alt=\"image-20220926174528428\" style=\"zoom:80%;\" />\n\n\n\n### 4.3 AODE\n\n- AODE（Averaged One-Dependent Estimator）是一种基于集成学习的ODE，与SPODE通过模型选择确定超父属性不同，**AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果**，即：\n\n$$\nP(c \\mid \\boldsymbol{x}) \\propto \\sum_{\\substack{i=1 \\\\\\left|D_{x_{i}}\\right| \\geqslant m^{\\prime}}}^{d} P\\left(c, x_{i}\\right) \\prod_{j=1}^{d} P\\left(x_{j} \\mid c, x_{i}\\right)\n$$\n\n其中$$D_{x_i}$$是在第$i$个属性上取值为$x_i$的样本集合，$m^{\\prime}$为阈值（默认为30）\n\n- 概率统计公式如下：\n\n$$\n\\begin{aligned}\n\\hat{P}\\left(c, x_{i}\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{|D|+ N \\times N_{i}} \\\\\n\\hat{P}\\left(x_{j} \\mid c, x_{i}\\right) &=\\frac{\\left|D_{c, x_{i}, x_{j}}\\right|+1}{\\left|D_{c, x_{i}}\\right|+N_{j}}\n\\end{aligned}\n$$\n\n- 注意：SPODE是假设类别和超父属性相互独立，所以连乘项前面是乘$P(c)$；而AODE则没有假设两者独立，所以连乘项前是乘$P(c|x_i)$\n\n\n\n\n\n# 5 贝叶斯网\n\n- 贝叶斯网亦称信念网，借助有向无环图（DAG）来刻画属性之间的依赖关系，并使用条件概率表（CPT）来描述属性间的联合概率分布\n\n- 具体来说，一个贝叶斯网B由结构G和参数$$\\Theta$$构成，即$$B = <G, \\Theta>$$。G是一个有向无环图，每个节点对应一个属性，若两个属性有直接依赖关系，则由一条边连接起来。$$\\Theta$$定量描述这种依赖关系，假设属性$$x_i$$的父节点集为$$\\pi_i$$，则$$\\Theta$$包含了每个属性的条件概率表$$\\theta_{x_i|\\pi_i} = P_B(x_i|\\pi_i)$$。如下图：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927112128167.png\" alt=\"image-20220927112128167\"  />\n\n\n\n### 5.1 结构\n\n- 贝叶斯网有效的表达了属性间的条件独立性，**给定父节点集，贝叶斯网假设每个属性与他的非后裔属性独立**，那么属性$$x_1, ...,x_d$$的联合概率分布为：\n\n$$\nP_{B}\\left(x_{1}, x_{2}, \\ldots, x_{d}\\right)=\\prod_{i=1}^{d} P_{B}\\left(x_{i} \\mid \\pi_{i}\\right)=\\prod_{i=1}^{d} \\theta_{x_{i} \\mid \\pi_{i}}\n$$\n\n- 贝叶斯网中有3种典型的依赖关系：\n\n\n\n![image-20220927161634626](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927161634626.png)\n\n1、同父结构中，给定父节点$x_1$的值，则$x_3$和$x_4$条件独立\n\n> **证明：**\n>\n> $$\n> P(x_1, x_3, x_4) = P(x_1)P(x_3|x_1)P(x_4|x_1) \\\\\n> P(x_3, x_4| x_1) = P(x_1, x_3, x_4) / P(x_1)\n> $$\n> 联立上述两式：\n> $$\n> P(x_3, x_4| x_1) = P(x_3|x_1)P(x_4|x_1)\n> $$\n\n2、顺序结构中，给定x的值，则y和z条件独立\n\n> **证明：**\n> $$\n> P(x, y, z) = P(z)P(x|z)P(y|x) = P(x)P(z|x)P(y|x) \\\\\n> P(z, y | x) = P(x, y, z) / P(x)\n> $$\n> 联立上述两式：\n> $$\n> P(z, y | x) = P(z|x)P(y|x)\n> $$\n\n3、V型结构中，给定$x_4$的取值，则$x_1$和$x_2$必不独立；但是若$x_4$取值完全未知，则$x_1$和$x_2$却是相互独立的（**边际独立性**）\n\n> **证明：**\n>\n> $$\n> P(x_1, x_2) = \\sum_{x_4}P(x_1, x_2, x_4) = \\sum_{x_4}P(x_1)P(x_2)P(x_4|x_1, x_2) = P(x_1)P(x_2)\n> $$\n\n\n\n### 5.2 有向分离\n\n- 可以使用**有向分离（D-separation）**分析有向图中变量间的条件独立性\n- 首先要把有向图转化为无向图，由此产生的无向图称为道德图：\n\n> 1. 找出有向图中的所有V型结构，在V型结构的两个父节点之间加上一条无向边\n> 2. 然后将所有有向边改为无向边\n\n- 基于道德图能直观迅速地找到变量间地条件独立性。假定道德图中有变量$x,y$和变量集合$$z = \\{z_i\\}$$。若变量x和y能在图上被z分开，即从道德图中将变量集合z去除后，x和y分属两个连通分支，则称x和y被有向分离，$$x \\perp y | z$$成立\n\n\n\n### 5.3 学习\n\n- 贝叶斯网学习的首要任务是根据训练数据来找出结构最恰当的贝叶斯网。**评分搜索**是求解的常用方法，具体来说，先定义一个评分函数，以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网\n\n- 常用评分函数通常基于信息论准则，此类准则将学习问题看作一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时**编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需的字节长度**。对贝叶斯网学习而言,模型就是一个贝叶斯网\n- 每个贝叶斯网描述了一个在训练数据上的概率分布，**自有一套编码机制能使那些经常出现的样本有更短的编码**。于是应**选择那个综合编码长度(包括描述网络和编码数据)最短的贝叶斯网**，这就是**最小描述长度(Minimal Description Length,MDL)准则**\n- 若给定训练集$$D = \\{x_1, ..., x_m\\}$$（每个样本向量中是包含了类别的），则贝叶斯网$$B = <G, \\Theta>$$在D上的评分函数可以写为：\n\n$$\ns(B \\mid D)=f(\\theta)|B|-L L(B \\mid D)\n$$\n\n其中|B|是贝叶斯网络的参数个数，$f(\\theta)$表示描述每个参数$\\theta$所需的编码位数；而第二项$$L L(B \\mid D)=\\sum_{i=1}^{m} \\log P_{B}\\left(x_{i}\\right)$$是贝叶斯网B的对数似然。**显然第一项是计算编码贝叶斯网B所需的编码位数，第二项是计算B所对应的概率分布$P_B$对D描述的有多好**\n\n> - 若$f(\\theta)=1$，则得到AIC评分函数：\n>\n> $$\n> \\operatorname{AIC}(B \\mid D)=|B|-L L(B \\mid D)\n> $$\n>\n> - 若$f(\\theta) = \\frac{1}{2}\\log m$，则得到BIC评分函数：\n>\n> $$\n> \\operatorname{BIC}(B \\mid D)=\\frac{\\log m}{2}|B|-L L(B \\mid D)\n> $$\n\n- 若贝叶斯网B的网络结构G固定，则评分函数第一项为常数，那么最小化$s(B|D)$等价于对参数$\\Theta$的极大似然估计，而此时每个参数$$\\theta_{x_i|\\pi_i}$$可以直接从D中通过频率统计获得。**所以，要最小化评分函数，只需对网络每种结构进行搜索，而候选结构的最优参数可直接在训练数据D上计算得到**\n\n> 但是搜索所有可能的结构是一个NP难问题。但是可以采用一些策略求得近似解，比如：\n>\n> 1. 贪心法，从某个网络结构出发，每次调整一条边（增加、删除、调整方向），直到评分函数不再降低\n> 2. 添加约束，比如将网络结构限定为树形结构（比如TAN）\n\n\n\n### 5.4 推断\n\n- 贝叶斯网训练好之后就能用来回答“查询”（query），即通过一些属性变量的观测值来推测其他属性变量的取值（类别也算作一个变量）。例如在西瓜问题中，若我们观测到西瓜色泽青绿、敲声浊响、根蒂蜷缩，想知道它是否成熟、甜度如何。**这样通过已知变量观测值来推测待查询变量的过程称为“推断”（inference），已知变量观测值称为“证据”（evidence）**\n\n- 理想情况下是直接通过贝叶斯网定义的联合概率分布来计算后验概率，但是在节点多、连接稠密时，难以进行这样的精确推断，这时需借助**近似推断**，尝试用**吉布斯采样（Gibbs sampling）**\n\n- 具体来说，$$Q = \\{Q_1, ..., Q_n\\}$$表示带查询变量，$$E = \\{E_1, ..., E_k\\}$$表示证据变量，其取值为$$e = \\{e_1, ..., e_k\\}$$。我们的任务是计算后验概率$$P(Q=q|E=e)$$，其中$$q = \\{q_1, ..., q_n\\}$$代表查询变量的一组取值\n\n- 吉布斯采样步骤如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927213559671.png\" alt=\"image-20220927213559671\" style=\"zoom:80%;\" />\n\n> 一开始先产生一个与证据$E = e$一致的样本$q^0$作为初始点，然后经过T次迭代，每次迭代都对非证据变量（即Z）逐个采样，然后由贝叶斯的概率分布推断其取值。若经过T次采样得到的与q一致的样本共有$n_q$个，则可近似估算出后验概率：\n> $$\n> P(\\mathbf{Q}=\\mathbf{q} \\mid \\mathbf{E}=\\mathbf{e}) \\simeq \\frac{n_{q}}{T}\n> $$\n\n\n\n\n\n# 6 EM算法\n\n- 上面的讨论中，都是认定训练样本是完整的，但是现实应用中往往有的属性值未知，这些未观测变量称为**隐变量（latent variable）**，在这种存在隐变量的情况下进行参数估计，可使用EM算法\n\n\n\n### 6.1 基本思想\n\n- EM 算法的核心思想非常简单，分为两步：Expectation-Step 和 Maximization-Step。E-Step 主要通过观察数据和现有模型来估计参数，然后用这个估计的参数值来计算似然函数的期望值；而 M-Step 是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后似然函数都会增加，所以函数最终会收敛\n- 于是以随机初始值$\\Theta^0$为起点，执行以下步骤直至收敛：\n\n> 1. 基于$\\Theta^t$推断隐变量Z的期望，记为$Z^t$\n> 2. 基于已观测变量X和$Z^t$对参数$\\Theta$做最大似然估计，记为$$\\Theta^{t+1}$$\n\n\n\n### 6.2 举个栗子\n\n- 有两枚硬币A、B，随机抛掷结果如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-4e19d89b47e21cf284644b0576e9af0f_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" />\n\n很容易估计出两枚硬币抛掷正面的概率：\n$$\n\\begin{array}{l}\n\\theta_{A}=24 / 30=0.8 \\\\\n\\theta_{B}=9 / 20=0.45\n\\end{array}\n$$\n\n- 现在加入隐变量，抹去每次投掷的硬币标记，即不知道这次投的是A还是B：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-caa896173185a8f527c037c122122258_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" />\n\n这种情况又如何估计$\\theta_A, \\theta_B$呢。我们多出了一个隐变量$$Z = \\{z_1, ..., z_5\\}$$，代表每次投掷的硬币类型。我们需要Z才能估计参数$\\theta_A, \\theta_B$，而又需要$\\theta_A, \\theta_B$才能估计Z。其解决方法就是先随机初始化$\\theta_A, \\theta_B$ ，然后用去估计 Z， 然后基于 Z 按照最大似然概率去估计新的$\\theta_A, \\theta_B$，循环至收敛。\n\n- 现在随机初始化$$\\theta_A = 0.6, \\theta_B = 0.5$$，以第一轮投掷来说，硬币A投出5H5T结果的概率是$$C_{10}^5 0.6^5 * 0.4^5$$，而B投出5H5T的概率为$$C_{10}^5 0.5^5 * 0.5^5$$，由此可以算出本次使用A或B硬币的概率：\n\n$$\n\\begin{array}{l}\nP_{A}=\\frac{0.6^{5} * 0.4^{5}}{\\left(0.6^{5} * 0.4^{5}\\right)+\\left(0.5^{5} * 0.5^{5}\\right)}=0.45 \\\\\nP_{B}=\\frac{0.5^{5} * 0.5^{5}}{\\left(0.6^{5} * 0.4^{5}\\right)+\\left(0.5^{5} * 0.5^{5}\\right)}=0.55\n\\end{array}\n$$\n\n对其他轮进行同样的操作，得到：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-b325de65a5bcac196fc0939f346410d7_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" />\n\n**这一步我们实际上是估计出了 Z 的概率分布，这部就是 E-Step**\n\n- 结合硬币 A 的概率和上一张投掷结果，我们利用期望可以求出硬币 A 和硬币 B 的贡献。以第二轮硬币 A 为例子，计算方式为：\n\n$$\n\\begin{array}{l}\nH: 0.80 * 9=7.2 \\\\\nT: 0.80 * 1=0.8\n\\end{array}\n$$\n\n对其他轮和硬币B进行同样的操作，得到：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-9b6e8c50c0761c6ac19909c26e0a71d4_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" />\n\n- 然后用极大似然估计来估计新的$\\theta_A, \\theta_B$：\n\n$$\n\\begin{aligned}\n\\theta_{A} &=\\frac{21.3}{21.3+8.6}=0.71 \\\\\n\\theta_{B} &=\\frac{11.7}{11.7+8.4}=0.58\n\\end{aligned}\n$$\n\n**这步就对应了 M-Step，重新估计出了参数值。**\n\n- 如此反复迭代，我们就可以算出最终的参数值。\n\n\n\n### 6.3 算法流程\n\n- 给定观测变量Y、隐变量Z，模型参数为$$\\theta$$\n\n> 1. 首先选定参数的初始值$$\\theta^{(0)}$$，开始迭代\n> 2. E步：在第$$i+1$$次迭代时，已知$$\\theta^{(i)}$$，计算：\n>\n> $$\n> \\begin{aligned}\n> Q\\left(\\theta, \\theta^{(i)}\\right) &=E_{Z}\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] \\\\\n> &=\\sum_{Z} \\log P(Y, Z \\mid \\theta) P\\left(Z \\mid Y, \\theta^{(i)}\\right)\n> \\end{aligned}\n> $$\n>\n> 3. M步：求得使$$Q(\\theta, \\theta^{(i)})$$最大化的$$\\theta$$，作为$$\\theta^{(i+1)}$$：\n>\n> $$\n> \\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)\n> $$\n>\n> 4. 重复上述的E步和M步，直至收敛\n\n- **注意：**初始值是可以随机选择的，但是**EM算法对初值敏感**，EM有可能收敛到局部最优点\n\n\n\n### 6.4 算法推导\n\n- 面对一个含隐变量的概率模型，目标是最大化：\n\n$$\n\\begin{aligned}\nL(\\theta) &=\\log P(Y \\mid \\theta)=\\log \\sum_{Z} P(Y, Z \\mid \\theta) \\\\\n&=\\log \\left(\\sum_{Z} P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)\n\\end{aligned}\n$$\n\n- 在第$$i+1$$次迭代时，我们是希望有所提升，即得到的$$\\theta$$的似然$$L(\\theta)$$要大于当前的似然$$L(\\theta^{(i)})$$，所以将两者相减：\n\n$$\n\\begin{aligned}\nL(\\theta)-L\\left(\\theta^{(i)}\\right) &= \\log \\left(\\sum_{Z} P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right)\\\\\n& =\\log \\left(\\sum_{Z} P\\left(Y \\mid Z, \\theta^{(i)}\\right) \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Y \\mid Z, \\theta^{(i)}\\right)}\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\n& \\geqslant \\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right)}-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\n&=\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\n\\end{aligned}\n$$\n\n上述放缩用到了**Jensen不等式**\n\n- 令：\n\n$$\nB\\left(\\theta, \\theta^{(i)}\\right) \\hat{=} L\\left(\\theta^{(i)}\\right)+\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\n$$\n\n那么：\n$$\nL(\\theta) \\geqslant B\\left(\\theta, \\theta^{(i)}\\right)\n$$\n即$$B(\\theta, \\theta^{(i)})$$为$$L(\\theta)$$的下界，并且在$$\\theta=\\theta^{(i)}$$时取等号：\n$$\nL\\left(\\theta^{(i)}\\right)=B\\left(\\theta^{(i)}, \\theta^{(i)}\\right)\n$$\n\n- 所以，增大下界$$B(\\theta, \\theta^{(i)})$$，同样可以使得$$L(\\theta)$$增大，而为了$$L(\\theta)$$增大得最多，选择$$\\theta^{(i+1)}$$使得$$B(\\theta, \\theta^{(i)})$$达到极大：\n\n$$\n\\theta^{(i+1)}=\\arg \\max _{\\theta} B\\left(\\theta, \\theta^{(i)}\\right)\n$$\n\n- 由上式就可以推出$$Q(\\theta, \\theta^{(i)})$$函数：\n\n$$\n\\begin{aligned}\n\\theta^{(i+1)} &=\\arg \\max _{\\theta}\\left(L\\left(\\theta^{(i)}\\right)+\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\\right) \\\\\n&=\\arg \\max _{\\theta}\\left(\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log (P(Y \\mid Z, \\theta) P(Z \\mid \\theta))\\right) \\\\\n&=\\arg \\max _{\\theta}\\left(\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log P(Y, Z \\mid \\theta)\\right) \\\\\n&=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)\n\\end{aligned}\n$$\n\n\n\n### 6.5 直观解释\n\n- 上方曲线为$$L(\\theta)$$，下方曲线为$$B(\\theta, \\theta^{(i)})$$，两者在$$\\theta=\\theta^{(i)}$$处相等，此时执行M步：找到$$\\theta^{(i+1)}=\\arg \\max _{\\theta} B\\left(\\theta, \\theta^{(i)}\\right)$$。函数$$B(\\theta, \\theta^{(i)})$$的增加同时也造成了$$L(\\theta)$$的增加。得到$$\\theta^{(i+1)}$$后再执行E步：在$$\\theta = \\theta^{(i+1)}$$点重新计算Q函数，然后进行下一次迭代\n\n![image-20221021194004500](https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221021194004500.png)","slug":"贝叶斯分类器","published":1,"updated":"2022-12-20T06:20:17.751Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1o000p7csz6aru1zan","content":"<h1 id=\"1-贝叶斯决策论\"><a href=\"#1-贝叶斯决策论\" class=\"headerlink\" title=\"1 贝叶斯决策论\"></a>1 贝叶斯决策论</h1><ul>\n<li>假设有N个可能的类别<script type=\"math/tex\">\\mathcal{Y} = \\{c_1, ..., c_N\\}</script>，$\\lambda_{ij}$是将一个$c_i$类样本误分类为$c_j$类的损失，则在样本x上的条件风险为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nR\\left(c_{i} \\mid \\boldsymbol{x}\\right)=\\sum_{j=1}^{N} \\lambda_{i j} P\\left(c_{j} \\mid \\boldsymbol{x}\\right)</script><ul>\n<li>我们的任务是寻找一个判定准则<script type=\"math/tex\">h: \\mathcal{X} \\mapsto \\mathcal{Y}</script>以最小化总体风险：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nR(h)=\\mathbb{E}_{\\boldsymbol{x}}[R(h(\\boldsymbol{x}) \\mid \\boldsymbol{x})]</script><ul>\n<li>显然，对每个样本x，若能最小化条件风险<script type=\"math/tex\">R(h(x) | x)</script>，则总体风险$R(h)$也被最小化。这就产生了<strong>贝叶斯判定准则（Bayes decision rule）</strong>：为最小化总体风险，只需在每个样本上选择那个能使条件风险最小的类别标记，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh^{*}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\min } R(c \\mid \\boldsymbol{x})</script><p><strong>此时，$h^*$称为贝叶斯最优分类器</strong></p>\n<ul>\n<li>具体来说，若目标是最小化分类错误率，则误判损失可以写为0/1损失：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\lambda_{ij}  = \\begin{cases}\n0, if \\quad i = j \\\\\n1, otherwise\n\\end {cases}</script><ul>\n<li>则此时条件风险为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nR(c|x) = 1 - P(c|x)</script><ul>\n<li>所以贝叶斯最优分类器为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh^*(x) = arg\\ max_{c \\in \\mathcal{Y}}P(c|x)</script><p><strong>即对每个样本x，选择能使后验概率$P(c|x)$最大的类别标记</strong></p>\n<ul>\n<li>所以首先要获得后验概率，然而这在现实任务中难以直接获得，所以机器学习的任务是<strong>基于有限的训练样本集尽可能准确地估计出后验概率</strong>，大体有两种策略：</li>\n</ul>\n<blockquote>\n<ul>\n<li><strong>判别式模型（discriminative models）：</strong>直接建模后验概率$P(c|x)$来预测c，决策树、SVM、神经网络等都是判别式模型</li>\n<li><strong>生成式模型（generative models）：</strong>先对联合概率分布$P(x, c)$建模，再由此得到后验概率$P(c|x)$</li>\n</ul>\n</blockquote>\n<ul>\n<li>对于生成式模型，必然考虑贝叶斯定理：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c|x) = \\frac{P(x, c)}{P(x)} \\\\ = \\frac{P(c)P(x|c)}{P(x)}</script><p>其中，$P(c)$是先验概率，$P(x|c)$是似然，$P(x)$是用于归一化的证据因子，与类标记无关，所以建模的时候都是把分母$P(x)$直接去掉。所以现在，<strong>估计后验概率$P(c|x)$的任务就会转化为如何基于训练集D来估计先验概率$P(c)$和似然$P(x|c)$ </strong>。在训练集足够大的时候可以直接用样本频率代替$P(c)$。而$P(x|c)$显然是无法通过频率估计的（不同属性的组合结果太多）</p>\n<blockquote>\n<p>基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上将会遭遇样本稀疏问题。属性数越多，问题越严重</p>\n</blockquote>\n<h1 id=\"2-极大似然估计\"><a href=\"#2-极大似然估计\" class=\"headerlink\" title=\"2 极大似然估计\"></a>2 极大似然估计</h1><ul>\n<li><p>求解$P(x|c)$的一个方法就是使用极大似然估计（MLE），这需要先假定其<strong>具有一种确定的概率分布形式</strong>，再基于训练样本对概率分布的参数进行估计</p>\n</li>\n<li><p>具体来说，记关于类别c的似然为$P(x|c)$，假设$P(x|c)$具有确定的形式并且被参数向量$\\theta_c$唯一确定，我们的任务就是通过训练集估计参数$\\theta_c$，为明确起见，将$P(x|c)$记为$P(x|\\theta_c)$。令$D_c$为数据集D中第c类样本的集合，假设这些样本独立同分布，则参数$\\theta_c$关于$D_c$的似然是：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(D_c|\\theta_c) = \\prod_{x \\in D_c}P(x|\\theta_c)</script><ul>\n<li>然后再对上式取负对数得到$LL(\\theta_c)$，最后得到极大估计值<script type=\"math/tex\">\\hat{\\theta_c}</script>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\hat{\\boldsymbol{\\theta}}_{c}=\\underset{\\boldsymbol{\\theta}_{c}}{\\arg \\max } L L\\left(\\boldsymbol{\\theta}_{c}\\right)</script><h1 id=\"3-朴素贝叶斯分类器\"><a href=\"#3-朴素贝叶斯分类器\" class=\"headerlink\" title=\"3 朴素贝叶斯分类器\"></a>3 朴素贝叶斯分类器</h1><h3 id=\"3-1-基本概念\"><a href=\"#3-1-基本概念\" class=\"headerlink\" title=\"3.1 基本概念\"></a>3.1 基本概念</h3><ul>\n<li><p>上面已经说过最大的困难在于$P(x|c)$难以从有限的样本中估计而得。而朴素贝叶斯采用了<strong>属性条件独立性假设：对已知类别，假设所有属性相互独立</strong></p>\n</li>\n<li><p>则生成式模型的目标可以重写为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c \\mid \\boldsymbol{x})=\\frac{P(c) P(\\boldsymbol{x} \\mid c)}{P(\\boldsymbol{x})}=\\frac{P(c)}{P(\\boldsymbol{x})} \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)</script><p>其中d为属性个数，$x_i$为样本$x$在第$i$个属性上的取值</p>\n<ul>\n<li>所以朴素贝叶斯分类器的表达式为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh_{n b}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\max } P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)</script><ul>\n<li>上式的概率都可以通过统计频率获得：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c) = \\frac{|D_c|}{|D|} \\\\ P(x_i|c) = \\frac{|D_{c, x_i}|}{|D_c|}</script><p>其中<script type=\"math/tex\">D_c</script>为类别为c的样本集，<script type=\"math/tex\">D_{c, x_i}</script>为类别为c并在第<script type=\"math/tex\">i</script>个属性上取值<script type=\"math/tex\">x_i</script>的样本集。若是对于连续属性可考虑概率密度函数，比如<script type=\"math/tex\">p\\left(x_{i} \\mid c\\right) \\sim \\mathcal{N}\\left(\\mu_{c, i}, \\sigma_{c, i}^{2}\\right)</script>，<script type=\"math/tex\">\\mu_{c, i}</script>和<script type=\"math/tex\">\\sigma^2_{c, i}</script>分别是第c类样本在第<script type=\"math/tex\">i</script>个属性上取值的均值和方差</p>\n<h3 id=\"3-2-引入先验分布\"><a href=\"#3-2-引入先验分布\" class=\"headerlink\" title=\"3.2 引入先验分布\"></a>3.2 引入先验分布</h3><ul>\n<li>在上面计算概率时，若某个属性值在训练集中没有与某个类同时出现过，则会导致0乘，频率估计将会出现问题。所以要进行平滑处理，常用<strong>拉普拉斯修正（Laplacian correction）</strong></li>\n<li>具体来说，令N表示可能的类别数，$N_i$表示第$i$个属性可能的取值数，则上面的概率计算式可以修正为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\hat{P}(c) &=\\frac{\\left|D_{c}\\right|+1}{|D|+N} \\\\\n\\hat{P}\\left(x_{i} \\mid c\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{\\left|D_{c}\\right|+N_{i}}\n\\end{aligned}</script><ul>\n<li>显然，拉普拉斯是<strong>引入了一个均匀分布的先验分布</strong>，避免了上述的0乘问题，并且在训练集变大时，修正过程索隐入的先验分布的影响也会逐渐变得可忽略，使得估值逐渐趋于实际概率值</li>\n</ul>\n<h1 id=\"4-半朴素贝叶斯分类器\"><a href=\"#4-半朴素贝叶斯分类器\" class=\"headerlink\" title=\"4 半朴素贝叶斯分类器\"></a>4 半朴素贝叶斯分类器</h1><ul>\n<li>朴素贝叶斯是采用了属性条件独立性假设，但是在现实任务中往往很难成立，于是尝试对这种假设进行一定的放松，由此产生了半朴素贝叶斯分类器。<strong>基本思想是适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系</strong></li>\n<li><strong>独依赖估计（One-Dependent Estimator, OED）</strong>是半朴素贝叶斯分类器最常用的一种策略，就是<strong>假设每个属性在类别之外最多仅依赖一个其他属性</strong>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c \\mid \\boldsymbol{x}) \\propto P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c, p a_{i}\\right)</script><p>其中$pa_i$为$x_i$的父属性，若$pa_i$已知，则可以通过前面的方法计算<script type=\"math/tex\">P(x_i|c, pa_i)</script>，所以问题就转化为如何确定每个属性的父属性</p>\n<h3 id=\"4-1-SPODE\"><a href=\"#4-1-SPODE\" class=\"headerlink\" title=\"4.1 SPODE\"></a>4.1 SPODE</h3><ul>\n<li>SPODE（Super-Parent ODE）方法是<strong>假设所有属性都依赖于同一个属性，成为超父，然后通过交叉验证等模型选择方法来确定超父属性</strong></li>\n</ul>\n<h3 id=\"4-2-TAN\"><a href=\"#4-2-TAN\" class=\"headerlink\" title=\"4.2 TAN\"></a>4.2 TAN</h3><ul>\n<li>TAN（Tree Augmented naive Bayes）是在最大带权生成树的基础上构建的依赖关系，具体步骤如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li>计算任意两个属性之间的条件互信息（conditional mutual information）：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nI\\left(x_{i}, x_{j} \\mid y\\right)=\\sum_{x_{i}, x_{j} ; c \\in \\mathcal{Y}} P\\left(x_{i}, x_{j} \\mid c\\right) \\log \\frac{P\\left(x_{i}, x_{j} \\mid c\\right)}{P\\left(x_{i} \\mid c\\right) P\\left(x_{j} \\mid c\\right)}</script><ol>\n<li>以属性作为结点构建完全图，每两个节点之间边的权重为<script type=\"math/tex\">I(x_i, x_j|y)</script></li>\n<li>构建此完全图的最大带权生成树，挑选根节点，并将边置为有向</li>\n<li>加入类别结点y，增加y到每个属性的有向边</li>\n</ol>\n</blockquote>\n<ul>\n<li>以下是朴素贝叶斯（NB）和两种ODE的对比：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220926174528428.png\" alt=\"image-20220926174528428\" style=\"zoom:80%;\" /></p>\n<h3 id=\"4-3-AODE\"><a href=\"#4-3-AODE\" class=\"headerlink\" title=\"4.3 AODE\"></a>4.3 AODE</h3><ul>\n<li>AODE（Averaged One-Dependent Estimator）是一种基于集成学习的ODE，与SPODE通过模型选择确定超父属性不同，<strong>AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果</strong>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c \\mid \\boldsymbol{x}) \\propto \\sum_{\\substack{i=1 \\\\\\left|D_{x_{i}}\\right| \\geqslant m^{\\prime}}}^{d} P\\left(c, x_{i}\\right) \\prod_{j=1}^{d} P\\left(x_{j} \\mid c, x_{i}\\right)</script><p>其中<script type=\"math/tex\">D_{x_i}</script>是在第$i$个属性上取值为$x_i$的样本集合，$m^{\\prime}$为阈值（默认为30）</p>\n<ul>\n<li>概率统计公式如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\hat{P}\\left(c, x_{i}\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{|D|+ N \\times N_{i}} \\\\\n\\hat{P}\\left(x_{j} \\mid c, x_{i}\\right) &=\\frac{\\left|D_{c, x_{i}, x_{j}}\\right|+1}{\\left|D_{c, x_{i}}\\right|+N_{j}}\n\\end{aligned}</script><ul>\n<li>注意：SPODE是假设类别和超父属性相互独立，所以连乘项前面是乘$P(c)$；而AODE则没有假设两者独立，所以连乘项前是乘$P(c|x_i)$</li>\n</ul>\n<h1 id=\"5-贝叶斯网\"><a href=\"#5-贝叶斯网\" class=\"headerlink\" title=\"5 贝叶斯网\"></a>5 贝叶斯网</h1><ul>\n<li><p>贝叶斯网亦称信念网，借助有向无环图（DAG）来刻画属性之间的依赖关系，并使用条件概率表（CPT）来描述属性间的联合概率分布</p>\n</li>\n<li><p>具体来说，一个贝叶斯网B由结构G和参数<script type=\"math/tex\">\\Theta</script>构成，即<script type=\"math/tex\">B = <G, \\Theta></script>。G是一个有向无环图，每个节点对应一个属性，若两个属性有直接依赖关系，则由一条边连接起来。<script type=\"math/tex\">\\Theta</script>定量描述这种依赖关系，假设属性<script type=\"math/tex\">x_i</script>的父节点集为<script type=\"math/tex\">\\pi_i</script>，则<script type=\"math/tex\">\\Theta</script>包含了每个属性的条件概率表<script type=\"math/tex\">\\theta_{x_i|\\pi_i} = P_B(x_i|\\pi_i)</script>。如下图：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927112128167.png\" alt=\"image-20220927112128167\"  /></p>\n<h3 id=\"5-1-结构\"><a href=\"#5-1-结构\" class=\"headerlink\" title=\"5.1 结构\"></a>5.1 结构</h3><ul>\n<li>贝叶斯网有效的表达了属性间的条件独立性，<strong>给定父节点集，贝叶斯网假设每个属性与他的非后裔属性独立</strong>，那么属性<script type=\"math/tex\">x_1, ...,x_d</script>的联合概率分布为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP_{B}\\left(x_{1}, x_{2}, \\ldots, x_{d}\\right)=\\prod_{i=1}^{d} P_{B}\\left(x_{i} \\mid \\pi_{i}\\right)=\\prod_{i=1}^{d} \\theta_{x_{i} \\mid \\pi_{i}}</script><ul>\n<li>贝叶斯网中有3种典型的依赖关系：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927161634626.png\" alt=\"image-20220927161634626\"></p>\n<p>1、同父结构中，给定父节点$x_1$的值，则$x_3$和$x_4$条件独立</p>\n<blockquote>\n<p><strong>证明：</strong></p>\n<script type=\"math/tex; mode=display\">\nP(x_1, x_3, x_4) = P(x_1)P(x_3|x_1)P(x_4|x_1) \\\\\nP(x_3, x_4| x_1) = P(x_1, x_3, x_4) / P(x_1)</script><p>联立上述两式：</p>\n<script type=\"math/tex; mode=display\">\nP(x_3, x_4| x_1) = P(x_3|x_1)P(x_4|x_1)</script></blockquote>\n<p>2、顺序结构中，给定x的值，则y和z条件独立</p>\n<blockquote>\n<p><strong>证明：</strong></p>\n<script type=\"math/tex; mode=display\">\nP(x, y, z) = P(z)P(x|z)P(y|x) = P(x)P(z|x)P(y|x) \\\\\nP(z, y | x) = P(x, y, z) / P(x)</script><p>联立上述两式：</p>\n<script type=\"math/tex; mode=display\">\nP(z, y | x) = P(z|x)P(y|x)</script></blockquote>\n<p>3、V型结构中，给定$x_4$的取值，则$x_1$和$x_2$必不独立；但是若$x_4$取值完全未知，则$x_1$和$x_2$却是相互独立的（<strong>边际独立性</strong>）</p>\n<blockquote>\n<p><strong>证明：</strong></p>\n<script type=\"math/tex; mode=display\">\nP(x_1, x_2) = \\sum_{x_4}P(x_1, x_2, x_4) = \\sum_{x_4}P(x_1)P(x_2)P(x_4|x_1, x_2) = P(x_1)P(x_2)</script></blockquote>\n<h3 id=\"5-2-有向分离\"><a href=\"#5-2-有向分离\" class=\"headerlink\" title=\"5.2 有向分离\"></a>5.2 有向分离</h3><ul>\n<li>可以使用<strong>有向分离（D-separation）</strong>分析有向图中变量间的条件独立性</li>\n<li>首先要把有向图转化为无向图，由此产生的无向图称为道德图：</li>\n</ul>\n<blockquote>\n<ol>\n<li>找出有向图中的所有V型结构，在V型结构的两个父节点之间加上一条无向边</li>\n<li>然后将所有有向边改为无向边</li>\n</ol>\n</blockquote>\n<ul>\n<li>基于道德图能直观迅速地找到变量间地条件独立性。假定道德图中有变量$x,y$和变量集合<script type=\"math/tex\">z = \\{z_i\\}</script>。若变量x和y能在图上被z分开，即从道德图中将变量集合z去除后，x和y分属两个连通分支，则称x和y被有向分离，<script type=\"math/tex\">x \\perp y | z</script>成立</li>\n</ul>\n<h3 id=\"5-3-学习\"><a href=\"#5-3-学习\" class=\"headerlink\" title=\"5.3 学习\"></a>5.3 学习</h3><ul>\n<li><p>贝叶斯网学习的首要任务是根据训练数据来找出结构最恰当的贝叶斯网。<strong>评分搜索</strong>是求解的常用方法，具体来说，先定义一个评分函数，以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网</p>\n</li>\n<li><p>常用评分函数通常基于信息论准则，此类准则将学习问题看作一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时<strong>编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需的字节长度</strong>。对贝叶斯网学习而言,模型就是一个贝叶斯网</p>\n</li>\n<li>每个贝叶斯网描述了一个在训练数据上的概率分布，<strong>自有一套编码机制能使那些经常出现的样本有更短的编码</strong>。于是应<strong>选择那个综合编码长度(包括描述网络和编码数据)最短的贝叶斯网</strong>，这就是<strong>最小描述长度(Minimal Description Length,MDL)准则</strong></li>\n<li>若给定训练集<script type=\"math/tex\">D = \\{x_1, ..., x_m\\}</script>（每个样本向量中是包含了类别的），则贝叶斯网<script type=\"math/tex\">B = <G, \\Theta></script>在D上的评分函数可以写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ns(B \\mid D)=f(\\theta)|B|-L L(B \\mid D)</script><p>其中|B|是贝叶斯网络的参数个数，$f(\\theta)$表示描述每个参数$\\theta$所需的编码位数；而第二项<script type=\"math/tex\">L L(B \\mid D)=\\sum_{i=1}^{m} \\log P_{B}\\left(x_{i}\\right)</script>是贝叶斯网B的对数似然。<strong>显然第一项是计算编码贝叶斯网B所需的编码位数，第二项是计算B所对应的概率分布$P_B$对D描述的有多好</strong></p>\n<blockquote>\n<ul>\n<li>若$f(\\theta)=1$，则得到AIC评分函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{AIC}(B \\mid D)=|B|-L L(B \\mid D)</script><ul>\n<li>若$f(\\theta) = \\frac{1}{2}\\log m$，则得到BIC评分函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{BIC}(B \\mid D)=\\frac{\\log m}{2}|B|-L L(B \\mid D)</script></blockquote>\n<ul>\n<li>若贝叶斯网B的网络结构G固定，则评分函数第一项为常数，那么最小化$s(B|D)$等价于对参数$\\Theta$的极大似然估计，而此时每个参数<script type=\"math/tex\">\\theta_{x_i|\\pi_i}</script>可以直接从D中通过频率统计获得。<strong>所以，要最小化评分函数，只需对网络每种结构进行搜索，而候选结构的最优参数可直接在训练数据D上计算得到</strong></li>\n</ul>\n<blockquote>\n<p>但是搜索所有可能的结构是一个NP难问题。但是可以采用一些策略求得近似解，比如：</p>\n<ol>\n<li>贪心法，从某个网络结构出发，每次调整一条边（增加、删除、调整方向），直到评分函数不再降低</li>\n<li>添加约束，比如将网络结构限定为树形结构（比如TAN）</li>\n</ol>\n</blockquote>\n<h3 id=\"5-4-推断\"><a href=\"#5-4-推断\" class=\"headerlink\" title=\"5.4 推断\"></a>5.4 推断</h3><ul>\n<li><p>贝叶斯网训练好之后就能用来回答“查询”（query），即通过一些属性变量的观测值来推测其他属性变量的取值（类别也算作一个变量）。例如在西瓜问题中，若我们观测到西瓜色泽青绿、敲声浊响、根蒂蜷缩，想知道它是否成熟、甜度如何。<strong>这样通过已知变量观测值来推测待查询变量的过程称为“推断”（inference），已知变量观测值称为“证据”（evidence）</strong></p>\n</li>\n<li><p>理想情况下是直接通过贝叶斯网定义的联合概率分布来计算后验概率，但是在节点多、连接稠密时，难以进行这样的精确推断，这时需借助<strong>近似推断</strong>，尝试用<strong>吉布斯采样（Gibbs sampling）</strong></p>\n</li>\n<li><p>具体来说，<script type=\"math/tex\">Q = \\{Q_1, ..., Q_n\\}</script>表示带查询变量，<script type=\"math/tex\">E = \\{E_1, ..., E_k\\}</script>表示证据变量，其取值为<script type=\"math/tex\">e = \\{e_1, ..., e_k\\}</script>。我们的任务是计算后验概率<script type=\"math/tex\">P(Q=q|E=e)</script>，其中<script type=\"math/tex\">q = \\{q_1, ..., q_n\\}</script>代表查询变量的一组取值</p>\n</li>\n<li><p>吉布斯采样步骤如下：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927213559671.png\" alt=\"image-20220927213559671\" style=\"zoom:80%;\" /></p>\n<blockquote>\n<p>一开始先产生一个与证据$E = e$一致的样本$q^0$作为初始点，然后经过T次迭代，每次迭代都对非证据变量（即Z）逐个采样，然后由贝叶斯的概率分布推断其取值。若经过T次采样得到的与q一致的样本共有$n_q$个，则可近似估算出后验概率：</p>\n<script type=\"math/tex; mode=display\">\nP(\\mathbf{Q}=\\mathbf{q} \\mid \\mathbf{E}=\\mathbf{e}) \\simeq \\frac{n_{q}}{T}</script></blockquote>\n<h1 id=\"6-EM算法\"><a href=\"#6-EM算法\" class=\"headerlink\" title=\"6 EM算法\"></a>6 EM算法</h1><ul>\n<li>上面的讨论中，都是认定训练样本是完整的，但是现实应用中往往有的属性值未知，这些未观测变量称为<strong>隐变量（latent variable）</strong>，在这种存在隐变量的情况下进行参数估计，可使用EM算法</li>\n</ul>\n<h3 id=\"6-1-基本思想\"><a href=\"#6-1-基本思想\" class=\"headerlink\" title=\"6.1 基本思想\"></a>6.1 基本思想</h3><ul>\n<li>EM 算法的核心思想非常简单，分为两步：Expectation-Step 和 Maximization-Step。E-Step 主要通过观察数据和现有模型来估计参数，然后用这个估计的参数值来计算似然函数的期望值；而 M-Step 是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后似然函数都会增加，所以函数最终会收敛</li>\n<li>于是以随机初始值$\\Theta^0$为起点，执行以下步骤直至收敛：</li>\n</ul>\n<blockquote>\n<ol>\n<li>基于$\\Theta^t$推断隐变量Z的期望，记为$Z^t$</li>\n<li>基于已观测变量X和$Z^t$对参数$\\Theta$做最大似然估计，记为<script type=\"math/tex\">\\Theta^{t+1}</script></li>\n</ol>\n</blockquote>\n<h3 id=\"6-2-举个栗子\"><a href=\"#6-2-举个栗子\" class=\"headerlink\" title=\"6.2 举个栗子\"></a>6.2 举个栗子</h3><ul>\n<li>有两枚硬币A、B，随机抛掷结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-4e19d89b47e21cf284644b0576e9af0f_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<p>很容易估计出两枚硬币抛掷正面的概率：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\theta_{A}=24 / 30=0.8 \\\\\n\\theta_{B}=9 / 20=0.45\n\\end{array}</script><ul>\n<li>现在加入隐变量，抹去每次投掷的硬币标记，即不知道这次投的是A还是B：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-caa896173185a8f527c037c122122258_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<p>这种情况又如何估计$\\theta_A, \\theta_B$呢。我们多出了一个隐变量<script type=\"math/tex\">Z = \\{z_1, ..., z_5\\}</script>，代表每次投掷的硬币类型。我们需要Z才能估计参数$\\theta_A, \\theta_B$，而又需要$\\theta_A, \\theta_B$才能估计Z。其解决方法就是先随机初始化$\\theta_A, \\theta_B$ ，然后用去估计 Z， 然后基于 Z 按照最大似然概率去估计新的$\\theta_A, \\theta_B$，循环至收敛。</p>\n<ul>\n<li>现在随机初始化<script type=\"math/tex\">\\theta_A = 0.6, \\theta_B = 0.5</script>，以第一轮投掷来说，硬币A投出5H5T结果的概率是<script type=\"math/tex\">C_{10}^5 0.6^5 * 0.4^5</script>，而B投出5H5T的概率为<script type=\"math/tex\">C_{10}^5 0.5^5 * 0.5^5</script>，由此可以算出本次使用A或B硬币的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nP_{A}=\\frac{0.6^{5} * 0.4^{5}}{\\left(0.6^{5} * 0.4^{5}\\right)+\\left(0.5^{5} * 0.5^{5}\\right)}=0.45 \\\\\nP_{B}=\\frac{0.5^{5} * 0.5^{5}}{\\left(0.6^{5} * 0.4^{5}\\right)+\\left(0.5^{5} * 0.5^{5}\\right)}=0.55\n\\end{array}</script><p>对其他轮进行同样的操作，得到：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-b325de65a5bcac196fc0939f346410d7_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<p><strong>这一步我们实际上是估计出了 Z 的概率分布，这部就是 E-Step</strong></p>\n<ul>\n<li>结合硬币 A 的概率和上一张投掷结果，我们利用期望可以求出硬币 A 和硬币 B 的贡献。以第二轮硬币 A 为例子，计算方式为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nH: 0.80 * 9=7.2 \\\\\nT: 0.80 * 1=0.8\n\\end{array}</script><p>对其他轮和硬币B进行同样的操作，得到：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-9b6e8c50c0761c6ac19909c26e0a71d4_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>然后用极大似然估计来估计新的$\\theta_A, \\theta_B$：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\theta_{A} &=\\frac{21.3}{21.3+8.6}=0.71 \\\\\n\\theta_{B} &=\\frac{11.7}{11.7+8.4}=0.58\n\\end{aligned}</script><p><strong>这步就对应了 M-Step，重新估计出了参数值。</strong></p>\n<ul>\n<li>如此反复迭代，我们就可以算出最终的参数值。</li>\n</ul>\n<h3 id=\"6-3-算法流程\"><a href=\"#6-3-算法流程\" class=\"headerlink\" title=\"6.3 算法流程\"></a>6.3 算法流程</h3><ul>\n<li>给定观测变量Y、隐变量Z，模型参数为<script type=\"math/tex\">\\theta</script></li>\n</ul>\n<blockquote>\n<ol>\n<li>首先选定参数的初始值<script type=\"math/tex\">\\theta^{(0)}</script>，开始迭代</li>\n<li>E步：在第<script type=\"math/tex\">i+1</script>次迭代时，已知<script type=\"math/tex\">\\theta^{(i)}</script>，计算：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nQ\\left(\\theta, \\theta^{(i)}\\right) &=E_{Z}\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] \\\\\n&=\\sum_{Z} \\log P(Y, Z \\mid \\theta) P\\left(Z \\mid Y, \\theta^{(i)}\\right)\n\\end{aligned}</script><ol>\n<li>M步：求得使<script type=\"math/tex\">Q(\\theta, \\theta^{(i)})</script>最大化的<script type=\"math/tex\">\\theta</script>，作为<script type=\"math/tex\">\\theta^{(i+1)}</script>：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)</script><ol>\n<li>重复上述的E步和M步，直至收敛</li>\n</ol>\n</blockquote>\n<ul>\n<li><strong>注意：</strong>初始值是可以随机选择的，但是<strong>EM算法对初值敏感</strong>，EM有可能收敛到局部最优点</li>\n</ul>\n<h3 id=\"6-4-算法推导\"><a href=\"#6-4-算法推导\" class=\"headerlink\" title=\"6.4 算法推导\"></a>6.4 算法推导</h3><ul>\n<li>面对一个含隐变量的概率模型，目标是最大化：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nL(\\theta) &=\\log P(Y \\mid \\theta)=\\log \\sum_{Z} P(Y, Z \\mid \\theta) \\\\\n&=\\log \\left(\\sum_{Z} P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)\n\\end{aligned}</script><ul>\n<li>在第<script type=\"math/tex\">i+1</script>次迭代时，我们是希望有所提升，即得到的<script type=\"math/tex\">\\theta</script>的似然<script type=\"math/tex\">L(\\theta)</script>要大于当前的似然<script type=\"math/tex\">L(\\theta^{(i)})</script>，所以将两者相减：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nL(\\theta)-L\\left(\\theta^{(i)}\\right) &= \\log \\left(\\sum_{Z} P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right)\\\\\n& =\\log \\left(\\sum_{Z} P\\left(Y \\mid Z, \\theta^{(i)}\\right) \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Y \\mid Z, \\theta^{(i)}\\right)}\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\n& \\geqslant \\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right)}-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\n&=\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\n\\end{aligned}</script><p>上述放缩用到了<strong>Jensen不等式</strong></p>\n<ul>\n<li>令：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nB\\left(\\theta, \\theta^{(i)}\\right) \\hat{=} L\\left(\\theta^{(i)}\\right)+\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}</script><p>那么：</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta) \\geqslant B\\left(\\theta, \\theta^{(i)}\\right)</script><p>即<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>为<script type=\"math/tex\">L(\\theta)</script>的下界，并且在<script type=\"math/tex\">\\theta=\\theta^{(i)}</script>时取等号：</p>\n<script type=\"math/tex; mode=display\">\nL\\left(\\theta^{(i)}\\right)=B\\left(\\theta^{(i)}, \\theta^{(i)}\\right)</script><ul>\n<li>所以，增大下界<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>，同样可以使得<script type=\"math/tex\">L(\\theta)</script>增大，而为了<script type=\"math/tex\">L(\\theta)</script>增大得最多，选择<script type=\"math/tex\">\\theta^{(i+1)}</script>使得<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>达到极大：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\theta^{(i+1)}=\\arg \\max _{\\theta} B\\left(\\theta, \\theta^{(i)}\\right)</script><ul>\n<li>由上式就可以推出<script type=\"math/tex\">Q(\\theta, \\theta^{(i)})</script>函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\theta^{(i+1)} &=\\arg \\max _{\\theta}\\left(L\\left(\\theta^{(i)}\\right)+\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\\right) \\\\\n&=\\arg \\max _{\\theta}\\left(\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log (P(Y \\mid Z, \\theta) P(Z \\mid \\theta))\\right) \\\\\n&=\\arg \\max _{\\theta}\\left(\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log P(Y, Z \\mid \\theta)\\right) \\\\\n&=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)\n\\end{aligned}</script><h3 id=\"6-5-直观解释\"><a href=\"#6-5-直观解释\" class=\"headerlink\" title=\"6.5 直观解释\"></a>6.5 直观解释</h3><ul>\n<li>上方曲线为<script type=\"math/tex\">L(\\theta)</script>，下方曲线为<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>，两者在<script type=\"math/tex\">\\theta=\\theta^{(i)}</script>处相等，此时执行M步：找到<script type=\"math/tex\">\\theta^{(i+1)}=\\arg \\max _{\\theta} B\\left(\\theta, \\theta^{(i)}\\right)</script>。函数<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>的增加同时也造成了<script type=\"math/tex\">L(\\theta)</script>的增加。得到<script type=\"math/tex\">\\theta^{(i+1)}</script>后再执行E步：在<script type=\"math/tex\">\\theta = \\theta^{(i+1)}</script>点重新计算Q函数，然后进行下一次迭代</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221021194004500.png\" alt=\"image-20221021194004500\"></p>\n","site":{"data":{}},"wordcount":11235,"excerpt":"","more":"<h1 id=\"1-贝叶斯决策论\"><a href=\"#1-贝叶斯决策论\" class=\"headerlink\" title=\"1 贝叶斯决策论\"></a>1 贝叶斯决策论</h1><ul>\n<li>假设有N个可能的类别<script type=\"math/tex\">\\mathcal{Y} = \\{c_1, ..., c_N\\}</script>，$\\lambda_{ij}$是将一个$c_i$类样本误分类为$c_j$类的损失，则在样本x上的条件风险为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nR\\left(c_{i} \\mid \\boldsymbol{x}\\right)=\\sum_{j=1}^{N} \\lambda_{i j} P\\left(c_{j} \\mid \\boldsymbol{x}\\right)</script><ul>\n<li>我们的任务是寻找一个判定准则<script type=\"math/tex\">h: \\mathcal{X} \\mapsto \\mathcal{Y}</script>以最小化总体风险：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nR(h)=\\mathbb{E}_{\\boldsymbol{x}}[R(h(\\boldsymbol{x}) \\mid \\boldsymbol{x})]</script><ul>\n<li>显然，对每个样本x，若能最小化条件风险<script type=\"math/tex\">R(h(x) | x)</script>，则总体风险$R(h)$也被最小化。这就产生了<strong>贝叶斯判定准则（Bayes decision rule）</strong>：为最小化总体风险，只需在每个样本上选择那个能使条件风险最小的类别标记，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh^{*}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\min } R(c \\mid \\boldsymbol{x})</script><p><strong>此时，$h^*$称为贝叶斯最优分类器</strong></p>\n<ul>\n<li>具体来说，若目标是最小化分类错误率，则误判损失可以写为0/1损失：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\lambda_{ij}  = \\begin{cases}\n0, if \\quad i = j \\\\\n1, otherwise\n\\end {cases}</script><ul>\n<li>则此时条件风险为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nR(c|x) = 1 - P(c|x)</script><ul>\n<li>所以贝叶斯最优分类器为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh^*(x) = arg\\ max_{c \\in \\mathcal{Y}}P(c|x)</script><p><strong>即对每个样本x，选择能使后验概率$P(c|x)$最大的类别标记</strong></p>\n<ul>\n<li>所以首先要获得后验概率，然而这在现实任务中难以直接获得，所以机器学习的任务是<strong>基于有限的训练样本集尽可能准确地估计出后验概率</strong>，大体有两种策略：</li>\n</ul>\n<blockquote>\n<ul>\n<li><strong>判别式模型（discriminative models）：</strong>直接建模后验概率$P(c|x)$来预测c，决策树、SVM、神经网络等都是判别式模型</li>\n<li><strong>生成式模型（generative models）：</strong>先对联合概率分布$P(x, c)$建模，再由此得到后验概率$P(c|x)$</li>\n</ul>\n</blockquote>\n<ul>\n<li>对于生成式模型，必然考虑贝叶斯定理：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c|x) = \\frac{P(x, c)}{P(x)} \\\\ = \\frac{P(c)P(x|c)}{P(x)}</script><p>其中，$P(c)$是先验概率，$P(x|c)$是似然，$P(x)$是用于归一化的证据因子，与类标记无关，所以建模的时候都是把分母$P(x)$直接去掉。所以现在，<strong>估计后验概率$P(c|x)$的任务就会转化为如何基于训练集D来估计先验概率$P(c)$和似然$P(x|c)$ </strong>。在训练集足够大的时候可以直接用样本频率代替$P(c)$。而$P(x|c)$显然是无法通过频率估计的（不同属性的组合结果太多）</p>\n<blockquote>\n<p>基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上将会遭遇样本稀疏问题。属性数越多，问题越严重</p>\n</blockquote>\n<h1 id=\"2-极大似然估计\"><a href=\"#2-极大似然估计\" class=\"headerlink\" title=\"2 极大似然估计\"></a>2 极大似然估计</h1><ul>\n<li><p>求解$P(x|c)$的一个方法就是使用极大似然估计（MLE），这需要先假定其<strong>具有一种确定的概率分布形式</strong>，再基于训练样本对概率分布的参数进行估计</p>\n</li>\n<li><p>具体来说，记关于类别c的似然为$P(x|c)$，假设$P(x|c)$具有确定的形式并且被参数向量$\\theta_c$唯一确定，我们的任务就是通过训练集估计参数$\\theta_c$，为明确起见，将$P(x|c)$记为$P(x|\\theta_c)$。令$D_c$为数据集D中第c类样本的集合，假设这些样本独立同分布，则参数$\\theta_c$关于$D_c$的似然是：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(D_c|\\theta_c) = \\prod_{x \\in D_c}P(x|\\theta_c)</script><ul>\n<li>然后再对上式取负对数得到$LL(\\theta_c)$，最后得到极大估计值<script type=\"math/tex\">\\hat{\\theta_c}</script>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\hat{\\boldsymbol{\\theta}}_{c}=\\underset{\\boldsymbol{\\theta}_{c}}{\\arg \\max } L L\\left(\\boldsymbol{\\theta}_{c}\\right)</script><h1 id=\"3-朴素贝叶斯分类器\"><a href=\"#3-朴素贝叶斯分类器\" class=\"headerlink\" title=\"3 朴素贝叶斯分类器\"></a>3 朴素贝叶斯分类器</h1><h3 id=\"3-1-基本概念\"><a href=\"#3-1-基本概念\" class=\"headerlink\" title=\"3.1 基本概念\"></a>3.1 基本概念</h3><ul>\n<li><p>上面已经说过最大的困难在于$P(x|c)$难以从有限的样本中估计而得。而朴素贝叶斯采用了<strong>属性条件独立性假设：对已知类别，假设所有属性相互独立</strong></p>\n</li>\n<li><p>则生成式模型的目标可以重写为：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c \\mid \\boldsymbol{x})=\\frac{P(c) P(\\boldsymbol{x} \\mid c)}{P(\\boldsymbol{x})}=\\frac{P(c)}{P(\\boldsymbol{x})} \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)</script><p>其中d为属性个数，$x_i$为样本$x$在第$i$个属性上的取值</p>\n<ul>\n<li>所以朴素贝叶斯分类器的表达式为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh_{n b}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\max } P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)</script><ul>\n<li>上式的概率都可以通过统计频率获得：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c) = \\frac{|D_c|}{|D|} \\\\ P(x_i|c) = \\frac{|D_{c, x_i}|}{|D_c|}</script><p>其中<script type=\"math/tex\">D_c</script>为类别为c的样本集，<script type=\"math/tex\">D_{c, x_i}</script>为类别为c并在第<script type=\"math/tex\">i</script>个属性上取值<script type=\"math/tex\">x_i</script>的样本集。若是对于连续属性可考虑概率密度函数，比如<script type=\"math/tex\">p\\left(x_{i} \\mid c\\right) \\sim \\mathcal{N}\\left(\\mu_{c, i}, \\sigma_{c, i}^{2}\\right)</script>，<script type=\"math/tex\">\\mu_{c, i}</script>和<script type=\"math/tex\">\\sigma^2_{c, i}</script>分别是第c类样本在第<script type=\"math/tex\">i</script>个属性上取值的均值和方差</p>\n<h3 id=\"3-2-引入先验分布\"><a href=\"#3-2-引入先验分布\" class=\"headerlink\" title=\"3.2 引入先验分布\"></a>3.2 引入先验分布</h3><ul>\n<li>在上面计算概率时，若某个属性值在训练集中没有与某个类同时出现过，则会导致0乘，频率估计将会出现问题。所以要进行平滑处理，常用<strong>拉普拉斯修正（Laplacian correction）</strong></li>\n<li>具体来说，令N表示可能的类别数，$N_i$表示第$i$个属性可能的取值数，则上面的概率计算式可以修正为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\hat{P}(c) &=\\frac{\\left|D_{c}\\right|+1}{|D|+N} \\\\\n\\hat{P}\\left(x_{i} \\mid c\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{\\left|D_{c}\\right|+N_{i}}\n\\end{aligned}</script><ul>\n<li>显然，拉普拉斯是<strong>引入了一个均匀分布的先验分布</strong>，避免了上述的0乘问题，并且在训练集变大时，修正过程索隐入的先验分布的影响也会逐渐变得可忽略，使得估值逐渐趋于实际概率值</li>\n</ul>\n<h1 id=\"4-半朴素贝叶斯分类器\"><a href=\"#4-半朴素贝叶斯分类器\" class=\"headerlink\" title=\"4 半朴素贝叶斯分类器\"></a>4 半朴素贝叶斯分类器</h1><ul>\n<li>朴素贝叶斯是采用了属性条件独立性假设，但是在现实任务中往往很难成立，于是尝试对这种假设进行一定的放松，由此产生了半朴素贝叶斯分类器。<strong>基本思想是适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系</strong></li>\n<li><strong>独依赖估计（One-Dependent Estimator, OED）</strong>是半朴素贝叶斯分类器最常用的一种策略，就是<strong>假设每个属性在类别之外最多仅依赖一个其他属性</strong>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c \\mid \\boldsymbol{x}) \\propto P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c, p a_{i}\\right)</script><p>其中$pa_i$为$x_i$的父属性，若$pa_i$已知，则可以通过前面的方法计算<script type=\"math/tex\">P(x_i|c, pa_i)</script>，所以问题就转化为如何确定每个属性的父属性</p>\n<h3 id=\"4-1-SPODE\"><a href=\"#4-1-SPODE\" class=\"headerlink\" title=\"4.1 SPODE\"></a>4.1 SPODE</h3><ul>\n<li>SPODE（Super-Parent ODE）方法是<strong>假设所有属性都依赖于同一个属性，成为超父，然后通过交叉验证等模型选择方法来确定超父属性</strong></li>\n</ul>\n<h3 id=\"4-2-TAN\"><a href=\"#4-2-TAN\" class=\"headerlink\" title=\"4.2 TAN\"></a>4.2 TAN</h3><ul>\n<li>TAN（Tree Augmented naive Bayes）是在最大带权生成树的基础上构建的依赖关系，具体步骤如下：</li>\n</ul>\n<blockquote>\n<ol>\n<li>计算任意两个属性之间的条件互信息（conditional mutual information）：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\nI\\left(x_{i}, x_{j} \\mid y\\right)=\\sum_{x_{i}, x_{j} ; c \\in \\mathcal{Y}} P\\left(x_{i}, x_{j} \\mid c\\right) \\log \\frac{P\\left(x_{i}, x_{j} \\mid c\\right)}{P\\left(x_{i} \\mid c\\right) P\\left(x_{j} \\mid c\\right)}</script><ol>\n<li>以属性作为结点构建完全图，每两个节点之间边的权重为<script type=\"math/tex\">I(x_i, x_j|y)</script></li>\n<li>构建此完全图的最大带权生成树，挑选根节点，并将边置为有向</li>\n<li>加入类别结点y，增加y到每个属性的有向边</li>\n</ol>\n</blockquote>\n<ul>\n<li>以下是朴素贝叶斯（NB）和两种ODE的对比：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220926174528428.png\" alt=\"image-20220926174528428\" style=\"zoom:80%;\" /></p>\n<h3 id=\"4-3-AODE\"><a href=\"#4-3-AODE\" class=\"headerlink\" title=\"4.3 AODE\"></a>4.3 AODE</h3><ul>\n<li>AODE（Averaged One-Dependent Estimator）是一种基于集成学习的ODE，与SPODE通过模型选择确定超父属性不同，<strong>AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果</strong>，即：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(c \\mid \\boldsymbol{x}) \\propto \\sum_{\\substack{i=1 \\\\\\left|D_{x_{i}}\\right| \\geqslant m^{\\prime}}}^{d} P\\left(c, x_{i}\\right) \\prod_{j=1}^{d} P\\left(x_{j} \\mid c, x_{i}\\right)</script><p>其中<script type=\"math/tex\">D_{x_i}</script>是在第$i$个属性上取值为$x_i$的样本集合，$m^{\\prime}$为阈值（默认为30）</p>\n<ul>\n<li>概率统计公式如下：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\hat{P}\\left(c, x_{i}\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{|D|+ N \\times N_{i}} \\\\\n\\hat{P}\\left(x_{j} \\mid c, x_{i}\\right) &=\\frac{\\left|D_{c, x_{i}, x_{j}}\\right|+1}{\\left|D_{c, x_{i}}\\right|+N_{j}}\n\\end{aligned}</script><ul>\n<li>注意：SPODE是假设类别和超父属性相互独立，所以连乘项前面是乘$P(c)$；而AODE则没有假设两者独立，所以连乘项前是乘$P(c|x_i)$</li>\n</ul>\n<h1 id=\"5-贝叶斯网\"><a href=\"#5-贝叶斯网\" class=\"headerlink\" title=\"5 贝叶斯网\"></a>5 贝叶斯网</h1><ul>\n<li><p>贝叶斯网亦称信念网，借助有向无环图（DAG）来刻画属性之间的依赖关系，并使用条件概率表（CPT）来描述属性间的联合概率分布</p>\n</li>\n<li><p>具体来说，一个贝叶斯网B由结构G和参数<script type=\"math/tex\">\\Theta</script>构成，即<script type=\"math/tex\">B = <G, \\Theta></script>。G是一个有向无环图，每个节点对应一个属性，若两个属性有直接依赖关系，则由一条边连接起来。<script type=\"math/tex\">\\Theta</script>定量描述这种依赖关系，假设属性<script type=\"math/tex\">x_i</script>的父节点集为<script type=\"math/tex\">\\pi_i</script>，则<script type=\"math/tex\">\\Theta</script>包含了每个属性的条件概率表<script type=\"math/tex\">\\theta_{x_i|\\pi_i} = P_B(x_i|\\pi_i)</script>。如下图：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927112128167.png\" alt=\"image-20220927112128167\"  /></p>\n<h3 id=\"5-1-结构\"><a href=\"#5-1-结构\" class=\"headerlink\" title=\"5.1 结构\"></a>5.1 结构</h3><ul>\n<li>贝叶斯网有效的表达了属性间的条件独立性，<strong>给定父节点集，贝叶斯网假设每个属性与他的非后裔属性独立</strong>，那么属性<script type=\"math/tex\">x_1, ...,x_d</script>的联合概率分布为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP_{B}\\left(x_{1}, x_{2}, \\ldots, x_{d}\\right)=\\prod_{i=1}^{d} P_{B}\\left(x_{i} \\mid \\pi_{i}\\right)=\\prod_{i=1}^{d} \\theta_{x_{i} \\mid \\pi_{i}}</script><ul>\n<li>贝叶斯网中有3种典型的依赖关系：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927161634626.png\" alt=\"image-20220927161634626\"></p>\n<p>1、同父结构中，给定父节点$x_1$的值，则$x_3$和$x_4$条件独立</p>\n<blockquote>\n<p><strong>证明：</strong></p>\n<script type=\"math/tex; mode=display\">\nP(x_1, x_3, x_4) = P(x_1)P(x_3|x_1)P(x_4|x_1) \\\\\nP(x_3, x_4| x_1) = P(x_1, x_3, x_4) / P(x_1)</script><p>联立上述两式：</p>\n<script type=\"math/tex; mode=display\">\nP(x_3, x_4| x_1) = P(x_3|x_1)P(x_4|x_1)</script></blockquote>\n<p>2、顺序结构中，给定x的值，则y和z条件独立</p>\n<blockquote>\n<p><strong>证明：</strong></p>\n<script type=\"math/tex; mode=display\">\nP(x, y, z) = P(z)P(x|z)P(y|x) = P(x)P(z|x)P(y|x) \\\\\nP(z, y | x) = P(x, y, z) / P(x)</script><p>联立上述两式：</p>\n<script type=\"math/tex; mode=display\">\nP(z, y | x) = P(z|x)P(y|x)</script></blockquote>\n<p>3、V型结构中，给定$x_4$的取值，则$x_1$和$x_2$必不独立；但是若$x_4$取值完全未知，则$x_1$和$x_2$却是相互独立的（<strong>边际独立性</strong>）</p>\n<blockquote>\n<p><strong>证明：</strong></p>\n<script type=\"math/tex; mode=display\">\nP(x_1, x_2) = \\sum_{x_4}P(x_1, x_2, x_4) = \\sum_{x_4}P(x_1)P(x_2)P(x_4|x_1, x_2) = P(x_1)P(x_2)</script></blockquote>\n<h3 id=\"5-2-有向分离\"><a href=\"#5-2-有向分离\" class=\"headerlink\" title=\"5.2 有向分离\"></a>5.2 有向分离</h3><ul>\n<li>可以使用<strong>有向分离（D-separation）</strong>分析有向图中变量间的条件独立性</li>\n<li>首先要把有向图转化为无向图，由此产生的无向图称为道德图：</li>\n</ul>\n<blockquote>\n<ol>\n<li>找出有向图中的所有V型结构，在V型结构的两个父节点之间加上一条无向边</li>\n<li>然后将所有有向边改为无向边</li>\n</ol>\n</blockquote>\n<ul>\n<li>基于道德图能直观迅速地找到变量间地条件独立性。假定道德图中有变量$x,y$和变量集合<script type=\"math/tex\">z = \\{z_i\\}</script>。若变量x和y能在图上被z分开，即从道德图中将变量集合z去除后，x和y分属两个连通分支，则称x和y被有向分离，<script type=\"math/tex\">x \\perp y | z</script>成立</li>\n</ul>\n<h3 id=\"5-3-学习\"><a href=\"#5-3-学习\" class=\"headerlink\" title=\"5.3 学习\"></a>5.3 学习</h3><ul>\n<li><p>贝叶斯网学习的首要任务是根据训练数据来找出结构最恰当的贝叶斯网。<strong>评分搜索</strong>是求解的常用方法，具体来说，先定义一个评分函数，以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网</p>\n</li>\n<li><p>常用评分函数通常基于信息论准则，此类准则将学习问题看作一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时<strong>编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需的字节长度</strong>。对贝叶斯网学习而言,模型就是一个贝叶斯网</p>\n</li>\n<li>每个贝叶斯网描述了一个在训练数据上的概率分布，<strong>自有一套编码机制能使那些经常出现的样本有更短的编码</strong>。于是应<strong>选择那个综合编码长度(包括描述网络和编码数据)最短的贝叶斯网</strong>，这就是<strong>最小描述长度(Minimal Description Length,MDL)准则</strong></li>\n<li>若给定训练集<script type=\"math/tex\">D = \\{x_1, ..., x_m\\}</script>（每个样本向量中是包含了类别的），则贝叶斯网<script type=\"math/tex\">B = <G, \\Theta></script>在D上的评分函数可以写为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\ns(B \\mid D)=f(\\theta)|B|-L L(B \\mid D)</script><p>其中|B|是贝叶斯网络的参数个数，$f(\\theta)$表示描述每个参数$\\theta$所需的编码位数；而第二项<script type=\"math/tex\">L L(B \\mid D)=\\sum_{i=1}^{m} \\log P_{B}\\left(x_{i}\\right)</script>是贝叶斯网B的对数似然。<strong>显然第一项是计算编码贝叶斯网B所需的编码位数，第二项是计算B所对应的概率分布$P_B$对D描述的有多好</strong></p>\n<blockquote>\n<ul>\n<li>若$f(\\theta)=1$，则得到AIC评分函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{AIC}(B \\mid D)=|B|-L L(B \\mid D)</script><ul>\n<li>若$f(\\theta) = \\frac{1}{2}\\log m$，则得到BIC评分函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\operatorname{BIC}(B \\mid D)=\\frac{\\log m}{2}|B|-L L(B \\mid D)</script></blockquote>\n<ul>\n<li>若贝叶斯网B的网络结构G固定，则评分函数第一项为常数，那么最小化$s(B|D)$等价于对参数$\\Theta$的极大似然估计，而此时每个参数<script type=\"math/tex\">\\theta_{x_i|\\pi_i}</script>可以直接从D中通过频率统计获得。<strong>所以，要最小化评分函数，只需对网络每种结构进行搜索，而候选结构的最优参数可直接在训练数据D上计算得到</strong></li>\n</ul>\n<blockquote>\n<p>但是搜索所有可能的结构是一个NP难问题。但是可以采用一些策略求得近似解，比如：</p>\n<ol>\n<li>贪心法，从某个网络结构出发，每次调整一条边（增加、删除、调整方向），直到评分函数不再降低</li>\n<li>添加约束，比如将网络结构限定为树形结构（比如TAN）</li>\n</ol>\n</blockquote>\n<h3 id=\"5-4-推断\"><a href=\"#5-4-推断\" class=\"headerlink\" title=\"5.4 推断\"></a>5.4 推断</h3><ul>\n<li><p>贝叶斯网训练好之后就能用来回答“查询”（query），即通过一些属性变量的观测值来推测其他属性变量的取值（类别也算作一个变量）。例如在西瓜问题中，若我们观测到西瓜色泽青绿、敲声浊响、根蒂蜷缩，想知道它是否成熟、甜度如何。<strong>这样通过已知变量观测值来推测待查询变量的过程称为“推断”（inference），已知变量观测值称为“证据”（evidence）</strong></p>\n</li>\n<li><p>理想情况下是直接通过贝叶斯网定义的联合概率分布来计算后验概率，但是在节点多、连接稠密时，难以进行这样的精确推断，这时需借助<strong>近似推断</strong>，尝试用<strong>吉布斯采样（Gibbs sampling）</strong></p>\n</li>\n<li><p>具体来说，<script type=\"math/tex\">Q = \\{Q_1, ..., Q_n\\}</script>表示带查询变量，<script type=\"math/tex\">E = \\{E_1, ..., E_k\\}</script>表示证据变量，其取值为<script type=\"math/tex\">e = \\{e_1, ..., e_k\\}</script>。我们的任务是计算后验概率<script type=\"math/tex\">P(Q=q|E=e)</script>，其中<script type=\"math/tex\">q = \\{q_1, ..., q_n\\}</script>代表查询变量的一组取值</p>\n</li>\n<li><p>吉布斯采样步骤如下：</p>\n</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20220927213559671.png\" alt=\"image-20220927213559671\" style=\"zoom:80%;\" /></p>\n<blockquote>\n<p>一开始先产生一个与证据$E = e$一致的样本$q^0$作为初始点，然后经过T次迭代，每次迭代都对非证据变量（即Z）逐个采样，然后由贝叶斯的概率分布推断其取值。若经过T次采样得到的与q一致的样本共有$n_q$个，则可近似估算出后验概率：</p>\n<script type=\"math/tex; mode=display\">\nP(\\mathbf{Q}=\\mathbf{q} \\mid \\mathbf{E}=\\mathbf{e}) \\simeq \\frac{n_{q}}{T}</script></blockquote>\n<h1 id=\"6-EM算法\"><a href=\"#6-EM算法\" class=\"headerlink\" title=\"6 EM算法\"></a>6 EM算法</h1><ul>\n<li>上面的讨论中，都是认定训练样本是完整的，但是现实应用中往往有的属性值未知，这些未观测变量称为<strong>隐变量（latent variable）</strong>，在这种存在隐变量的情况下进行参数估计，可使用EM算法</li>\n</ul>\n<h3 id=\"6-1-基本思想\"><a href=\"#6-1-基本思想\" class=\"headerlink\" title=\"6.1 基本思想\"></a>6.1 基本思想</h3><ul>\n<li>EM 算法的核心思想非常简单，分为两步：Expectation-Step 和 Maximization-Step。E-Step 主要通过观察数据和现有模型来估计参数，然后用这个估计的参数值来计算似然函数的期望值；而 M-Step 是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后似然函数都会增加，所以函数最终会收敛</li>\n<li>于是以随机初始值$\\Theta^0$为起点，执行以下步骤直至收敛：</li>\n</ul>\n<blockquote>\n<ol>\n<li>基于$\\Theta^t$推断隐变量Z的期望，记为$Z^t$</li>\n<li>基于已观测变量X和$Z^t$对参数$\\Theta$做最大似然估计，记为<script type=\"math/tex\">\\Theta^{t+1}</script></li>\n</ol>\n</blockquote>\n<h3 id=\"6-2-举个栗子\"><a href=\"#6-2-举个栗子\" class=\"headerlink\" title=\"6.2 举个栗子\"></a>6.2 举个栗子</h3><ul>\n<li>有两枚硬币A、B，随机抛掷结果如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-4e19d89b47e21cf284644b0576e9af0f_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<p>很容易估计出两枚硬币抛掷正面的概率：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\n\\theta_{A}=24 / 30=0.8 \\\\\n\\theta_{B}=9 / 20=0.45\n\\end{array}</script><ul>\n<li>现在加入隐变量，抹去每次投掷的硬币标记，即不知道这次投的是A还是B：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-caa896173185a8f527c037c122122258_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<p>这种情况又如何估计$\\theta_A, \\theta_B$呢。我们多出了一个隐变量<script type=\"math/tex\">Z = \\{z_1, ..., z_5\\}</script>，代表每次投掷的硬币类型。我们需要Z才能估计参数$\\theta_A, \\theta_B$，而又需要$\\theta_A, \\theta_B$才能估计Z。其解决方法就是先随机初始化$\\theta_A, \\theta_B$ ，然后用去估计 Z， 然后基于 Z 按照最大似然概率去估计新的$\\theta_A, \\theta_B$，循环至收敛。</p>\n<ul>\n<li>现在随机初始化<script type=\"math/tex\">\\theta_A = 0.6, \\theta_B = 0.5</script>，以第一轮投掷来说，硬币A投出5H5T结果的概率是<script type=\"math/tex\">C_{10}^5 0.6^5 * 0.4^5</script>，而B投出5H5T的概率为<script type=\"math/tex\">C_{10}^5 0.5^5 * 0.5^5</script>，由此可以算出本次使用A或B硬币的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nP_{A}=\\frac{0.6^{5} * 0.4^{5}}{\\left(0.6^{5} * 0.4^{5}\\right)+\\left(0.5^{5} * 0.5^{5}\\right)}=0.45 \\\\\nP_{B}=\\frac{0.5^{5} * 0.5^{5}}{\\left(0.6^{5} * 0.4^{5}\\right)+\\left(0.5^{5} * 0.5^{5}\\right)}=0.55\n\\end{array}</script><p>对其他轮进行同样的操作，得到：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-b325de65a5bcac196fc0939f346410d7_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<p><strong>这一步我们实际上是估计出了 Z 的概率分布，这部就是 E-Step</strong></p>\n<ul>\n<li>结合硬币 A 的概率和上一张投掷结果，我们利用期望可以求出硬币 A 和硬币 B 的贡献。以第二轮硬币 A 为例子，计算方式为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nH: 0.80 * 9=7.2 \\\\\nT: 0.80 * 1=0.8\n\\end{array}</script><p>对其他轮和硬币B进行同样的操作，得到：</p>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/v2-9b6e8c50c0761c6ac19909c26e0a71d4_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /></p>\n<ul>\n<li>然后用极大似然估计来估计新的$\\theta_A, \\theta_B$：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\theta_{A} &=\\frac{21.3}{21.3+8.6}=0.71 \\\\\n\\theta_{B} &=\\frac{11.7}{11.7+8.4}=0.58\n\\end{aligned}</script><p><strong>这步就对应了 M-Step，重新估计出了参数值。</strong></p>\n<ul>\n<li>如此反复迭代，我们就可以算出最终的参数值。</li>\n</ul>\n<h3 id=\"6-3-算法流程\"><a href=\"#6-3-算法流程\" class=\"headerlink\" title=\"6.3 算法流程\"></a>6.3 算法流程</h3><ul>\n<li>给定观测变量Y、隐变量Z，模型参数为<script type=\"math/tex\">\\theta</script></li>\n</ul>\n<blockquote>\n<ol>\n<li>首先选定参数的初始值<script type=\"math/tex\">\\theta^{(0)}</script>，开始迭代</li>\n<li>E步：在第<script type=\"math/tex\">i+1</script>次迭代时，已知<script type=\"math/tex\">\\theta^{(i)}</script>，计算：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nQ\\left(\\theta, \\theta^{(i)}\\right) &=E_{Z}\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] \\\\\n&=\\sum_{Z} \\log P(Y, Z \\mid \\theta) P\\left(Z \\mid Y, \\theta^{(i)}\\right)\n\\end{aligned}</script><ol>\n<li>M步：求得使<script type=\"math/tex\">Q(\\theta, \\theta^{(i)})</script>最大化的<script type=\"math/tex\">\\theta</script>，作为<script type=\"math/tex\">\\theta^{(i+1)}</script>：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)</script><ol>\n<li>重复上述的E步和M步，直至收敛</li>\n</ol>\n</blockquote>\n<ul>\n<li><strong>注意：</strong>初始值是可以随机选择的，但是<strong>EM算法对初值敏感</strong>，EM有可能收敛到局部最优点</li>\n</ul>\n<h3 id=\"6-4-算法推导\"><a href=\"#6-4-算法推导\" class=\"headerlink\" title=\"6.4 算法推导\"></a>6.4 算法推导</h3><ul>\n<li>面对一个含隐变量的概率模型，目标是最大化：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nL(\\theta) &=\\log P(Y \\mid \\theta)=\\log \\sum_{Z} P(Y, Z \\mid \\theta) \\\\\n&=\\log \\left(\\sum_{Z} P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)\n\\end{aligned}</script><ul>\n<li>在第<script type=\"math/tex\">i+1</script>次迭代时，我们是希望有所提升，即得到的<script type=\"math/tex\">\\theta</script>的似然<script type=\"math/tex\">L(\\theta)</script>要大于当前的似然<script type=\"math/tex\">L(\\theta^{(i)})</script>，所以将两者相减：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nL(\\theta)-L\\left(\\theta^{(i)}\\right) &= \\log \\left(\\sum_{Z} P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right)\\\\\n& =\\log \\left(\\sum_{Z} P\\left(Y \\mid Z, \\theta^{(i)}\\right) \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Y \\mid Z, \\theta^{(i)}\\right)}\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\n& \\geqslant \\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right)}-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\n&=\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\n\\end{aligned}</script><p>上述放缩用到了<strong>Jensen不等式</strong></p>\n<ul>\n<li>令：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nB\\left(\\theta, \\theta^{(i)}\\right) \\hat{=} L\\left(\\theta^{(i)}\\right)+\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}</script><p>那么：</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta) \\geqslant B\\left(\\theta, \\theta^{(i)}\\right)</script><p>即<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>为<script type=\"math/tex\">L(\\theta)</script>的下界，并且在<script type=\"math/tex\">\\theta=\\theta^{(i)}</script>时取等号：</p>\n<script type=\"math/tex; mode=display\">\nL\\left(\\theta^{(i)}\\right)=B\\left(\\theta^{(i)}, \\theta^{(i)}\\right)</script><ul>\n<li>所以，增大下界<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>，同样可以使得<script type=\"math/tex\">L(\\theta)</script>增大，而为了<script type=\"math/tex\">L(\\theta)</script>增大得最多，选择<script type=\"math/tex\">\\theta^{(i+1)}</script>使得<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>达到极大：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\theta^{(i+1)}=\\arg \\max _{\\theta} B\\left(\\theta, \\theta^{(i)}\\right)</script><ul>\n<li>由上式就可以推出<script type=\"math/tex\">Q(\\theta, \\theta^{(i)})</script>函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\theta^{(i+1)} &=\\arg \\max _{\\theta}\\left(L\\left(\\theta^{(i)}\\right)+\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\\right) \\\\\n&=\\arg \\max _{\\theta}\\left(\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log (P(Y \\mid Z, \\theta) P(Z \\mid \\theta))\\right) \\\\\n&=\\arg \\max _{\\theta}\\left(\\sum_{Z} P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log P(Y, Z \\mid \\theta)\\right) \\\\\n&=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)\n\\end{aligned}</script><h3 id=\"6-5-直观解释\"><a href=\"#6-5-直观解释\" class=\"headerlink\" title=\"6.5 直观解释\"></a>6.5 直观解释</h3><ul>\n<li>上方曲线为<script type=\"math/tex\">L(\\theta)</script>，下方曲线为<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>，两者在<script type=\"math/tex\">\\theta=\\theta^{(i)}</script>处相等，此时执行M步：找到<script type=\"math/tex\">\\theta^{(i+1)}=\\arg \\max _{\\theta} B\\left(\\theta, \\theta^{(i)}\\right)</script>。函数<script type=\"math/tex\">B(\\theta, \\theta^{(i)})</script>的增加同时也造成了<script type=\"math/tex\">L(\\theta)</script>的增加。得到<script type=\"math/tex\">\\theta^{(i+1)}</script>后再执行E步：在<script type=\"math/tex\">\\theta = \\theta^{(i+1)}</script>点重新计算Q函数，然后进行下一次迭代</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221021194004500.png\" alt=\"image-20221021194004500\"></p>\n"},{"title":"集成学习","math":true,"date":"2022-05-16T16:00:00.000Z","_content":"\n\n\n# 1 基本概念\n\n- 集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务。其中每个**个体学习器**如果都使用同样的算法，则称这种集成是**同质**的，同质集成中的个体学习器亦称**基学习器**，反之则为**异质**的\n- 集成学习的相比于单一学习器可获得更优越的泛化性能，这对**弱学习器（泛化性能略优于随机猜测的学习器）**尤为明显\n\n- **对于每个个体学习器，要有一定的准确性，即学习器不能太坏，也要有多样性，即学习器间具有差异**\n\n- 做一个简单的分析：\n\n> - 考虑一个二分类问题，假设**每个基学习器的错误率相互独立**且为$$\\epsilon$$，即对每个基学习器$$h_i$$有：\n>\n> $$\n> P(h_i(x) \\neq f(x)) = \\epsilon\n> $$\n>\n> - 假设集成时使用简单投票法结合T个基学习器：\n>\n> $$\n> H(\\boldsymbol{x})=\\operatorname{sign}\\left(\\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})\\right)\n> $$\n>\n> - 由于每个基学习器的错误率相互独立，则由Hoeffding不等式可知，集成错误率为：\n>\n> $$\n> \\begin{aligned}\n> P(H(\\boldsymbol{x}) \\neq f(\\boldsymbol{x})) &=\\sum_{k=0}^{\\lfloor T / 2\\rfloor}C_T^k(1-\\epsilon)^{k} \\epsilon^{T-k} \\\\\n> & \\leqslant \\exp \\left(-\\frac{1}{2} T(1-2 \\epsilon)^{2}\\right)\n> \\end{aligned}\n> $$\n>\n> - **由上式可得：随着T的增大，集成的错误率降指数级下降，最终趋于0**\n\n- 在上面的分析中提到了一个关键的假设：基学习器的误差相互独立。但是在现实任务中是不可能的。目前的集成学习方法可大致分为两类：\n\n> 1. 个体学习器之间存在强依赖关系，必须串行生成的序列化方法，代表是Boosting\n> 2. 个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林（RF）\n\n\n\n\n\n# 2 Boosting\n\n- Boosting是一族可将弱学习器提升为强学习器的算法，工作机制为：**先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此重复直至得到事先指定的T**\n- 从偏差-方差的角度看，**Boosting主要关注降低偏差**\n\n\n\n###  2.1 AdaBoost\n\n#### 2.1.1 算法流程\n\n- 其中最著名的代表就是AdaBoost，考虑一个二分类任务，其算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221015122613734.png\" alt=\"image-20221015122613734\" style=\"zoom:80%;\" />\n\n其中的$$D_t$$为样本权重分布，反应了每个样本对基学习器的重要程度（反映在对loss的贡献上）。$$\\alpha_t$$为每个基学习器的权重。在每次更新样本权重分布的时候提升分类错误样本的权重，其中$$Z_t$$是一个规范化因子\n\n\n\n#### 2.1.2 损失函数\n\n- AdaBoost是基于加法模型，即基于学习器的线性组合：\n\n$$\nH(\\boldsymbol{x})=\\sum_{t=1}^{T} \\alpha_{t} h_{t}(\\boldsymbol{x})\n$$\n\n来最小化指数损失函数：\n$$\n\\ell_{\\exp }(H \\mid \\mathcal{D})=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\\right]\n$$\n其实**指数函数是分类任务原本0/1损失函数的一致的替代损失函数**（因为其拥有更好的数学性质）\n\n> **证明：**\n>\n> - 求$$H(x)$$关于损失函数的偏导：\n>\n> $$\n> \\frac{\\partial \\ell_{\\exp }(H \\mid \\mathcal{D})}{\\partial H(\\boldsymbol{x})}=-e^{-H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=1 \\mid \\boldsymbol{x})+e^{H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=-1 \\mid \\boldsymbol{x})\n> $$\n>\n> - 偏导数为0可求得极点：\n>\n> $$\n> H(\\boldsymbol{x})=\\frac{1}{2} \\ln \\frac{P(f(x)=1 \\mid \\boldsymbol{x})}{P(f(x)=-1 \\mid \\boldsymbol{x})}\n> $$\n>\n> - 因此有：\n>\n> $$\n> \\begin{array}{l}\n> sign(H(x)) =\\left\\{\\begin{array}{l}\n> 1, \\quad P(f(x)=1 \\mid \\boldsymbol{x})>P(f(x)=-1 \\mid \\boldsymbol{x}) \\\\\n> -1, \\quad P(f(x)=1 \\mid \\boldsymbol{x})<P(f(x)=-1 \\mid \\boldsymbol{x})\n> \\end{array}\\right. \\\\\n> =\\underset{y \\in\\{-1,1\\}}{\\arg \\max } P(f(x)=y \\mid \\boldsymbol{x})\n> \\end{array}\n> $$\n\n\n\n#### 2.1.3 基学习器的权重\n\n- 在通过$$D_t$$产生$$h_t$$后，该基分类器的权重$$\\alpha_t$$应使得$$\\alpha_th_t$$最小化指数损失函数（在$$D_t$$分布上而非D分布）：\n\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(\\alpha_{t} h_{t} \\mid \\mathcal{D}_{t}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right)\\right] \\\\\n&=e^{-\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right) \\\\\n&=e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t}\n\\end{aligned}\n$$\n\n- 关于$$\\alpha_t$$求导：\n\n$$\n\\frac{\\partial \\ell_{\\exp }\\left(\\alpha_{t} h_{t} \\mid \\mathcal{D}_{t}\\right)}{\\partial \\alpha_{t}}=-e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t} = 0 \\\\\n\\alpha_t = \\frac{1}{2}\\ln(\\frac{1 - \\epsilon_t}{\\epsilon_t})\n$$\n\n\n\n#### 2.1.4 样本权重分布的更新\n\n- 在获得$$H_{t-1}$$后样本分布将进行调整，使下一轮的$$h_t$$能纠正$$H_{t-1}$$的一些错误，理想情况下能纠正$$H_{t-1}$$的全部错误，即最小化$$\\ell_{\\exp }\\left(H_{t-1}+\\alpha_th_{t} \\mid \\mathcal{D}\\right)$$，可简化为：\n\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} \\mid \\mathcal{D}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x})\\left(H_{t-1}(\\boldsymbol{x})+h_{t}(\\boldsymbol{x})\\right)}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})}\\right]\n\\end{aligned}\n$$\n\n其中的$$e^{-f(x)h_t(x)}$$可用泰勒展示近似：\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} \\mid \\mathcal{D}\\right) & \\simeq \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{f^{2}(\\boldsymbol{x}) h_{t}^{2}(\\boldsymbol{x})}{2}\\right)\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{1}{2}\\right)\\right]\n\\end{aligned}\n$$\n\n- 于是理想的基学习器为：\n\n$$\n\\begin{array}{l}\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\ell_{\\exp }\\left(H_{t-1}+h \\mid \\mathcal{D}\\right) \\\\\n=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\\n=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right]\n\\end{array}\n$$\n\n注意最后添加的分母是一个常数。令$$D_t$$表示分布：\n$$\n\\mathcal{D}_{t}(\\boldsymbol{x})=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}\n$$\n则：\n$$\n\\begin{aligned}\nh_{t}(\\boldsymbol{x}) &=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\\n&=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[f(\\boldsymbol{x}) h(\\boldsymbol{x})]\n\\end{aligned}\n$$\n\n- 而由于$$f(\\boldsymbol{x}) h(\\boldsymbol{x})=1-2 \\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))$$，所以：\n\n$$\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))]\n$$\n\n**所以理想的$$h_t$$将在$$D_t$$分布下最小化分类误差**。因此，弱分类器将基于分布$$D_t$$来训练\n\n- 考虑由$$D_t$$推导到$$D_{t-1}$$：\n\n$$\n\\begin{aligned}\n\\mathcal{D}_{t+1}(\\boldsymbol{x}) &=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\mathcal{D}_{t}(\\boldsymbol{x}) \\cdot e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})} \\frac{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]}\n\\end{aligned}\n$$\n\n恰好对应算法流程总的更新公式\n\n\n\n### 2.2 调整数据分布的方法\n\n- 在上述算法流程中，是通过对每个样本赋予不同的权重来调整数据分布，称为**重赋权法（re-weighting）**\n- 但是某些基学习算法无法接受带权样本，这时可采样**重采样法（re-sampling）**，即在每一轮中根据样本分布对训练集进行重新采样，然后根据采样到的样本进行训练。**一般而言，两种算法没有显著的优劣差别**\n\n- 注意在算法流程中，如果得到的$$\\epsilon_t > 0.5$$，则整个算法就终止了，如果采用的是重赋权法，可能导致过早停止致使因基学习器过少而导致的性能不佳。但是如果使用重采样法，可以通过重启动避免过早的停止，即如果当前分布训练出来的基学习器不好，则抛弃，然后重新采样再训练\n\n\n\n\n\n# 3 Bagging\n\n- 欲得到泛化性能强的集成，个体学习器之间应该尽可能相互独立，虽然在现实任务中无法得到，但是可以设法令其具有较大差异。一种方法就是改变每个个体学习器的训练数据集，使每个训练集差异较大，但是又不能过大（每个训练集之间都没有交集），这样只用到了很少的训练数据进行训练。\n- 所以可用**自助采样法（bootstrap sampling）**：给定包含m个样本的数据集，每次随机取出一个样本后，又把该样本放回去，使得下次采样同样有可能采到该样本，这样采样m次得到一个同样大小的数据集。使用这样的采样方法，初始数据集中约有63.2%的样本出现在采样集中\n\n- 自助采样法还有一个优点就是：对于每个基学习器，仅使用了约63.2%的样本，剩下的样本正好可作为每个基学习器的验证集\n- 从偏差-方差分解的角度看，**Bagging主要关注降低方差**（即在不同数据集上表现的稳定性）。因此他在不剪枝的决策树、神经网络等易受样本扰动的学习器上效用更为明显\n\n\n\n### 3.1 算法流程\n\n- 算法是采样出T个采样集，然后基于每个采样集训练出一个基学习器（所以可以并行操作）。最后进行简单投票（即每个基学习器使用相同权重），流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221015150202207.png\" alt=\"image-20221015150202207\" style=\"zoom:80%;\" />\n\nBagging的时间复杂度约等于一个基学习器的时间复杂度，所以他是一个很高效的算法。另外，AdaBoost需进行一些算法的更改才可应用于多分类和回归任务，而Bagging算法可直接应用\n\n\n\n### 3.2 随机森林\n\n- 随机森林（Random Forest，RF）是Bagging的一个扩展变体，其在以决策树为基学习器构建Bagging的基础上，进一步在决策树的训练过程中引入**随机属性选择**。\n- 具体来说，**在每个结点选择一个划分属性时，传统决策树是从所有当前结点中（假设有d个）选择一个最优的，而RF先随机选择k个属性的子集，然后再在该子集中选择一个最优的**。推荐$$k=\\log_2d$$\n\n- RF使用了非常小的额外计算开销，但却在许多任务中展现出了强大的性能。其不仅像Bagging一样通过**样本扰动**来增加基学习器的多样性，还通过**属性扰动**进一步增加多样性，这就使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步增加\n- RF的收敛性和Bagging类似，但起始性能相对较差，最终结果更佳：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018114212479.png\" alt=\"image-20221018114212479\" style=\"zoom:70%;\" />\n\n\n\n\n\n# 4 结合策略\n\n### 4.1 集成的好处\n\n- **统计方面：**因为学习任务的假设空间很大，所以可能有多个假设在训练集上性能一样，使用单学习器可能因为误选导致泛化性能不佳，结合多个学习器则会减少这一风险\n- **计算方面：**算法可能会陷入局部最小点，而多次运行后结合可降低这种风险\n- **表示方面：**某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，结合多个学习器，由于相应的假设空间有所扩大，有可能学得更好的近似\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018115801453.png\" alt=\"image-20221018115801453\" style=\"zoom:70%;\" />\n\n\n\n### 4.2 平均法\n\n- 针对回归任务\n\n- **简单平均法：**\n\n$$\nH(\\boldsymbol{x})=\\frac{1}{T} \\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})\n$$\n\n- **加权平均法：**\n\n$$\nH(\\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} h_{i}(\\boldsymbol{x})\n$$\n\n- 加权平均法的权重一般是通过训练数据学习而得，现实任务中的训练样本通常不充分或存在噪声，所以**学出的权重不一定可靠**。因此，**加权平均法未必优于简单平均法**\n\n- 一般而言，**个体学习器性能相差较大时使用加权平均，性能相近时使用简单平均**\n\n\n\n### 4.3 投票法\n\n- 针对分类任务\n\n- 规定第$$i$$个学习器$$h_i$$在样本x上的预测输出表示为$$(h_i^1(x), ..., h_i^N(x))$$，N为类别个数\n- **绝对多数投票法：**\n\n$$\nH(\\boldsymbol{x})=\\left\\{\\begin{array}{ll}\nc_{j}, & \\text { if } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})>0.5 \\sum_{k=1}^{N} \\sum_{i=1}^{T} h_{i}^{k}(\\boldsymbol{x}) \\\\\n\\text { reject, } & \\text { otherwise }\n\\end{array}\\right.\n$$\n\n即某类得到票数过半，才预测为该类，否则拒绝预测\n\n- **相对多数投票法：**\n\n$$\nH(\\boldsymbol{x})=c_{j}^{\\arg \\max } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})\n$$\n\n- **加权投票法：**\n\n$$\nH(\\boldsymbol{x})=c_{j}^{\\arg \\max } \\sum_{i=1}^{T} w_{i} h_{i}^{j}(\\boldsymbol{x}) .\n$$\n\n- 上面的式子中并没有限制个体学习器输出值的类型，一般为类标记（$$h_i^j(x) \\in \\{0,1\\}$$）或类概率（$$h_i^j(x) \\in [0,1]$$）。不能类型的输出不能混用，一般基于类概率效果往往比类标记更好。**若基学习器的类型不同，其类概率之间不能直接进行比较，需要先转化为类标记**\n\n\n\n### 4.4 学习法\n\n- **当训练数据很多时**，一种更为强大的结合策略是学习法，即通过另一个学习器来进行结合。其中Stacking是一种典型代表。把个体学习器称为**初级学习器**，用于结合的学习器称为**次级学习器**或**元学习器**\n- Stacking算法先从初始数据集中训练出多个初级学习器，然后以此生成一个新的数据集：初级学习器的输出被当作样例输入特征，而初始样本的label仍作为新数据集的label，算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018165945814.png\" alt=\"image-20221018165945814\" style=\"zoom:70%;\" />\n\n- 但是上述算法流程有个问题：**次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险较大**，因此一般采用交叉验证或留一法，用初级学习器未使用的样本来产生次级学习器的训练样本\n\n> - 以k折交叉验证法为例，训练集为D，对于每个初级学习器都会有k折（所以一共会迭代$$T * k $$次），在第$$i$$个学习器的第$$j$$折的时候，令$$D_j$$和$$\\bar{D}_{j}=D \\backslash D_{j}$$为此时的验证集和训练集，通过$$\\bar{D}_j$$训练出$$h_i$$后，在使用$$D_j$$进行验证和生成次级训练集\n\n- 次级学习器的输入和学习算法有很大的影响，效果比较好的是：**将初级学习器的输出类概率作为输入，再采用多响应线性回归（MLR）作为次级学习算法**\n\n> - **MLR：**对每个类分别进行**线性回归**，训练样例要输入进每一个回归模型，若训练样例属于该类，则对应回归的输出label为1，若不属于该类，label则为0。预测时取输出值最大的那个类\n\n\n\n\n\n# 5 多样性\n\n### 5.1 误差-分歧分解\n\n- 以一个回归问题使用加权平均法进行集成为例，对于样例x，定义学习器$$h_i$$的**分歧（ambiguity）**为：\n\n$$\nA\\left(h_{i} \\mid \\boldsymbol{x}\\right)=\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\n$$\n\n集成的分歧为：\n$$\n\\begin{aligned}\n\\bar{A}(h \\mid \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} A\\left(h_{i} \\mid \\boldsymbol{x}\\right) \\\\\n&=\\sum_{i=1}^{T} w_{i}\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\n\\end{aligned}\n$$\n**上式表征了个体学习器之间在样本x上的不一致性，即在一定程度上反映了个体学习器之间的多样性**\n\n- 个体学习器和集成后的MSE误差为：\n\n$$\n\\begin{array}{l}\nE\\left(h_{i} \\mid \\boldsymbol{x}\\right)=\\left(f(\\boldsymbol{x})-h_{i}(\\boldsymbol{x})\\right)^{2} \\\\\nE(H \\mid \\boldsymbol{x})=(f(\\boldsymbol{x})-H(\\boldsymbol{x}))^{2}\n\\end{array}\n$$\n\n并且表示出个体学习器误差的加权平均：\n$$\n\\bar{E}(h \\mid \\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} \\cdot E\\left(h_{i} \\mid \\boldsymbol{x}\\right)\n$$\n\n- 由上式可得：\n\n$$\n\\begin{aligned}\n\\bar{A}(h \\mid \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} E\\left(h_{i} \\mid \\boldsymbol{x}\\right)-E(H \\mid \\boldsymbol{x}) \\\\\n&=\\bar{E}(h \\mid \\boldsymbol{x})-E(H \\mid \\boldsymbol{x})\n\\end{aligned}\n$$\n\n- 上式对于所有x都成立，引入概率密度函数$$p(x)$$，则在全样本上可将上式扩展成：\n\n$$\n\\sum_{i=1}^{T} w_{i} \\int A\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}=\\sum_{i=1}^{T} w_{i} \\int E\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}-\\int E(H \\mid \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x} .\n$$\n\n- 同样，将泛化误差和分歧扩展在全样本上：\n\n$$\n\\begin{array}{l}\nE(h_i) = E_{i}=\\int E\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x} \\\\\nA(h_i) = A_{i}=\\int A\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x} \\\\\nE(H) = E=\\int E(H \\mid \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x}\n\\end{array}\n$$\n\n再取个学习器的加权误差和加权分歧：\n$$\n\\bar{E}=\\sum_{i=1}^{T} w_{i} E_{i} \\\\\n\\bar{A}=\\sum_{i=1}^{T} w_{i} A_{i}\n$$\n\n- 通过上面这些式子可以得到：\n\n$$\nE=\\bar{E}-\\bar{A}\n$$\n\n这个式子表明：**个体学习器准确率越高、多样性越大，则集成效果越好**\n\n\n\n### 5.2 多样性度量\n\n- 简单介绍几个多样性的度量标准，给定数据集$$D=\\left\\{\\left(\\boldsymbol{x}_{1}, y_{1}\\right),\\left(\\boldsymbol{x}_{2}, y_{2}\\right), \\ldots,\\left(\\boldsymbol{x}_{m}, y_{m}\\right)\\right\\}$$，假定为二分类任务，则两个分类器的预测结果列联表为：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018180852374.png\" alt=\"image-20221018180852374\" style=\"zoom:80%;\" />\n\n该表中，比如c为$$h_i$$预测为负类而$$h_j$$预测为正类的样本数\n\n- **不合度量（disagreement measure）：**\n\n$$\nd i s_{i j}=\\frac{b+c}{m}\n$$\n\n值越大多样性越大\n\n- **相关系数（correlation coefficient）：**\n\n$$\n\\rho_{i j}=\\frac{a d-b c}{\\sqrt{(a+b)(a+c)(c+d)(b+d)}}\n$$\n\n若$$h_i, h_j$$无关，则值为0，若正相关则值为正，若负相关则值为负。绝对值越大，相关性越强\n\n- **Q-统计量（Q-statistic）：**\n\n$$\nQ_{i j}=\\frac{a d-b c}{a d+b c}\n$$\n\n和$$\\rho_{ij}$$类似\n\n- **$$\\kappa$$-统计量（k-statistic）：**\n\n$$\n\\kappa=\\frac{p_{1}-p_{2}}{1-p_{2}}\n$$\n\n其中$$p_i$$为两个分类器取得一致的概率，$$p_2$$为两个分类器偶然达成一致的概率，可由数据集D直接统计估算：\n$$\np_1 = \\frac{a+d}{a+b+c+d} \\\\\np_2 = \\frac{(a+b)(a+c) + (c+d)(b+d)}{(a+b+c+d)^2}\n$$\n若两个分类器在D上完全一致，则$$\\kappa = 1$$，若只是偶然达成一致，$$\\kappa = 0$$，$$\\kappa$$一般非负，尽在两个分类器达成一致的概率甚至低于偶然性的情况下取负值。$$\\kappa$$越大，多样性越小","source":"_posts/集成学习.md","raw":"---\ntitle: 集成学习\nmath: true\ndate: 2022-5-17\n---\n\n\n\n# 1 基本概念\n\n- 集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务。其中每个**个体学习器**如果都使用同样的算法，则称这种集成是**同质**的，同质集成中的个体学习器亦称**基学习器**，反之则为**异质**的\n- 集成学习的相比于单一学习器可获得更优越的泛化性能，这对**弱学习器（泛化性能略优于随机猜测的学习器）**尤为明显\n\n- **对于每个个体学习器，要有一定的准确性，即学习器不能太坏，也要有多样性，即学习器间具有差异**\n\n- 做一个简单的分析：\n\n> - 考虑一个二分类问题，假设**每个基学习器的错误率相互独立**且为$$\\epsilon$$，即对每个基学习器$$h_i$$有：\n>\n> $$\n> P(h_i(x) \\neq f(x)) = \\epsilon\n> $$\n>\n> - 假设集成时使用简单投票法结合T个基学习器：\n>\n> $$\n> H(\\boldsymbol{x})=\\operatorname{sign}\\left(\\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})\\right)\n> $$\n>\n> - 由于每个基学习器的错误率相互独立，则由Hoeffding不等式可知，集成错误率为：\n>\n> $$\n> \\begin{aligned}\n> P(H(\\boldsymbol{x}) \\neq f(\\boldsymbol{x})) &=\\sum_{k=0}^{\\lfloor T / 2\\rfloor}C_T^k(1-\\epsilon)^{k} \\epsilon^{T-k} \\\\\n> & \\leqslant \\exp \\left(-\\frac{1}{2} T(1-2 \\epsilon)^{2}\\right)\n> \\end{aligned}\n> $$\n>\n> - **由上式可得：随着T的增大，集成的错误率降指数级下降，最终趋于0**\n\n- 在上面的分析中提到了一个关键的假设：基学习器的误差相互独立。但是在现实任务中是不可能的。目前的集成学习方法可大致分为两类：\n\n> 1. 个体学习器之间存在强依赖关系，必须串行生成的序列化方法，代表是Boosting\n> 2. 个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林（RF）\n\n\n\n\n\n# 2 Boosting\n\n- Boosting是一族可将弱学习器提升为强学习器的算法，工作机制为：**先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此重复直至得到事先指定的T**\n- 从偏差-方差的角度看，**Boosting主要关注降低偏差**\n\n\n\n###  2.1 AdaBoost\n\n#### 2.1.1 算法流程\n\n- 其中最著名的代表就是AdaBoost，考虑一个二分类任务，其算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221015122613734.png\" alt=\"image-20221015122613734\" style=\"zoom:80%;\" />\n\n其中的$$D_t$$为样本权重分布，反应了每个样本对基学习器的重要程度（反映在对loss的贡献上）。$$\\alpha_t$$为每个基学习器的权重。在每次更新样本权重分布的时候提升分类错误样本的权重，其中$$Z_t$$是一个规范化因子\n\n\n\n#### 2.1.2 损失函数\n\n- AdaBoost是基于加法模型，即基于学习器的线性组合：\n\n$$\nH(\\boldsymbol{x})=\\sum_{t=1}^{T} \\alpha_{t} h_{t}(\\boldsymbol{x})\n$$\n\n来最小化指数损失函数：\n$$\n\\ell_{\\exp }(H \\mid \\mathcal{D})=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\\right]\n$$\n其实**指数函数是分类任务原本0/1损失函数的一致的替代损失函数**（因为其拥有更好的数学性质）\n\n> **证明：**\n>\n> - 求$$H(x)$$关于损失函数的偏导：\n>\n> $$\n> \\frac{\\partial \\ell_{\\exp }(H \\mid \\mathcal{D})}{\\partial H(\\boldsymbol{x})}=-e^{-H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=1 \\mid \\boldsymbol{x})+e^{H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=-1 \\mid \\boldsymbol{x})\n> $$\n>\n> - 偏导数为0可求得极点：\n>\n> $$\n> H(\\boldsymbol{x})=\\frac{1}{2} \\ln \\frac{P(f(x)=1 \\mid \\boldsymbol{x})}{P(f(x)=-1 \\mid \\boldsymbol{x})}\n> $$\n>\n> - 因此有：\n>\n> $$\n> \\begin{array}{l}\n> sign(H(x)) =\\left\\{\\begin{array}{l}\n> 1, \\quad P(f(x)=1 \\mid \\boldsymbol{x})>P(f(x)=-1 \\mid \\boldsymbol{x}) \\\\\n> -1, \\quad P(f(x)=1 \\mid \\boldsymbol{x})<P(f(x)=-1 \\mid \\boldsymbol{x})\n> \\end{array}\\right. \\\\\n> =\\underset{y \\in\\{-1,1\\}}{\\arg \\max } P(f(x)=y \\mid \\boldsymbol{x})\n> \\end{array}\n> $$\n\n\n\n#### 2.1.3 基学习器的权重\n\n- 在通过$$D_t$$产生$$h_t$$后，该基分类器的权重$$\\alpha_t$$应使得$$\\alpha_th_t$$最小化指数损失函数（在$$D_t$$分布上而非D分布）：\n\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(\\alpha_{t} h_{t} \\mid \\mathcal{D}_{t}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right)\\right] \\\\\n&=e^{-\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right) \\\\\n&=e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t}\n\\end{aligned}\n$$\n\n- 关于$$\\alpha_t$$求导：\n\n$$\n\\frac{\\partial \\ell_{\\exp }\\left(\\alpha_{t} h_{t} \\mid \\mathcal{D}_{t}\\right)}{\\partial \\alpha_{t}}=-e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t} = 0 \\\\\n\\alpha_t = \\frac{1}{2}\\ln(\\frac{1 - \\epsilon_t}{\\epsilon_t})\n$$\n\n\n\n#### 2.1.4 样本权重分布的更新\n\n- 在获得$$H_{t-1}$$后样本分布将进行调整，使下一轮的$$h_t$$能纠正$$H_{t-1}$$的一些错误，理想情况下能纠正$$H_{t-1}$$的全部错误，即最小化$$\\ell_{\\exp }\\left(H_{t-1}+\\alpha_th_{t} \\mid \\mathcal{D}\\right)$$，可简化为：\n\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} \\mid \\mathcal{D}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x})\\left(H_{t-1}(\\boldsymbol{x})+h_{t}(\\boldsymbol{x})\\right)}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})}\\right]\n\\end{aligned}\n$$\n\n其中的$$e^{-f(x)h_t(x)}$$可用泰勒展示近似：\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} \\mid \\mathcal{D}\\right) & \\simeq \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{f^{2}(\\boldsymbol{x}) h_{t}^{2}(\\boldsymbol{x})}{2}\\right)\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{1}{2}\\right)\\right]\n\\end{aligned}\n$$\n\n- 于是理想的基学习器为：\n\n$$\n\\begin{array}{l}\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\ell_{\\exp }\\left(H_{t-1}+h \\mid \\mathcal{D}\\right) \\\\\n=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\\n=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right]\n\\end{array}\n$$\n\n注意最后添加的分母是一个常数。令$$D_t$$表示分布：\n$$\n\\mathcal{D}_{t}(\\boldsymbol{x})=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}\n$$\n则：\n$$\n\\begin{aligned}\nh_{t}(\\boldsymbol{x}) &=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\\n&=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[f(\\boldsymbol{x}) h(\\boldsymbol{x})]\n\\end{aligned}\n$$\n\n- 而由于$$f(\\boldsymbol{x}) h(\\boldsymbol{x})=1-2 \\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))$$，所以：\n\n$$\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))]\n$$\n\n**所以理想的$$h_t$$将在$$D_t$$分布下最小化分类误差**。因此，弱分类器将基于分布$$D_t$$来训练\n\n- 考虑由$$D_t$$推导到$$D_{t-1}$$：\n\n$$\n\\begin{aligned}\n\\mathcal{D}_{t+1}(\\boldsymbol{x}) &=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\mathcal{D}_{t}(\\boldsymbol{x}) \\cdot e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})} \\frac{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]}\n\\end{aligned}\n$$\n\n恰好对应算法流程总的更新公式\n\n\n\n### 2.2 调整数据分布的方法\n\n- 在上述算法流程中，是通过对每个样本赋予不同的权重来调整数据分布，称为**重赋权法（re-weighting）**\n- 但是某些基学习算法无法接受带权样本，这时可采样**重采样法（re-sampling）**，即在每一轮中根据样本分布对训练集进行重新采样，然后根据采样到的样本进行训练。**一般而言，两种算法没有显著的优劣差别**\n\n- 注意在算法流程中，如果得到的$$\\epsilon_t > 0.5$$，则整个算法就终止了，如果采用的是重赋权法，可能导致过早停止致使因基学习器过少而导致的性能不佳。但是如果使用重采样法，可以通过重启动避免过早的停止，即如果当前分布训练出来的基学习器不好，则抛弃，然后重新采样再训练\n\n\n\n\n\n# 3 Bagging\n\n- 欲得到泛化性能强的集成，个体学习器之间应该尽可能相互独立，虽然在现实任务中无法得到，但是可以设法令其具有较大差异。一种方法就是改变每个个体学习器的训练数据集，使每个训练集差异较大，但是又不能过大（每个训练集之间都没有交集），这样只用到了很少的训练数据进行训练。\n- 所以可用**自助采样法（bootstrap sampling）**：给定包含m个样本的数据集，每次随机取出一个样本后，又把该样本放回去，使得下次采样同样有可能采到该样本，这样采样m次得到一个同样大小的数据集。使用这样的采样方法，初始数据集中约有63.2%的样本出现在采样集中\n\n- 自助采样法还有一个优点就是：对于每个基学习器，仅使用了约63.2%的样本，剩下的样本正好可作为每个基学习器的验证集\n- 从偏差-方差分解的角度看，**Bagging主要关注降低方差**（即在不同数据集上表现的稳定性）。因此他在不剪枝的决策树、神经网络等易受样本扰动的学习器上效用更为明显\n\n\n\n### 3.1 算法流程\n\n- 算法是采样出T个采样集，然后基于每个采样集训练出一个基学习器（所以可以并行操作）。最后进行简单投票（即每个基学习器使用相同权重），流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221015150202207.png\" alt=\"image-20221015150202207\" style=\"zoom:80%;\" />\n\nBagging的时间复杂度约等于一个基学习器的时间复杂度，所以他是一个很高效的算法。另外，AdaBoost需进行一些算法的更改才可应用于多分类和回归任务，而Bagging算法可直接应用\n\n\n\n### 3.2 随机森林\n\n- 随机森林（Random Forest，RF）是Bagging的一个扩展变体，其在以决策树为基学习器构建Bagging的基础上，进一步在决策树的训练过程中引入**随机属性选择**。\n- 具体来说，**在每个结点选择一个划分属性时，传统决策树是从所有当前结点中（假设有d个）选择一个最优的，而RF先随机选择k个属性的子集，然后再在该子集中选择一个最优的**。推荐$$k=\\log_2d$$\n\n- RF使用了非常小的额外计算开销，但却在许多任务中展现出了强大的性能。其不仅像Bagging一样通过**样本扰动**来增加基学习器的多样性，还通过**属性扰动**进一步增加多样性，这就使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步增加\n- RF的收敛性和Bagging类似，但起始性能相对较差，最终结果更佳：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018114212479.png\" alt=\"image-20221018114212479\" style=\"zoom:70%;\" />\n\n\n\n\n\n# 4 结合策略\n\n### 4.1 集成的好处\n\n- **统计方面：**因为学习任务的假设空间很大，所以可能有多个假设在训练集上性能一样，使用单学习器可能因为误选导致泛化性能不佳，结合多个学习器则会减少这一风险\n- **计算方面：**算法可能会陷入局部最小点，而多次运行后结合可降低这种风险\n- **表示方面：**某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，结合多个学习器，由于相应的假设空间有所扩大，有可能学得更好的近似\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018115801453.png\" alt=\"image-20221018115801453\" style=\"zoom:70%;\" />\n\n\n\n### 4.2 平均法\n\n- 针对回归任务\n\n- **简单平均法：**\n\n$$\nH(\\boldsymbol{x})=\\frac{1}{T} \\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})\n$$\n\n- **加权平均法：**\n\n$$\nH(\\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} h_{i}(\\boldsymbol{x})\n$$\n\n- 加权平均法的权重一般是通过训练数据学习而得，现实任务中的训练样本通常不充分或存在噪声，所以**学出的权重不一定可靠**。因此，**加权平均法未必优于简单平均法**\n\n- 一般而言，**个体学习器性能相差较大时使用加权平均，性能相近时使用简单平均**\n\n\n\n### 4.3 投票法\n\n- 针对分类任务\n\n- 规定第$$i$$个学习器$$h_i$$在样本x上的预测输出表示为$$(h_i^1(x), ..., h_i^N(x))$$，N为类别个数\n- **绝对多数投票法：**\n\n$$\nH(\\boldsymbol{x})=\\left\\{\\begin{array}{ll}\nc_{j}, & \\text { if } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})>0.5 \\sum_{k=1}^{N} \\sum_{i=1}^{T} h_{i}^{k}(\\boldsymbol{x}) \\\\\n\\text { reject, } & \\text { otherwise }\n\\end{array}\\right.\n$$\n\n即某类得到票数过半，才预测为该类，否则拒绝预测\n\n- **相对多数投票法：**\n\n$$\nH(\\boldsymbol{x})=c_{j}^{\\arg \\max } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})\n$$\n\n- **加权投票法：**\n\n$$\nH(\\boldsymbol{x})=c_{j}^{\\arg \\max } \\sum_{i=1}^{T} w_{i} h_{i}^{j}(\\boldsymbol{x}) .\n$$\n\n- 上面的式子中并没有限制个体学习器输出值的类型，一般为类标记（$$h_i^j(x) \\in \\{0,1\\}$$）或类概率（$$h_i^j(x) \\in [0,1]$$）。不能类型的输出不能混用，一般基于类概率效果往往比类标记更好。**若基学习器的类型不同，其类概率之间不能直接进行比较，需要先转化为类标记**\n\n\n\n### 4.4 学习法\n\n- **当训练数据很多时**，一种更为强大的结合策略是学习法，即通过另一个学习器来进行结合。其中Stacking是一种典型代表。把个体学习器称为**初级学习器**，用于结合的学习器称为**次级学习器**或**元学习器**\n- Stacking算法先从初始数据集中训练出多个初级学习器，然后以此生成一个新的数据集：初级学习器的输出被当作样例输入特征，而初始样本的label仍作为新数据集的label，算法流程如下：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018165945814.png\" alt=\"image-20221018165945814\" style=\"zoom:70%;\" />\n\n- 但是上述算法流程有个问题：**次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险较大**，因此一般采用交叉验证或留一法，用初级学习器未使用的样本来产生次级学习器的训练样本\n\n> - 以k折交叉验证法为例，训练集为D，对于每个初级学习器都会有k折（所以一共会迭代$$T * k $$次），在第$$i$$个学习器的第$$j$$折的时候，令$$D_j$$和$$\\bar{D}_{j}=D \\backslash D_{j}$$为此时的验证集和训练集，通过$$\\bar{D}_j$$训练出$$h_i$$后，在使用$$D_j$$进行验证和生成次级训练集\n\n- 次级学习器的输入和学习算法有很大的影响，效果比较好的是：**将初级学习器的输出类概率作为输入，再采用多响应线性回归（MLR）作为次级学习算法**\n\n> - **MLR：**对每个类分别进行**线性回归**，训练样例要输入进每一个回归模型，若训练样例属于该类，则对应回归的输出label为1，若不属于该类，label则为0。预测时取输出值最大的那个类\n\n\n\n\n\n# 5 多样性\n\n### 5.1 误差-分歧分解\n\n- 以一个回归问题使用加权平均法进行集成为例，对于样例x，定义学习器$$h_i$$的**分歧（ambiguity）**为：\n\n$$\nA\\left(h_{i} \\mid \\boldsymbol{x}\\right)=\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\n$$\n\n集成的分歧为：\n$$\n\\begin{aligned}\n\\bar{A}(h \\mid \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} A\\left(h_{i} \\mid \\boldsymbol{x}\\right) \\\\\n&=\\sum_{i=1}^{T} w_{i}\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\n\\end{aligned}\n$$\n**上式表征了个体学习器之间在样本x上的不一致性，即在一定程度上反映了个体学习器之间的多样性**\n\n- 个体学习器和集成后的MSE误差为：\n\n$$\n\\begin{array}{l}\nE\\left(h_{i} \\mid \\boldsymbol{x}\\right)=\\left(f(\\boldsymbol{x})-h_{i}(\\boldsymbol{x})\\right)^{2} \\\\\nE(H \\mid \\boldsymbol{x})=(f(\\boldsymbol{x})-H(\\boldsymbol{x}))^{2}\n\\end{array}\n$$\n\n并且表示出个体学习器误差的加权平均：\n$$\n\\bar{E}(h \\mid \\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} \\cdot E\\left(h_{i} \\mid \\boldsymbol{x}\\right)\n$$\n\n- 由上式可得：\n\n$$\n\\begin{aligned}\n\\bar{A}(h \\mid \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} E\\left(h_{i} \\mid \\boldsymbol{x}\\right)-E(H \\mid \\boldsymbol{x}) \\\\\n&=\\bar{E}(h \\mid \\boldsymbol{x})-E(H \\mid \\boldsymbol{x})\n\\end{aligned}\n$$\n\n- 上式对于所有x都成立，引入概率密度函数$$p(x)$$，则在全样本上可将上式扩展成：\n\n$$\n\\sum_{i=1}^{T} w_{i} \\int A\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}=\\sum_{i=1}^{T} w_{i} \\int E\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}-\\int E(H \\mid \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x} .\n$$\n\n- 同样，将泛化误差和分歧扩展在全样本上：\n\n$$\n\\begin{array}{l}\nE(h_i) = E_{i}=\\int E\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x} \\\\\nA(h_i) = A_{i}=\\int A\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x} \\\\\nE(H) = E=\\int E(H \\mid \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x}\n\\end{array}\n$$\n\n再取个学习器的加权误差和加权分歧：\n$$\n\\bar{E}=\\sum_{i=1}^{T} w_{i} E_{i} \\\\\n\\bar{A}=\\sum_{i=1}^{T} w_{i} A_{i}\n$$\n\n- 通过上面这些式子可以得到：\n\n$$\nE=\\bar{E}-\\bar{A}\n$$\n\n这个式子表明：**个体学习器准确率越高、多样性越大，则集成效果越好**\n\n\n\n### 5.2 多样性度量\n\n- 简单介绍几个多样性的度量标准，给定数据集$$D=\\left\\{\\left(\\boldsymbol{x}_{1}, y_{1}\\right),\\left(\\boldsymbol{x}_{2}, y_{2}\\right), \\ldots,\\left(\\boldsymbol{x}_{m}, y_{m}\\right)\\right\\}$$，假定为二分类任务，则两个分类器的预测结果列联表为：\n\n<img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018180852374.png\" alt=\"image-20221018180852374\" style=\"zoom:80%;\" />\n\n该表中，比如c为$$h_i$$预测为负类而$$h_j$$预测为正类的样本数\n\n- **不合度量（disagreement measure）：**\n\n$$\nd i s_{i j}=\\frac{b+c}{m}\n$$\n\n值越大多样性越大\n\n- **相关系数（correlation coefficient）：**\n\n$$\n\\rho_{i j}=\\frac{a d-b c}{\\sqrt{(a+b)(a+c)(c+d)(b+d)}}\n$$\n\n若$$h_i, h_j$$无关，则值为0，若正相关则值为正，若负相关则值为负。绝对值越大，相关性越强\n\n- **Q-统计量（Q-statistic）：**\n\n$$\nQ_{i j}=\\frac{a d-b c}{a d+b c}\n$$\n\n和$$\\rho_{ij}$$类似\n\n- **$$\\kappa$$-统计量（k-statistic）：**\n\n$$\n\\kappa=\\frac{p_{1}-p_{2}}{1-p_{2}}\n$$\n\n其中$$p_i$$为两个分类器取得一致的概率，$$p_2$$为两个分类器偶然达成一致的概率，可由数据集D直接统计估算：\n$$\np_1 = \\frac{a+d}{a+b+c+d} \\\\\np_2 = \\frac{(a+b)(a+c) + (c+d)(b+d)}{(a+b+c+d)^2}\n$$\n若两个分类器在D上完全一致，则$$\\kappa = 1$$，若只是偶然达成一致，$$\\kappa = 0$$，$$\\kappa$$一般非负，尽在两个分类器达成一致的概率甚至低于偶然性的情况下取负值。$$\\kappa$$越大，多样性越小","slug":"集成学习","published":1,"updated":"2022-12-20T06:20:58.835Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clftw6p1p000q7csz9i6chxta","content":"<h1 id=\"1-基本概念\"><a href=\"#1-基本概念\" class=\"headerlink\" title=\"1 基本概念\"></a>1 基本概念</h1><ul>\n<li>集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务。其中每个<strong>个体学习器</strong>如果都使用同样的算法，则称这种集成是<strong>同质</strong>的，同质集成中的个体学习器亦称<strong>基学习器</strong>，反之则为<strong>异质</strong>的</li>\n<li><p>集成学习的相比于单一学习器可获得更优越的泛化性能，这对<strong>弱学习器（泛化性能略优于随机猜测的学习器）</strong>尤为明显</p>\n</li>\n<li><p><strong>对于每个个体学习器，要有一定的准确性，即学习器不能太坏，也要有多样性，即学习器间具有差异</strong></p>\n</li>\n<li><p>做一个简单的分析：</p>\n</li>\n</ul>\n<blockquote>\n<ul>\n<li>考虑一个二分类问题，假设<strong>每个基学习器的错误率相互独立</strong>且为<script type=\"math/tex\">\\epsilon</script>，即对每个基学习器<script type=\"math/tex\">h_i</script>有：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(h_i(x) \\neq f(x)) = \\epsilon</script><ul>\n<li>假设集成时使用简单投票法结合T个基学习器：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\operatorname{sign}\\left(\\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})\\right)</script><ul>\n<li>由于每个基学习器的错误率相互独立，则由Hoeffding不等式可知，集成错误率为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(H(\\boldsymbol{x}) \\neq f(\\boldsymbol{x})) &=\\sum_{k=0}^{\\lfloor T / 2\\rfloor}C_T^k(1-\\epsilon)^{k} \\epsilon^{T-k} \\\\\n& \\leqslant \\exp \\left(-\\frac{1}{2} T(1-2 \\epsilon)^{2}\\right)\n\\end{aligned}</script><ul>\n<li><strong>由上式可得：随着T的增大，集成的错误率降指数级下降，最终趋于0</strong></li>\n</ul>\n</blockquote>\n<ul>\n<li>在上面的分析中提到了一个关键的假设：基学习器的误差相互独立。但是在现实任务中是不可能的。目前的集成学习方法可大致分为两类：</li>\n</ul>\n<blockquote>\n<ol>\n<li>个体学习器之间存在强依赖关系，必须串行生成的序列化方法，代表是Boosting</li>\n<li>个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林（RF）</li>\n</ol>\n</blockquote>\n<h1 id=\"2-Boosting\"><a href=\"#2-Boosting\" class=\"headerlink\" title=\"2 Boosting\"></a>2 Boosting</h1><ul>\n<li>Boosting是一族可将弱学习器提升为强学习器的算法，工作机制为：<strong>先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此重复直至得到事先指定的T</strong></li>\n<li>从偏差-方差的角度看，<strong>Boosting主要关注降低偏差</strong></li>\n</ul>\n<h3 id=\"2-1-AdaBoost\"><a href=\"#2-1-AdaBoost\" class=\"headerlink\" title=\"2.1 AdaBoost\"></a>2.1 AdaBoost</h3><h4 id=\"2-1-1-算法流程\"><a href=\"#2-1-1-算法流程\" class=\"headerlink\" title=\"2.1.1 算法流程\"></a>2.1.1 算法流程</h4><ul>\n<li>其中最著名的代表就是AdaBoost，考虑一个二分类任务，其算法流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221015122613734.png\" alt=\"image-20221015122613734\" style=\"zoom:80%;\" /></p>\n<p>其中的<script type=\"math/tex\">D_t</script>为样本权重分布，反应了每个样本对基学习器的重要程度（反映在对loss的贡献上）。<script type=\"math/tex\">\\alpha_t</script>为每个基学习器的权重。在每次更新样本权重分布的时候提升分类错误样本的权重，其中<script type=\"math/tex\">Z_t</script>是一个规范化因子</p>\n<h4 id=\"2-1-2-损失函数\"><a href=\"#2-1-2-损失函数\" class=\"headerlink\" title=\"2.1.2 损失函数\"></a>2.1.2 损失函数</h4><ul>\n<li>AdaBoost是基于加法模型，即基于学习器的线性组合：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\sum_{t=1}^{T} \\alpha_{t} h_{t}(\\boldsymbol{x})</script><p>来最小化指数损失函数：</p>\n<script type=\"math/tex; mode=display\">\n\\ell_{\\exp }(H \\mid \\mathcal{D})=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\\right]</script><p>其实<strong>指数函数是分类任务原本0/1损失函数的一致的替代损失函数</strong>（因为其拥有更好的数学性质）</p>\n<blockquote>\n<p><strong>证明：</strong></p>\n<ul>\n<li>求<script type=\"math/tex\">H(x)</script>关于损失函数的偏导：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\ell_{\\exp }(H \\mid \\mathcal{D})}{\\partial H(\\boldsymbol{x})}=-e^{-H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=1 \\mid \\boldsymbol{x})+e^{H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=-1 \\mid \\boldsymbol{x})</script><ul>\n<li>偏导数为0可求得极点：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\frac{1}{2} \\ln \\frac{P(f(x)=1 \\mid \\boldsymbol{x})}{P(f(x)=-1 \\mid \\boldsymbol{x})}</script><ul>\n<li>因此有：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nsign(H(x)) =\\left\\{\\begin{array}{l}\n1, \\quad P(f(x)=1 \\mid \\boldsymbol{x})>P(f(x)=-1 \\mid \\boldsymbol{x}) \\\\\n-1, \\quad P(f(x)=1 \\mid \\boldsymbol{x})<P(f(x)=-1 \\mid \\boldsymbol{x})\n\\end{array}\\right. \\\\\n=\\underset{y \\in\\{-1,1\\}}{\\arg \\max } P(f(x)=y \\mid \\boldsymbol{x})\n\\end{array}</script></blockquote>\n<h4 id=\"2-1-3-基学习器的权重\"><a href=\"#2-1-3-基学习器的权重\" class=\"headerlink\" title=\"2.1.3 基学习器的权重\"></a>2.1.3 基学习器的权重</h4><ul>\n<li>在通过<script type=\"math/tex\">D_t</script>产生<script type=\"math/tex\">h_t</script>后，该基分类器的权重<script type=\"math/tex\">\\alpha_t</script>应使得<script type=\"math/tex\">\\alpha_th_t</script>最小化指数损失函数（在<script type=\"math/tex\">D_t</script>分布上而非D分布）：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\ell_{\\exp }\\left(\\alpha_{t} h_{t} \\mid \\mathcal{D}_{t}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right)\\right] \\\\\n&=e^{-\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right) \\\\\n&=e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t}\n\\end{aligned}</script><ul>\n<li>关于<script type=\"math/tex\">\\alpha_t</script>求导：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\ell_{\\exp }\\left(\\alpha_{t} h_{t} \\mid \\mathcal{D}_{t}\\right)}{\\partial \\alpha_{t}}=-e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t} = 0 \\\\\n\\alpha_t = \\frac{1}{2}\\ln(\\frac{1 - \\epsilon_t}{\\epsilon_t})</script><h4 id=\"2-1-4-样本权重分布的更新\"><a href=\"#2-1-4-样本权重分布的更新\" class=\"headerlink\" title=\"2.1.4 样本权重分布的更新\"></a>2.1.4 样本权重分布的更新</h4><ul>\n<li>在获得<script type=\"math/tex\">H_{t-1}</script>后样本分布将进行调整，使下一轮的<script type=\"math/tex\">h_t</script>能纠正<script type=\"math/tex\">H_{t-1}</script>的一些错误，理想情况下能纠正<script type=\"math/tex\">H_{t-1}</script>的全部错误，即最小化<script type=\"math/tex\">\\ell_{\\exp }\\left(H_{t-1}+\\alpha_th_{t} \\mid \\mathcal{D}\\right)</script>，可简化为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} \\mid \\mathcal{D}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x})\\left(H_{t-1}(\\boldsymbol{x})+h_{t}(\\boldsymbol{x})\\right)}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})}\\right]\n\\end{aligned}</script><p>其中的<script type=\"math/tex\">e^{-f(x)h_t(x)}</script>可用泰勒展示近似：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} \\mid \\mathcal{D}\\right) & \\simeq \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{f^{2}(\\boldsymbol{x}) h_{t}^{2}(\\boldsymbol{x})}{2}\\right)\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{1}{2}\\right)\\right]\n\\end{aligned}</script><ul>\n<li>于是理想的基学习器为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\ell_{\\exp }\\left(H_{t-1}+h \\mid \\mathcal{D}\\right) \\\\\n=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\\n=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right]\n\\end{array}</script><p>注意最后添加的分母是一个常数。令<script type=\"math/tex\">D_t</script>表示分布：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{D}_{t}(\\boldsymbol{x})=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}</script><p>则：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nh_{t}(\\boldsymbol{x}) &=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\\n&=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[f(\\boldsymbol{x}) h(\\boldsymbol{x})]\n\\end{aligned}</script><ul>\n<li>而由于<script type=\"math/tex\">f(\\boldsymbol{x}) h(\\boldsymbol{x})=1-2 \\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))</script>，所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))]</script><p><strong>所以理想的<script type=\"math/tex\">h_t</script>将在<script type=\"math/tex\">D_t</script>分布下最小化分类误差</strong>。因此，弱分类器将基于分布<script type=\"math/tex\">D_t</script>来训练</p>\n<ul>\n<li>考虑由<script type=\"math/tex\">D_t</script>推导到<script type=\"math/tex\">D_{t-1}</script>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\mathcal{D}_{t+1}(\\boldsymbol{x}) &=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\mathcal{D}_{t}(\\boldsymbol{x}) \\cdot e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})} \\frac{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]}\n\\end{aligned}</script><p>恰好对应算法流程总的更新公式</p>\n<h3 id=\"2-2-调整数据分布的方法\"><a href=\"#2-2-调整数据分布的方法\" class=\"headerlink\" title=\"2.2 调整数据分布的方法\"></a>2.2 调整数据分布的方法</h3><ul>\n<li>在上述算法流程中，是通过对每个样本赋予不同的权重来调整数据分布，称为<strong>重赋权法（re-weighting）</strong></li>\n<li><p>但是某些基学习算法无法接受带权样本，这时可采样<strong>重采样法（re-sampling）</strong>，即在每一轮中根据样本分布对训练集进行重新采样，然后根据采样到的样本进行训练。<strong>一般而言，两种算法没有显著的优劣差别</strong></p>\n</li>\n<li><p>注意在算法流程中，如果得到的<script type=\"math/tex\">\\epsilon_t > 0.5</script>，则整个算法就终止了，如果采用的是重赋权法，可能导致过早停止致使因基学习器过少而导致的性能不佳。但是如果使用重采样法，可以通过重启动避免过早的停止，即如果当前分布训练出来的基学习器不好，则抛弃，然后重新采样再训练</p>\n</li>\n</ul>\n<h1 id=\"3-Bagging\"><a href=\"#3-Bagging\" class=\"headerlink\" title=\"3 Bagging\"></a>3 Bagging</h1><ul>\n<li>欲得到泛化性能强的集成，个体学习器之间应该尽可能相互独立，虽然在现实任务中无法得到，但是可以设法令其具有较大差异。一种方法就是改变每个个体学习器的训练数据集，使每个训练集差异较大，但是又不能过大（每个训练集之间都没有交集），这样只用到了很少的训练数据进行训练。</li>\n<li><p>所以可用<strong>自助采样法（bootstrap sampling）</strong>：给定包含m个样本的数据集，每次随机取出一个样本后，又把该样本放回去，使得下次采样同样有可能采到该样本，这样采样m次得到一个同样大小的数据集。使用这样的采样方法，初始数据集中约有63.2%的样本出现在采样集中</p>\n</li>\n<li><p>自助采样法还有一个优点就是：对于每个基学习器，仅使用了约63.2%的样本，剩下的样本正好可作为每个基学习器的验证集</p>\n</li>\n<li>从偏差-方差分解的角度看，<strong>Bagging主要关注降低方差</strong>（即在不同数据集上表现的稳定性）。因此他在不剪枝的决策树、神经网络等易受样本扰动的学习器上效用更为明显</li>\n</ul>\n<h3 id=\"3-1-算法流程\"><a href=\"#3-1-算法流程\" class=\"headerlink\" title=\"3.1 算法流程\"></a>3.1 算法流程</h3><ul>\n<li>算法是采样出T个采样集，然后基于每个采样集训练出一个基学习器（所以可以并行操作）。最后进行简单投票（即每个基学习器使用相同权重），流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221015150202207.png\" alt=\"image-20221015150202207\" style=\"zoom:80%;\" /></p>\n<p>Bagging的时间复杂度约等于一个基学习器的时间复杂度，所以他是一个很高效的算法。另外，AdaBoost需进行一些算法的更改才可应用于多分类和回归任务，而Bagging算法可直接应用</p>\n<h3 id=\"3-2-随机森林\"><a href=\"#3-2-随机森林\" class=\"headerlink\" title=\"3.2 随机森林\"></a>3.2 随机森林</h3><ul>\n<li>随机森林（Random Forest，RF）是Bagging的一个扩展变体，其在以决策树为基学习器构建Bagging的基础上，进一步在决策树的训练过程中引入<strong>随机属性选择</strong>。</li>\n<li><p>具体来说，<strong>在每个结点选择一个划分属性时，传统决策树是从所有当前结点中（假设有d个）选择一个最优的，而RF先随机选择k个属性的子集，然后再在该子集中选择一个最优的</strong>。推荐<script type=\"math/tex\">k=\\log_2d</script></p>\n</li>\n<li><p>RF使用了非常小的额外计算开销，但却在许多任务中展现出了强大的性能。其不仅像Bagging一样通过<strong>样本扰动</strong>来增加基学习器的多样性，还通过<strong>属性扰动</strong>进一步增加多样性，这就使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步增加</p>\n</li>\n<li>RF的收敛性和Bagging类似，但起始性能相对较差，最终结果更佳：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018114212479.png\" alt=\"image-20221018114212479\" style=\"zoom:70%;\" /></p>\n<h1 id=\"4-结合策略\"><a href=\"#4-结合策略\" class=\"headerlink\" title=\"4 结合策略\"></a>4 结合策略</h1><h3 id=\"4-1-集成的好处\"><a href=\"#4-1-集成的好处\" class=\"headerlink\" title=\"4.1 集成的好处\"></a>4.1 集成的好处</h3><ul>\n<li><strong>统计方面：</strong>因为学习任务的假设空间很大，所以可能有多个假设在训练集上性能一样，使用单学习器可能因为误选导致泛化性能不佳，结合多个学习器则会减少这一风险</li>\n<li><strong>计算方面：</strong>算法可能会陷入局部最小点，而多次运行后结合可降低这种风险</li>\n<li><strong>表示方面：</strong>某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，结合多个学习器，由于相应的假设空间有所扩大，有可能学得更好的近似</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018115801453.png\" alt=\"image-20221018115801453\" style=\"zoom:70%;\" /></p>\n<h3 id=\"4-2-平均法\"><a href=\"#4-2-平均法\" class=\"headerlink\" title=\"4.2 平均法\"></a>4.2 平均法</h3><ul>\n<li><p>针对回归任务</p>\n</li>\n<li><p><strong>简单平均法：</strong></p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\frac{1}{T} \\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})</script><ul>\n<li><strong>加权平均法：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} h_{i}(\\boldsymbol{x})</script><ul>\n<li><p>加权平均法的权重一般是通过训练数据学习而得，现实任务中的训练样本通常不充分或存在噪声，所以<strong>学出的权重不一定可靠</strong>。因此，<strong>加权平均法未必优于简单平均法</strong></p>\n</li>\n<li><p>一般而言，<strong>个体学习器性能相差较大时使用加权平均，性能相近时使用简单平均</strong></p>\n</li>\n</ul>\n<h3 id=\"4-3-投票法\"><a href=\"#4-3-投票法\" class=\"headerlink\" title=\"4.3 投票法\"></a>4.3 投票法</h3><ul>\n<li><p>针对分类任务</p>\n</li>\n<li><p>规定第<script type=\"math/tex\">i</script>个学习器<script type=\"math/tex\">h_i</script>在样本x上的预测输出表示为<script type=\"math/tex\">(h_i^1(x), ..., h_i^N(x))</script>，N为类别个数</p>\n</li>\n<li><strong>绝对多数投票法：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\left\\{\\begin{array}{ll}\nc_{j}, & \\text { if } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})>0.5 \\sum_{k=1}^{N} \\sum_{i=1}^{T} h_{i}^{k}(\\boldsymbol{x}) \\\\\n\\text { reject, } & \\text { otherwise }\n\\end{array}\\right.</script><p>即某类得到票数过半，才预测为该类，否则拒绝预测</p>\n<ul>\n<li><strong>相对多数投票法：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=c_{j}^{\\arg \\max } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})</script><ul>\n<li><strong>加权投票法：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=c_{j}^{\\arg \\max } \\sum_{i=1}^{T} w_{i} h_{i}^{j}(\\boldsymbol{x}) .</script><ul>\n<li>上面的式子中并没有限制个体学习器输出值的类型，一般为类标记（<script type=\"math/tex\">h_i^j(x) \\in \\{0,1\\}</script>）或类概率（<script type=\"math/tex\">h_i^j(x) \\in [0,1]</script>）。不能类型的输出不能混用，一般基于类概率效果往往比类标记更好。<strong>若基学习器的类型不同，其类概率之间不能直接进行比较，需要先转化为类标记</strong></li>\n</ul>\n<h3 id=\"4-4-学习法\"><a href=\"#4-4-学习法\" class=\"headerlink\" title=\"4.4 学习法\"></a>4.4 学习法</h3><ul>\n<li><strong>当训练数据很多时</strong>，一种更为强大的结合策略是学习法，即通过另一个学习器来进行结合。其中Stacking是一种典型代表。把个体学习器称为<strong>初级学习器</strong>，用于结合的学习器称为<strong>次级学习器</strong>或<strong>元学习器</strong></li>\n<li>Stacking算法先从初始数据集中训练出多个初级学习器，然后以此生成一个新的数据集：初级学习器的输出被当作样例输入特征，而初始样本的label仍作为新数据集的label，算法流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018165945814.png\" alt=\"image-20221018165945814\" style=\"zoom:70%;\" /></p>\n<ul>\n<li>但是上述算法流程有个问题：<strong>次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险较大</strong>，因此一般采用交叉验证或留一法，用初级学习器未使用的样本来产生次级学习器的训练样本</li>\n</ul>\n<blockquote>\n<ul>\n<li>以k折交叉验证法为例，训练集为D，对于每个初级学习器都会有k折（所以一共会迭代<script type=\"math/tex\">T * k</script>次），在第<script type=\"math/tex\">i</script>个学习器的第<script type=\"math/tex\">j</script>折的时候，令<script type=\"math/tex\">D_j</script>和<script type=\"math/tex\">\\bar{D}_{j}=D \\backslash D_{j}</script>为此时的验证集和训练集，通过<script type=\"math/tex\">\\bar{D}_j</script>训练出<script type=\"math/tex\">h_i</script>后，在使用<script type=\"math/tex\">D_j</script>进行验证和生成次级训练集</li>\n</ul>\n</blockquote>\n<ul>\n<li>次级学习器的输入和学习算法有很大的影响，效果比较好的是：<strong>将初级学习器的输出类概率作为输入，再采用多响应线性回归（MLR）作为次级学习算法</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li><strong>MLR：</strong>对每个类分别进行<strong>线性回归</strong>，训练样例要输入进每一个回归模型，若训练样例属于该类，则对应回归的输出label为1，若不属于该类，label则为0。预测时取输出值最大的那个类</li>\n</ul>\n</blockquote>\n<h1 id=\"5-多样性\"><a href=\"#5-多样性\" class=\"headerlink\" title=\"5 多样性\"></a>5 多样性</h1><h3 id=\"5-1-误差-分歧分解\"><a href=\"#5-1-误差-分歧分解\" class=\"headerlink\" title=\"5.1 误差-分歧分解\"></a>5.1 误差-分歧分解</h3><ul>\n<li>以一个回归问题使用加权平均法进行集成为例，对于样例x，定义学习器<script type=\"math/tex\">h_i</script>的<strong>分歧（ambiguity）</strong>为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nA\\left(h_{i} \\mid \\boldsymbol{x}\\right)=\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}</script><p>集成的分歧为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\bar{A}(h \\mid \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} A\\left(h_{i} \\mid \\boldsymbol{x}\\right) \\\\\n&=\\sum_{i=1}^{T} w_{i}\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\n\\end{aligned}</script><p><strong>上式表征了个体学习器之间在样本x上的不一致性，即在一定程度上反映了个体学习器之间的多样性</strong></p>\n<ul>\n<li>个体学习器和集成后的MSE误差为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nE\\left(h_{i} \\mid \\boldsymbol{x}\\right)=\\left(f(\\boldsymbol{x})-h_{i}(\\boldsymbol{x})\\right)^{2} \\\\\nE(H \\mid \\boldsymbol{x})=(f(\\boldsymbol{x})-H(\\boldsymbol{x}))^{2}\n\\end{array}</script><p>并且表示出个体学习器误差的加权平均：</p>\n<script type=\"math/tex; mode=display\">\n\\bar{E}(h \\mid \\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} \\cdot E\\left(h_{i} \\mid \\boldsymbol{x}\\right)</script><ul>\n<li>由上式可得：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\bar{A}(h \\mid \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} E\\left(h_{i} \\mid \\boldsymbol{x}\\right)-E(H \\mid \\boldsymbol{x}) \\\\\n&=\\bar{E}(h \\mid \\boldsymbol{x})-E(H \\mid \\boldsymbol{x})\n\\end{aligned}</script><ul>\n<li>上式对于所有x都成立，引入概率密度函数<script type=\"math/tex\">p(x)</script>，则在全样本上可将上式扩展成：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{T} w_{i} \\int A\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}=\\sum_{i=1}^{T} w_{i} \\int E\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}-\\int E(H \\mid \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x} .</script><ul>\n<li>同样，将泛化误差和分歧扩展在全样本上：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nE(h_i) = E_{i}=\\int E\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x} \\\\\nA(h_i) = A_{i}=\\int A\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x} \\\\\nE(H) = E=\\int E(H \\mid \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x}\n\\end{array}</script><p>再取个学习器的加权误差和加权分歧：</p>\n<script type=\"math/tex; mode=display\">\n\\bar{E}=\\sum_{i=1}^{T} w_{i} E_{i} \\\\\n\\bar{A}=\\sum_{i=1}^{T} w_{i} A_{i}</script><ul>\n<li>通过上面这些式子可以得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE=\\bar{E}-\\bar{A}</script><p>这个式子表明：<strong>个体学习器准确率越高、多样性越大，则集成效果越好</strong></p>\n<h3 id=\"5-2-多样性度量\"><a href=\"#5-2-多样性度量\" class=\"headerlink\" title=\"5.2 多样性度量\"></a>5.2 多样性度量</h3><ul>\n<li>简单介绍几个多样性的度量标准，给定数据集<script type=\"math/tex\">D=\\left\\{\\left(\\boldsymbol{x}_{1}, y_{1}\\right),\\left(\\boldsymbol{x}_{2}, y_{2}\\right), \\ldots,\\left(\\boldsymbol{x}_{m}, y_{m}\\right)\\right\\}</script>，假定为二分类任务，则两个分类器的预测结果列联表为：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018180852374.png\" alt=\"image-20221018180852374\" style=\"zoom:80%;\" /></p>\n<p>该表中，比如c为<script type=\"math/tex\">h_i</script>预测为负类而<script type=\"math/tex\">h_j</script>预测为正类的样本数</p>\n<ul>\n<li><strong>不合度量（disagreement measure）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nd i s_{i j}=\\frac{b+c}{m}</script><p>值越大多样性越大</p>\n<ul>\n<li><strong>相关系数（correlation coefficient）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\rho_{i j}=\\frac{a d-b c}{\\sqrt{(a+b)(a+c)(c+d)(b+d)}}</script><p>若<script type=\"math/tex\">h_i, h_j</script>无关，则值为0，若正相关则值为正，若负相关则值为负。绝对值越大，相关性越强</p>\n<ul>\n<li><strong>Q-统计量（Q-statistic）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nQ_{i j}=\\frac{a d-b c}{a d+b c}</script><p>和<script type=\"math/tex\">\\rho_{ij}</script>类似</p>\n<ul>\n<li><strong><script type=\"math/tex\">\\kappa</script>-统计量（k-statistic）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\kappa=\\frac{p_{1}-p_{2}}{1-p_{2}}</script><p>其中<script type=\"math/tex\">p_i</script>为两个分类器取得一致的概率，<script type=\"math/tex\">p_2</script>为两个分类器偶然达成一致的概率，可由数据集D直接统计估算：</p>\n<script type=\"math/tex; mode=display\">\np_1 = \\frac{a+d}{a+b+c+d} \\\\\np_2 = \\frac{(a+b)(a+c) + (c+d)(b+d)}{(a+b+c+d)^2}</script><p>若两个分类器在D上完全一致，则<script type=\"math/tex\">\\kappa = 1</script>，若只是偶然达成一致，<script type=\"math/tex\">\\kappa = 0</script>，<script type=\"math/tex\">\\kappa</script>一般非负，尽在两个分类器达成一致的概率甚至低于偶然性的情况下取负值。<script type=\"math/tex\">\\kappa</script>越大，多样性越小</p>\n","site":{"data":{}},"wordcount":6908,"excerpt":"","more":"<h1 id=\"1-基本概念\"><a href=\"#1-基本概念\" class=\"headerlink\" title=\"1 基本概念\"></a>1 基本概念</h1><ul>\n<li>集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务。其中每个<strong>个体学习器</strong>如果都使用同样的算法，则称这种集成是<strong>同质</strong>的，同质集成中的个体学习器亦称<strong>基学习器</strong>，反之则为<strong>异质</strong>的</li>\n<li><p>集成学习的相比于单一学习器可获得更优越的泛化性能，这对<strong>弱学习器（泛化性能略优于随机猜测的学习器）</strong>尤为明显</p>\n</li>\n<li><p><strong>对于每个个体学习器，要有一定的准确性，即学习器不能太坏，也要有多样性，即学习器间具有差异</strong></p>\n</li>\n<li><p>做一个简单的分析：</p>\n</li>\n</ul>\n<blockquote>\n<ul>\n<li>考虑一个二分类问题，假设<strong>每个基学习器的错误率相互独立</strong>且为<script type=\"math/tex\">\\epsilon</script>，即对每个基学习器<script type=\"math/tex\">h_i</script>有：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(h_i(x) \\neq f(x)) = \\epsilon</script><ul>\n<li>假设集成时使用简单投票法结合T个基学习器：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\operatorname{sign}\\left(\\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})\\right)</script><ul>\n<li>由于每个基学习器的错误率相互独立，则由Hoeffding不等式可知，集成错误率为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(H(\\boldsymbol{x}) \\neq f(\\boldsymbol{x})) &=\\sum_{k=0}^{\\lfloor T / 2\\rfloor}C_T^k(1-\\epsilon)^{k} \\epsilon^{T-k} \\\\\n& \\leqslant \\exp \\left(-\\frac{1}{2} T(1-2 \\epsilon)^{2}\\right)\n\\end{aligned}</script><ul>\n<li><strong>由上式可得：随着T的增大，集成的错误率降指数级下降，最终趋于0</strong></li>\n</ul>\n</blockquote>\n<ul>\n<li>在上面的分析中提到了一个关键的假设：基学习器的误差相互独立。但是在现实任务中是不可能的。目前的集成学习方法可大致分为两类：</li>\n</ul>\n<blockquote>\n<ol>\n<li>个体学习器之间存在强依赖关系，必须串行生成的序列化方法，代表是Boosting</li>\n<li>个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林（RF）</li>\n</ol>\n</blockquote>\n<h1 id=\"2-Boosting\"><a href=\"#2-Boosting\" class=\"headerlink\" title=\"2 Boosting\"></a>2 Boosting</h1><ul>\n<li>Boosting是一族可将弱学习器提升为强学习器的算法，工作机制为：<strong>先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此重复直至得到事先指定的T</strong></li>\n<li>从偏差-方差的角度看，<strong>Boosting主要关注降低偏差</strong></li>\n</ul>\n<h3 id=\"2-1-AdaBoost\"><a href=\"#2-1-AdaBoost\" class=\"headerlink\" title=\"2.1 AdaBoost\"></a>2.1 AdaBoost</h3><h4 id=\"2-1-1-算法流程\"><a href=\"#2-1-1-算法流程\" class=\"headerlink\" title=\"2.1.1 算法流程\"></a>2.1.1 算法流程</h4><ul>\n<li>其中最著名的代表就是AdaBoost，考虑一个二分类任务，其算法流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221015122613734.png\" alt=\"image-20221015122613734\" style=\"zoom:80%;\" /></p>\n<p>其中的<script type=\"math/tex\">D_t</script>为样本权重分布，反应了每个样本对基学习器的重要程度（反映在对loss的贡献上）。<script type=\"math/tex\">\\alpha_t</script>为每个基学习器的权重。在每次更新样本权重分布的时候提升分类错误样本的权重，其中<script type=\"math/tex\">Z_t</script>是一个规范化因子</p>\n<h4 id=\"2-1-2-损失函数\"><a href=\"#2-1-2-损失函数\" class=\"headerlink\" title=\"2.1.2 损失函数\"></a>2.1.2 损失函数</h4><ul>\n<li>AdaBoost是基于加法模型，即基于学习器的线性组合：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\sum_{t=1}^{T} \\alpha_{t} h_{t}(\\boldsymbol{x})</script><p>来最小化指数损失函数：</p>\n<script type=\"math/tex; mode=display\">\n\\ell_{\\exp }(H \\mid \\mathcal{D})=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\\right]</script><p>其实<strong>指数函数是分类任务原本0/1损失函数的一致的替代损失函数</strong>（因为其拥有更好的数学性质）</p>\n<blockquote>\n<p><strong>证明：</strong></p>\n<ul>\n<li>求<script type=\"math/tex\">H(x)</script>关于损失函数的偏导：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\ell_{\\exp }(H \\mid \\mathcal{D})}{\\partial H(\\boldsymbol{x})}=-e^{-H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=1 \\mid \\boldsymbol{x})+e^{H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=-1 \\mid \\boldsymbol{x})</script><ul>\n<li>偏导数为0可求得极点：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\frac{1}{2} \\ln \\frac{P(f(x)=1 \\mid \\boldsymbol{x})}{P(f(x)=-1 \\mid \\boldsymbol{x})}</script><ul>\n<li>因此有：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nsign(H(x)) =\\left\\{\\begin{array}{l}\n1, \\quad P(f(x)=1 \\mid \\boldsymbol{x})>P(f(x)=-1 \\mid \\boldsymbol{x}) \\\\\n-1, \\quad P(f(x)=1 \\mid \\boldsymbol{x})<P(f(x)=-1 \\mid \\boldsymbol{x})\n\\end{array}\\right. \\\\\n=\\underset{y \\in\\{-1,1\\}}{\\arg \\max } P(f(x)=y \\mid \\boldsymbol{x})\n\\end{array}</script></blockquote>\n<h4 id=\"2-1-3-基学习器的权重\"><a href=\"#2-1-3-基学习器的权重\" class=\"headerlink\" title=\"2.1.3 基学习器的权重\"></a>2.1.3 基学习器的权重</h4><ul>\n<li>在通过<script type=\"math/tex\">D_t</script>产生<script type=\"math/tex\">h_t</script>后，该基分类器的权重<script type=\"math/tex\">\\alpha_t</script>应使得<script type=\"math/tex\">\\alpha_th_t</script>最小化指数损失函数（在<script type=\"math/tex\">D_t</script>分布上而非D分布）：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\ell_{\\exp }\\left(\\alpha_{t} h_{t} \\mid \\mathcal{D}_{t}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right)\\right] \\\\\n&=e^{-\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right) \\\\\n&=e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t}\n\\end{aligned}</script><ul>\n<li>关于<script type=\"math/tex\">\\alpha_t</script>求导：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\ell_{\\exp }\\left(\\alpha_{t} h_{t} \\mid \\mathcal{D}_{t}\\right)}{\\partial \\alpha_{t}}=-e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t} = 0 \\\\\n\\alpha_t = \\frac{1}{2}\\ln(\\frac{1 - \\epsilon_t}{\\epsilon_t})</script><h4 id=\"2-1-4-样本权重分布的更新\"><a href=\"#2-1-4-样本权重分布的更新\" class=\"headerlink\" title=\"2.1.4 样本权重分布的更新\"></a>2.1.4 样本权重分布的更新</h4><ul>\n<li>在获得<script type=\"math/tex\">H_{t-1}</script>后样本分布将进行调整，使下一轮的<script type=\"math/tex\">h_t</script>能纠正<script type=\"math/tex\">H_{t-1}</script>的一些错误，理想情况下能纠正<script type=\"math/tex\">H_{t-1}</script>的全部错误，即最小化<script type=\"math/tex\">\\ell_{\\exp }\\left(H_{t-1}+\\alpha_th_{t} \\mid \\mathcal{D}\\right)</script>，可简化为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} \\mid \\mathcal{D}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x})\\left(H_{t-1}(\\boldsymbol{x})+h_{t}(\\boldsymbol{x})\\right)}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})}\\right]\n\\end{aligned}</script><p>其中的<script type=\"math/tex\">e^{-f(x)h_t(x)}</script>可用泰勒展示近似：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} \\mid \\mathcal{D}\\right) & \\simeq \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{f^{2}(\\boldsymbol{x}) h_{t}^{2}(\\boldsymbol{x})}{2}\\right)\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{1}{2}\\right)\\right]\n\\end{aligned}</script><ul>\n<li>于是理想的基学习器为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\ell_{\\exp }\\left(H_{t-1}+h \\mid \\mathcal{D}\\right) \\\\\n=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\\n=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right]\n\\end{array}</script><p>注意最后添加的分母是一个常数。令<script type=\"math/tex\">D_t</script>表示分布：</p>\n<script type=\"math/tex; mode=display\">\n\\mathcal{D}_{t}(\\boldsymbol{x})=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}</script><p>则：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nh_{t}(\\boldsymbol{x}) &=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\\n&=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[f(\\boldsymbol{x}) h(\\boldsymbol{x})]\n\\end{aligned}</script><ul>\n<li>而由于<script type=\"math/tex\">f(\\boldsymbol{x}) h(\\boldsymbol{x})=1-2 \\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))</script>，所以：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))]</script><p><strong>所以理想的<script type=\"math/tex\">h_t</script>将在<script type=\"math/tex\">D_t</script>分布下最小化分类误差</strong>。因此，弱分类器将基于分布<script type=\"math/tex\">D_t</script>来训练</p>\n<ul>\n<li>考虑由<script type=\"math/tex\">D_t</script>推导到<script type=\"math/tex\">D_{t-1}</script>：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\mathcal{D}_{t+1}(\\boldsymbol{x}) &=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\mathcal{D}_{t}(\\boldsymbol{x}) \\cdot e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})} \\frac{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]}\n\\end{aligned}</script><p>恰好对应算法流程总的更新公式</p>\n<h3 id=\"2-2-调整数据分布的方法\"><a href=\"#2-2-调整数据分布的方法\" class=\"headerlink\" title=\"2.2 调整数据分布的方法\"></a>2.2 调整数据分布的方法</h3><ul>\n<li>在上述算法流程中，是通过对每个样本赋予不同的权重来调整数据分布，称为<strong>重赋权法（re-weighting）</strong></li>\n<li><p>但是某些基学习算法无法接受带权样本，这时可采样<strong>重采样法（re-sampling）</strong>，即在每一轮中根据样本分布对训练集进行重新采样，然后根据采样到的样本进行训练。<strong>一般而言，两种算法没有显著的优劣差别</strong></p>\n</li>\n<li><p>注意在算法流程中，如果得到的<script type=\"math/tex\">\\epsilon_t > 0.5</script>，则整个算法就终止了，如果采用的是重赋权法，可能导致过早停止致使因基学习器过少而导致的性能不佳。但是如果使用重采样法，可以通过重启动避免过早的停止，即如果当前分布训练出来的基学习器不好，则抛弃，然后重新采样再训练</p>\n</li>\n</ul>\n<h1 id=\"3-Bagging\"><a href=\"#3-Bagging\" class=\"headerlink\" title=\"3 Bagging\"></a>3 Bagging</h1><ul>\n<li>欲得到泛化性能强的集成，个体学习器之间应该尽可能相互独立，虽然在现实任务中无法得到，但是可以设法令其具有较大差异。一种方法就是改变每个个体学习器的训练数据集，使每个训练集差异较大，但是又不能过大（每个训练集之间都没有交集），这样只用到了很少的训练数据进行训练。</li>\n<li><p>所以可用<strong>自助采样法（bootstrap sampling）</strong>：给定包含m个样本的数据集，每次随机取出一个样本后，又把该样本放回去，使得下次采样同样有可能采到该样本，这样采样m次得到一个同样大小的数据集。使用这样的采样方法，初始数据集中约有63.2%的样本出现在采样集中</p>\n</li>\n<li><p>自助采样法还有一个优点就是：对于每个基学习器，仅使用了约63.2%的样本，剩下的样本正好可作为每个基学习器的验证集</p>\n</li>\n<li>从偏差-方差分解的角度看，<strong>Bagging主要关注降低方差</strong>（即在不同数据集上表现的稳定性）。因此他在不剪枝的决策树、神经网络等易受样本扰动的学习器上效用更为明显</li>\n</ul>\n<h3 id=\"3-1-算法流程\"><a href=\"#3-1-算法流程\" class=\"headerlink\" title=\"3.1 算法流程\"></a>3.1 算法流程</h3><ul>\n<li>算法是采样出T个采样集，然后基于每个采样集训练出一个基学习器（所以可以并行操作）。最后进行简单投票（即每个基学习器使用相同权重），流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221015150202207.png\" alt=\"image-20221015150202207\" style=\"zoom:80%;\" /></p>\n<p>Bagging的时间复杂度约等于一个基学习器的时间复杂度，所以他是一个很高效的算法。另外，AdaBoost需进行一些算法的更改才可应用于多分类和回归任务，而Bagging算法可直接应用</p>\n<h3 id=\"3-2-随机森林\"><a href=\"#3-2-随机森林\" class=\"headerlink\" title=\"3.2 随机森林\"></a>3.2 随机森林</h3><ul>\n<li>随机森林（Random Forest，RF）是Bagging的一个扩展变体，其在以决策树为基学习器构建Bagging的基础上，进一步在决策树的训练过程中引入<strong>随机属性选择</strong>。</li>\n<li><p>具体来说，<strong>在每个结点选择一个划分属性时，传统决策树是从所有当前结点中（假设有d个）选择一个最优的，而RF先随机选择k个属性的子集，然后再在该子集中选择一个最优的</strong>。推荐<script type=\"math/tex\">k=\\log_2d</script></p>\n</li>\n<li><p>RF使用了非常小的额外计算开销，但却在许多任务中展现出了强大的性能。其不仅像Bagging一样通过<strong>样本扰动</strong>来增加基学习器的多样性，还通过<strong>属性扰动</strong>进一步增加多样性，这就使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步增加</p>\n</li>\n<li>RF的收敛性和Bagging类似，但起始性能相对较差，最终结果更佳：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018114212479.png\" alt=\"image-20221018114212479\" style=\"zoom:70%;\" /></p>\n<h1 id=\"4-结合策略\"><a href=\"#4-结合策略\" class=\"headerlink\" title=\"4 结合策略\"></a>4 结合策略</h1><h3 id=\"4-1-集成的好处\"><a href=\"#4-1-集成的好处\" class=\"headerlink\" title=\"4.1 集成的好处\"></a>4.1 集成的好处</h3><ul>\n<li><strong>统计方面：</strong>因为学习任务的假设空间很大，所以可能有多个假设在训练集上性能一样，使用单学习器可能因为误选导致泛化性能不佳，结合多个学习器则会减少这一风险</li>\n<li><strong>计算方面：</strong>算法可能会陷入局部最小点，而多次运行后结合可降低这种风险</li>\n<li><strong>表示方面：</strong>某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，结合多个学习器，由于相应的假设空间有所扩大，有可能学得更好的近似</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018115801453.png\" alt=\"image-20221018115801453\" style=\"zoom:70%;\" /></p>\n<h3 id=\"4-2-平均法\"><a href=\"#4-2-平均法\" class=\"headerlink\" title=\"4.2 平均法\"></a>4.2 平均法</h3><ul>\n<li><p>针对回归任务</p>\n</li>\n<li><p><strong>简单平均法：</strong></p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\frac{1}{T} \\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})</script><ul>\n<li><strong>加权平均法：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} h_{i}(\\boldsymbol{x})</script><ul>\n<li><p>加权平均法的权重一般是通过训练数据学习而得，现实任务中的训练样本通常不充分或存在噪声，所以<strong>学出的权重不一定可靠</strong>。因此，<strong>加权平均法未必优于简单平均法</strong></p>\n</li>\n<li><p>一般而言，<strong>个体学习器性能相差较大时使用加权平均，性能相近时使用简单平均</strong></p>\n</li>\n</ul>\n<h3 id=\"4-3-投票法\"><a href=\"#4-3-投票法\" class=\"headerlink\" title=\"4.3 投票法\"></a>4.3 投票法</h3><ul>\n<li><p>针对分类任务</p>\n</li>\n<li><p>规定第<script type=\"math/tex\">i</script>个学习器<script type=\"math/tex\">h_i</script>在样本x上的预测输出表示为<script type=\"math/tex\">(h_i^1(x), ..., h_i^N(x))</script>，N为类别个数</p>\n</li>\n<li><strong>绝对多数投票法：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=\\left\\{\\begin{array}{ll}\nc_{j}, & \\text { if } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})>0.5 \\sum_{k=1}^{N} \\sum_{i=1}^{T} h_{i}^{k}(\\boldsymbol{x}) \\\\\n\\text { reject, } & \\text { otherwise }\n\\end{array}\\right.</script><p>即某类得到票数过半，才预测为该类，否则拒绝预测</p>\n<ul>\n<li><strong>相对多数投票法：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=c_{j}^{\\arg \\max } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})</script><ul>\n<li><strong>加权投票法：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nH(\\boldsymbol{x})=c_{j}^{\\arg \\max } \\sum_{i=1}^{T} w_{i} h_{i}^{j}(\\boldsymbol{x}) .</script><ul>\n<li>上面的式子中并没有限制个体学习器输出值的类型，一般为类标记（<script type=\"math/tex\">h_i^j(x) \\in \\{0,1\\}</script>）或类概率（<script type=\"math/tex\">h_i^j(x) \\in [0,1]</script>）。不能类型的输出不能混用，一般基于类概率效果往往比类标记更好。<strong>若基学习器的类型不同，其类概率之间不能直接进行比较，需要先转化为类标记</strong></li>\n</ul>\n<h3 id=\"4-4-学习法\"><a href=\"#4-4-学习法\" class=\"headerlink\" title=\"4.4 学习法\"></a>4.4 学习法</h3><ul>\n<li><strong>当训练数据很多时</strong>，一种更为强大的结合策略是学习法，即通过另一个学习器来进行结合。其中Stacking是一种典型代表。把个体学习器称为<strong>初级学习器</strong>，用于结合的学习器称为<strong>次级学习器</strong>或<strong>元学习器</strong></li>\n<li>Stacking算法先从初始数据集中训练出多个初级学习器，然后以此生成一个新的数据集：初级学习器的输出被当作样例输入特征，而初始样本的label仍作为新数据集的label，算法流程如下：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018165945814.png\" alt=\"image-20221018165945814\" style=\"zoom:70%;\" /></p>\n<ul>\n<li>但是上述算法流程有个问题：<strong>次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险较大</strong>，因此一般采用交叉验证或留一法，用初级学习器未使用的样本来产生次级学习器的训练样本</li>\n</ul>\n<blockquote>\n<ul>\n<li>以k折交叉验证法为例，训练集为D，对于每个初级学习器都会有k折（所以一共会迭代<script type=\"math/tex\">T * k</script>次），在第<script type=\"math/tex\">i</script>个学习器的第<script type=\"math/tex\">j</script>折的时候，令<script type=\"math/tex\">D_j</script>和<script type=\"math/tex\">\\bar{D}_{j}=D \\backslash D_{j}</script>为此时的验证集和训练集，通过<script type=\"math/tex\">\\bar{D}_j</script>训练出<script type=\"math/tex\">h_i</script>后，在使用<script type=\"math/tex\">D_j</script>进行验证和生成次级训练集</li>\n</ul>\n</blockquote>\n<ul>\n<li>次级学习器的输入和学习算法有很大的影响，效果比较好的是：<strong>将初级学习器的输出类概率作为输入，再采用多响应线性回归（MLR）作为次级学习算法</strong></li>\n</ul>\n<blockquote>\n<ul>\n<li><strong>MLR：</strong>对每个类分别进行<strong>线性回归</strong>，训练样例要输入进每一个回归模型，若训练样例属于该类，则对应回归的输出label为1，若不属于该类，label则为0。预测时取输出值最大的那个类</li>\n</ul>\n</blockquote>\n<h1 id=\"5-多样性\"><a href=\"#5-多样性\" class=\"headerlink\" title=\"5 多样性\"></a>5 多样性</h1><h3 id=\"5-1-误差-分歧分解\"><a href=\"#5-1-误差-分歧分解\" class=\"headerlink\" title=\"5.1 误差-分歧分解\"></a>5.1 误差-分歧分解</h3><ul>\n<li>以一个回归问题使用加权平均法进行集成为例，对于样例x，定义学习器<script type=\"math/tex\">h_i</script>的<strong>分歧（ambiguity）</strong>为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nA\\left(h_{i} \\mid \\boldsymbol{x}\\right)=\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}</script><p>集成的分歧为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\bar{A}(h \\mid \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} A\\left(h_{i} \\mid \\boldsymbol{x}\\right) \\\\\n&=\\sum_{i=1}^{T} w_{i}\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\n\\end{aligned}</script><p><strong>上式表征了个体学习器之间在样本x上的不一致性，即在一定程度上反映了个体学习器之间的多样性</strong></p>\n<ul>\n<li>个体学习器和集成后的MSE误差为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nE\\left(h_{i} \\mid \\boldsymbol{x}\\right)=\\left(f(\\boldsymbol{x})-h_{i}(\\boldsymbol{x})\\right)^{2} \\\\\nE(H \\mid \\boldsymbol{x})=(f(\\boldsymbol{x})-H(\\boldsymbol{x}))^{2}\n\\end{array}</script><p>并且表示出个体学习器误差的加权平均：</p>\n<script type=\"math/tex; mode=display\">\n\\bar{E}(h \\mid \\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} \\cdot E\\left(h_{i} \\mid \\boldsymbol{x}\\right)</script><ul>\n<li>由上式可得：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\bar{A}(h \\mid \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} E\\left(h_{i} \\mid \\boldsymbol{x}\\right)-E(H \\mid \\boldsymbol{x}) \\\\\n&=\\bar{E}(h \\mid \\boldsymbol{x})-E(H \\mid \\boldsymbol{x})\n\\end{aligned}</script><ul>\n<li>上式对于所有x都成立，引入概率密度函数<script type=\"math/tex\">p(x)</script>，则在全样本上可将上式扩展成：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{T} w_{i} \\int A\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}=\\sum_{i=1}^{T} w_{i} \\int E\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}-\\int E(H \\mid \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x} .</script><ul>\n<li>同样，将泛化误差和分歧扩展在全样本上：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{l}\nE(h_i) = E_{i}=\\int E\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x} \\\\\nA(h_i) = A_{i}=\\int A\\left(h_{i} \\mid \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x} \\\\\nE(H) = E=\\int E(H \\mid \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x}\n\\end{array}</script><p>再取个学习器的加权误差和加权分歧：</p>\n<script type=\"math/tex; mode=display\">\n\\bar{E}=\\sum_{i=1}^{T} w_{i} E_{i} \\\\\n\\bar{A}=\\sum_{i=1}^{T} w_{i} A_{i}</script><ul>\n<li>通过上面这些式子可以得到：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nE=\\bar{E}-\\bar{A}</script><p>这个式子表明：<strong>个体学习器准确率越高、多样性越大，则集成效果越好</strong></p>\n<h3 id=\"5-2-多样性度量\"><a href=\"#5-2-多样性度量\" class=\"headerlink\" title=\"5.2 多样性度量\"></a>5.2 多样性度量</h3><ul>\n<li>简单介绍几个多样性的度量标准，给定数据集<script type=\"math/tex\">D=\\left\\{\\left(\\boldsymbol{x}_{1}, y_{1}\\right),\\left(\\boldsymbol{x}_{2}, y_{2}\\right), \\ldots,\\left(\\boldsymbol{x}_{m}, y_{m}\\right)\\right\\}</script>，假定为二分类任务，则两个分类器的预测结果列联表为：</li>\n</ul>\n<p><img src=\"https://zlkqzimg-1310374208.cos.ap-chengdu.myqcloud.com/image-20221018180852374.png\" alt=\"image-20221018180852374\" style=\"zoom:80%;\" /></p>\n<p>该表中，比如c为<script type=\"math/tex\">h_i</script>预测为负类而<script type=\"math/tex\">h_j</script>预测为正类的样本数</p>\n<ul>\n<li><strong>不合度量（disagreement measure）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nd i s_{i j}=\\frac{b+c}{m}</script><p>值越大多样性越大</p>\n<ul>\n<li><strong>相关系数（correlation coefficient）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\rho_{i j}=\\frac{a d-b c}{\\sqrt{(a+b)(a+c)(c+d)(b+d)}}</script><p>若<script type=\"math/tex\">h_i, h_j</script>无关，则值为0，若正相关则值为正，若负相关则值为负。绝对值越大，相关性越强</p>\n<ul>\n<li><strong>Q-统计量（Q-statistic）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nQ_{i j}=\\frac{a d-b c}{a d+b c}</script><p>和<script type=\"math/tex\">\\rho_{ij}</script>类似</p>\n<ul>\n<li><strong><script type=\"math/tex\">\\kappa</script>-统计量（k-statistic）：</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\kappa=\\frac{p_{1}-p_{2}}{1-p_{2}}</script><p>其中<script type=\"math/tex\">p_i</script>为两个分类器取得一致的概率，<script type=\"math/tex\">p_2</script>为两个分类器偶然达成一致的概率，可由数据集D直接统计估算：</p>\n<script type=\"math/tex; mode=display\">\np_1 = \\frac{a+d}{a+b+c+d} \\\\\np_2 = \\frac{(a+b)(a+c) + (c+d)(b+d)}{(a+b+c+d)^2}</script><p>若两个分类器在D上完全一致，则<script type=\"math/tex\">\\kappa = 1</script>，若只是偶然达成一致，<script type=\"math/tex\">\\kappa = 0</script>，<script type=\"math/tex\">\\kappa</script>一般非负，尽在两个分类器达成一致的概率甚至低于偶然性的情况下取负值。<script type=\"math/tex\">\\kappa</script>越大，多样性越小</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}