

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="bia">
  <meta name="author" content="zlk">
  <meta name="keywords" content="">
  <meta name="description" content="与多层感知机和能有效处理空间信息的卷积神经⽹络不同，循环神经⽹络是为更好地处理时序信息而设计的。它引⼊状态变量来存储过去的信息，并⽤其与当前的输⼊共同决定当前的输出  1 语言模型1.1 语言模型的计算 我们可以把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。假设⼀段⻓度为 $T$ 的⽂本中的词 依次为 $w_1, w_2, . . . , w_T$，那么在离散的时间序列中，$w_t（1 ≤ t ≤">
<meta property="og:type" content="article">
<meta property="og:title" content="RNN基本概念">
<meta property="og:url" content="https://zlkqz.github.io/2021/12/10/RNN/index.html">
<meta property="og:site_name" content="ZLK">
<meta property="og:description" content="与多层感知机和能有效处理空间信息的卷积神经⽹络不同，循环神经⽹络是为更好地处理时序信息而设计的。它引⼊状态变量来存储过去的信息，并⽤其与当前的输⼊共同决定当前的输出  1 语言模型1.1 语言模型的计算 我们可以把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。假设⼀段⻓度为 $T$ 的⽂本中的词 依次为 $w_1, w_2, . . . , w_T$，那么在离散的时间序列中，$w_t（1 ≤ t ≤">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/oAbwO43tDSTWpVg.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/OYZwANsxl2oEQaI.png">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_720w.jpg?source=1940ef5c">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/Feq2PM3nCslpGO7.png">
<meta property="og:image" content="c:/Users/LENOVO/AppData/Roaming/Typora/typora-user-images/image-20211210135706726.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/L7O69pm3Yjzbno5.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/jzn954wmEqRfyIg.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/mlYhrVbA47MByvG.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/PUK7CRjyemEbzLM.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/VtFmigZ58qw2aeb.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/FQfVxdq2wuC7aSs.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/JUgILXFCjWviaQw.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/zSKDLamFWqseyuN.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/QEBb3gUVxmuDS7O.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/10/5kGV8uqeaYn3wlF.png">
<meta property="article:published_time" content="2021-12-10T11:05:28.597Z">
<meta property="article:modified_time" content="2021-12-11T07:55:31.135Z">
<meta property="article:author" content="zlk">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2021/12/10/oAbwO43tDSTWpVg.png">
  
  <title>RNN基本概念 - ZLK</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"zlkqz.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ZLK</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="RNN基本概念">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-12-10 19:05" pubdate>
        2021年12月10日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      15k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      48 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">RNN基本概念</h1>
            
            <div class="markdown-body">
              <ul>
<li>与多层感知机和能有效<strong>处理空间信息</strong>的卷积神经⽹络不同，循环神经⽹络是为更好地<strong>处理时序信息</strong>而设计的。它<strong>引⼊状态变量来存储过去的信息</strong>，并⽤其<strong>与当前的输⼊共同决定当前的输出</strong></li>
</ul>
<h1 id="1-语言模型"><a href="#1-语言模型" class="headerlink" title="1 语言模型"></a>1 语言模型</h1><h3 id="1-1-语言模型的计算"><a href="#1-1-语言模型的计算" class="headerlink" title="1.1 语言模型的计算"></a>1.1 语言模型的计算</h3><ul>
<li>我们可以把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。假设⼀段⻓度为 $T$ 的⽂本中的词 依次为 $w_1, w_2, . . . , w_T$，那么在离散的时间序列中，$w_t（1 ≤ t ≤ T）$可看作在时间步（time step）$ t$ 的输出或标签。给定⼀个⻓度为 $T$ 的词的序列 $w_1, w_2, . . . , w_T$，语⾔模型将计算该序列的概率：</li>
</ul>
<script type="math/tex; mode=display">
P(w_1, w_2, ...,w_T)</script><ul>
<li><p>语⾔模型可⽤于提升语⾳识别和机器翻译的性能。例如，在语⾳识别中，给定⼀段“厨房⾥⻝油 ⽤完了”的语⾳，有可能会输出“厨房⾥⻝油⽤完了”和“厨房⾥⽯油⽤完了”这两个读⾳完全 ⼀样的⽂本序列。如果语⾔模型判断出前者的概率⼤于后者的概率，我们就可以根据相同读⾳的 语⾳输出“厨房⾥⻝油⽤完了”的⽂本序列。</p>
</li>
<li><p><strong>由于 $w_1, w_2, . . . , w_T$ 是依次生成的，所以有：</strong></p>
</li>
</ul>
<script type="math/tex; mode=display">
P(w_1, w_2, ...,w_T) = \prod_{t=1}^T P(w_t | w_1, ..., w_{t-1})</script><ul>
<li>为了计算语⾔模型，我们需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即<strong>语⾔模型参数</strong>。<strong>词的概率可以通过该词在训练数据集中的相对词频来计算</strong></li>
</ul>
<h3 id="1-2-n元语法"><a href="#1-2-n元语法" class="headerlink" title="1.2 n元语法"></a>1.2 n元语法</h3><ul>
<li><strong>当序列⻓度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加</strong></li>
<li>n元语法通过⻢尔可夫假设（<strong>虽然并不⼀定成立</strong>）简化了语⾔模型的计算。这⾥的<strong>⻢尔可夫假设是指⼀个词的出现只与前⾯n个词相关</strong>，即n阶⻢尔可夫链（Markov chain of order n）</li>
<li>如果基于$n-1$阶马尔科夫链，我们可以将语言模型改写为：</li>
</ul>
<script type="math/tex; mode=display">
P(w_1, w_2, ...,w_T) \approx \prod_{t=1}^T P(w_t | w_{t - (n - 1)}, ..., w_{t-1})</script><ul>
<li><p>以上也叫<strong>n元语法</strong>（n-grams）。它是基于n−1阶⻢尔可夫链的概率语⾔模型</p>
</li>
<li><p>当n较小时，n元语法往往并不准确。当n较⼤时，n元语法需要计算并存储⼤量的词频和多词相邻频率。<strong>n权衡了计算复杂度和模型准确性</strong></p>
</li>
</ul>
<h1 id="2-循环神经网络（RNN）"><a href="#2-循环神经网络（RNN）" class="headerlink" title="2 循环神经网络（RNN）"></a>2 循环神经网络（RNN）</h1><ul>
<li>在n元语法中，时间步t的词 $w_t$ 基于前⾯所有词的条件概率只考虑了最近时间步的n − 1个词。如果要考虑⽐t − (n − 1)更早时间步的词对$w_t$的可能影响，我们需要增⼤n。但这样模型参数的数量将随之呈指数级增⻓</li>
<li>但是对于循环神经网络，它<strong>并⾮刚性地记忆所有固定⻓度的序列，而是通过隐藏状态来存储之前时间步的信息</strong></li>
</ul>
<h3 id="2-1-循环神经⽹络"><a href="#2-1-循环神经⽹络" class="headerlink" title="2.1 循环神经⽹络"></a>2.1 循环神经⽹络</h3><ul>
<li>假设<script type="math/tex">X_t \in \mathbb{R}^{n \times d}</script>是序列中时间步t的小批量输⼊， <script type="math/tex">H_t \in \mathbb{R}^{n \times h}</script>是该时间步的隐藏变量。与多层感知机不同的是，这⾥我们保存上⼀时间步的隐藏变量<script type="math/tex">H_{t−1}</script>，并引⼊⼀个新的权重参数<script type="math/tex">W_{hh} \in \mathbb{R}^{h×h}</script>，该参数⽤来描述在当前时间步如何使⽤上⼀时间步的隐藏变量。时间步t的隐藏变量的计算由当前时间步的输⼊和上⼀时间步的隐藏变量共同决定：</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{H}_{t}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h h}+\boldsymbol{b}_{h}\right)</script><p>$\phi$是激活函数</p>
<ul>
<li><strong>这⾥的隐藏变量能够捕捉截⾄当前时间步的序列的历史信息</strong></li>
<li>每个时间步还有一个对应的输出：</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{O}_{t}=\boldsymbol{H}_{t} \boldsymbol{W}_{h q}+\boldsymbol{b}_{q}</script><p>在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差</p>
<ul>
<li><strong>即便在不同时间步，循环神经⽹络也始终使⽤W， b这些模型参数。因此，循环神经⽹络模型参数的数量不随时间步的增加而增⻓</strong></li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/oAbwO43tDSTWpVg.png" srcset="/img/loading.gif" lazyload alt="image-20211210101227481"></p>
<ul>
<li>隐藏状态中<script type="math/tex">X_{t}W_{x h}+H_{t-1} W_{h h}</script>的计算等价于<script type="math/tex">X_t</script>与<script type="math/tex">H_{t−1}</script>连结后的矩阵乘以<script type="math/tex">W_{xh}</script>与<script type="math/tex">W_{hh}</script>连结后的矩阵</li>
</ul>
<ul>
<li>举一个栗子：基于字符级循环神经⽹络的语⾔模型</li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/OYZwANsxl2oEQaI.png" srcset="/img/loading.gif" lazyload alt="image-20211210101914126"></p>
<p><strong>标签序列依次为输入序列的下一个</strong>，并且在训练时，我们对每个时间步的输出层输出使⽤softmax运算，然后使⽤交叉熵损失函数来计算它与标签的误差</p>
<h3 id="2-2-时序数据的采样"><a href="#2-2-时序数据的采样" class="headerlink" title="2.2 时序数据的采样"></a>2.2 时序数据的采样</h3><ul>
<li>时序数据的⼀个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想” “要” “有” “直” “升”。<strong>该样本的标签序列为这些字符分别在训练集中的下⼀个字符</strong>，即 “要” “有” “直” “升” “机”。</li>
<li>我们有两种⽅式对时序数据进⾏采样，分别是随机采样和相邻采样</li>
</ul>
<h4 id="2-2-1-随机采样"><a href="#2-2-1-随机采样" class="headerlink" title="2.2.1 随机采样"></a>2.2.1 随机采样</h4><ul>
<li><p>在随机采样中，每个样本是原始序列上任意截取的⼀段序列。</p>
</li>
<li><p>相邻的两个随机小批量在原始序列上的位置不⼀定相毗邻。因此，我们⽆法⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态。<strong>在训练模型时，每次随机采样前都需要重新初始化隐藏状态</strong></p>
</li>
</ul>
<h4 id="2-2-2-相邻采样"><a href="#2-2-2-相邻采样" class="headerlink" title="2.2.2 相邻采样"></a>2.2.2 相邻采样</h4><ul>
<li>除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就<strong>可以⽤⼀个小批量最终时间步的隐藏状态来初始化下⼀个小批量的隐藏状态</strong>，从而使下⼀个小批量的输出也取决于当前小批量的输⼊</li>
</ul>
<ul>
<li><strong>两种采样方式的区别：</strong><ol>
<li>采用相邻采样，在训练模型时，我们只需在每⼀个迭代周期开始时初始化隐藏状态</li>
<li>当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同⼀迭代周期中，随着迭代次数的增加，梯度的<strong>计算开销会越来越⼤</strong>。为了使模型参数的梯度计算只依赖⼀次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来</li>
</ol>
</li>
</ul>
<h3 id="2-3-RNN的训练、预测"><a href="#2-3-RNN的训练、预测" class="headerlink" title="2.3 RNN的训练、预测"></a>2.3 RNN的训练、预测</h3><h4 id="2-3-1-输入输出"><a href="#2-3-1-输入输出" class="headerlink" title="2.3.1 输入输出"></a>2.3.1 输入输出</h4><ul>
<li>首先是RNN的输入输出，应该包含time_steps、batch_size、vocab_size（时间步、批量大小、词典大小）的维度，之所以有一个维度是词典大小，是因为我们会<strong>把输入输出用独热向量</strong>表示</li>
</ul>
<h4 id="2-3-2-激活函数的选择"><a href="#2-3-2-激活函数的选择" class="headerlink" title="2.3.2 激活函数的选择"></a>2.3.2 激活函数的选择</h4><ul>
<li><p>对于下一个时间步隐藏状态的计算，激活函数一般选用Tanh或者ReLu。对输出的运算，一般做softmax运算。</p>
</li>
<li><p>但是很多时候还是选用Tanh而不是ReLu，这一点在Hinton的<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1504.00941">IRNN论文</a>里面是很明确的提到的：</p>
</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_720w.jpg?source=1940ef5c" srcset="/img/loading.gif" lazyload alt="img"></p>
<p><strong>也就是说在RNN中直接把激活函数换成ReLu会导致非常大的输出值</strong></p>
<ul>
<li>下面具体来看一下：</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{c}
\text { net }_{t}=U x_{t}+W h_{t-1} \\
h_{t}=f\left(\text { net }_{t}\right)
\end{array}</script><p>假设ReLu函数一直处于激活区域（即输入大于0），则有$f(x) = x, net<em>t = Ux_t + W(Ux</em>{t-1} + Wh_{t-2})$，继续将其展开，最终会包含多个W的连乘，如果W不是单位矩阵， 那么最终的结果将会区域0或无穷。</p>
<p>但是在CNN中就不会发生这种问题，因为CNN中每一层的W是不同的，并且在初始化时它们是独立同分布的，因此可以相互抵消，在多层之后一般不会出现这种严重的数值问题</p>
<p>我们再来看看反向传播的时候：</p>
<script type="math/tex; mode=display">
\frac{\partial net_t}{\partial net_{t-1}} = W</script><p>所以：</p>
<script type="math/tex; mode=display">
\frac{\partial net_t}{\partial net_{1}} = W^n</script><p>可以看到只要W不是单位矩阵，梯度会出现消失或者爆炸的现象</p>
<ul>
<li>综上所述：当采用ReLu作为激活函数时，只有W在单位矩阵附近时才能取得比较好的结果，因此需要将W初始化为单位矩阵。但是ReLu也拥有自己的优点，那就是计算量更小，收敛更快</li>
</ul>
<h4 id="2-3-3-预测"><a href="#2-3-3-预测" class="headerlink" title="2.3.3 预测"></a>2.3.3 预测</h4><ul>
<li><strong>预测时和训练时的最大区别就是预测将上⼀时间步的输出作为当前时间步的输⼊</strong></li>
</ul>
<h4 id="2-3-4-困惑度"><a href="#2-3-4-困惑度" class="headerlink" title="2.3.4 困惑度"></a>2.3.4 困惑度</h4><ul>
<li>我们通常使⽤困惑度（perplexity）来评价语⾔模型的好坏</li>
<li>困惑度是对交叉熵损失函数做指数运算后得到的值</li>
<li>特别的：</li>
</ul>
<ol>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数</li>
</ol>
<p><strong>显然，任何一个有效模型的困惑度必须小于类别个数（要不然模型的预测效果还不如随机点一个类别来的好）</strong></p>
<h4 id="2-3-5-通过时间反向传播"><a href="#2-3-5-通过时间反向传播" class="headerlink" title="2.3.5 通过时间反向传播"></a>2.3.5 通过时间反向传播</h4><ul>
<li>我们需要将循环神经⽹络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应⽤反向传播计算并存储梯度</li>
<li>简单起⻅，我们考虑⼀个⽆偏差项的循环神经⽹络，且激活函数为恒等映射$（\phi(x) = x）$。设时间步t的输⼊为单样本$x_t \in R^d$，标签为$y_t$，那么隐藏状态$h_t \in R^h$的计算表达式为:</li>
</ul>
<script type="math/tex; mode=display">
h_t = W_{hx}X_t + W_{hh}h_{t-1}</script><p>输出层变量$o_t \in \mathbb{R}^q$为：</p>
<script type="math/tex; mode=display">
o_t = W_{qh}h_t</script><p>设时间步t的损失为$\ell(o_t, y_t)$。则总时间步数为T的损失函数L定义为：</p>
<script type="math/tex; mode=display">
L = \frac{1}{T}\sum_{t = 1}^T \ell(o_t, y_t)</script><ul>
<li><p>我们假设一共有3个时间步数，那么可以做出计算图：<br><img src="https://s2.loli.net/2021/12/10/Feq2PM3nCslpGO7.png" srcset="/img/loading.gif" lazyload alt="image-20211210132156070"></p>
</li>
<li><p>现在我们开始反向传播：</p>
</li>
</ul>
<p>易得：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial \boldsymbol{o}_{t}}=\frac{\partial \ell\left(\boldsymbol{o}_{t}, y_{t}\right)}{T \cdot \partial \boldsymbol{o}_{t}}</script><p>然后可以得到：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial \boldsymbol{W}_{q h}}=\sum_{t=1}^{T} \operatorname{prod}\left(\frac{\partial L}{\partial \boldsymbol{o}_{t}}, \frac{\partial \boldsymbol{o}_{t}}{\partial \boldsymbol{W}_{q h}}\right)=\sum_{t=1}^{T} \frac{\partial L}{\partial \boldsymbol{o}_{t}} \boldsymbol{h}_{t}^{\top} .</script><p>接下来就是计算关于<script type="math/tex">W_{hx}</script>和<script type="math/tex">W_{hh}</script>的梯度，那么必然要求关于各时间步隐藏状态的梯度，我们注意到隐藏状态之间也存在依赖关系，，L只通过<script type="math/tex">o_T</script>依赖最终时间步T的隐藏状态<script type="math/tex">h_T</script>。因此，我们先计算⽬标函数有关最终时间步隐藏状态的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial \boldsymbol{h}_{T}}=\operatorname{prod}\left(\frac{\partial L}{\partial \boldsymbol{o}_{T}}, \frac{\partial \boldsymbol{o}_{T}}{\partial \boldsymbol{h}_{T}}\right)=\boldsymbol{W}_{q h}^{\top} \frac{\partial L}{\partial \boldsymbol{o}_{T}}</script><p>接下来对于时间步t &lt; T，在图6.3中，$L$通过$h_{t+1}$和$o_t$依赖$h_t$。依据链式法则，⽬标函数有关时间步t &lt; T的隐藏状态的梯度$\partial L / \partial h_t \in R^h$需要按照时间步从⼤到小依次计算:</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial \boldsymbol{h}_{t}}=\operatorname{prod}\left(\frac{\partial L}{\partial \boldsymbol{h}_{t+1}}, \frac{\partial \boldsymbol{h}_{t+1}}{\partial \boldsymbol{h}_{t}}\right)+\operatorname{prod}\left(\frac{\partial L}{\partial \boldsymbol{o}_{t}}, \frac{\partial \boldsymbol{o}_{t}}{\partial \boldsymbol{h}_{t}}\right)=\boldsymbol{W}_{h h}^{\top} \frac{\partial L}{\partial \boldsymbol{h}_{t+1}}+\boldsymbol{W}_{q h}^{\top} \frac{\partial L}{\partial \boldsymbol{o}_{t}} .</script><p>对次递归公式展开，我们可以得到对任意时间步1 ≤ t ≤ T，我们可以得到⽬标函数有关隐藏状态梯度的通项公式：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial \boldsymbol{h}_{t}}=\sum_{i=t}^{T}\left(\boldsymbol{W}_{h h}^{\top}\right)^{T-i} \boldsymbol{W}_{q h}^{\top} \frac{\partial L}{\partial \boldsymbol{o}_{T+t-i}}</script><ul>
<li><p><strong>由上式中的指数项可⻅，当时间步数T较⼤或者时间步t较小时，⽬标函数有关隐藏状态的梯度较容易出现衰减和爆炸</strong></p>
</li>
<li><p>RNN和一般的网络一样，我们在正向传播和反向传播的时候会进行一些数据的储存，避免重复计算</p>
</li>
</ul>
<h3 id="2-4-梯度裁剪"><a href="#2-4-梯度裁剪" class="headerlink" title="2.4 梯度裁剪"></a>2.4 梯度裁剪</h3><ul>
<li>循环神经⽹络中较容易出现梯度衰减或梯度爆炸。<strong>为了应对梯度爆炸</strong>，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成⼀个向量 $g$，并设裁剪的阈值是$\theta$。裁剪后的梯度：</li>
</ul>
<script type="math/tex; mode=display">
\min \left(\frac{\theta}{\|g\|}, 1\right) g</script><p>的$L_2$范数不超过$\theta$</p>
<ul>
<li><strong>但是梯度裁剪无法应对梯度衰减</strong></li>
</ul>
<h1 id="3-门控循环单元（GRU）"><a href="#3-门控循环单元（GRU）" class="headerlink" title="3 门控循环单元（GRU）"></a>3 门控循环单元（GRU）</h1><ul>
<li>我们发现，当时间步数较⼤或者时间步较小时，循环神经⽹络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但⽆法解决梯度衰减的问题。通常由于这个原因，<strong>循环神经⽹络在实际中较难捕捉时间序列中时间步距离较⼤的依赖关系</strong></li>
<li>门控循环神经⽹络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较⼤的依赖关系。它通过可以学习的⻔来控制信息的流动。其中，门控循环单元（gated recurrent unit，GRU）是⼀种常⽤的⻔控循环神经⽹</li>
</ul>
<h3 id="3-1-重置门和更新门"><a href="#3-1-重置门和更新门" class="headerlink" title="3.1 重置门和更新门"></a>3.1 重置门和更新门</h3><ul>
<li>⻔控循环单元中的重置⻔和更新⻔的输⼊均为当前时间步输⼊<script type="math/tex">X_t</script>与上⼀时间步隐藏状态<script type="math/tex">H_{t−1}</script>，输出由激活函数为sigmoid函数的全连接层计算得到。</li>
</ul>
<p><img src="C:\Users\LENOVO\AppData\Roaming\Typora\typora-user-images\image-20211210135706726.png" srcset="/img/loading.gif" lazyload alt="image-20211210135706726"></p>
<p>具体来说，重置门$R_t \in \mathbb{R}^{n \times h}$和更新门$Z_t \in \mathbb{R}^{n \times h}$的计算如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\boldsymbol{R}_{t}=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x r}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h r}+\boldsymbol{b}_{r}\right) \\
\boldsymbol{Z}_{t}=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x z}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h z}+\boldsymbol{b}_{z}\right)
\end{array}</script><h3 id="3-2-候选隐藏状态"><a href="#3-2-候选隐藏状态" class="headerlink" title="3.2 候选隐藏状态"></a>3.2 候选隐藏状态</h3><ul>
<li><p>我们将当前时间步重置⻔的输出与上⼀时间步隐藏状态做按元素乘法（符号为$\odot$）。如果重置⻔中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上⼀时间步的隐藏状态。如果元素值接近1，那么表⽰保留上⼀时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输⼊连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态</p>
</li>
<li><p>具体来说，时间步t的候选隐藏状态$\tilde{H}_t \in \mathbb{R}^{n \times h}$的计算为：</p>
</li>
</ul>
<script type="math/tex; mode=display">
\tilde{\boldsymbol{H}}_{t}=\tanh \left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}+\left(\boldsymbol{R}_{t} \odot \boldsymbol{H}_{t-1}\right) \boldsymbol{W}_{h h}+\boldsymbol{b}_{h}\right),</script><ul>
<li><p><strong>重置门控制了上⼀时间步的隐藏状态如何流⼊当前时间步的候选隐藏状态。而上⼀时间步的隐藏状态可能包含了时间序列截⾄上⼀时间步的全部历史信息。因此，重置门可以⽤来丢弃与预测⽆关的历史信息</strong></p>
</li>
<li><p>最后，时间步t的隐藏状态<script type="math/tex">H_t \in \mathbb{R}^{n×h}</script>的计算使⽤当前时间步的更新门<script type="math/tex">Z_t</script>来对上⼀时间步的隐藏状态<script type="math/tex">H_{t−1}</script>和当前时间步的候选隐藏状态<script type="math/tex">\tilde{H}_t</script>做组合：</p>
</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{H}_{t}=\boldsymbol{Z}_{t} \odot \boldsymbol{H}_{t-1}+\left(1-\boldsymbol{Z}_{t}\right) \odot \tilde{\boldsymbol{H}}_{t}</script><p><img src="https://s2.loli.net/2021/12/10/L7O69pm3Yjzbno5.png" srcset="/img/loading.gif" lazyload alt="image-20211210141824572"></p>
<ul>
<li><strong>更新⻔可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新</strong>。假设更新⻔在时间步$t’$到$t（t’ &lt; t）$之间⼀直近似1。那么，<strong>在时间步$t’$到$t$之间的输⼊信息⼏乎没有流⼊时间步$t$的隐藏状态$H_t$</strong>。实际上，<strong>这可以看作是较早时刻的隐藏状态$H_{t’−1}$⼀直通过时间保存并传递⾄当前时间步$t$</strong>。这个设计可以应对循环神经⽹络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较⼤的依赖关系</li>
<li>我们稍作总结：</li>
</ul>
<ol>
<li>重置⻔有助于捕捉时间序列⾥短期的依赖关系</li>
<li>更新⻔有助于捕捉时间序列⾥⻓期的依赖关系</li>
</ol>
<h1 id="4-长短期记忆（LSTM）"><a href="#4-长短期记忆（LSTM）" class="headerlink" title="4 长短期记忆（LSTM）"></a>4 长短期记忆（LSTM）</h1><ul>
<li>还有另外一种十分常用的门控循环神经网络：⻓短期记忆（long short-term memory，LSTM）</li>
</ul>
<h3 id="4-1-输⼊门、遗忘门、输出门和候选记忆细胞"><a href="#4-1-输⼊门、遗忘门、输出门和候选记忆细胞" class="headerlink" title="4.1 输⼊门、遗忘门、输出门和候选记忆细胞"></a>4.1 输⼊门、遗忘门、输出门和候选记忆细胞</h3><ul>
<li>与门控循环单元中的重置门和更新门⼀样，⻓短期记忆的门的输⼊均为当前时间步输⼊<script type="math/tex">X_t</script>与上⼀时间步隐藏状态<script type="math/tex">H_{t−1}</script>，输出由激活函数为sigmoid函数的全连接层计算得到</li>
<li>⻓短期记忆需要计算候选记忆细胞$\tilde{C}_t$。它的计算与上⾯介绍的3个⻔类似，但使⽤了值域在[−1, 1]的tanh函数作为激活函数:</li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/jzn954wmEqRfyIg.png" srcset="/img/loading.gif" lazyload alt="image-20211210144009641"></p>
<ul>
<li>时间步t的输⼊⻔$I_t \in R^{n×h}$、遗忘⻔$F_t \in R^{n×h}$和输出⻔$O_t \in R^{n×h}$分别计算如下：</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{I}_{t} &=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x i}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h i}+\boldsymbol{b}_{i}\right), \\
\boldsymbol{F}_{t} &=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x f}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h f}+\boldsymbol{b}_{f}\right), \\
\boldsymbol{O}_{t} &=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x o}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h o}+\boldsymbol{b}_{o}\right), \\
\tilde{\boldsymbol{C}}_{t}&=\tanh \left(\boldsymbol{X}_{t} \boldsymbol{W}_{x c}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h c}+\boldsymbol{b}_{c}\right),
\end{aligned}</script><h3 id="4-2-记忆细胞"><a href="#4-2-记忆细胞" class="headerlink" title="4.2 记忆细胞"></a>4.2 记忆细胞</h3><ul>
<li>当前时间步记忆细胞$C_t \in R^{n\times h}$的计算组合了上⼀时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘⻔和输⼊⻔来控制信息的流动：</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{C}_{t}=\boldsymbol{F}_{t} \odot \boldsymbol{C}_{t-1}+\boldsymbol{I}_{t} \odot \tilde{\boldsymbol{C}}_{t}</script><ul>
<li>遗忘门控制上⼀时间步的记忆细胞$C_{t−1}$中的信息是否传递到当前时间步，而输⼊门则控制当前时间步的输⼊$X_t$通过候选记忆细胞$\tilde{C}_t$如何流⼊当前时间步的记忆细胞。如果遗忘门⼀直近似1且输⼊门⼀直近似0，过去的记忆细胞将⼀直通过时间保存并传递⾄当前时间步</li>
</ul>
<h3 id="4-3-隐藏状态"><a href="#4-3-隐藏状态" class="headerlink" title="4.3 隐藏状态"></a>4.3 隐藏状态</h3><ul>
<li>有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$H_t \in R^{n×h}$的信息的流动：</li>
</ul>
<script type="math/tex; mode=display">
H_t = O_t \odot tanh(C_t)</script><ul>
<li>当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使⽤；当输出门近似0时，记忆细胞信息只⾃⼰保留</li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/mlYhrVbA47MByvG.png" srcset="/img/loading.gif" lazyload alt="image-20211210145754660"></p>
<h3 id="4-4-三种门的作用"><a href="#4-4-三种门的作用" class="headerlink" title="4.4 三种门的作用"></a>4.4 三种门的作用</h3><ul>
<li><strong>输入门控制当前计算的新状态以多大程度更新到记忆单元中</strong></li>
<li><strong>遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉</strong></li>
<li><strong>输出门控制当前的输出有多大程度取决于当前的记忆单元</strong></li>
<li>在一个训练好的网络中：</li>
</ul>
<ol>
<li>当输入的序列中没有重要信息时，LSTM的遗忘门接近于1，输入门接近于0，此时过去的记忆会被保存，从而实现了长期记忆功能</li>
<li>当输入的序列中出现了重要的信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近1</li>
<li>当输入的序列中出现了重要的信息，且该信息意味着之前的记忆不再重要时，输入门的值接近1，而遗忘门的值接近于0，这样旧的记忆被遗忘，新的重要信息被记忆</li>
</ol>
<h3 id="4-5-LSTM各模块激活函数"><a href="#4-5-LSTM各模块激活函数" class="headerlink" title="4.5 LSTM各模块激活函数"></a>4.5 LSTM各模块激活函数</h3><ul>
<li>LSTM中，三种门都用的Sigmoid函数，生成候选记忆时，使用Tanh作为激活函数。值得注意的是，<strong>这两个激活函数都是饱和的</strong>，也就是说在输入达到一定值的情况下，输出就不发生明显变化了。如果使用的是非饱和的激活函数，例如ReLu，将难以实现门控的效果</li>
<li>Sigmoid的输出在0到1之间，符合门控的物理定义。且当输入较大或较小时，输出会非常接近1或0，从而保证门开或门关</li>
<li>在生成候选记忆时，使用Tanh函数，是因为其输出在-1到1之间，这与大多数场景下特征分布是zero-centered吻合</li>
<li>此为Tanh函数在输入为0附近相比于Sigmoid有更大的梯度，通常收敛更快</li>
</ul>
<h1 id="5-深度循环神经网络"><a href="#5-深度循环神经网络" class="headerlink" title="5 深度循环神经网络"></a>5 深度循环神经网络</h1><ul>
<li>到⽬前为⽌介绍的循环神经⽹络只有⼀个单向的隐藏层，在深度学习应⽤⾥，我们通常会⽤到含有多个隐藏层的循环神经⽹络，也称作深度循环神经网络</li>
<li>下图演⽰了⼀个有L个隐藏层的深度循环神经⽹络，<strong>每个隐藏状态不断传递⾄当前层的下⼀时间步和当前时间步的下⼀层</strong></li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/PUK7CRjyemEbzLM.png" srcset="/img/loading.gif" lazyload alt="image-20211210150125737"></p>
<ul>
<li>第1隐藏层的隐藏状态和之前的计算⼀样：</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{H}_{t}^{(1)}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}^{(1)}+\boldsymbol{H}_{t-1}^{(1)} \boldsymbol{W}_{h h}^{(1)}+\boldsymbol{b}_{h}^{(1)}\right)</script><ul>
<li>当1 &lt; l ≤ L时，第l隐藏层的隐藏状态的表达式为:</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{H}_{t}^{(l)}=\phi\left(\boldsymbol{H}_{t}^{(l-1)} \boldsymbol{W}_{x h}^{(l)}+\boldsymbol{H}_{t-1}^{(l)} \boldsymbol{W}_{h h}^{(l)}+\boldsymbol{b}_{h}^{(l)}\right)</script><ul>
<li>最终，输出层的输出只需基于第L隐藏层的隐藏状态：</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{O}_{t}=\boldsymbol{H}_{t}^{(L)} \boldsymbol{W}_{h q}+\boldsymbol{b}_{q}</script><ul>
<li><p>同多层感知机⼀样，隐藏层个数L是超参数</p>
</li>
<li><p><strong>RNN中选择深度网络的原因和一般前馈神经网络一样，都是为了能够用来表征更复杂的情况</strong></p>
</li>
</ul>
<h1 id="6-双向循环神经⽹络"><a href="#6-双向循环神经⽹络" class="headerlink" title="6 双向循环神经⽹络"></a>6 双向循环神经⽹络</h1><ul>
<li>之前介绍的循环神经⽹络模型都是假设当前时间步是由前⾯的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后⾯时间步决定。例如，当我们写下⼀个句⼦时，可能会根据句⼦后⾯的词来修改句⼦前⾯的⽤词</li>
<li><strong>双向循环神经⽹络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息</strong>，下图演⽰了⼀个含单隐藏层的双向循环神经⽹络的架构</li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/VtFmigZ58qw2aeb.png" srcset="/img/loading.gif" lazyload alt="image-20211210151934211"></p>
<ul>
<li>具体来看，设时间步t正向隐藏状态为$\overrightarrow{H}_t \in R^{n×h}$（正向隐藏单元个数为h），反向隐藏状态为$\overleftarrow{H}_t \in R^{n×h}$（反向隐藏单元个数为h）。我们可以分别计算正向隐藏状态和反向隐藏状态：</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{l}
\overrightarrow{\boldsymbol{H}}_{t}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}^{(f)}+\overrightarrow{\boldsymbol{H}}_{t-1} \boldsymbol{W}_{h h}^{(f)}+\boldsymbol{b}_{h}^{(f)}\right) \\
\overleftarrow{\boldsymbol{H}}_{t}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}^{(b)}+\overleftarrow{\boldsymbol{H}}_{t+1} \boldsymbol{W}_{h h}^{(b)}+\boldsymbol{b}_{h}^{(b)}\right)
\end{array}</script><p><strong>然后我们连结两个⽅向的隐藏状态$\overrightarrow{H}_t$和 $\overleftarrow{H}_t$来得到隐藏状态$H_t \in R^{n×2h}$</strong>，并将其输⼊到输出层。 输出层计算输出$O_t \in R^{n×q}$（输出个数为q）：</p>
<script type="math/tex; mode=display">
O_t = H_tW_{hq} + b_q</script><ul>
<li><strong>不同方向的隐藏状态的隐藏单元个数也可以不同</strong></li>
<li>双向循环神经⽹络在每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊）</li>
</ul>
<h1 id="7-Seq2Seq模型"><a href="#7-Seq2Seq模型" class="headerlink" title="7 Seq2Seq模型"></a>7 Seq2Seq模型</h1><ul>
<li>在许多时候，输入和输出都是不定长序列，这采用一般的RNN肯定是行不通的。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，例如：英语输⼊：“They”、“are”、“watching”、“.” ； 法语输出：“Ils”、“regardent”、“.”</li>
<li>当输⼊和输出都是不定⻓序列时，我们可以使⽤seq2seq模型，本质上都⽤到了两个循环神经网络，分别叫做编码器和解码器。<strong>编码器用来分析输⼊序列，解码器⽤来生成输出序列</strong></li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/FQfVxdq2wuC7aSs.png" srcset="/img/loading.gif" lazyload alt="image-20211210170200879"></p>
<ul>
<li>&lt; bos &gt;（beginning of sequence）和 &lt; eos &gt;（end of sequence）分别表示序列的开始和结束</li>
<li>编码器每个时间步的输⼊依次为英语句⼦中的单词、标点和特殊符号。上图中使⽤了编码器在最终时间步的隐藏状态作为输⼊句⼦的表征或编码信息。解码器在各个时间步中使⽤输⼊句⼦的编码信息和上个时间步的输出以及隐藏状态作为输⼊</li>
</ul>
<h3 id="7-1-编码器"><a href="#7-1-编码器" class="headerlink" title="7.1 编码器"></a>7.1 编码器</h3><ul>
<li><strong>编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量$c$，并在该背景变量中编码输⼊序列信息</strong>。编码器可以使⽤循环神经网络</li>
<li>编码器通过⾃定义函数q将各个时间步的隐藏状态变换为背景变量</li>
</ul>
<script type="math/tex; mode=display">
c = q(h_1, ...,h_T)</script><ul>
<li><strong>也可以使⽤双向循环神经⽹络构造编码器</strong>，在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊），并编码了整个序列的信息。</li>
</ul>
<h3 id="7-2-解码器"><a href="#7-2-解码器" class="headerlink" title="7.2 解码器"></a>7.2 解码器</h3><ul>
<li>解码器就是通过编码器输出的背景向量$c$，和每一个时间步的输入，输出所需要的结果</li>
<li><strong>解码器在预测和训练时是不一样的</strong>，我们先介绍<strong>预测</strong>时的解码器：</li>
</ul>
<p>编码器输出的背景变量$c$编码了整个输⼊序列<script type="math/tex">x_1, . . . , x_T</script>的信息。给定训练样本中的输出序列<script type="math/tex">y_1, y_2, . . . , y_{T'}</script>，对每个时间步<script type="math/tex">t'</script>（符号与输⼊序列或编码器的时间步<script type="math/tex">t</script>有区别），解码器输出<script type="math/tex">y_{t'}</script>的条件概率将基于之前的输出序列<script type="math/tex">y_1, . . . , y_{t'−1}</script>和背景变量<script type="math/tex">c</script>，即<script type="math/tex">P(y_{t'} | y_1, . . . , y_{t'−1}, c)</script></p>
<p>为此，我们可以使⽤另⼀个循环神经网络作为解码器。在输出序列的时间步<script type="math/tex">t'</script>，解码器将上⼀时间步的输出<script type="math/tex">y_{t'−1}</script>以及背景变量<script type="math/tex">c</script>作为输⼊，并将它们与上⼀时间步的隐藏状态<script type="math/tex">s_{t'−1}</script>变换为当前时间步的隐藏状态<script type="math/tex">s_{t'}</script>：</p>
<script type="math/tex; mode=display">
s_{t'} = g(y_{t' -1}, c, s_{t' - 1})</script><p>有了解码器的隐藏状态后，我们可以使⽤⾃定义的输出层和softmax运算来计算<script type="math/tex">P(y_{t'} | y_1, . . . , y_{t'−1}, c)</script></p>
<ul>
<li><strong>训练</strong>时的解码器，和预测不一样的地方在于，每一时间步的输入序列不是上一时间步的输出，而是上一时间步的真实标签序列。这叫做<strong>强制教学</strong></li>
</ul>
<p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(y_{1}, \ldots, y_{T^{\prime}} \mid x_{1}, \ldots, x_{T}\right) &=\prod_{t^{\prime}=1}^{T^{\prime}} P\left(y_{t^{\prime}} \mid y_{1}, \ldots, y_{t^{\prime}-1}, x_{1}, \ldots, x_{T}\right) \\
&=\prod_{t^{\prime}=1}^{T^{\prime}} P\left(y_{t^{\prime}} \mid y_{1}, \ldots, y_{t^{\prime}-1}, c\right)
\end{aligned}</script><p>并得到该输出序列的损失：</p>
<script type="math/tex; mode=display">
-\log P\left(y_{1}, \ldots, y_{T^{\prime}} \mid x_{1}, \ldots, x_{T}\right)=-\sum_{t^{\prime}=1}^{T^{\prime}} \log P\left(y_{t^{\prime}} \mid y_{1}, \ldots, y_{t^{\prime}-1}, c\right)</script><p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数</p>
<h3 id="7-3-预测时的搜索方式"><a href="#7-3-预测时的搜索方式" class="headerlink" title="7.3 预测时的搜索方式"></a>7.3 预测时的搜索方式</h3><h4 id="7-3-1-贪婪搜索"><a href="#7-3-1-贪婪搜索" class="headerlink" title="7.3.1 贪婪搜索"></a>7.3.1 贪婪搜索</h4><ul>
<li>贪婪搜索（greedy search）。对于输出序列任⼀时间步$t’$，我 们从$|Y|$（Y为词典）个词中搜索出条件概率最⼤的词：</li>
</ul>
<script type="math/tex; mode=display">
y_{t'} = argmax_{y \in Y}P(y|y_1, ..., y_{t' - 1}, c)</script><p>作为输出。⼀旦搜索出“&lt; eos &gt;”符号，或者输出序列⻓度已经达到了最⼤⻓度$T’$，便完成输出。</p>
<ul>
<li>我们将该条件概率最⼤的输出序列称为<strong>最优输出序列</strong>，<strong>而贪婪搜索无法保证得到最优输出序列</strong>，下面举个栗子：</li>
</ul>
<blockquote>
<p>假设输出词典⾥⾯有“A” “B” “C” 和“&lt; eos &gt;”这4个词。下图中每个时间步下的4个数字分别代表了该时间步⽣成“A” “B” “C” 和“&lt; eos &gt;”这4个词的条件概率。在每个 间步，贪婪搜索选取条件概率最⼤的词。因此，将⽣成输出序列“A” “B” “C” “&lt; eos &gt;”。 该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 = 0.048</p>
</blockquote>
<p><img src="https://s2.loli.net/2021/12/10/JUgILXFCjWviaQw.png" srcset="/img/loading.gif" lazyload alt="image-20211210180148791"></p>
<blockquote>
<p>但是现在我们考虑下面一种情况，在时间步2中选取了条件概率第⼆⼤的词“C”，<strong>由于之后时间步的概率值是基于前面的时间步的，所以时间步2结果选取的改变，会导致后续的概率发生改变</strong>，如下图，所以我们现在的输出序列“A” “C” “B” “&lt; eos &gt;”的条件概率是0.5 × 0.3 × 0.6 × 0.6 = 0.054，大于贪婪搜索的概率，所以贪婪搜索得到的不是最优的</p>
</blockquote>
<p><img src="https://s2.loli.net/2021/12/10/zSKDLamFWqseyuN.png" srcset="/img/loading.gif" lazyload alt="image-20211210180404095"></p>
<h4 id="7-3-2-穷举搜索"><a href="#7-3-2-穷举搜索" class="headerlink" title="7.3.2 穷举搜索"></a>7.3.2 穷举搜索</h4><ul>
<li>我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最⼤的序列</li>
<li><strong>虽然穷举搜索可以得到最优输出序列，但它的计算开销$O(|Y|^{T’})$很容易过⼤</strong>，而贪婪搜索的开销为$O(|Y|T’)$，明显小于穷举搜索</li>
</ul>
<h4 id="7-3-3-束搜索"><a href="#7-3-3-束搜索" class="headerlink" title="7.3.3 束搜索"></a>7.3.3 束搜索</h4><ul>
<li><p>束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个<strong>束宽</strong>（beam size）超参数。我们将它设为k。在时间步1时，选取当前时间步条件概率最⼤的k个词，分别组成k个候选输出序列的⾸词。在之后的每个时间步，基于上个时间步的k个候选输出序列，从k$ |Y|$个可能的输出序列中选取条件概率最⼤的k个，作为该时间步的候选输出序列。最终，<strong>我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt; eos &gt;”的序列，并将它们中所有特殊符号“&lt; eos &gt;”后⾯的⼦序列舍弃</strong>，得到最终候选输出序列的集合</p>
</li>
<li><p>下面举个栗子：</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/QEBb3gUVxmuDS7O.png" srcset="/img/loading.gif" lazyload alt="image-20211210182131362"></p>
<p>第一个时间步找出概率最大的”A”和”C”，然后再根据”A”和”C”，在时间步2中寻找概率最大的2个，分别为”AB”和”CE”，再根据这两个，在时间步3，输出”ABD”和”CED”，最后减去&lt; eos &gt;符号以后的内容，得出输出序列</p>
<ul>
<li>在最终候选输出序列的集合中，我们取以下分数最⾼的序列作为输出序列：</li>
</ul>
<script type="math/tex; mode=display">
\frac{1}{L^{\alpha}} \log P\left(y_{1}, \ldots, y_{L}\right)=\frac{1}{L^{\alpha}} \sum_{t^{\prime}=1}^{L} \log P\left(y_{t^{\prime}} \mid y_{1}, \ldots, y_{t^{\prime}-1}, c\right)</script><p>其中$L$为最终候选序列⻓度，$\alpha$⼀般可选为0.75。分⺟上的$L^{\alpha}$是为了惩罚较⻓序列在以上分数中较多的对数相加项</p>
<ul>
<li>束搜索的计算开销为$O(k|Y|T’)$，介于贪婪搜索和穷举搜索之间</li>
<li><strong>束搜索通过灵活的束宽来权衡计算开销和搜索质量</strong></li>
</ul>
<h3 id="7-4-注意力机制"><a href="#7-4-注意力机制" class="headerlink" title="7.4 注意力机制"></a>7.4 注意力机制</h3><ul>
<li><strong>解码器在⽣成输出序列中的每⼀个词时可能只需利⽤输⼊序列某⼀部分的信息， 而不需要由整个输入序列生成的背景向量</strong>，例如，在输出序列的时间步1，解码器可以主要依赖“They” “are” 的信息来⽣成“Ils”，在时间步2则主要使⽤来⾃“watching”的编码信息⽣成“regardent”，而不需要”They are watching”整个句子</li>
<li><p>若没引入注意力机制，在实际使用中，那么会发现随着输入序列的增长，模型的性能发生了显著下降。<strong>这是因为编码时输入序列的全部信息压缩到了一个向量中，随着序列的增长，越前面的词丢失越严重</strong>。<strong>同时，seq2seq模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词对应的源语言词的上下文信息和位置信息在编解码过程中丢失了</strong></p>
</li>
<li><p>注意⼒机制通过对编码器所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量。解码器在每⼀时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2021/12/10/5kGV8uqeaYn3wlF.png" srcset="/img/loading.gif" lazyload alt="image-20211210184622950"></p>
<p>⾸先，函数a根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输⼊。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量</p>
<ul>
<li>具体来说，令编码器在时间步t的隐藏状态为$h_t$，且总时间步数为T。那么解码器在时间步$t’$（$t’$代表的是解码器的时间步数）的背景变量为所有编码器隐藏状态的加权平均：</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{c}_{t^{\prime}}=\sum_{t=1}^{T} \alpha_{t^{\prime} t} \boldsymbol{h}_{t}</script><p>其中给定$t’$时，权重$\alpha_{t’t}$在$t = 1, . . . , T$的值是⼀个概率分布。为了得到概率分布，我们可以使⽤softmax运算:</p>
<script type="math/tex; mode=display">
\alpha_{t^{\prime} t}=\frac{\exp \left(e_{t^{\prime} t}\right)}{\sum_{k=1}^{T} \exp \left(e_{t^{\prime} k}\right)}, \quad t=1, \ldots, T</script><p>由于<script type="math/tex">e_{t't}</script>同时取决于解码器的时间步<script type="math/tex">t'</script>和编码器的时间步t，我们不妨以解码器在时间步<script type="math/tex">t'− 1</script>的隐藏状态<script type="math/tex">s_{t'−1}</script>与编码器在时间步t的隐藏状态<script type="math/tex">h_t</script>为输⼊，并通过函数a计算<script type="math/tex">e_{t't}</script>：</p>
<script type="math/tex; mode=display">
e_{t't} = a(s_{t'-1}, h_t)</script><p>其中a是一个自定义函数，有多种选择。如果两个输⼊向量⻓度相同，⼀个简单的选择是计算它们的内积$a(s,h) = s^⊤h$</p>
<ul>
<li>我们对注意力机制可以有一个直观的理解：<strong>在生成一个输出词时，会考虑每一个输入词和当前输出词的对应关系，对齐越好的词，会有越大的权重</strong></li>
<li>还可以结合双向RNN和注意力机制，每个隐状态包含了$\overleftarrow{h_t}$和$\overrightarrow{h_t}$</li>
</ul>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/11/26/CNN/">
                        <span class="hidden-mobile">CNN基本概念</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://zlkqz.github.io/2021/12/10/RNN/';
          this.page.identifier = '/2021/12/10/RNN/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'fluid' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
